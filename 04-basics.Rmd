
```{r, child='in_all.Rmd'}
```

# (PART) Data Programming {-}

# Basics of Programming with Data {#data-programming-basics}

## Module Objectives  {- #module4-objectives}

- Write basic functions and procedures to create simple plots and data summaries
- Apply syntax knowledge to reference variables and observations in common data structures
- Create new variables and columns or reformat existing columns in provided data structures

## Introduction

At this point in the course, you've learned how to write functions. You know the basics of how to create new variables, how data frames and lists work, and how to use markdown.

And yet... these are skills that take some practice when applied to new data. So this week, we're going to focus on working with datasets, but we're not going to learn much in the way of new material (though I'll provide sample code for tasks like basic plots and tables). This week is about **reinforcing the skills you've already learned**, and helping you find your feet a bit as you work through a data analysis. It will also provide a preview of some of the packages we're going to work with in the coming weeks (because I'm going to show you how to e.g. summarize a dataset and plot a few things, even without having covered that material). 

As you've probably guessed by now, this week's reading will primarily be focused on examples.

## Example 1: Art
The Tate Art Museum assembled a collection of 70,000 artworks (last updated in 2014). They catalogued information including accession number, artwork dimensions, units, title, date, medium, inscription, and even URLs for images of the art. 

```{r read-art-data}
library(readr)
library(skimr)
artwork <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')

skim(artwork)

```

When you first access a new dataset, it's fun to explore it a bit. I've shown a summary of the variables (character variables summarized with completion rates and # unique values, numeric variables summarized with quantiles and mean/sd) generated using the `skimr` package (which we'll talk about next week). 

<details class="ex"><summary>First, let's pull out the year for each piece of artwork in the dataset and see what we can do with it...</summary>
```{r access-col-data-head}
head(artwork$year)
```

We reference a column of the dataset by name using `dataset_name$column_name`, and since our data is stored in `artwork`, and we want the column named `year`, we use `artwork$year` to get access to the data we want.

I've used the `head` command to show only the first few values (so that the output isn't overwhelming). When we have output like this, it is useful to summarize the output in some way:

```{r summary-data-col}
summary(artwork$year)
```

That's much less output, but we might want to instead make a chart:
```{r hist-data-col}
hist(artwork$year)
```
Personally, I much prefer the graphical version. It's informative (though it does leave out NA values) and shows that there are pieces going back to the 1500s, but that most pieces were made in the early 1800s or late 1900s. 

</details>

<details class="ex"><summary>We might be interested in the aspect ratio of the artwork - let's take a look at the input variables and define new variables related to aspect ratio(s)</summary>

```{r hist-dims-art, fig.show = 'hold', collapse = T, out.width = "33%", fig.width = 4, fig.height = 4}
hist(artwork$width)
hist(artwork$depth)
hist(artwork$height)
```

So all of our variables are skewed quite a bit, and we know from the existence of the `units` column that they may not be in the same unit, either...

```{r table-art-units}
table(artwork$units)
```

Except apparently they are, so ... cool. That does make life easier.

To define a new variable that exists on its own, we might do something like this:
```{r hist-aspect-ratio-calc, fig.show = 'hold', collapse = T, out.width = "49%", fig.width = 4, fig.height = 4}
aspect_hw <- artwork$height/artwork$width
hist(aspect_hw)
hist(log(aspect_hw))
```

Ok, interesting. Most things are pretty square-ish, but there are obviously quite a few exceptions in both directions.

The one problem with how we've done this is that we now have a data frame with all of our data in it, and a separate variable `aspect_hw`, that is not attached to our data frame. That's not ideal - it's easy to lose track of the variable, it's easy to accidentally "sort" the variable so that the row order isn't the same as in the original data frame... there are all sorts of potential issues.

So, the better way to define a new variable is to add a new **column** to the data frame:
```{r art-assign-column-var}
artwork$aspect_hw <- artwork$height/artwork$width
```

(We'll learn an easier way to do this later, but this is functional, if not pretty, for now).

The downside to this is that we have to write out `artwork$aspect_hw` each time we want to reference the variable. That is a pain, but one that's relatively temporary (we'll get to a better way to do this in a couple of weeks). A little bit of extra typing is definitely worth it if you don't lose data you want to keep.

</details>

One mistake I see people make frequently is to calculate `artwork$height/artwork$width`, but then not assign that value to a variable. If you're not using `<-` (or `=` or `->` if you're a total heathen) then you're not saving that information to be referenced later - you're just calculating values temporarily. It's important to keep track of where you're putting the pieces you create during an analysis - just as important as keeping track of the different sub-components when you're putting a lego set together or making a complex recipe in the kitchen. Forgetting to assign your calculation to a variable is like dumping your mixture down the sink or throwing that small lego component into the trash.


## Example 2: Dogs of NYC

New York City provides a whole host of open-data resources, including a [dataset of dogs licensed in the city on an annual basis](https://data.cityofnewyork.us/Health/NYC-Dog-Licensing-Dataset/nu7n-tubp) (link is to the NYC Open Data Page). 

[CSV link](https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD) (this data is ~23 MB)

```{r read-dog-data}
library(readr)

if (!file.exists("data/NYC_dogs.csv")) {
  # if the file doesn't exist, download it!
  download.file(
    "https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD", # url for download
    destfile = "data/NYC_dogs.csv", # location to store the file
    mode = "wb" # need this to get downloads to work on windows
  )
}

dogs <- read_csv("data/NYC_dogs.csv")
head(dogs)
```

One thing we might want to do first is to transform the license dates (`LicenseIssuedDate`, `LicenseExpiredDate`) into actual dates instead of characters. We will use the `lubridate` package to do this, because it is designed to make working with dates and times very easy.

```{r dog-data-dates}
library(lubridate)
head(dogs$LicenseExpiredDate) # Dates are in month-day-year format


dogs$LicenseExpiredDate <- mdy(dogs$LicenseExpiredDate)
dogs$LicenseIssuedDate <- mdy(dogs$LicenseIssuedDate)
```

It might be interesting to see when licenses have been issued over time, so let's make a histogram:

```{r dog-license-hist}
library(ggplot2)

ggplot(
  data = dogs, 
  aes(x = LicenseIssuedDate) # Specify we want LicenseIssueDate on the x-axis
) + 
  geom_histogram() # Create a histogram

```

There is an interesting periodicity to the license issue dates. 

I'm also curious about how long a license tends to be held for - we can get this information by subtracting the issue date from the expiration date.

```{r dog-license-length}
dogs$LicenseLength <- dogs$LicenseExpiredDate - dogs$LicenseIssuedDate
summary(dogs$LicenseLength)
head(dogs$LicenseLength)
```

We can see that directly subtracting date-times gives us a license length in days. That's useful enough, I guess, but it might be more useful in years... unfortunately, that's not an option for `difftime()`


```{r dog-license-length-2}
dogs$LicenseLength <- difftime(dogs$LicenseExpiredDate, dogs$LicenseIssuedDate, units = "weeks")

# 52 weeks in a year so we'll just convert as we plot
ggplot(data = dogs, aes(x = LicenseLength / 52 )) + geom_histogram() + 
  scale_x_continuous(breaks = 0:10)
```

Another question that I have when looking at this dataset is a bit more superficial - are the characteristics of different areas different? The `dogs` data frame has a Borough column, but it's not actually filled in, so we'll need to get rid of it and then add Borough back in by zip code. 

To look at this, we'll need a bit more data. I found a list of NYC zip codes by borough, which we can merge in with the data we already have to get puppy registrations by borough. Then, we can see if e.g. the top 10 breeds are different for different boroughs. To simplify this, I'm going to link to a file to merge in, and not show you the specifics of how I read the table from [this site](https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php) into R.

```{r get-nyc-zip-borough, echo = F, include = F}
library(xml2)
library(tidyverse)
page <- read_html("https://www.nycbynatives.com/nyc_info/new_york_city_zip_codes.php")
nyc_zip_borough <- data.frame(ZipCode = c(xml_find_all(page, ".//tr/td[1]"), 
                                          xml_find_all(page, ".//tr/td[4]")) %>%
                                purrr::map_chr(xml_text) %>% 
                                stringr::str_trim(),
                              Borough = c(xml_find_all(page, ".//tr/td[2]"), 
                                          xml_find_all(page, ".//tr/td[5]")) %>%
                                purrr::map_chr(xml_text) %>% 
                                stringr::str_trim()) %>%
  unique()
write_csv(nyc_zip_borough, "data/nyc_zip_borough.csv")
```

```{r merge-dog-borough-info}
borough_zip <- read_csv("https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/nyc_zip_borough.csv")

# Remove the Borough column from dogs
dogs <- dogs[, which(names(dogs) != "Borough")]
dogs <- merge(dogs, borough_zip, by = "ZipCode")
head(dogs)
```

Now that we have borough, let's write a function that will take a dataset and spit out a list of the top 5 dog breeds registered in that area.

```{r dog-top-5-breeds-function}
top_5_breeds <- function(data) {
  # Inside the function, our dataset is called data, not dogs
  tmp <- table(data$BreedName) 
  return(sort(tmp, decreasing = T)[1:5]) # top 5 breeds with counts
}
```

Now, using that function, lets write a for loop that loops through the 5 boroughs and spits out the top 5 breeds in each borough:

```{r dog-for-loop-breeds}
boroughs <- unique(borough_zip$Borough) # get a list of the 5 boroughs
for (i in boroughs) {
  # Get subset of data frame corresponding to the Borough
  dogs_sub <- subset(dogs, Borough == i)
  # Get top 5 dog breeds
  result <- as.data.frame(top_5_breeds(dogs_sub))
  # set names
  names(result) <- c("Breed", "Freq")
  # Add Borough as a new column
  result$Borough <- i
  # Add rank as a new column
  result$rank <- 1:5
  
  print(result)
}
```

If we wanted to save these results as a summary data frame, we could totally do that!

```{r dog-for-loop-breeds2}
breeds_by_borough <- data.frame() # create a blank data frame

for (i in boroughs) {
  # Get subset of data frame corresponding to the Borough
  dogs_sub <- subset(dogs, Borough == i)
  # Get top 5 dog breeds
  result <- as.data.frame(top_5_breeds(dogs_sub))
  # set names
  names(result) <- c("Breed", "Freq")
  # Add Borough as a new column
  result$Borough <- i
  # Add rank as a new column
  result$rank <- 1:5
  
  breeds_by_borough <- rbind(breeds_by_borough, result)
}

breeds_by_borough
```

We could even sort our data by the rank and Borough for easier comparisons:

```{r dog-sort-results}

breeds_by_borough[order(breeds_by_borough$rank, 
                        breeds_by_borough$Borough),]

```

::: tryitout
Look at the name, age, or gender of dogs registered in NYC and see if you can come up with a similar function and way of summarizing the data in a for-loop.
:::

In 2 weeks, we'll learn a much shorter set of commands to get these types of summaries, but it's important to know how a for loop connects to the concept of summarizing data by a factor (in this case, by borough).
