{
  "hash": "fd2b39f5b9dcd5260fb5248db9374b63",
  "result": {
    "markdown": "\n# Loading External Data {#reading-data}\n\n## Module Objectives  {- #module7-objectives}\n\n- Read in data from common formats into R or Python\n- Identify delimiters, headers, and other essential components of files\n\n    \nIt may be helpful to print out this [data import with the tidyverse](https://github.com/rstudio/cheatsheets/raw/main/data-import.pdf) cheat sheet for your work in R in this module.\n\n## An Overview of External Data Formats\n\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we'll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\n- **as raw text**, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable. \n\n\n- **in a spreadsheet**. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they're also not raw text files either. They're a hybrid - a special type of markup that is specific to the filetype and the program it's designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.     \n\n::: callout-note\nThere is a collection of spreadsheet horror stories [here](https://github.com/jennybc/scary-excel-stories) and a series of even more horrifying tweets [here](https://twitter.com/JennyBryan/status/722954354198597632).    \nAlso, there's this amazing comic:    \n[![](https://imgs.xkcd.com/comics/algorithms.png)](https://xkcd.com/1667/)\n:::\n\n- **as a binary file**. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n    - R, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as `foreign` in R will let you read data from other programs, and packages such as `haven` in R will let you write data into binary formats used by other programs. To read data from R into SAS, the easiest way is probably to [call R from PROC IML](http://proc-x.com/2015/05/import-rdata-to-sas-along-with-labels/). \n    - [Here](https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/) is a very thorough explanation of why binary file formats exist, and why they're not necessarily optimal.\n\n- **in a database**. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with. \n\nThere are, of course, many other non-tabular data formats -- some open and easy to work with, some inpenetrable. A few which may be more common:\n\n- **Web related data structures**: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.\n\n- **Spatial files**: Shapefiles are by far the most common version of spatial files^[though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times]. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. We'll explore these a bit more when we talk about maps. \n\n\nTo be minimally functional in R and Python, it's important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly cover binary files and databases, but it is less critical to remember how to read these in without consulting one or more online references. \n\n\n## Text Files\n\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What's the difference, you say?\n\n### Fixed-width files\n\n```\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\n```\n\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don't have to read the whole thing into memory). \n\n::: panel-tabset\n\n#### Base R {.unnumbered}\n\nIn base R (no extra packages), you can read fwf files in using `read.fwf`, but you must specify the column breaks yourself, which can be painful.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## url <- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\ndata <- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n```\n:::\n\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you. \n\n::: {.callout-warning collapse=true}\n##### Code for counting field width {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp <- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars <- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars <- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space <- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space <- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks <- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths <- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata <- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names <- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) <- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n```\n:::\n\n\n:::\n\nYou can also write fixed-width files if you *really* want to:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\n\nlibrary(gdata)\n\nwrite.fwf(mtcars, file = \"data/04_mtcars-fixed-width.txt\")\n```\n:::\n\n\n#### `readr` {-}\n\nThe `readr` package creates data-frame like objects called tibbles (a souped-up data frame), but it is *much* friendlier to use. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 × 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    <chr> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # … with 110 more rows, and 12 more variables: PRES <dbl>, SRAD <dbl>,\n## #   TA9M <dbl>, WS2M <dbl>, TS10 <dbl>, TB10 <dbl>, TS05 <dbl>, TS25 <dbl>,\n## #   TS60 <dbl>, TR05 <dbl>, TR25 <dbl>, TR60 <dbl>\n```\n:::\n\n\n#### Python {-}\n\nBy default, pandas' `read_fwf` will guess at the format of your fixed-width file. \n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\n```\n:::\n\n\n#### SAS {-}\n\nIn SAS, it's a bit more complicated, but not that much - the biggest difference is that you generally have to specify the column names for SAS. For complicated data, as in R, you may also have to specify the column widths.\n\n```\n/* This downloads the file to my machine */\n/* x \"curl https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/ \n> data/mesodata.txt\" */\n/* only run this once */\n\n/* Specifying WORK.mesodata means the dataset will cease to exist after this chunk exits */\ndata WORK.mesodata;\n\ninfile  \"data/mesodata.txt\" firstobs = 4; \n/* Skip the first 3 rows */\n  length STID $ 4; /* define ID length */\n  input STID $ STNM TIME RELH TAIR \n        WSPD WVEC WDIR WDSD WSSD WMAX \n        RAIN PRES SRAD TA9M WS2M TS10 \n        TB10 TS05 TS25 TS60 TR05 TR25 TR60;\nrun;\n\nproc print data=mesodata (obs=10); /* print the first 10 observations */\n  run;\n```\n\nIn SAS data statements, you generally need to specify the data names explicitly. \n\nIn theory you can also get SAS to write out a fixed-width file, but it's much easier to just... not. You can generally use a CSV or format of your choice -- and you should definitely do that, because delimited files are much easier to work with. \n\n:::\n\n### Delimited Text Files\n\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file. \n\n::: panel-tabset\n\n#### Base R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info <- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n##   X pokedex_number          name german_name            japanese_name\n## 1 0              1     Bulbasaur     Bisasam フシギダネ (Fushigidane)\n## 2 1              2       Ivysaur   Bisaknosp  フシギソウ (Fushigisou)\n## 3 2              3      Venusaur    Bisaflor フシギバナ (Fushigibana)\n## 4 3              3 Mega Venusaur    Bisaflor フシギバナ (Fushigibana)\n## 5 4              4    Charmander    Glumanda      ヒトカゲ (Hitokage)\n## 6 5              5    Charmeleon     Glutexo       リザード (Lizardo)\n##   generation\n## 1          1\n## 2          1\n## 3          1\n## 4          1\n## 5          1\n## 6          1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = 'data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('data/NE_Features.zip', exdir = 'data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations <- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n```\n:::\n\n\n#### `readr` {-}\n\nThere is a family of `read_xxx` functions in `readr` to read files delimited with commas (`read_csv`), tabs (`read_tsv`), or generic delimited files (`read_delim`).\n\nThe most common delimited text format is CSV: comma-separated value. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\nurl <- \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info <- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n## # A tibble: 6 × 6\n##    ...1 pokedex_number name          german_name japanese_name        generation\n##   <dbl>          <dbl> <chr>         <chr>       <chr>                     <dbl>\n## 1     0              1 Bulbasaur     Bisasam     フシギダネ (Fushigi…          1\n## 2     1              2 Ivysaur       Bisaknosp   フシギソウ (Fushigi…          1\n## 3     2              3 Venusaur      Bisaflor    フシギバナ (Fushigi…          1\n## 4     3              3 Mega Venusaur Bisaflor    フシギバナ (Fushigi…          1\n## 5     4              4 Charmander    Glumanda    ヒトカゲ (Hitokage)           1\n## 6     5              5 Charmeleon    Glutexo     リザード (Lizardo)            1\n```\n:::\n\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can't let it keep us from accessing the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = 'data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('data/NE_Features.zip', exdir = 'data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations <- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        <dbl> <chr>           <chr>         <chr>       <chr>         <chr>      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre… Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n```\n:::\n\n\nWe can actually read in the file without unzipping it, so long as we download it first - `readr` does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnebraska_locations <- read_delim(\"data/NE_Features.zip\", delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME    FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME\n##        <dbl> <chr>           <chr>         <chr>       <chr>         <chr>      \n## 1     171013 Peetz Table     Area          CO          08            Logan      \n## 2     171029 Sidney Draw     Valley        NE          31            Cheyenne   \n## 3     182687 Highline Canal  Canal         CO          08            Sedgwick   \n## 4     182688 Cottonwood Cre… Stream        CO          08            Sedgwick   \n## 5     182689 Sand Draw       Valley        CO          08            Sedgwick   \n## 6     182690 Sedgwick Draw   Valley        CO          08            Sedgwick\n```\n:::\n\n\n\n#### Python {-}\n\nThere is a family of `read_xxx` functions in `pandas` including functions to read files delimited with commas (`read_csv`) as well as generic delimited files (`read_table`).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n##                                 name german_name  ... against_steel  against_fairy\n## 0                          Bulbasaur     Bisasam  ...           1.0            0.5\n## 1                            Ivysaur   Bisaknosp  ...           1.0            0.5\n## 2                           Venusaur    Bisaflor  ...           1.0            0.5\n## 3                      Mega Venusaur    Bisaflor  ...           1.0            0.5\n## 4                         Charmander    Glumanda  ...           0.5            0.5\n## ...                              ...         ...  ...           ...            ...\n## 1023     Zacian Hero of Many Battles         NaN  ...           2.0            1.0\n## 1024        Zamazenta Crowned Shield         NaN  ...           0.5            1.0\n## 1025  Zamazenta Hero of Many Battles         NaN  ...           1.0            2.0\n## 1026                       Eternatus         NaN  ...           1.0            1.0\n## 1027             Eternatus Eternamax         NaN  ...           1.0            1.0\n## \n## [1028 rows x 49 columns]\n```\n:::\n\n\n\nPandas can actually access zipped data files and unzip them while reading the data in, so we don't have to download the file and unzip it first.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\nnebraska_locations\n##        FEATURE_ID  ... DATE_EDITED\n## 0          171013  ...         NaN\n## 1          171029  ...  03/08/2018\n## 2          182687  ...         NaN\n## 3          182688  ...  10/23/2009\n## 4          182689  ...  12/20/2017\n## ...           ...  ...         ...\n## 31473     2806916  ...  08/12/2021\n## 31474     2806917  ...  08/12/2021\n## 31475     2806918  ...  08/12/2021\n## 31476     2806919  ...         NaN\n## 31477     2806920  ...  08/12/2021\n## \n## [31478 rows x 20 columns]\n```\n:::\n\n\n#### SAS {-}\n\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters). \n\n::: callout-note\nDon't know what UTF-8 is? [Watch this excellent YouTube video explaining the history of file encoding!](https://www.youtube.com/watch?v=MijmeoH9LT4)\n:::\n\nWhile writing this code, I got an error of \"Invalid logical name\" because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS. \n\n```\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv > data/pokemon.csv\";\nonly run this once to download the file... */\nfilename pokeloc 'data/pokemon.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\n```\n\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\n\nIf I fix the file in R (because I know how to fix it there... another option is to fix it manually), \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\ntmp <- read_csv(\"data/pokemon.csv\")[,-1]\n# You'll learn how to do this later\ntmp <- select(tmp, -japanese_name) %>%\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\nwrite_csv(tmp, \"data/pokemon_ascii.csv\", na='.')\n```\n:::\n\n\nThen, reading in the new file allows us to actually see the output.\n```\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"data/pokemon_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \n```\n\nThis trick works in so many different situations. It's very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate. \n\nTo read in a pipe delimited file (the '|' character), we have to make some changes. Here is the proc import code. Note that I am reading in a version of the file that I've converted to ASCII (see details below) because while the import works with the original file, it causes the SAS -> R pipeline that the book is built on to break. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp <- readLines(\"data/NE_Features_20200501.txt\")\ntmp_ascii <- iconv(tmp, to = \"ASCII//TRANSLIT\")\nwriteLines(tmp_ascii, \"data/NE_Features_ascii.txt\")\n```\n:::\n\n\n```\n/* Without specifying the library to store the data in, it is stored in WORK */\nproc import datafile = \"data/NE_Features_ascii.txt\" out=nefeatures\n  DBMS = DLM /* delimited file */\n  replace;\n  GETNAMES = YES;\n  DELIMITER = '|';\n  GUESSINGROWS = 31582;\nrun;\n\nproc print data=nefeatures (obs=10); /* print the first 10 observations */\n  run;\n\n```\n\nUnder the hood, proc import is just writing code for a data step. So when proc import doesn't work, we can just write the code ourselves. It requires a bit more work (specifying column names, for example) but it also doesn't fail nearly as often. \n\n```\n/* x \"curl https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt\n> data/NE_Features_20200501.txt\"; */\n/* only run this once... */\n\ndata nefeatures;\n/*infile \"data/NE_Features_20200501.txt\"*/\ninfile \"data/NE_Features_ascii.txt\"\ndlm='|' /* specify delimiter */\n  encoding=\"utf-8\" /* specify encoding */\n  DSD /* delimiter sensitive data */\n  missover /* keep going if missing obs encountered */\n  firstobs=2; /* skip header row */\n  input FEATURE_ID $\n  FEATURE_NAME $\n  FEATURE_CLASS $\n  STATE_ALPHA $\n  STATE_NUMERIC\nCOUNTY_NAME $\n  COUNTY_NUMERIC $\n  PRIMARY_LAT_DMS $\n  PRIM_LONG_DMS $\n  PRIM_LAT_DEC\nPRIM_LONG_DEC\nSOURCE_LAT_DMS $\n  SOURCE_LONG_DMS $\n  SOURCE_LAT_DEC\nSOURCE_LONG_DEC\nELEV_IN_M\nELEV_IN_FT\nMAP_NAME $\n  DATE_CREATED $\n  DATE_EDITED $\n  ;\nrun;\n\nproc print data=nefeatures (obs=10); /* print the first 10 observations */\n  run;\n```\n\n:::\n\n\n::: callout-tip\n\n### Try it out: Reading CSV files\n\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveninently available at their [download page](https://rebrickable.com/downloads/). Because these datasets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV. \n\n\n\n\n\n::: panel-tabset\n\n#### Problem {-}\n\nThe `readr` package and `pandas` can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n#### R Solution {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlegosets <- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"data/lego_sets.csv\")\n```\n:::\n\n\n\n#### Python Solution {-}\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"data/lego_sets_py.csv\")\n```\n:::\n\n\n#### SAS Problem {-}\n\nIn SAS, it is also possible to read gzip files directly; however, it is tricky to get PROC IMPORT to work with gzip files. The code below will 1) download the file (uncomment that part), 2) create a link to the gzip file, 3) create a link to where the unzipped file will go, and 4) unzip the file to the link specified in (3). \n\n```\n/* x \"curl https://cdn.rebrickable.com/media/downloads/sets.csv.gz > \\ \ndata/lego_sets.csv.gz\";\nonly run this once... */\n\nfilename legofile ZIP \"data/lego_sets.csv.gz\" GZIP;\nfilename target \"data/lego_sets.csv\";\n\ndata _null_;\n  infile legofile;\n  file target;\n  input;\n  put _infile_;\nrun;\n```\n\nCan you write two different statements (one using proc import on the unzipped file, one using a datastep on the zipped file) to read the data in? Note that you may have to specify the length of character fields in the data step version using `length var_name $ 100;` before the input statement to set variable var_name to have maximum length of 100 characters. \n\n#### SAS Solution {-}\n\n```\nlibname classdat \"sas/\";\n/* Work with the library of class data */\n\nfilename legofile ZIP \"data/lego_sets.csv.gz\" GZIP;\nfilename target \"data/lego_sets.csv\";\n\ndata _null_;\n  infile legofile;\n  file target;\n  input;\n  put _infile_;\nrun;\n\nproc import datafile = target out=classdat.legoset DBMS=csv replace;\nGETNAMES=YES;\nGUESSINGROWS=15424;\nrun;\n\n/* This dataset will be stored in WORK */\ndata legoset2;\n  infile legofile dsd firstobs=2\n  dlm=\",\";\n  length set_num $20;\n  length name $100;\n  input set_num $ name $ year theme_id num_parts;\n  run;\n  \nproc print data=classdat.legoset (obs=10);\n  run;\n  \nproc print data=legoset2 (obs=10);\n  run;\n```\n\n:::\n\n## Spreadsheets\n\nThis example uses data downloaded from [\"Mapping Police Violence\"](https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx), which now links to a google sheet that has to be downloaded manually. Save that file to a path that you can find again - in the code below it lives in `data/police_violence.xlsx`. \n\n::: panel-tabset\n\n### R `readxl` {-}\n\nIn R, the easiest way to read Excel data in is to use the `readxl` package. There are many other packages with different features, however - I have used `openxlsx` in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, skip = 1)\npolice_violence[1:10, 1:6]\n## # A tibble: 10 × 6\n##    `Victim's name`         `Victim's age` `Victim's gender` `Victim's race`\n##    <chr>                   <chr>          <chr>             <chr>          \n##  1 Name withheld by police 27.0           Male              Unknown race   \n##  2 Name withheld by police Unknown        Unknown           Unknown race   \n##  3 Name withheld by police Unknown        Male              Unknown race   \n##  4 Ronald W. Flowers II    Unknown        Male              Unknown race   \n##  5 Name withheld by police Unknown        Male              Unknown race   \n##  6 Name withheld by police 31.0           Male              Unknown race   \n##  7 Name withheld by police Unknown        Male              Unknown race   \n##  8 Name withheld by police Unknown        Unknown           Unknown race   \n##  9 Name withheld by police Unknown        Male              Unknown race   \n## 10 Name withheld by police Unknown        Male              Unknown race   \n## # … with 2 more variables: `URL of image of victim` <chr>,\n## #   `Date of Incident (month/day/year)` <dttm>\n```\n:::\n\n\n\n### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows = 1)\npolice_violence\n##                 Victim's name  ... Killed by Police 2013-21\n## 0     Name withheld by police  ...                      NaN\n## 1     Name withheld by police  ...                      NaN\n## 2     Name withheld by police  ...                      NaN\n## 3        Ronald W. Flowers II  ...                      NaN\n## 4     Name withheld by police  ...                      NaN\n## ...                       ...  ...                      ...\n## 9937               Tyree Bell  ...                      1.0\n## 9938             Abel Gurrola  ...                      1.0\n## 9939      Christopher Tavares  ...                      1.0\n## 9940        Andrew L. Closson  ...                      1.0\n## 9941            Andrew Layton  ...                      1.0\n## \n## [9942 rows x 63 columns]\n```\n:::\n\n\n\n### SAS {-}\n\nIn SAS, PROC IMPORT is one easy way to read in xlsx files. In this code chunk, we have to handle the fact that one of the columns in the spreadsheet contains dates. [SAS and Excel handle dates a bit differently](https://support.sas.com/resources/papers/proceedings/proceedings/sugi29/068-29.pdf), so we have to transform the date variable -- and we may as well relabel it at the same time. To do this, we use a DATA statement that outputs to the same dataset it references. We define a new variable date, adjust the Excel dates so that they conform to SAS's standard, and tell SAS how to format the date. (We'll talk more about dates and times later)\n\n```\nlibname classdat \"sas/\";\n\nPROC IMPORT OUT=classdat.police \n    DATAFILE=\"data/police_violence.xlsx\" \n    DBMS=xlsx /* Tell SAS what type of file it's reading */ \n    REPLACE; /* replace the dataset if it already exists */\n  SHEET=\"2013-2019 Police Killings\"; /* SAS reads the first sheet by default */\n  GETNAMES=yes;\n    informat VAR6 mmddyy10.; /* tell SAS what format the date is in */\nRUN;\n\nDATA classdat.police;\n  SET classdat.police; /* modify the dataset and write back out to it */\n  \n  date = VAR6 - 21916; /* Conversion to SAS date standard from Excel */\n  FORMAT date MMDDYY10.; /* Tell SAS how to format the data when printing it */ \n  DROP VAR6; /* Get rid of the original data */\n  \n  num_age = INPUT(Victim_s_age, 3.); /* create numeric age variable */\n  \n  DROP \n    A_brief_description_of_the_circu \n    URL_of_image_of_victim \n    Link_to_news_article_or_photo_of; \n    /* drop longer variable to save space so the file fits on GitHub */\n    /* Size went from 100 MB to 6.7 MB without these 3 vars */\nRUN;\n\n\nPROC PRINT DATA=classdat.police (obs=10); /* print the first 10 observations */\n  VAR Victim_s_name Victim_s_age num_age Victim_s_gender Victim_s_race date;\nRUN;\n```\n\n::: callout-note\n[Here](https://stats.idre.ucla.edu/sas/faq/how-do-i-readwrite-excel-files-in-sas/) is some additional information about reading and writing Excel files in SAS. \n:::\n\n:::\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier. \n\n### Try it out {- .tryitout}\n\n::: panel-tabset\n\n#### Problem {-}\nThe Nebraska Department of Motor Vehicles [publishes a database of vehicle registrations](https://dmv.nebraska.gov/about/dmv-searchable-data) by type of license plate. [Link](https://dmv.nebraska.gov/about/dmv-searchable-data)\n\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n#### R Solution {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates <- read_xls(path = \"data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 × 6\n##    Age   `\\nOperator's \\nLi…` `Operator's\\nL…` `Motor-\\ncycle…` `Commercial Dr…`\n##    <chr>                <dbl>            <dbl>            <dbl> <chr>           \n##  1 <NA>                    NA               NA               NA CDL A           \n##  2 14                       0                0                0 0               \n##  3 15                       0                0                0 0               \n##  4 16                       0                0                0 0               \n##  5 17                     961               33                0 0               \n##  6 18                   18903              174                0 25              \n##  7 19                   22159              251                0 97              \n##  8 20                   22844              326                1 144             \n##  9 21                   21589              428                0 233             \n## 10 22                   22478              588                0 292             \n## # … with 1 more variable: ...6 <chr>\n```\n:::\n\n\n#### Python Solution {-}\n\nYou may need to install `xlrd` via pip for this code to work.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\nne_plates\n##     Unnamed: 0  ... Total\\nLicensed\\n Drivers\n## 0          NaN  ...                       NaN\n## 1          NaN  ...                    3279.0\n## 2          NaN  ...                   14902.0\n## 3          NaN  ...                   22339.0\n## 4          NaN  ...                   24341.0\n## 5          NaN  ...                   21447.0\n## 6          NaN  ...                   23761.0\n## 7          NaN  ...                   24269.0\n## 8          NaN  ...                   23039.0\n## 9          NaN  ...                   23990.0\n## 10         NaN  ...                   24717.0\n## 11         NaN  ...                   25283.0\n## 12         NaN  ...                  125153.0\n## 13         NaN  ...                  128563.0\n## 14         NaN  ...                  126432.0\n## 15         NaN  ...                  117878.0\n## 16         NaN  ...                  102261.0\n## 17         NaN  ...                  104622.0\n## 18         NaN  ...                  111702.0\n## 19         NaN  ...                  118860.0\n## 20         NaN  ...                  106939.0\n## 21         NaN  ...                   86649.0\n## 22         NaN  ...                   56675.0\n## 23         NaN  ...                   35481.0\n## 24         NaN  ...                   20288.0\n## 25         NaN  ...                    8555.0\n## 26         NaN  ...                    1864.0\n## 27         NaN  ...                      82.0\n## 28         NaN  ...                 1483162.0\n## \n## [29 rows x 16 columns]\n```\n:::\n\n\n#### SAS Solution {-}\n```\nPROC IMPORT OUT=WORK.licplate \n    DATAFILE=\"data/2019_Vehicle_Registration_Plates_NE.xlsx\" \n    DBMS=xlsx /* Tell SAS what type of file it's reading */ \n    REPLACE; /* replace the dataset if it already exists */\n    RANGE=\"'Reg By Plate Type'$A2:0\"\n  GETNAMES=yes;\nRUN;\n\n/* just a few columns... way too many to handle */\nPROC PRINT DATA=WORK.licplate (obs=10); /* print the first 10 observations */\nVar County Amateur__Radio Breast__Cancer Choose__Life County__Gov Comm__Truck Passenger Total;\nRUN;\n```\n\n:::\n\n\n### Google Sheets\n\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We're not going to get into the power of these packages just now, but it's worth a look if you're working with collaborators who use Google sheets.\n\n::: callout-note\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\n\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via `GSpread` or `googlesheets4` in python and R respectively.\n:::\n\nFor now, let's just demonstrate reading in data from google sheets in R and python using the [Data Is Plural](https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0) archive.\n\n::: panel-tabset\n\n#### Python {-}\n\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in @schaferReadDataGoogle2020. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n```\n:::\n\n\nThis method would likely work just as well in R and would not require the `googlesheets4` package.\n\n#### R {-}\n\nThis method is described in @schaferReadDataGoogle2020 for Python, but I have adapted the code to use in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n```\n:::\n\n\n\n#### R: `googlesheets4` {-}\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\n\n<!-- How to authenticate on google actions/rsconnect: https://gargle.r-lib.org/articles/non-interactive-auth.html -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural <- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n```\n:::\n\n\n#### Python: `GSpread` {-}\n\n\nThese instructions are derived from [@clarkeHowReadGoogle2021]. \nWe will have to install the `GSpread` package: type `pip install gspread` into the terminal.\n\nThen, you will need to obtain a client token JSON file following [these instructions](https://towardsdatascience.com/how-to-import-google-sheets-data-into-a-pandas-dataframe-using-googles-api-v4-2020-f50e84ea4530). \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport gspread as gs\nimport pandas as pd\n\n```\n:::\n\n\nI've stopped here because I can't get the authentication working. \n<!-- XXX TODO XXX -->\n\n:::\n\n## Binary Files\n\nBoth R and SAS have binary data files that store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see @maierleLoadingBinaryData2020.\n\n::: panel-tabset\n\n\n### R formats in R {-}\n\n`.Rdata` is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlegos <- read_csv(\"data/lego_sets.csv\")\nmy_var <- \"This variable contains a string\"\nsave(legos, my_var, file = \"data/R_binary.Rdata\")\n```\n:::\n\n\nIf we look at the file sizes of `lego_sets.csv` (619 KB) and `R_binary.Rdata`(227.8 KB), the size difference between binary and flat file formats is obvious. \n\nWe can load the R binary file back in using the `load()` function.\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"mesodata\"           \"mesodata_names\"    \n##  [7] \"ne_plates\"          \"nebraska_locations\" \"pokemon_info\"      \n## [10] \"police_violence\"    \"sheet_id\"           \"sheet_name\"        \n## [13] \"tmp\"                \"tmp_chars\"          \"tmp_chars_space\"   \n## [16] \"tmp_space\"          \"url\"                \"widths\"\n\nload(\"data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"mesodata\"          \n##  [7] \"mesodata_names\"     \"my_var\"             \"ne_plates\"         \n## [10] \"nebraska_locations\" \"pokemon_info\"       \"police_violence\"   \n## [13] \"sheet_id\"           \"sheet_name\"         \"tmp\"               \n## [16] \"tmp_chars\"          \"tmp_chars_space\"    \"tmp_space\"         \n## [19] \"url\"                \"widths\"\n```\n:::\n\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(legos, \"data/RDSlego.rds\")\n\nother_lego <- readRDS(\"data/RDSlego.rds\")\n```\n:::\n\n\nBecause RDS formats don't save the object name, you can be sure that you're not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately. \n\n### R formats in Python {-}\n\nWe first need to install the `pyreadr` package by running `pip install pyreadr` in the terminal.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pyreadr\n\nrdata_result = pyreadr.read_r('data/R_binary.Rdata')\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n##            set_num                                 name  ...  theme_id  num_parts\n## 0            001-1                                Gears  ...       1.0       43.0\n## 1           0011-2                    Town Mini-Figures  ...      84.0       12.0\n## 2           0011-3           Castle 2 for 1 Bonus Offer  ...     199.0        0.0\n## 3           0012-1                   Space Mini-Figures  ...     143.0       12.0\n## 4           0013-1                   Space Mini-Figures  ...     143.0       12.0\n## ...            ...                                  ...  ...       ...        ...\n## 15419      wwgp1-1  Wild West Limited Edition Gift Pack  ...     476.0        0.0\n## 15420   XMASTREE-1                       Christmas Tree  ...     410.0       26.0\n## 15421      XWING-1                  Mini X-Wing Fighter  ...     158.0       60.0\n## 15422      XWING-2                    X-Wing Trench Run  ...     158.0       52.0\n## 15423  YODACHRON-1      Yoda Chronicles Promotional Set  ...     158.0      413.0\n## \n## [15424 rows x 5 columns]\nrdata_result[\"my_var\"]\n##                             my_var\n## 0  This variable contains a string\nrds_result = pyreadr.read_r('data/RDSlego.rds')\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n##            set_num                                 name  ...  theme_id  num_parts\n## 0            001-1                                Gears  ...       1.0       43.0\n## 1           0011-2                    Town Mini-Figures  ...      84.0       12.0\n## 2           0011-3           Castle 2 for 1 Bonus Offer  ...     199.0        0.0\n## 3           0012-1                   Space Mini-Figures  ...     143.0       12.0\n## 4           0013-1                   Space Mini-Figures  ...     143.0       12.0\n## ...            ...                                  ...  ...       ...        ...\n## 15419      wwgp1-1  Wild West Limited Edition Gift Pack  ...     476.0        0.0\n## 15420   XMASTREE-1                       Christmas Tree  ...     410.0       26.0\n## 15421      XWING-1                  Mini X-Wing Fighter  ...     158.0       60.0\n## 15422      XWING-2                    X-Wing Trench Run  ...     158.0       52.0\n## 15423  YODACHRON-1      Yoda Chronicles Promotional Set  ...     158.0      413.0\n## \n## [15424 rows x 5 columns]\n```\n:::\n\n\n### R format in SAS\n\nThere are [theoretically ways to read R data into SAS via the R subsystem](https://blogs.sas.com/content/iml/2011/05/13/calling-r-from-sasiml-software.html) [@wicklinCallingSASIML2011]. Feel free to do that on your own machine.^[I tried, and it crashed SAS on my machine.]\n\n### SAS format in R {-}\n\nFirst, let's download the NHTS data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\n# Download the file and write to disk\nres <- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"data/cen10pub.sas7bdat\", overwrite = T))\n```\n:::\n\n\nYou can see more information about this data [here](https://data.world/dot/national-household-travel-survey-nhts-2009) [@usdepartmentoftransportationNationalHouseholdTravel2018].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata <- read.sas7bdat(\"data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n```\n:::\n\n\n\nIf you are curious about what this data means, then by all means, take a look at the [codebook](https://query.data.world/s/k4g2u42ltuqhkw56htbfheg5ouoyiu) (XLSX file). For now, it's enough that we can see roughly how it's structured.\n\n### SAS format in Python {-}\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"data/cen10pub.sas7bdat\")\n## 'data/cen10pub.sas7bdat'\n```\n:::\n\n\nYou can see more information about this data [here](https://data.world/dot/national-household-travel-survey-nhts-2009) [@usdepartmentoftransportationNationalHouseholdTravel2018].\n\nTo read SAS files, we use the `read_sas` function in Pandas.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\ndata = pd.read_sas(\"data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n```\n:::\n\n\n### SAS format in SAS {-}\n\nLet's read in the data from the 2009 National Household Travel Survey:\n```\nlibname classdat \"data\";\n/* this tells SAS where to look for (a bunch of) data files */\n\nproc contents data=classdat.cen10pub; /* This tells sas to access the specific file */\nrun;\n```\n\nIf you are curious about what this data means, then by all means, take a look at the [codebook](https://github.com/srvanderplas/unl-stat850/raw/master/data/cen10_codebook.xlsx) (XLSX file). For now, it's enough that we can see roughly how it's structured.\n\n:::\n\n\n::: callout-tip\n### Try it out \n\n::: panel-tabset\n\n#### Problem {-}\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\n\nIn RStudio, go to Session -> Clear Workspace. (This will clear your environment)\n\nNow, using your RDS files, load the objects back into R with different names. \n\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with `all.equal()` if you're curious)\n\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n#### R Solution {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(readr)\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000)\nlegos <- read_csv(\"data/lego_sets.csv\")\n\nsave(police_violence, legos, file = \"data/04_Try_Binary.Rdata\")\nsaveRDS(police_violence, \"data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"data/04_Try_Binary2.rds\")\n\nrm(police_violence, legos) # Limited clearing of workspace... \n\n\nload(\"data/04_Try_Binary.Rdata\")\n\npv_compare <- readRDS(\"data/04_Try_Binary1.rds\")\nlego_compare <- readRDS(\"data/04_Try_Binary2.rds\")\n\nall.equal(police_violence, pv_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n```\n:::\n\n\n\n#### Python Solution {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pyreadr\n\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\npolice = robjs[\"police_violence\"]\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n\npolice_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n\npolice.equals(police_compare)\n## True\nlegos.equals(lego_compare)\n## True\n```\n:::\n\n\n:::\n\n:::\n\n## Databases\n\nThere are many different database formats. Some of the most common databases are SQL* related formats and Microsoft Access files. \n\n::: callout-note\nYou can get through this class without this section. Feel free to skip it and come back when/if you need it.\n:::\n\n\n[This excellent GitHub repo contains code to connect to multiple types of databases in R, python, PHP, Java, SAS, and VBA](https://github.com/ParfaitG/DATABASE_CONNECTIONS)\n\n\n### Microsoft Access\n\nTo get access to MS Access databases, you will need to become familiar with how to install ODBC drivers. These drivers tell your operating system how to connect to each type of database (so you need a different driver to get to MS Access databases than to get to SQL databases). \n\n::: {.callout-note collapse=\"true\"}\n#### Install ODBC Drivers on your machine {-}\n\n<i class=\"fa-brands fa-windows\"></i> and <i class=\"fa-brands fa-apple\"></i> [This set of instructions](https://exploratory.io/note/exploratory/How-to-import-Data-from-Microsoft-Access-Database-with-ODBC-zIJ2bjs2) appears to contain all of the right steps for Windows and Mac and has been updated recently (Feb 2022) [@teamexploratoryHowImportData2022]. \n\n<i class=\"fa-brands fa-linux\"></i> I installed `mdbtools` on Ubuntu and have the following entry in my `/etc/odbcinst.ini` file:\n\n```\n[MDBTools]\nDescription=MDBTools Driver\nDriver=libmdbodbc.so\nSetup=libmdbodbc.so\nFileUsage=1\nUsageCount=1\n```\n\nAdding this entry to the file may be part of the `mdbtools` installation - I certainly have no memory of doing it myself, but this may help if you're troubleshooting, so I've included it.\n:::\n\nFor this demo, we'll be using the [Scottish Witchcraft Database](http://witches.shca.ed.ac.uk/index.cfm?fuseaction=home.register)[@juliangoodareSurveyScottishWitchcraft2003], which you can download from their website, or acquire from the [course data folder](https://github.com/srvanderplas/unl-stat850/raw/main/data/Witchcraftsurvey_download.mdb) if you don't want to register with the authors. A description of the dataset is also [available](https://github.com/srvanderplas/unl-stat850/raw/main/data/Witchcraftsurvey_download.doc). \n\n::: panel-tabset\n\n#### R {-}\n\nIn R, we can read in MS Access files using the `Hmisc` package, as long as the mdbtools library is available on your computer^[A currently maintained version of the library is [here](https://github.com/cyberemissary/mdbtools) and should work for UNIX platforms. It may be possible to install the library on Windows using the UNIX subsystem, per [this thread](https://github.com/brianb/mdbtools/issues/107)]. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"Hmisc\" %in% installed.packages()) install.packages(\"Hmisc\")\nlibrary(Hmisc)\ndb_loc <- \"data/Witchcraftsurvey_download.mdb\"\n\nmdb.get(db_loc, tables = TRUE) # get table list\n##  [1] \"WDB_Accused\"             \"WDB_Accused_family\"     \n##  [3] \"WDB_Appeal\"              \"WDB_CalendarCustom\"     \n##  [5] \"WDB_Case\"                \"WDB_Case_person\"        \n##  [7] \"WDB_Commission\"          \"WDB_Complaint\"          \n##  [9] \"WDB_Confession\"          \"WDB_CounterStrategy\"    \n## [11] \"WDB_DemonicPact\"         \"WDB_Denunciation\"       \n## [13] \"WDB_DevilAppearance\"     \"WDB_Elf_FairyElements\"  \n## [15] \"WDB_Imprisonment\"        \"WDB_LinkedTrial\"        \n## [17] \"WDB_Malice\"              \"WDB_MentionedAsWitch\"   \n## [19] \"WDB_MovestoHLA\"          \"WDB_MusicalInstrument\"  \n## [21] \"WDB_Ordeal\"              \"WDB_OtherCharges\"       \n## [23] \"WDB_OtherNamedwitch\"     \"WDB_Person\"             \n## [25] \"WDB_PrevCommission\"      \"WDB_PropertyDamage\"     \n## [27] \"WDB_Ref_Parish\"          \"WDB_Reference\"          \n## [29] \"WDB_ReligiousMotif\"      \"WDB_RitualObject\"       \n## [31] \"WDB_ShapeChanging\"       \"WDB_Source\"             \n## [33] \"WDB_Torture\"             \"WDB_Trial\"              \n## [35] \"WDB_Trial_Person\"        \"WDB_WeatherModification\"\n## [37] \"WDB_WhiteMagic\"          \"WDB_WitchesMeetingPlace\"\nmdb.get(db_loc, tables = \"WDB_Trial\")[1:6,1:10] # get table of trials, print first 6 rows and 10 cols\n##    Trialref TrialId TrialSystemId    CaseRef TrialType Trial.settlement\n## 1    T/JO/1       1            JO C/EGD/2120         2                 \n## 2  T/JO/100     100            JO  C/JO/2669         2                 \n## 3 T/JO/1000    1000            JO C/EGD/1474         2                 \n## 4 T/JO/1001    1001            JO C/EGD/1558         2                 \n## 5 T/JO/1002    1002            JO C/EGD/1681         2                 \n## 6 T/JO/1003    1003            JO C/EGD/1680         2                 \n##   Trial.parish Trial.presbytery Trial.county Trial.burgh\n## 1                      Aberdeen     Aberdeen    Aberdeen\n## 2                                                       \n## 3                                                       \n## 4                                                       \n## 5                                                       \n## 6\n```\n:::\n\nMany databases have multiple tables with **keys** that connect information in each table. We'll spend more time on databases later in the semester - for now, it's enough to be able to get data out of one. \n#### Python {-}\n\nThere are several tutorials out there to access MS Access databases using packages like `pyodbc` [e.g. @datatofishHowConnectPython2021]. I couldn't quite get these working on Linux, but it is possible you may have better luck on another OS. With that said, the solution using [pandas_access](https://pypi.org/project/pandas_access/) seems to be much simpler and require less OS configuration, so it's what I'll show here.\n\nFirst, we have to install `pandas_access` using `pip install pandas_access`. \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas_access as mdb\ndb_filename = 'data/Witchcraftsurvey_download.mdb'\n\n# List tables\nfor tbl in mdb.list_tables(db_filename):\n  print(tbl)\n\n# Read a small table.\n## WDB_Accused\n## WDB_Accused_family\n## WDB_Appeal\n## WDB_CalendarCustom\n## WDB_Case\n## WDB_Case_person\n## WDB_Commission\n## WDB_Complaint\n## WDB_Confession\n## WDB_CounterStrategy\n## WDB_DemonicPact\n## WDB_Denunciation\n## WDB_DevilAppearance\n## WDB_Elf_FairyElements\n## WDB_Imprisonment\n## WDB_LinkedTrial\n## WDB_Malice\n## WDB_MentionedAsWitch\n## WDB_MovestoHLA\n## WDB_MusicalInstrument\n## WDB_Ordeal\n## WDB_OtherCharges\n## WDB_OtherNamedwitch\n## WDB_Person\n## WDB_PrevCommission\n## WDB_PropertyDamage\n## WDB_Ref_Parish\n## WDB_Reference\n## WDB_ReligiousMotif\n## WDB_RitualObject\n## WDB_ShapeChanging\n## WDB_Source\n## WDB_Torture\n## WDB_Trial\n## WDB_Trial_Person\n## WDB_WeatherModification\n## WDB_WhiteMagic\n## WDB_WitchesMeetingPlace\ntrials = mdb.read_table(db_filename, \"WDB_Trial_Person\")\n```\n:::\n\n\nThis isn't perfectly stable - I tried to read `WDB_Trial` and got errors about NA values in an integer field - but it does at least work.\n\n\n#### SAS {-}\n\nUnfortunately, it appears that SAS on Linux doesn't allow you to read in Access files. So I can't demonstrate that for you. But, since you know how to do it in R, worst case you can open up R and export all of the tables to separate CSV files, then read those into SAS. 😢\n\n:::\n\nMy hope is that you never actually need to get at data in an MS Access database - the format seems to be largely dying out.\n\n### SQLite\n\nSQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. They function as databases but are more portable than SQL databases that require a server instance to run and connecting over a network (or running a server on your machine locally). As a result, they provide an opportunity to demonstrate most of the skills required for working with databases without all of the configuration overhead. \n\n\n::: panel-tabset\n#### R {-}\n\nLet's try working with a sqlite file that has only one table in R: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"RSQLite\" %in% installed.packages()) install.packages(\"RSQLite\")\nif (!\"DBI\" %in% installed.packages()) install.packages(\"DBI\")\nlibrary(RSQLite)\nlibrary(DBI)\n\n# Download the baby names file:\ndownload.file(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", destfile = \"data/ssa-\")\n\ncon <- dbConnect(RSQLite::SQLite(), \"data/ssa-babynames-for-2015.sqlite\")\ndbListTables(con) # List all the tables\n## [1] \"babynames\"\nbabyname <- dbReadTable(con, \"babynames\")\nhead(babyname, 10) # show the first 10 obs\n##    state year    name sex count rank_within_sex per_100k_within_sex\n## 1     AK 2015  Olivia   F    56               1              2367.9\n## 2     AK 2015    Liam   M    53               1              1590.6\n## 3     AK 2015    Emma   F    49               2              2071.9\n## 4     AK 2015    Noah   M    46               2              1380.6\n## 5     AK 2015  Aurora   F    46               3              1945.0\n## 6     AK 2015   James   M    45               3              1350.5\n## 7     AK 2015  Amelia   F    39               4              1649.0\n## 8     AK 2015     Ava   F    39               4              1649.0\n## 9     AK 2015 William   M    44               4              1320.5\n## 10    AK 2015  Oliver   M    41               5              1230.5\n```\n:::\n\n\n\nYou can of course write formal queries using the DBI package, but for many databases, it's easier to do the querying in R. We'll cover both options later - the R version will be in the next module.\n\n#### Python {-}\n\nThis example was created using @datacarpentryAccessingSQLiteDatabases2019 as a primary reference.\n\nIf you haven't already downloaded the database file, you can do that automatically in python using this code:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport urllib.request\nurllib.request.urlretrieve(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", \"data/ssa-babynames-for-2015.sqlite\")\n```\n:::\n\n\nYou don't have to install the `sqlite3` module in python using pip because it's been included in base python since Python 2.5.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport sqlite3\n\ncon = sqlite3.connect('data/ssa-babynames-for-2015.sqlite')\n\nbabyname = pd.read_sql_query(\"SELECT * from babynames\", con)\nbabyname\n##        state  year      name sex  count  rank_within_sex  per_100k_within_sex\n## 0         AK  2015    Olivia   F     56                1               2367.9\n## 1         AK  2015      Liam   M     53                1               1590.6\n## 2         AK  2015      Emma   F     49                2               2071.9\n## 3         AK  2015      Noah   M     46                2               1380.6\n## 4         AK  2015    Aurora   F     46                3               1945.0\n## ...      ...   ...       ...  ..    ...              ...                  ...\n## 127661    WY  2015  Sterling   M      5              159                248.9\n## 127662    WY  2015    Steven   M      5              159                248.9\n## 127663    WY  2015     Trace   M      5              159                248.9\n## 127664    WY  2015   Tristan   M      5              159                248.9\n## 127665    WY  2015     Tyson   M      5              159                248.9\n## \n## [127666 rows x 7 columns]\ncon.close() # You must close any connection you open!\n```\n:::\n\n\n#### SAS {-}\n\nIn SAS, you can theoretically connect to SQLite databases, but there are very specific instructions for how to do that for each operating system. \n\nYou'll need to acquire the [SQLite ODBC Driver](http://www.ch-werner.de/sqliteodbc/) for your operating system. You may also need to set up a DSN (Data Source Name) ([Windows](https://support.exagoinc.com/hc/en-us/articles/115005848908-Using-SQLite-Data-Sources), [Mac and Linux](https://db.rstudio.com/best-prsactices/rdivers/#setting-up-database-connections-1)).^[On one of my machines, I also had to make sure the file libodbc.so existed - it was named libodbc.so.1 on my laptop, so a symbolic link fixed the issue.] \n\nHere is my .odbc.ini file as I've configured it for my Ubuntu 20.04 machine. A similar file should work for any Mac or Linux machine. In windows, you'll need to use the ODBC Data Source Administrator to set this up. \n\n\n````\n[babyname]\nDescription = 2015 SSA baby names\nDriver = SQLite3\nDatabase = data/ssa-babynames-for-2015.sqlite\n````\n\n```\n/* This code requires that Ive set up a DSN connecting the sqlite file to */\n/* a specific driver on my computer. Youll have to set up your machine to */\n/* have a configuration that is appropriate for your setup */\n  \nlibname mydat odbc complete = \"dsn=babyname; Database=data/ssa-babynames-for-2015.sqlite\"; \n\nproc print data=mydat.babynames (obs=10);\nrun; \n```\n\n:::\n\n::: callout-note\n## Learn more\n\n- [RSQLite vignette](https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html)\n- [Slides from Jenny Bryan's talk on spreadsheets](https://speakerdeck.com/jennybc/spreadsheets) (sadly, no audio. It was a good talk.)\n- The [`vroom` package](https://www.tidyverse.org/blog/2019/05/vroom-1-0-0/) works like `read_csv` but allows you to read in and write to many files at incredible speeds. \n:::\n\n## References\n",
    "supporting": [
      "reading-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}