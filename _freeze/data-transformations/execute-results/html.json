{
  "hash": "bf9d06ed72175924131ad30f99eb97d0",
  "result": {
    "markdown": "# Data Transformations {#sec-data-transformations}\n\n## Module Objectives {#module10-objectives .unnumbered}\n\nBroadly, your objective while reading this chapter is to be able to identify datasets which have \"messy\" formats and determine a sequence of operations to transition the data into \"tidy\" format. To do this, you should be master the following concepts:\n\n-   Determine what data format is necessary to generate a desired plot or statistical model\n-   Join and split data columns using string operations\n-   Understand the differences between \"wide\" and \"long\" format data and how to transition between the two structures\n-   Understand relational data formats and how to use data joins to assemble data from multiple tables into a single table.\n\n## Tidy and Messy Data\n\n### Motivating Example\n\nConsider the spreadsheet screenshot in @fig-human-readable.\n\n![Spreadsheet intended for human consumption, from @mccallumBadDataHandbook2013 (Chapter 3)](images/data-transformations/Data-human-consumption.png){#fig-human-readable}\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. @fig-machine-readable shows a restructured version of this data in a more compact rectangular format.\n\n![Spreadsheet reorganized for data analysis](images/data-transformations/Data-machine-consumption.png){#fig-machine-readable}\n\nIn @fig-machine-readable, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as \"tidy\" data: it has a single column for each variable and a single row for each observation.\n\n### Defining Tidy data\n\nThe illustrations below are lifted from an [excellent blog post](https://www.openscapes.org/blog/2020/10/12/tidy-data/) [@lowndesTidyDataEfficiency2020] about tidy data; they're reproduced here because\n\n1.  they're beautiful and licensed as CCA-4.0-by, and\n2.  they might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\n-   Tables may be laid out for easy data entry, so that there are multiple observations in a single row\n-   It may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\n\nThe structure of **tidy data** reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n![Tidy data format, illustrated.](https://www.openscapes.org/img/blog/tidydata/tidydata_1.jpg){#fig-tidy-data-definition fig-alt=\"Stylized text providing an overview of Tidy Data. The top reads “Tidy data is a standard way of mapping the meaning of a dataset to its structure. - Hadley Wickham.” On the left reads “In tidy data: each variable forms a column; each observation forms a row; each cell is a single measurement.” There is an example table on the lower right with columns ‘id’, ‘name’ and ‘color’ with observations for different cats, illustrating tidy data structure.\"}\n\n![An illustration of the principle that every messy dataset is messy in its own way.](https://www.openscapes.org/img/blog/tidydata/tidydata_2.jpg){fig-alt=\"There are two sets of anthropomorphized data tables. The top group of three tables are all rectangular and smiling, with a shared speech bubble reading “our columns are variables and our rows are observations!”. Text to the left of that group reads “The standard structure of tidy data means that “tidy datasets are all alike…” The lower group of four tables are all different shapes, look ragged and concerned, and have different speech bubbles reading (from left to right) “my column are values and my rows are variables”, “I have variables in columns AND in rows”, “I have multiple variables in a single column”, and “I don’t even KNOW what my deal is.” Next to the frazzled data tables is text “...but every messy dataset is messy in its own way. -Hadley Wickham.”\"}\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n![Tidy data is easier to manage because the same tools and approaches apply to multiple datasets.](https://www.openscapes.org/img/blog/tidydata/tidydata_3.jpg){fig-alt=\"On the left is a happy cute fuzzy monster holding a rectangular data frame with a tool that fits the data frame shape. On the workbench behind the monster are other data frames of similar rectangular shape, and neatly arranged tools that also look like they would fit those data frames. The workbench looks uncluttered and tidy. The text above the tidy workbench reads “When working with tidy data, we can use the same tools in similar ways for different datasets…” On the right is a cute monster looking very frustrated, using duct tape and other tools to haphazardly tie data tables together, each in a different way. The monster is in front of a messy, cluttered workbench. The text above the frustrated monster reads “...but working with untidy data often means reinventing the wheel with one-time approaches that are hard to iterate or reuse.”\"}\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator's analysis and easily apply them to your own dataset.\n\n::: {#fig-tidy-data-advantages layout-ncol=\"2\"}\n![Collaboration with tidy data.](https://www.openscapes.org/img/blog/tidydata/tidydata_4.jpg){fig-alt=\"Two happy looking round fuzzy monsters, each holding a similarly shaped wrench with the word “wrangle” on it. Between their tools is held up a rectangular data table labeled “TIDY.”\"}\n\n![Tidy data enables standardized workflows.](https://www.openscapes.org/img/blog/tidydata/tidydata_5.jpg){fig-alt=\"Cute fuzzy monsters putting rectangular data tables onto a conveyor belt. Along the conveyor belt line are different automated “stations” that update the data, reading “WRANGLE”, “VISUALIZE”, and “MODEL”. A monster at the end of the conveyor belt is carrying away a table that reads “Complete analysis.”\"}\n\nTidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n:::\n\n::: callout-warning\n### Examples: Messy Data {.unnumbered}\n\n\n\n\n\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n::: panel-tabset\n#### Table 1 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 1\n\n|country     | year|  cases| population|\n|:-----------|----:|------:|----------:|\n|Afghanistan | 1999|    745|   19987071|\n|Afghanistan | 2000|   2666|   20595360|\n|Brazil      | 1999|  37737|  172006362|\n|Brazil      | 2000|  80488|  174504898|\n|China       | 1999| 212258| 1272915272|\n|China       | 2000| 213766| 1280428583|\n:::\n:::\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population \\* 100000 (this would define a new column).\n\n#### 2 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 2\n\n|country     | year|type       |      count|\n|:-----------|----:|:----------|----------:|\n|Afghanistan | 1999|cases      |        745|\n|Afghanistan | 1999|population |   19987071|\n|Afghanistan | 2000|cases      |       2666|\n|Afghanistan | 2000|population |   20595360|\n|Brazil      | 1999|cases      |      37737|\n|Brazil      | 1999|population |  172006362|\n|Brazil      | 2000|cases      |      80488|\n|Brazil      | 2000|population |  174504898|\n|China       | 1999|cases      |     212258|\n|China       | 1999|population | 1272915272|\n|China       | 2000|cases      |     213766|\n|China       | 2000|population | 1280428583|\n:::\n:::\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n#### 3 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 3\n\n|country     | year|rate              |\n|:-----------|----:|:-----------------|\n|Afghanistan | 1999|745/19987071      |\n|Afghanistan | 2000|2666/20595360     |\n|Brazil      | 1999|37737/172006362   |\n|Brazil      | 2000|80488/174504898   |\n|China       | 1999|212258/1272915272 |\n|China       | 2000|213766/1280428583 |\n:::\n:::\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can't do *anything* with this format as it stands, because we can't do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n#### 4 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 4a\n\n|country     |   1999|   2000|\n|:-----------|------:|------:|\n|Afghanistan |    745|   2666|\n|Brazil      |  37737|  80488|\n|China       | 212258| 213766|\n:::\n\n::: {.cell-output-display}\nTable: Table 4b\n\n|country     |       1999|       2000|\n|:-----------|----------:|----------:|\n|Afghanistan |   19987071|   20595360|\n|Brazil      |  172006362|  174504898|\n|China       | 1272915272| 1280428583|\n:::\n:::\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year's observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we'll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n#### 5 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 5\n\n|country     |century |year |rate              |\n|:-----------|:-------|:----|:-----------------|\n|Afghanistan |19      |99   |745/19987071      |\n|Afghanistan |20      |00   |2666/20595360     |\n|Brazil      |19      |99   |37737/172006362   |\n|Brazil      |20      |00   |80488/174504898   |\n|China       |19      |99   |212258/1272915272 |\n|China       |20      |00   |213766/1280428583 |\n:::\n:::\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don't always handle dates the way you'd hope they would.\n:::\n:::\n\n::: callout-tip\n### Try it out: Classifying Messy Data\n\n::: panel-tabset\n#### Problem\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\n\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n#### Table 1 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 1\n\n|country     | year|  cases| population|\n|:-----------|----:|------:|----------:|\n|Afghanistan | 1999|    745|   19987071|\n|Afghanistan | 2000|   2666|   20595360|\n|Brazil      | 1999|  37737|  172006362|\n|Brazil      | 2000|  80488|  174504898|\n|China       | 1999| 212258| 1272915272|\n|China       | 2000| 213766| 1280428583|\n:::\n:::\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population\\*100,000.\n\n#### 2 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 2\n\n|country     | year|type       |      count|\n|:-----------|----:|:----------|----------:|\n|Afghanistan | 1999|cases      |        745|\n|Afghanistan | 1999|population |   19987071|\n|Afghanistan | 2000|cases      |       2666|\n|Afghanistan | 2000|population |   20595360|\n|Brazil      | 1999|cases      |      37737|\n|Brazil      | 1999|population |  172006362|\n|Brazil      | 2000|cases      |      80488|\n|Brazil      | 2000|population |  174504898|\n|China       | 1999|cases      |     212258|\n|China       | 1999|population | 1272915272|\n|China       | 2000|cases      |     213766|\n|China       | 2000|population | 1280428583|\n:::\n:::\n\n\nEach variable does not have its own column (so a single year's observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n#### 3 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 3\n\n|country     | year|rate              |\n|:-----------|----:|:-----------------|\n|Afghanistan | 1999|745/19987071      |\n|Afghanistan | 2000|2666/20595360     |\n|Brazil      | 1999|37737/172006362   |\n|Brazil      | 2000|80488/174504898   |\n|China       | 1999|212258/1272915272 |\n|China       | 2000|213766/1280428583 |\n:::\n:::\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you'd have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n#### 4 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 4a\n\n|country     |   1999|   2000|\n|:-----------|------:|------:|\n|Afghanistan |    745|   2666|\n|Brazil      |  37737|  80488|\n|China       | 212258| 213766|\n:::\n\n::: {.cell-output-display}\nTable: Table 4b\n\n|country     |       1999|       2000|\n|:-----------|----------:|----------:|\n|Afghanistan |   19987071|   20595360|\n|Brazil      |  172006362|  174504898|\n|China       | 1272915272| 1280428583|\n:::\n:::\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you'd need to \"stack\" the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n#### 5 {.unnumbered}\n\n\n::: {.cell}\n::: {.cell-output-display}\nTable: Table 5\n\n|country     |century |year |rate              |\n|:-----------|:-------|:----|:-----------------|\n|Afghanistan |19      |99   |745/19987071      |\n|Afghanistan |20      |00   |2666/20595360     |\n|Brazil      |19      |99   |37737/172006362   |\n|Brazil      |20      |00   |80488/174504898   |\n|China       |19      |99   |212258/1272915272 |\n|China       |20      |00   |213766/1280428583 |\n:::\n:::\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren't actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you'd need to combine the two year columns together first).\n:::\n:::\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\n1.  Each dataset goes into its own table (or tibble, if you are using R)\n2.  Each variable gets its own column\n\n::: callout-note\n## Additional reading\n\n@internationalbusinessmachinesRisksUsingSpreadsheets2018 - IBM SPSS ad that talks about the perils of spreadsheets\n\n@obeirneHorrorStories2020 - assembled news stories involving spreadsheet mishaps\n:::\n\nBy the end of this chapter, you will have the skills needed to wrangle the most common \"messy\" data sets into \"tidy\" form.\n\n## String operations\n\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different \"levels\" of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\n> Some people, when confronted with a problem, think \"I know, I'll use regular expressions.\" Now they have two problems. - Jamie Zawinski\n\n![Alternately, the xkcd version of the above quote](https://imgs.xkcd.com/comics/perl_problems.png)\n\nThe [stringr cheatsheet](https://github.com/rstudio/cheatsheets/blob/main/strings.pdf) by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Task                                                       | R                                                                                              | Python                                                                                     |\n+============================================================+================================================================================================+============================================================================================+\n| Replace `pattern` with `replacement`                       | base: `gsub(pattern, replacement, x)`                                                          | pandas: `x.str.replace(pattern, replacement)` (not vectorized over pattern or replacement) |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_replace(x, pattern, replacement)` and `str_replace_all(x, pattern, replacement)` |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Convert case                                               | base: `tolower(x)`, `toupper(x)`                                                               | pandas: `x.str.lower()`, `x.str.upper()`                                                   |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_to_lower(x)`, `str_to_upper(x)` , `str_to_title(x)`                              |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Strip whitespace from start/end                            | base: `trimws(x)`                                                                              | pandas: `x.str.strip()`                                                                    |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_trim(x)` , `str_squish(x)`                                                       |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Pad strings to a specific length                           | base: `sprintf(format, x)`                                                                     | pandas: `x.str.pad()`                                                                      |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_pad(x, …)`                                                                       |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Test if the string contains a pattern                      | base: `grep(pattern, x)` or `grepl(pattern, x)`                                                | pandas: `x.str.contains(pattern)`                                                          |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_detect(x, pattern)`                                                              |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Count how many times a pattern appears in the string       | base: `gregexpr(pattern, x)` + `sapply` to count length of the returned list                   | pandas: `x.str.count(pattern)`                                                             |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringi: `stri_count(x, pattern)`                                                              |                                                                                            |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_count(x, pattern)`                                                               |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Find the first appearance of the pattern within the string | base: `regexpr(pattern, x)`                                                                    | pandas: `x.str.find(pattern)`                                                              |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_locate(x, pattern)`                                                              |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Find all appearances of the pattern within the string      | base: `gregexpr`                                                                               | pandas: `x.str.findall(pattern)`                                                           |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_locate_all(x, pattern)`                                                          |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Detect a match at the start/end of the string              | base: use regular expr.                                                                        | pandas: `x.str.startswith(pattern)` , `x.str.endswith(pattern)`                            |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_starts(x, pattern)` ,`str_ends(x, pattern)`                                      |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Subset a string from index a to b                          | base: `substr(x, a, b)`                                                                        | pandas: `x.str.slice(a, b, step)`                                                          |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_sub(x, a, b)`                                                                    |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n| Convert string encoding                                    | base: `iconv(x, encoding)`                                                                     | pandas: `x.str.encode(encoding)`                                                           |\n|                                                            |                                                                                                |                                                                                            |\n|                                                            | stringr: `str_conv(x, encoding)`                                                               |                                                                                            |\n+------------------------------------------------------------+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+\n\n: Table of string functions in R and python. `x` is the string or vector of strings, `pattern` is a pattern to be found within the string, `a` and `b` are indexes, and `encoding` is a string encoding, such as UTF8 or ASCII. {#tbl-string-function}\n\nIn @tbl-string-function, multiple functions are provided for e.g. common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the `stringr` package, which is included in the tidyverse[^data-transformations-1].\n\n[^data-transformations-1]: Many functions from `stringr` have somewhat faster functional equivalents in the `stringi` package, but the `stringi` package has a less \"tidy\" API, so it may be worth the slight slowdown to use `stringr` if your data isn't huge because your code will be more readable.\n\n### Converting strings to numbers {.unnumbered}\n\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\n::: {#tbl-parse-numbers .cell tbl-cap='Different \"messy\" number formats'}\n      int_col    float_col  mix_col      missing_col  money_col    eu_numbers    boolean_col    custom\n--  ---------  -----------  ---------  -------------  -----------  ------------  -------------  --------\n 0          1          1.1  a                      1  £1,000.00    1.000.000,00  True           Y\n 1          2          1.2  2                      2  £2,400.00    2.000.342,00  False          Y\n 2          3          1.3  3                      3  £2,400.00    3.141,59      True           N\n 3          4          4.7  4                    nan  £2,400.00    34,25         True           N\n:::\n\n\n\n\nNote that [numbers](https://docs.oracle.com/cd/E19455-01/806-0169/overview-8/index.html), currencies, dates, and times are written differently based on what country you're in [@ashourConciseGuideNumber2022]. In computer terms, this is the \"locale\", and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [@LocaleComputerSoftware2022].\n\n[If you've never had to deal with the complexities of working on a laptop designed for one country using another country's conventions, know that it isn't necessarily the easiest thing to do.]{.aside}\n\n::: {.callout-note collapse=\"true\"}\n##### Optional: Locales {.unnumbered}\n\n###### Find your locale {.unnumbered}\n\n-   <i class=\"fa-brands fa-windows\"></i> Type [`Get-WinSystemLocale`](https://docs.microsoft.com/en-us/powershell/module/international/get-winsystemlocale?view=windowsserver2022-ps#syntax) into your CMD or powershell terminal.\n-   <i class=\"fa-brands fa-apple\"></i> (10.4 and later) and <i class=\"fa-brands fa-linux\"></i> Type `locale` into your terminal\n\n###### Get set up to work with locales {.unnumbered}\n\nWhile this isn't required, it may be useful and is definitely good practice if you're planning to work with data generated internationally.\n\n[This article](https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html) tells you how to set things up in linux <i class=\"fa-brands fa-linux\"></i>. The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n-   <i class=\"fa-brands fa-windows\"></i> [Installing languages](https://support.microsoft.com/en-us/windows/install-a-language-for-windows-ccd853d3-9ecd-7da7-9ef0-72b4a055410a)\n-   <i class=\"fa-brands fa-apple\"></i> [Change locales](https://9to5mac.com/2018/08/09/mac-how-to-change-language-and-region/). Installing or creating new locales seems to be [more complicated](https://stackoverflow.com/questions/9991603/add-a-locale-in-mac-osx), and since I do not have a mac, I can't test this out easily myself.\n:::\n\nWe'll use @tbl-parse-numbers to explore different string operations focused specifically on converting strings to numbers.\n\n::: panel-tabset\n##### Get the data: Python {.unnumbered}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\")\n```\n:::\n\n\n##### R {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\", colClasses = \"character\")\n```\n:::\n\n\nBy default, R tries to outsmart us and read the data in as numbers. I've disabled this behavior by setting `colClasses='character'` so that you can see how these functions work... but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don't know how to disable it.\n:::\n\n::: callout-caution\n##### Converting Columns Using Your Best Guess {.unnumbered}\n\nBoth R and Python have ways to \"guess\" what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you're working with data that is already read in, how do you get R and Python to guess what type something is?\n\n::: panel-tabset\n###### R\n\nHere, R gets everything \"right\" except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn't clearly numeric or doesn't match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle `mix_col`, we would have to explicitly convert to numeric, causing the a to be converted to `NA`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(readr)\ndf_guess <- type_convert(df)\nstr(df_guess)\n## 'data.frame':\t4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n```\n:::\n\n\nThe `type_convert` function has a `locale` argument; `readr` includes a `locale()` function that you can pass to `type_convert` that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(readr)\nfixed_df <- type_convert(df) \nfixed_df2 <- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':\t4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n```\n:::\n\n\n###### Python\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n```\n:::\n\n\nAs in R, we can set the locale in Python to change how things are read in.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n```\n:::\n\n:::\n:::\n\n::: callout-caution\n##### Converting Columns Directly {.unnumbered}\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in @sec-type-conversions. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of 'missing' or NA variables that are not necessarily compatible.\n\n::: panel-tabset\n###### R\n\nHere, we use the `across` helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don't work here, because they contain characters R doesn't recognize as numeric characters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\ndf_numeric <- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':\t4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  1 0 1 1\n##  $ custom     : num  NA NA NA NA\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n```\n:::\n\n\n###### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n```\n:::\n\n:::\n:::\n\n::: callout-caution\n##### Example: Converting Y/N data\n\nThe next thing we might want to do is convert our `custom` column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\n-   We could use factors/categorical variables, which have numeric values \"under the hood\", but show up as labeled.\n-   We could (in this particular case) test for equality with \"Y\", but this approach would not generalize well if we had more than 2 categories.\n-   We could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I've used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n::: panel-tabset\n###### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %>%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %>%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':\t4 obs. of  11 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n```\n:::\n\n\n###### Python\n\nWe've already done a brief demonstration of string methods in Python when we trimmed off the £ character. In this situation, it's better to use the pandas `replace` method, which allows you to pass in a list of values and a list of replacements.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers      float64\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object\n```\n:::\n\n:::\n:::\n\n### Using find and replace\n\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution[^data-transformations-2], and may introduce bugs if you use the same code to analyze new data with characters you haven't anticipated, but in so many cases it's also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\n\n[^data-transformations-2]: It's particularly hackish when you're working with locale-specific settings [@herrmannHowDealInternational2021], and in many cases you can handle locale issues much more elegantly.\n\n[I'll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading @herrmannHowDealInternational2021 so that you know why find-and-replace isn't (necessarily) the best option for locale-specific formatting.]{.aside}\n\n::: callout-caution\n##### Example: find and replace\n\nLet's start with the money column.\n\n::: panel-tabset\n###### R\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn't work by default with type_convert, but as long as we `mutate` and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ',' for decimal and '.' for thousands separators.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n```\n:::\n\n\n###### Python\n\nIn python, a similar approach doesn't work out, because the pound sign is not handled correctly.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): babel.numbers.NumberFormatError: '£1,000.00' is not a valid decimal number\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# Remove £ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"£\")\n# Then parse the number\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n```\n:::\n\n:::\n:::\n\n::: callout-caution\n#### Example: Locale find-and-replace\n\nWe could also handle the locale issues using find-and-replace, if we wanted to...\n\n::: panel-tabset\n###### R\n\nNote that `str_remove` is shorthand for `str_replace(x, pattern, \"\")`. There is a little bit of additional complexity in switching \",\" for \".\" and vice versa - we have to change \",\" to something else first, so that we can replace \".\" with \",\". This is *not* elegant but it does work. It also doesn't generalize - it will mess up numbers formatted using the US/UK convention, and it won't handle numbers formatted using other conventions from other locales.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = str_remove(money_col, \"£\") %>% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %>%\n           parse_number())\n```\n:::\n\n\n###### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\n## <string>:1: FutureWarning: The default value of regex will change from True to False in a future version.\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"£\").\\\nstr.replace(',', '')\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...  eu_numbers  boolean_col  custom\n## 0        1        1.1       a  ...  1000000.00         True       Y\n## 1        2        1.2       2  ...  2000342.00        False       Y\n## 2        3        1.3       3  ...     3141.59         True       N\n## 3        4        4.7       4  ...       34.25         True       N\n## \n## [4 rows x 8 columns]\n```\n:::\n\n:::\n:::\n\n### Separating multi-variable columns\n\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\n\nIn pandas, we use `x.str.split()` to split columns in a DataFrame, in R we use the `tidyr` package's `separate` function.\n\n::: callout-caution\n#### Example: Separating columns\n\nWe'll use the `table3` object included in `dplyr` for this example. You can load it in R and then load the `reticuate` package to be able to access the object in python as `r.table3`.\n\n::: panel-tabset\n##### Picture the operation\n\n![We want to separate the rate column into two new columns, cases and population.](images/data-transformations/tidyr_separate.png){fig-alt=\"An image showing table 3 from the messy data examples, with the rate column containing data formatted as xxx/yyy. The picture shows the transition to a similarly structured data with two new columns: cases, which contains the xxx data, and pop, which contains the yyy data.\"}\n\n##### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate(table3, rate, into = c('cases', 'pop'), sep = \"/\", remove = F)\n## # A tibble: 6 × 5\n##   country      year rate              cases  pop       \n##   <chr>       <int> <chr>             <chr>  <chr>     \n## 1 Afghanistan  1999 745/19987071      745    19987071  \n## 2 Afghanistan  2000 2666/20595360     2666   20595360  \n## 3 Brazil       1999 37737/172006362   37737  172006362 \n## 4 Brazil       2000 80488/174504898   80488  174504898 \n## 5 China        1999 212258/1272915272 212258 1272915272\n## 6 China        2000 213766/1280428583 213766 1280428583\n```\n:::\n\n\n##### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country  year               rate   cases         pop\n## 0  Afghanistan  1999       745/19987071     745    19987071\n## 1  Afghanistan  2000      2666/20595360    2666    20595360\n## 2       Brazil  1999    37737/172006362   37737   172006362\n## 3       Brazil  2000    80488/174504898   80488   174504898\n## 4        China  1999  212258/1272915272  212258  1272915272\n## 5        China  2000  213766/1280428583  213766  1280428583\n```\n:::\n\n\nThis uses python's multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g. cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time.\n:::\n:::\n\n### Joining columns\n\nIt's also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a 'name' column that has both pieces of information together.\n\n::: callout-caution\n#### Example: Joining columns\n\nWe'll use the `table3` object included in `dplyr` for this example. You can load it in R and then load the `reticuate` package to be able to access the object in python as `r.table3`.\n\n::: panel-tabset\n##### Picture the operation\n\n![We want to join the century and year columns into a new column, yyyy.](images/data-transformations/tidyr_unite.png){fig-alt=\"An image showing table 5 from the messy data examples, with century and year columns. The picture shows the transition to a similarly structured data set with a single column, year, which contains the century and the year pasted together as a single number.\"}\n\n##### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %>%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 × 5\n##   country      yyyy century year  rate             \n##   <chr>       <dbl>   <dbl> <chr> <chr>            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n```\n:::\n\n\n##### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000\n```\n:::\n\n:::\n:::\n\n### Regular Expressions\n\nMatching exact strings is easy - it's just like using find and replace.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears <- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n```\n:::\n\n\nBut, if you can master even a small amount of regular expression notation, you'll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you're creative, but often they're much simpler.\n\n::: {.callout-note collapse=\"true\"}\n#### Optional: Short Regular Expressions Primer\n\nYou may find it helpful to follow along with this section using this [web app](https://spannbaueradam.shinyapps.io/r_regex_tester/) built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found [here](https://regex101.com/). The subset of regular expression syntax we're going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but [you can find regular expressions to do just about anything string-related](https://stackoverflow.com/questions/tagged/regex?tab=Votes). As with any tool, there are situations where it's useful, and situations where you should not use a regular expression, no matter how much you want to.\n\nHere are the basics of regular expressions:\n\n-   `[]` enclose sets of characters\\\n    Ex: `[abc]` will match any single character `a`, `b`, `c`\n    -   `-` specifies a range of characters (`A-z` matches all upper and lower case letters)\n    -   to match `-` exactly, precede with a backslash (outside of `[]`) or put the `-` last (inside `[]`)\n-   `.` matches any character (except a newline)\n-   To match special characters, escape them using `\\` (in most languages) or `\\\\` (in R). So `\\.` or `\\\\.` will match a literal `.`, `\\$` or `\\\\$` will match a literal `$`.\n\n::: panel-tabset\n#### R {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn <- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n```\n:::\n\n\n#### Python {.unnumbered}\n\nIn python, a regular expression is indicated by putting the character 'r' right before the quoted expression. This tells python that any backslashes in the string should be left alone -- if R had that feature, we wouldn't have to escape all the backslashes!\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\n```\n:::\n\n:::\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n-   `*` means repeat between 0 and inf times\n-   `+` means 1 or more times\n-   `?` means 0 or 1 times -- most useful when you're looking for something optional\n-   `{a, b}` means repeat between `a` and `b` times, where `a` and `b` are integers. `b` can be blank. So `[abc]{3,}` will match `abc`, `aaaa`, `cbbaa`, but not `ab`, `bb`, or `a`. For a single number of repeated characters, you can use `{a}`. So `{3, }` means \"3 or more times\" and `{3}` means \"exactly 3 times\"\n\n::: panel-tabset\n#### R {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn <- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone <- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid <- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance <- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n```\n:::\n\n\n#### Python {.unnumbered}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## <re.Match object; span=(0, 6), match='banana'>\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## <re.Match object; span=(0, 2), match='ba'>\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## <re.Match object; span=(1, 5), match='anan'>\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## <re.Match object; span=(7, 19), match='123-456-7890'>\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## <re.Match object; span=(27, 35), match='12345678'>\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## <re.Match object; span=(77, 91), match='$50,000,000.23'>\n```\n:::\n\n:::\n\nThere are also ways to \"anchor\" a pattern to a part of the string (e.g. the beginning or the end)\n\n-   `^` has multiple meanings:\n    -   if it's the first character in a pattern, `^` matches the beginning of a string\n    -   if it follows `[`, e.g. `[^abc]`, `^` means \"not\" - for instance, \"the collection of all characters that aren't a, b, or c\".\n-   `$` means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n::: panel-tabset\n#### R {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\naddress <- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num <- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet <- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet <- str_remove(street, house_num) %>% str_trim() # remove house number\n\ncity <- str_extract(address, \",.*,\") %>% str_remove_all(\",\") %>% str_trim()\n\nzip <- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n```\n:::\n\n\n#### Python {.unnumbered}\n\nPython match objects contain 3 things: `.span()`, which has the start and end positions of the match, `.string`, which contains the original string passed into the function, and `.group()`, which contains the actual matching portion of the string.\n\n\n::: {.cell}\n\n```{.python .cell-code}\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n```\n:::\n\n:::\n\n-   `()` are used to capture information. So `([0-9]{4})` captures any 4-digit number\n-   `a|b` will select a or b.\n\nIf you've captured information using (), you can reference that information using backreferences. In most languages, those look like this: `\\1` for the first reference, `\\9` for the ninth. In R, backreferences are `\\\\1` through `\\\\9`.\n\n::: panel-tabset\n#### R {.unnumbered}\n\nIn R, the `\\` character is special, so you have to escape it. So in R, `\\\\1` is the first reference, and `\\\\2` is the second, and so on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nphone_num_variants <- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears <- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n```\n:::\n\n\n#### Python {.unnumbered}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n```\n:::\n\n:::\n\nPutting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n::: panel-tabset\n#### R {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstrings <- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex <- \"(walk|treat)\"\naddr_regex <- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex <- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 × 5\n##   text                                                   phone dog   addr  abab \n##   <chr>                                                  <lgl> <lgl> <lgl> <lgl>\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n```\n:::\n\n\n#### Python {.unnumbered}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]\n## \n## <string>:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## <string>:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## <string>:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## <string>:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n```\n:::\n\n:::\n:::\n\n## Pivot operations\n\nIt's fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren't necessarily the most friendly for analysis.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/static/png/original-dfs-tidy.png)\n\nThe two operations we'll learn here are wide -\\> long and long -\\> wide.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/tidyr-pivoting.gif)\n\nThis animation uses the R functions pivot_wider() and pivot_longer() [Animation source](https://github.com/kelseygonzalez/tidyexplain/tree/wider_longer), but the concept is the same in both R and python.\n\n### Longer\n\nIn many cases, the data come in what we might call \"wide\" form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n::: panel-tabset\n#### Picture the Operation\n\nTables 4a and 4b are good examples of data which is in \"wide\" form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n## * <chr>        <int>  <int>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n## * <chr>            <int>      <int>\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n```\n:::\n\n\nThe solution to this is to rearrange the data into \"long form\": to take the columns which contain values and \"stack\" them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn't being stacked (e.g. country, in both the example above and the image below).\n\n![A visual representation of what the pivot_longer operation looks like in practice.](images/data-transformations/tidyr_pivot_longer.png){fig-alt=\"A wide-to-long transformation operation, where the values of the id variables are repeated for each column which is used as a key; the values in each column are moved into a value column. There is a row of data in the transformed data frame for each combination of id variables and key variables.\"}\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we'll have tidy(er) data.\n\n#### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntba <- table4a %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb <- table4b %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %>%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n```\n:::\n\n\nThe columns are moved to a variable with the name passed to the argument \"names_to\" (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument \"values_to\" (again, hopefully easy to remember).\n\nWe identify ID variables (variables which we don't want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\n-   select only variables we want to pivot: `pivot_longer(table4a, cols =`1999`:`2000`, names_to = \"year\", values_to = \"cases\")`\n-   select variables we don't want to pivot, using `-` to remove them. (see above, where `-country` excludes country from the pivot operation)\n\nWhich option is easier depends how many things you're pivoting (and how the columns are structured).\n\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values \"cases\" and \"population\" respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ID columns\ntable4a.x <- table4a %>% mutate(id = \"cases\")\ntable4b.x <- table4b %>% mutate(id = \"population\")\n# Create one table\ntable4 <- bind_rows(table4a.x, table4b.x)\n\ntable4_long <- table4 %>%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %>%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    <chr>       <chr>      <chr>      <int>\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy <- table4_long %>%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   <chr>       <chr>  <int>      <int>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n```\n:::\n\n\n#### Python\n\nIn Pandas, `pandas.melt(...)` takes `id_vars`, `value_vars`, `var_name`, and `value_name`. Otherwise, it functions nearly exactly the same as `pivot_longer`; the biggest difference is that column selection works differently in python than it does in the tidyverse.\n\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n```\n:::\n\n\nHere's the melt/pivot_longer + pivot/pivot_wider version:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year   cases  population\n## 0   Afghanistan  1999     745    19987071\n## 1   Afghanistan  2000    2666    20595360\n## 2        Brazil  1999   37737   172006362\n## 3        Brazil  2000   80488   174504898\n## 4         China  1999  212258  1272915272\n## 5         China  2000  213766  1280428583\n```\n:::\n\n:::\n\n### Wider\n\nWhile it's very common to need to transform data into a longer format, it's not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n::: panel-tabset\n#### Picture the Operation\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be \"tidy\" - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n![A visual representation of what the pivot_wider operation looks like in practice.](images/data-transformations/tidyr_pivot_wider.png){fig-alt=\"An illustration of the transition from long data to wide data. In the long data frame, there are alternating rows of cases and populations, with corresponding counts. In the wide data frame, for each combination of id variables country and year, there are two columns: cases, and pop, each with corresponding values. That is, the key variables (cases, pop) in the long data frame become columns in the wide data frame.\"}\n\n#### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <int>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n```\n:::\n\n\n#### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country  year   cases  population\n## 0     Afghanistan  1999     745    19987071\n## 1     Afghanistan  2000    2666    20595360\n## 2          Brazil  1999   37737   172006362\n## 3          Brazil  2000   80488   174504898\n## 4           China  1999  212258  1272915272\n## 5           China  2000  213766  1280428583\n```\n:::\n\n:::\n\n::: callout-tip\n### Try it Out!\n\nIn the next section, we'll be using the WHO surveillance of disease incidence data ([link](https://immunizationdata.who.int/)). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form ([Excel link](http://www.who.int/entity/immunization/monitoring_surveillance/data/incidence_series.xls)). For demonstration purposes, I'll continue using the messier 2020 data, but the link is no longer available on the WHO's site.\n\nIt will require some preprocessing before it's suitable for a demonstration. I'll do some of it, but in this section, you're going to do the rest.\n\n::: panel-tabset\n#### Preprocessing\n\nYou don't have to understand what this code is doing just yet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\n\nsheets <- excel_sheets(\"data/incidence_series.xls\")\nsheets <- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence <- map_df(sheets, ~read_xls(path =\"data/incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 <- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 <- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"data/incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"data/who_disease_incidence.csv\")\n```\n:::\n\n\n#### Problem\n\nDownload the exported data [here](data/who_disease_incidence.csv) and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\n\nCan you make a line plot of cases of measles in Bangladesh over time?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(disease_incidence)\n## # A tibble: 6 × 43\n##   WHO_R…¹ ISO_c…² Cname Disease `2018` `2017` `2016` `2015` `2014` `2013` `2012`\n##   <chr>   <chr>   <chr> <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n## 1 EMR     AFG     Afgh… CRS         NA     NA     NA      0      0      0     NA\n## 2 EUR     ALB     Alba… CRS          0      0     NA     NA     NA      0      0\n## 3 AFR     DZA     Alge… CRS         NA     NA      0      0     NA     NA      0\n## 4 EUR     AND     Ando… CRS          0      0      0     NA     NA      0      0\n## 5 AFR     AGO     Ango… CRS         NA     NA     NA     NA     NA     NA     NA\n## 6 AMR     ATG     Anti… CRS          0      0      0      0      0      0      0\n## # … with 32 more variables: `2011` <dbl>, `2010` <dbl>, `2009` <dbl>,\n## #   `2008` <dbl>, `2007` <dbl>, `2006` <dbl>, `2005` <dbl>, `2004` <dbl>,\n## #   `2003` <dbl>, `2002` <dbl>, `2001` <dbl>, `2000` <dbl>, `1999` <dbl>,\n## #   `1998` <dbl>, `1997` <dbl>, `1996` <dbl>, `1995` <dbl>, `1994` <dbl>,\n## #   `1993` <dbl>, `1992` <dbl>, `1991` <dbl>, `1990` <dbl>, `1989` <dbl>,\n## #   `1988` <dbl>, `1987` <dbl>, `1986` <dbl>, `1985` <dbl>, `1984` <dbl>,\n## #   `1983` <dbl>, `1982` <dbl>, `1981` <dbl>, `1980` <dbl>, and abbreviated …\n## # ℹ Use `colnames()` to see all variable names\n```\n:::\n\n\n#### R solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nwho_disease <- read_csv(\"data/who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long <- who_disease %>%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %>%\n  rename(Country = Cname) %>%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year),\n         cases = as.numeric(cases))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %>%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n```\n\n::: {.cell-output-display}\n![](data-transformations_files/figure-html/tryitout-surveillance-cleaning-1.png){width=2100}\n:::\n:::\n\n\n#### Python solution\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"data/who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()\n```\n:::\n\n:::\n:::\n\n## Merging Tables\n\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized **relationally** - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\n::: callout-caution\n### Relational Data Example: Primary School Records\n\nEach individual has certain characteristics:\n\n-   full_name\n-   gender\n-   birth date\n-   ID number\n\nEach student has specific characteristics:\n\n-   ID number\n-   parent name\n-   parent phone number\n-   medical information\n-   Class ID\n\nTeachers may also have additional information:\n\n-   ID number\n-   Class ID\n-   employment start date\n-   education level\n-   compensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\n-   ID number\n-   Student ID\n-   Class ID\n-   year\n-   term number\n-   subject\n-   grade\n-   comment\n\nAnd for teachers, there are employment records on a yearly basis\n\n-   ID number\n-   Employee ID\n-   year\n-   rating\n-   comment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\n-   location ID\n-   class ID\n-   meeting time\n-   grade level\n\nEach location might also have some logistical information attached:\n\n-   location ID\n-   room number\n-   building\n-   number of seats\n-   AV equipment\n\n![Primary School Database Schema](images/data-transformations/PrimarySchoolExample.png) <!-- <a href=\"https://dbdiagram.io/embed/5ef387179ea313663b3b048e\">Link to diagram of the database</a> -->\n\nWe could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student's individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\n\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\n-   get the Class ID\n-   get any teachers that are assigned that Class ID - specifically, get their ID number\n-   get any students that are assigned that Class ID - specifically, get their ID number\n-   append the results from teachers and students so that there is a list of all individuals in the class\n-   look through the \"individual data\" table to find any individuals with matching ID numbers, and keep those individuals' birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren't well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\n:::\n\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\n\n**keys** are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A **primary key** identifies an observation in its own table. A **foreign key** identifies an observation in another table.\n\nThere are 3 main types of table joins:\n\n-   Mutating joins, which add columns from one table to matching rows in another table\\\n    Ex: adding birthday to the table of all individuals in a class\n\n-   Filtering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don't change)\\\n    Ex: finding all teachers or students who have class ClassID\n\n-   Set operations, which treat observations as set elements (e.g. union, intersection, etc.)\\\n    Ex: taking the union of all student and teacher IDs to get a list of individual IDs\n\n### Animating Joins\n\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\n\nIf we start with two tables, x and y,\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/static/png/original-dfs.png)\n\n#### Mutating Joins\n\nWe're primarily going to focus on mutating joins, as filtering joins can be accomplished by ... filtering ... rather than by table joins.\n\n::: panel-tabset\n##### Inner Join\n\nWe can do a filtering `inner_join` to keep only rows which are in both tables (but we keep all columns)\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/inner-join.gif)\n\n##### Left Join\n\nBut what if we want to keep all of the rows in x? We would do a `left_join`\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/left-join.gif)\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/left-join-extra.gif)\n\n##### Right Join\n\nIf we wanted to keep all of the rows in y, we would do a `right_join`:\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/right-join.gif)\n\n(or, we could do a left join with y and x, but... either way is fine).\n\n##### Full Join\n\nAnd finally, if we want to keep all of the rows, we'd do a `full_join`:\n\n![](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/full-join.gif)\n\nYou can find other animations corresponding to filtering joins and set operations [here](https://raw.githubusercontent.com/gadenbuie/tidyexplain/master/images/full-join.gif)\n:::\n\nEvery join has a \"left side\" and a \"right side\" - so in `some_join(A, B)`, A is the left side, B is the right side.\n\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n+-------+-----------+------------+----------+\n|       | Left Side | Right Side |          |\n+-------+-----------+------------+----------+\n|       | Join Type | Rows       | Cols     |\n+-------+-----------+------------+----------+\n| inner | matching  | all        | matching |\n+-------+-----------+------------+----------+\n| left  | all       | all        | matching |\n+-------+-----------+------------+----------+\n| right | matching  | all        | all      |\n+-------+-----------+------------+----------+\n| outer | all       | all        | all      |\n+-------+-----------+------------+----------+\n\n::: callout-caution\n##### Demonstration: Mutating Joins\n\n::: panel-tabset\n###### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 <- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n```\n:::\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_join(t1, t2)\n## # A tibble: 2 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n```\n:::\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don't match get filled in with NAs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nleft_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n```\n:::\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\n\n::: {.cell}\n\n```{.r .cell-code}\nright_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n```\n:::\n\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it's known as a `full_join`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_join(t1, t2)\n## # A tibble: 4 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n```\n:::\n\n\n###### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n```\n:::\n\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n```\n:::\n\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don't match get filled in with NAs.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n```\n:::\n\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n```\n:::\n\n\nAn outer join keeps everything - all rows, all columns.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\n## 3  C  NaN  4.0\n```\n:::\n\n:::\n:::\n\nI've included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins [here](https://r4ds.had.co.nz/relational-data.html#filtering-joins) [@r4ds].\n\n#### Filtering Joins\n\n::: panel-tabset\n##### Semi Join\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/semi-join.gif)\n\n##### Anti Join\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/anti-join.gif)\n:::\n\n#### Set Operations\n\n::: panel-tabset\n##### Union\n\nAll unique rows from x and y\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/union.gif)\n\nOr, all unique rows from y and x.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/union-rev.gif)\n\n##### Union All\n\nAll rows from x and y, keeping duplicate rows.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/union-all.gif)\n\n##### Intersection\n\nCommon rows in x and y, keeping only unique rows.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/intersect.gif)\n\n##### Set Difference\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/setdiff.gif)\n\n![](https://github.com/gadenbuie/tidyexplain/raw/main/images/setdiff-rev.gif)\n:::\n\n### Example: NYC Flights\n\nWe'll use the `nycflights13` package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you'll recall, I had to use a small sample the last time we played with this data...). So before we can work with this data, we have to load the tables into Python.\n\n::: panel-tabset\n#### Loading Data: R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"data/\")\n## <SQLiteConnection>\n##   Path: /home/susan/Projects/Class/unl-stat850/stat850-textbook/data/nycflights13.sqlite\n##   Extensions: TRUE\n```\n:::\n\n\n#### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sqlite3\ncon = sqlite3.connect(\"data/nycflights13.sqlite\")\ncur = con.cursor()\n```\n:::\n\n:::\n\n[I am not going to cover SQLITE commands here - I'm just going to use the bare minimum, but you can find a very nice [introduction to python and SQLITE at datacarpentry](https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html) [@thecarpentriesAccessingSQLiteDatabases2022], and an [introduction to the dbplyr package](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html) for a nice R-SQLITE interface.]{.aside}\n\n::: callout-tip\n#### Try it out: Understanding Relational Data\n\n::: panel-tabset\n##### Problem\n\nSketch a diagram of which fields in each table match fields in other tables. Use the [data documentation](https://nycflights13.tidyverse.org/reference/index.html) to help you with your sketch.\n\n##### Solution\n\n![The nycflights database schema](https://d33wubrfki0l68.cloudfront.net/245292d1ea724f6c3fd8a92063dcd7bfb9758d02/5751b/diagrams/relational-nycflights.png) [here](https://r4ds.had.co.nz/relational-data.html#nycflights13-relational) (scroll down a bit).\n:::\n:::\n\n::: callout-caution\n#### Mutating Joins\n\nThese functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let's determine whether there's a relationship between the age of a plane and its delays.\n\n::: panel-tabset\n##### R\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age <- planes %>%\n  mutate(age = 2013 - year) %>% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane <- flights %>%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres <- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n```\n\n::: {.cell-output-display}\n![](data-transformations_files/figure-html/flights-delay-age-R-1.png){width=45%}\n:::\n\n::: {.cell-output-display}\n![](data-transformations_files/figure-html/flights-delay-age-R-2.png){width=45%}\n:::\n:::\n\n\nIt doesn't look like there's much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren't enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n##### Python\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## <ggplot: (8736452972029)>\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:324: PlotnineWarning: stat_boxplot : Removed 9374 rows containing non-finite values.\n```\n\n::: {.cell-output-display}\n![](data-transformations_files/figure-html/flights-delay-age-py-1.png){width=45%}\n:::\n\n```{.python .cell-code}\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## <ggplot: (8736452706792)>\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:324: PlotnineWarning: stat_boxplot : Removed 10317 rows containing non-finite values.\n```\n\n::: {.cell-output-display}\n![](data-transformations_files/figure-html/flights-delay-age-py-2.png){width=45%}\n:::\n:::\n\n:::\n:::\n\n## Example: Gas Prices Data\n\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to [this site](https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w) to see a nice graph of gas prices, along with a corresponding table. <!-- (or you can look at the screenshot below, as I don't really trust that the site design will stay the same...) -->\n\n![Gas prices at US EIA site](images/data-transformations/06_gas_prices_screenshot.png)\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\n\nBut looking at the chart at the top of the page, it's not clear how we might get that chart from the data in the format it's presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n::: callout-tip\n### Try it out: Manual Formatting in Excel\n\n::: panel-tabset\n#### Problem\n\nAn excel spreadsheet of the data as downloaded in July 2022 is available [here](data/gas_prices.xlsx). Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\n\nWhat steps are involved?\n\n#### Solution\n\n1.  Copy the year-month column, creating one vertical copy for every set of columns\n\n2.  Move each block of two columns down to the corresponding vertical copy\n\n3.  Delete empty rows\n\n4.  Format dates\n\n5.  Delete empty columns\n\n#### Video\n\n::: {#fig-excel-demo-video}\n<iframe width=\"736\" height=\"414\" src=\"https://www.youtube.com/embed/n70eAKJmzRo\" title=\"06 Tidying Gas Price Data (in Excel)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen>\n\n</iframe>\n\nHere is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n:::\n:::\n:::\n\n### Setup: Gas Price Data Cleaning\n\nFor the next two examples, we'll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl <- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc <- read_html(url)\ngas_prices_html <- html_table(htmldoc, fill = T, trim = T)[[5]]\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\nTable: First 6 rows of gas prices data as read into R\n\n|Year-Month |Week 1   |Week 1 |Week 2   |Week 2 |Week 3   |Week 3 |Week 4   |Week 4 |Week 5   |Week 5 |   |   |\n|:----------|:--------|:------|:--------|:------|:--------|:------|:--------|:------|:--------|:------|:--|:--|\n|Year-Month |End Date |Value  |End Date |Value  |End Date |Value  |End Date |Value  |End Date |Value  |NA |NA |\n|1994-Nov   |         |       |         |       |         |       |11/28    |1.175  |         |       |NA |NA |\n|1994-Dec   |12/05    |1.143  |12/12    |1.118  |12/19    |1.099  |12/26    |1.088  |         |       |NA |NA |\n|           |         |       |         |       |         |       |         |       |         |       |NA |NA |\n|1995-Jan   |01/02    |1.104  |01/09    |1.111  |01/16    |1.102  |01/23    |1.110  |01/30    |1.109  |NA |NA |\n|1995-Feb   |02/06    |1.103  |02/13    |1.099  |02/20    |1.093  |02/27    |1.101  |         |       |NA |NA |\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n```\n:::\n\n    ('Year-Month', 'Year-Month')    ('Week 1', 'End Date')      ('Week 1', 'Value')  ('Week 2', 'End Date')      ('Week 2', 'Value')  ('Week 3', 'End Date')      ('Week 3', 'Value')  ('Week 4', 'End Date')      ('Week 4', 'Value')  ('Week 5', 'End Date')      ('Week 5', 'Value')    ('Unnamed: 11_level_0', 'Unnamed: 11_level_1')    ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')\n--  ------------------------------  ------------------------  ---------------------  ------------------------  ---------------------  ------------------------  ---------------------  ------------------------  ---------------------  ------------------------  ---------------------  ------------------------------------------------  ------------------------------------------------\n 0  1994-Nov                        nan                                     nan      nan                                     nan      nan                                     nan      11/28                                     1.175  nan                                     nan                                                   nan                                               nan\n 1  1994-Dec                        12/05                                     1.143  12/12                                     1.118  12/19                                     1.099  12/26                                     1.088  nan                                     nan                                                   nan                                               nan\n 2  nan                             nan                                     nan      nan                                     nan      nan                                     nan      nan                                     nan      nan                                     nan                                                   nan                                               nan\n 3  1995-Jan                        01/02                                     1.104  01/09                                     1.111  01/16                                     1.102  01/23                                     1.11   01/30                                     1.109                                               nan                                               nan\n 4  1995-Feb                        02/06                                     1.103  02/13                                     1.099  02/20                                     1.093  02/27                                     1.101  nan                                     nan                                                   nan                                               nan\n\n\n::: callout-tip\n### Try it out: Formatting with Pivot Operations\n\n::: panel-tabset\n#### Problem\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\n\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\n\nHow do your steps compare to the steps you used for the manual approach?\n\n#### Sketch\n\n![Steps to work through the gas prices data cleaning process](images/data-transformations/gas-prices-steps.png){fig-alt=\"Step 1: set row names to be more descriptive and remove header row. Step 2: Remove empty columns and pivot to long form, with dates and values in the same column and a description column that indicates what type of data is in the value column. Step 3: separate the week and variable information into different columns, discarding the week label. Step 4: pivot wider, so that date and value information are each in a single column. Step 5: remove rows with no values and create a yyyy-mm-dd format date. Step 6: Convert date and value into appropriate types (date, numeric).\"}\n\n#### R solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names <- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %>%\n    # trim leading/trailing spaces\n    str_trim() %>%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 13\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   <chr>      <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 4 more variables: Week.5.Date <chr>, Week.5.Value <chr>, X <lgl>,\n## #   Date <lgl>, and abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date,\n## #   ³​Week.2.Value, ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n## # ℹ Use `colnames()` to see all variable names\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_raw <- select(gas_prices_raw, -c(X, Date))\ngas_prices_long <- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 × 3\n##   Year.Month variable     value\n##   <chr>      <chr>        <chr>\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_sep <- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %>%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 × 4\n##   Year.Month week  variable value\n##   <chr>      <chr> <chr>    <chr>\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_wide <- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 × 4\n##   Year.Month week  Date    Value  \n##   <chr>      <chr> <chr>   <chr>  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_date <- gas_prices_wide %>%\n  filter(nchar(Value) > 0) %>%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %>%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %>%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 × 2\n##   Date       Value\n##   <chr>      <chr>\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate)\ngas_prices <- gas_prices_date %>%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %>%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       Price.per.gallon\n##   <date>                <dbl>\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n```\n:::\n\n\n#### Python solution\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## MultiIndex([(         'Year-Month',          'Year-Month'),\n##             (             'Week 1',            'End Date'),\n##             (             'Week 1',               'Value'),\n##             (             'Week 2',            'End Date'),\n##             (             'Week 2',               'Value'),\n##             (             'Week 3',            'End Date'),\n##             (             'Week 3',               'Value'),\n##             (             'Week 4',            'End Date'),\n##             (             'Week 4',               'Value'),\n##             (             'Week 5',            'End Date'),\n##             (             'Week 5',               'Value'),\n##             ('Unnamed: 11_level_0', 'Unnamed: 11_level_1'),\n##             ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')],\n##            )\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date', 'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date', 'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\ngas_prices_raw.head()\n##   Year-Month Week.1.Date  ...  Unnamed:.11_level_0 Unnamed:.12_level_0\n## 0   1994-Nov         NaN  ...                  NaN                 NaN\n## 1   1994-Dec       12/05  ...                  NaN                 NaN\n## 3   1995-Jan       01/02  ...                  NaN                 NaN\n## 4   1995-Feb       02/06  ...                  NaN                 NaN\n## 5   1995-Mar       03/06  ...                  NaN                 NaN\n## \n## [5 rows x 13 columns]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\ngas_prices_long.head()\n##   Year-Month     variable  value\n## 0   1994-Nov  Week.1.Date    NaN\n## 1   1994-Dec  Week.1.Date  12/05\n## 2   1995-Jan  Week.1.Date  01/02\n## 3   1995-Feb  Week.1.Date  02/06\n## 4   1995-Mar  Week.1.Date  03/06\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_sep = gas_prices_long\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\ngas_prices_sep.head()\n##   Year-Month variable  value week\n## 0   1994-Nov     Date    NaN    1\n## 1   1994-Dec     Date  12/05    1\n## 2   1995-Jan     Date  01/02    1\n## 3   1995-Feb     Date  02/06    1\n## 4   1995-Mar     Date  03/06    1\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\ngas_prices_wide.head()\n## variable          Date  Value\n## Year-Month week              \n## 1994-Dec   1     12/05  1.143\n##            2     12/12  1.118\n##            3     12/19  1.099\n##            4     12/26  1.088\n##            5       NaN    NaN\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n\ngas_prices_date.head()\n## variable Year-Month week       Date  Value  Year Month\n## 0          1994-Dec    1 1994-12-05  1.143  1994   Dec\n## 1          1994-Dec    2 1994-12-12  1.118  1994   Dec\n## 2          1994-Dec    3 1994-12-19  1.099  1994   Dec\n## 3          1994-Dec    4 1994-12-26  1.088  1994   Dec\n## 4          1994-Nov    4 1994-11-28  1.175  1994   Nov\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\ngas_prices['Price_per_gallon'] = gas_prices.Value\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\ngas_prices.head()\n## variable       Date Price_per_gallon\n## 0        1994-12-05            1.143\n## 1        1994-12-12            1.118\n## 2        1994-12-19            1.099\n## 3        1994-12-26            1.088\n## 4        1994-11-28            1.175\n```\n:::\n\n:::\n:::\n\n::: callout-tip\n### Try it out: Formatting using merge + pivot\n\n::: panel-tabset\n#### Problem\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\n\nYou can start with the `gas_prices_raw`\n\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\n\nHow do your steps compare to the steps you used for the manual approach?\n\n#### Sketch\n\n![](images/data-transformations/gas-prices-steps2.png) \\#### R solution\n\nWe'll use the same data cleaning function as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 13\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   <chr>      <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 4 more variables: Week.5.Date <chr>, Week.5.Value <chr>, X <lgl>,\n## #   Date <lgl>, and abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date,\n## #   ³​Week.2.Value, ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n## # ℹ Use `colnames()` to see all variable names\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_dates <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\ngas_prices_values <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n\nhead(gas_prices_dates)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Date Week.2.Date Week.3.Date Week.4.Date Week.5.Date\n##   <chr>      <chr>       <chr>       <chr>       <chr>       <chr>      \n## 1 1994-Nov   \"\"          \"\"          \"\"          11/28       \"\"         \n## 2 1994-Dec   \"12/05\"     \"12/12\"     \"12/19\"     12/26       \"\"         \n## 3 1995-Jan   \"01/02\"     \"01/09\"     \"01/16\"     01/23       \"01/30\"    \n## 4 1995-Feb   \"02/06\"     \"02/13\"     \"02/20\"     02/27       \"\"         \n## 5 1995-Mar   \"03/06\"     \"03/13\"     \"03/20\"     03/27       \"\"         \n## 6 1995-Apr   \"04/03\"     \"04/10\"     \"04/17\"     04/24       \"\"\nhead(gas_prices_values)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Value Week.2.Value Week.3.Value Week.4.Value Week.5.Value\n##   <chr>      <chr>        <chr>        <chr>        <chr>        <chr>       \n## 1 1994-Nov   \"\"           \"\"           \"\"           1.175        \"\"          \n## 2 1994-Dec   \"1.143\"      \"1.118\"      \"1.099\"      1.088        \"\"          \n## 3 1995-Jan   \"1.104\"      \"1.111\"      \"1.102\"      1.110        \"1.109\"     \n## 4 1995-Feb   \"1.103\"      \"1.099\"      \"1.093\"      1.101        \"\"          \n## 5 1995-Mar   \"1.103\"      \"1.096\"      \"1.095\"      1.102        \"\"          \n## 6 1995-Apr   \"1.116\"      \"1.134\"      \"1.149\"      1.173        \"\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices_dates_long <- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\ngas_prices_values_long <- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n\nhead(gas_prices_dates_long)\n## # A tibble: 6 × 3\n##   Year.Month week        month_day\n##   <chr>      <chr>       <chr>    \n## 1 1994-Nov   Week.1.Date \"\"       \n## 2 1994-Nov   Week.2.Date \"\"       \n## 3 1994-Nov   Week.3.Date \"\"       \n## 4 1994-Nov   Week.4.Date \"11/28\"  \n## 5 1994-Nov   Week.5.Date \"\"       \n## 6 1994-Dec   Week.1.Date \"12/05\"\nhead(gas_prices_values_long)\n## # A tibble: 6 × 3\n##   Year.Month week         price_per_gallon\n##   <chr>      <chr>        <chr>           \n## 1 1994-Nov   Week.1.Value \"\"              \n## 2 1994-Nov   Week.2.Value \"\"              \n## 3 1994-Nov   Week.3.Value \"\"              \n## 4 1994-Nov   Week.4.Value \"1.175\"         \n## 5 1994-Nov   Week.5.Value \"\"              \n## 6 1994-Dec   Week.1.Value \"1.143\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean <- gas_prices_dates_long %>%\n  filter(month_day != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %>% \n           ymd())\n\ngas_prices_values_long_clean <- gas_prices_values_long %>%\n  filter(price_per_gallon != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n\nhead(gas_prices_dates_long_clean)\n## # A tibble: 6 × 5\n##   Year.Month  week month_day year  Date      \n##   <chr>      <dbl> <chr>     <chr> <date>    \n## 1 1994-Nov       4 11/28     1994  1994-11-28\n## 2 1994-Dec       1 12/05     1994  1994-12-05\n## 3 1994-Dec       2 12/12     1994  1994-12-12\n## 4 1994-Dec       3 12/19     1994  1994-12-19\n## 5 1994-Dec       4 12/26     1994  1994-12-26\n## 6 1995-Jan       1 01/02     1995  1995-01-02\nhead(gas_prices_values_long_clean)\n## # A tibble: 6 × 3\n##   Year.Month  week price_per_gallon\n##   <chr>      <dbl>            <dbl>\n## 1 1994-Nov       4             1.18\n## 2 1994-Dec       1             1.14\n## 3 1994-Dec       2             1.12\n## 4 1994-Dec       3             1.10\n## 5 1994-Dec       4             1.09\n## 6 1995-Jan       1             1.10\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngas_prices <- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %>%\n  select(Date, price_per_gallon)\nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       price_per_gallon\n##   <date>                <dbl>\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n```\n:::\n\n\n#### Python solution\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_raw = gas_prices_html\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## Index(['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date',\n##        'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date',\n##        'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0',\n##        'Unnamed:.12_level_0'],\n##       dtype='object')\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date.Date', 'Week.1.Value.Value', 'Week.2.Date.Date', 'Week.2.Value.Value', 'Week.3.Date.Date', 'Week.3.Value.Value', 'Week.4.Date.Date', 'Week.4.Value.Value', 'Week.5.Date.Date', 'Week.5.Value.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\ngas_prices_raw.head()\n##   Year-Month Week.1.Date.Date  ...  Unnamed:.11_level_0 Unnamed:.12_level_0\n## 0   1994-Nov              NaN  ...                  NaN                 NaN\n## 1   1994-Dec            12/05  ...                  NaN                 NaN\n## 3   1995-Jan            01/02  ...                  NaN                 NaN\n## 4   1995-Feb            02/06  ...                  NaN                 NaN\n## 5   1995-Mar            03/06  ...                  NaN                 NaN\n## \n## [5 rows x 13 columns]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n\ngas_prices_dates.head()\n##   Year-Month Week.1.Date.Date  ... Week.4.Date.Date Week.5.Date.Date\n## 0   1994-Nov              NaN  ...            11/28              NaN\n## 1   1994-Dec            12/05  ...            12/26              NaN\n## 3   1995-Jan            01/02  ...            01/23            01/30\n## 4   1995-Feb            02/06  ...            02/27              NaN\n## 5   1995-Mar            03/06  ...            03/27              NaN\n## \n## [5 rows x 6 columns]\ngas_prices_values.head()\n##   Year-Month  Week.1.Value.Value  ...  Week.4.Value.Value  Week.5.Value.Value\n## 0   1994-Nov                 NaN  ...               1.175                 NaN\n## 1   1994-Dec               1.143  ...               1.088                 NaN\n## 3   1995-Jan               1.104  ...               1.110               1.109\n## 4   1995-Feb               1.103  ...               1.101                 NaN\n## 5   1995-Mar               1.103  ...               1.102                 NaN\n## \n## [5 rows x 6 columns]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n\ngas_prices_dates_long.head()\n##   Year-Month              week month_day\n## 0   1994-Nov  Week.1.Date.Date       NaN\n## 1   1994-Dec  Week.1.Date.Date     12/05\n## 2   1995-Jan  Week.1.Date.Date     01/02\n## 3   1995-Feb  Week.1.Date.Date     02/06\n## 4   1995-Mar  Week.1.Date.Date     03/06\ngas_prices_values_long.head()\n##   Year-Month                week  price_per_gallon\n## 0   1994-Nov  Week.1.Value.Value               NaN\n## 1   1994-Dec  Week.1.Value.Value             1.143\n## 2   1995-Jan  Week.1.Value.Value             1.104\n## 3   1995-Feb  Week.1.Value.Value             1.103\n## 4   1995-Mar  Week.1.Value.Value             1.103\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna(subset = \"month_day\")\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## <string>:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## <string>:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## <string>:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n\n## <string>:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_values_long_clean = gas_prices_values_long.dropna(subset = \"price_per_gallon\")\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## <string>:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n\ngas_prices_dates_long_clean.head()\n##   Year-Month week month_day  year       Date\n## 1   1994-Dec    1     12/05  1994 1994-12-05\n## 2   1995-Jan    1     01/02  1995 1995-01-02\n## 3   1995-Feb    1     02/06  1995 1995-02-06\n## 4   1995-Mar    1     03/06  1995 1995-03-06\n## 5   1995-Apr    1     04/03  1995 1995-04-03\ngas_prices_values_long_clean.head()\n##   Year-Month week  price_per_gallon\n## 1   1994-Dec    1             1.143\n## 2   1995-Jan    1             1.104\n## 3   1995-Feb    1             1.103\n## 4   1995-Mar    1             1.103\n## 5   1995-Apr    1             1.116\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\ngas_prices.head()\n##         Date  price_per_gallon\n## 0 1994-12-05             1.143\n## 1 1995-01-02             1.104\n## 2 1995-02-06             1.103\n## 3 1995-03-06             1.103\n## 4 1995-04-03             1.116\n```\n:::\n\n:::\n:::\n\n::: callout-note\n## Other resources\n\n@doughertyCleanMessyData2021 - very nice task-oriented chapter that's below the level addressed in this course but still useful\n:::\n\n## References\n",
    "supporting": [
      "data-transformations_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}