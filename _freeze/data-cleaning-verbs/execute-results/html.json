{
  "hash": "0613842c2353e80f82b870424151c916",
  "result": {
    "markdown": "# Data Cleaning and Manipulation {#manipulating-data}\n\n## Module Objectives  {- #module9-objectives}\n\n- Apply data manipulation verbs (filter, select, group by, summarize, mutate) to prepare data for analysis\n- Identify required sequence of steps for data cleaning\n- Describe step-by-step data cleaning process in lay terms appropriately and understand the consequences of data cleaning steps\n- Create summaries of data appropriate for analysis or display using data manipulation techniques\n\n## Introduction\n\nIn this section, we're going start learning how to work with data. Generally speaking, data doesn't come in a form suitable for analysis^[See [this twitter thread](https://twitter.com/JennyBryan/status/722954354198597632) for some horror stories. [This tweet](https://twitter.com/jengolbeck/status/1153064308483510272?s=20) is also pretty good at showing one type of messiness.] - you have to clean it up, create the variables you care about, get rid of those you don't care about, and so on. \n\nSome people call the process of cleaning and organizing your data \"data wrangling\", which is a fantastic way to think about chasing down all of the issues in the data. \n\n![Data wrangling (by Allison Horst)](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/data_cowboy.png)\n\nIn R, we'll be using the `tidyverse` for this. It's a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy^[The philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.] and interface (basically, the commands will use predictable argument names and structure). You've already been introduced to parts of the tidyverse - specifically, `readr` and `ggplot2`. \n\n`dplyr` (one of the packages in the tidyverse) creates a \"grammar of data manipulation\" to make it easier to describe different operations. I find the `dplyr` grammar to be extremely useful when talking about data operations, so I'm going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework). \n\nEach `dplyr` verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, **data comes first** -- literally, as it's the first argument to any function. In addition, you don't use df$variable to access a variable - you refer to the variable by its name alone (\"bare\" names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy. \n\n<!-- In SAS, there is no tidyverse, but there is a relatively consistent structure for how to accomplish each task. Most data cleaning in SAS is accomplished in data steps. In the interests of not confusing terms too much between languages, I'm going to use the tidyverse \"verbs\" to describe operations in both SAS and R. This will help all of us to focus on the essentials of the operations, instead of the specific language-based syntax. -->\n\nIn Python, most data manipulation tasks are handled using `pandas`[@pandasIndexingSelectingData2022]. In the interests of using a single consistent \"language\" for describing data manipulation tasks, I'll use the tidyverse \"verbs\" to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\n\nThere is also the `datar` python package[@pwwangDatarGrammarData2022], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, `datar` may be more useful if you prefer the `dplyr` way of handling things using a data-first API. \n\n::: aside\nI haven't had the chance to add the `datar` package to this book, but it looks promising and may be worth your time to figure out. It's a bit too new for me to take the time to add it into this book - I want packages that will be maintained long-term if I'm going to teach them to others.\n:::\n\n::: {.callout-note}\n\n[There is an excellent dplyr cheatsheet available from RStudio](https://github.com/rstudio/cheatsheets/blob/master/data-transformation.pdf). You may want to print it out to have a copy to reference as you work through this chapter.\n\n[Here is a data wrangling with pandas cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) that is formatted similarly to the dplyr cheat sheet.\n\n:::\n\n## Tidy Data\n\nThere are infinitely many ways to configure \"messy\" data, but data that is \"tidy\" has 3 attributes:\n\n1. Each variable has its own column\n2. Each observation has its own row\n3. Each value has its own cell\n\nThese attributes aren't sufficient to define \"clean\" data, but they work to define \"tidy\" data (in the same way that you can have a \"tidy\" room because all of your clothes are folded, but they aren't clean just because they're folded; you could have folded a pile of dirty clothes). \n\nWe'll get more into how to work with different \"messy\" data configurations in the next module, but it's worth keeping rules 1 and 3 in mind while working through this module. \n\n## Filter: Pick cases (rows) based on their values\n\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we're interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from `filter()`. Note that unless we store the result from `filter()` in the original object, we don't change the original. \n\n![dplyr filter() by Allison Horst](https://github.com/allisonhorst/stats-illustrations/raw/main/rstats-artwork/dplyr_filter.jpg){fig-alt=\"Cartoon showing three fuzzy monsters either selecting or crossing out rows of a data table. If the type of animal in the table is “otter” and the site is “bay”, a monster is drawing a purple rectangle around the row. If those conditions are not met, another monster is putting a line through the column indicating it will be excluded. Stylized text reads “dplyr::filter() - keep rows that satisfy your conditions.”\"}\n\n\n::: {.callout-caution}\n\n\nLet's explore how it works, using the `starwars` dataset, which contains a comprehensive list of the characters in the Star Wars movies. \n\nIn the interests of demonstrating the process on the same data, I've exported the starwars data to a CSV file using the `readr` package. I had to remove the list-columns (films, vehicles, starships) because that format isn't supported by CSV files. You can access the csv data [here](data/starwars.csv). \n\n\n\n\n\n::: panel-tabset\n### R {-}\n\nThis data set is included in the `dplyr` package, so we load that package and then use the `data()` function to load dataset into memory. The loading isn't complete until we actually use the dataset though... so let's print the first few rows. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n##  3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n##  4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n##  9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n## # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n```\n:::\n\n\n### Python {-}\n\nWe have to use the exported CSV data in python.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nstarwars = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padmé Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\nfrom skimpy import skim\nskim(starwars)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 87     │ │ object      │ 8     │                       │\n## │ │ Number of columns │ 11     │ │ float64     │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━┳━━━━┳━━━━━┳━━━━━┳━━━━━━┳━━━━━━━━┓  │\n## │ ┃        ┃ missing ┃ comple ┃ mean ┃ sd  ┃ p0 ┃ p25 ┃ p75 ┃ p100 ┃ hist   ┃  │\n## │ ┃        ┃         ┃ te     ┃      ┃     ┃    ┃     ┃     ┃      ┃        ┃  │\n## │ ┃        ┃         ┃ rate   ┃      ┃     ┃    ┃     ┃     ┃      ┃        ┃  │\n## │ ┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━╇━━━━╇━━━━━╇━━━━━╇━━━━━━╇━━━━━━━━┩  │\n## │ │ height │       6 │   0.93 │  170 │  35 │ 66 │ 170 │ 190 │  260 │ ▁ ▁█▂  │  │\n## │ │ mass   │      28 │   0.68 │   97 │ 170 │ 15 │  56 │  84 │ 1400 │   █    │  │\n## │ │ birth_ │      44 │   0.49 │   88 │ 150 │  8 │  35 │  72 │  900 │   █    │  │\n## │ │ year   │         │        │      │     │    │     │     │      │        │  │\n## │ └────────┴─────────┴────────┴──────┴─────┴────┴─────┴─────┴──────┴────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n```\n:::\n\n:::\n\n\nOnce the data is set up, filtering the data (selecting certain **rows**) is actually very simple. Of course, we've talked about how to use logical indexing before in @sec-indexing and @sec-data-programming, but here we'll focus on using specific functions to perform the same operation. \n\n::: panel-tabset\n\n### R: `dplyr`\n\nThe dplyr verb for selecting rows is `filter`. `filter` takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n\n# Get only the people who come from Tattoine\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cordé   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dormé   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padmé Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n```\n:::\n\n\n### Base R\n\nIn base R, you would perform a filtering operation using `subset`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n```\n:::\n\n\nNotice that with `subset`, you have to use `&` to join two logical statements; it does not by default take multiple successive arguments. \n\n:::\n:::\n\n\n### Common Row Selection Tasks\n\nIn `dplyr`, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I'll do my best to show you easy ways to handle each task in each language.\n\n#### Filtering by row number\n\n::: panel-tabset\n\n##### R: `dplyr` {-}\n\n`row_number()` is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke <- read_csv(\"data/pokemon_ascii.csv\")\nfilter(poke, (row_number() %% 2 == 0)) \n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          2 Ivys… Bisakn…       1 Normal Seed P…       2 Grass  Poison     1  \n##  2          3 Mega… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2.4\n##  3          5 Char… Glutexo       1 Normal Flame …       1 Fire   .          1.1\n##  4          6 Mega… Glurak        1 Normal Flame …       2 Fire   Dragon     1.7\n##  5          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  6          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  7         10 Cate… Raupy         1 Normal Worm P…       1 Bug    .          0.3\n##  8         12 Butt… Smettbo       1 Normal Butter…       2 Bug    Flying     1.1\n##  9         14 Kaku… Kokuna        1 Normal Cocoon…       2 Bug    Poison     0.6\n## 10         15 Mega… Bibor         1 Normal Poison…       2 Bug    Poison     1.4\n## # … with 504 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n```\n:::\n\n\n##### Python {-}\n\nIn python, the easiest way to accomplish filtering by row number is by using `.iloc`. But, up until now, we've only talked about how Python creates slices using `start:(end+1)` notation. There is an additional option with slicing - `start:(end+1):by`. So if we want to get only even rows, we can use the index `[::2]`, which will give us row 0, 2, 4, 6, ... through the end of the dataset, because we didn't specify the start and end portions of the slice. \n\nBecause Python is 0-indexed, using `::2` will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\npoke = pd.read_csv(\"data/pokemon_ascii.csv\")\npoke.iloc[0::2]\n##       pokedex_number                      name  ... against_steel  against_fairy\n## 0                  1                 Bulbasaur  ...           1.0            0.5\n## 2                  3                  Venusaur  ...           1.0            0.5\n## 4                  4                Charmander  ...           0.5            0.5\n## 6                  6                 Charizard  ...           0.5            0.5\n## 8                  6          Mega Charizard Y  ...           0.5            0.5\n## ...              ...                       ...  ...           ...            ...\n## 1018             884                 Duraludon  ...           0.5            1.0\n## 1020             886                  Drakloak  ...           1.0            2.0\n## 1022             888      Zacian Crowned Sword  ...           1.0            0.5\n## 1024             889  Zamazenta Crowned Shield  ...           0.5            1.0\n## 1026             890                 Eternatus  ...           1.0            1.0\n## \n## [514 rows x 49 columns]\n```\n:::\n\nIf we want to get only odd rows, we can use the index `[1::2]`, which will start at row 1 and give us 1, 3, 5, ...\n\n##### Base R {-}\n\nIn base R, we'd use `seq()` to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use `subset`, which requires a logical condition, and use `1:nrow(poke)` to create an index which we then use for deciding whether each row is even or odd.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke[seq(1, nrow(poke), 2),]\n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          1 Bulb… Bisasam       1 Normal Seed P…       2 Grass  Poison     0.7\n##  2          3 Venu… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2  \n##  3          4 Char… Gluman…       1 Normal Lizard…       1 Fire   .          0.6\n##  4          6 Char… Glurak        1 Normal Flame …       2 Fire   Flying     1.7\n##  5          6 Mega… Glurak        1 Normal Flame …       2 Fire   Flying     1.7\n##  6          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  7          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  8         11 Meta… Safcon        1 Normal Cocoon…       1 Bug    .          0.7\n##  9         13 Weed… Hornliu       1 Normal Hairy …       2 Bug    Poison     0.3\n## 10         15 Beed… Bibor         1 Normal Poison…       2 Bug    Poison     1  \n## # … with 504 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          2 Ivys… Bisakn…       1 Normal Seed P…       2 Grass  Poison     1  \n##  2          3 Mega… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2.4\n##  3          5 Char… Glutexo       1 Normal Flame …       1 Fire   .          1.1\n##  4          6 Mega… Glurak        1 Normal Flame …       2 Fire   Dragon     1.7\n##  5          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  6          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  7         10 Cate… Raupy         1 Normal Worm P…       1 Bug    .          0.3\n##  8         12 Butt… Smettbo       1 Normal Butter…       2 Bug    Flying     1.1\n##  9         14 Kaku… Kokuna        1 Normal Cocoon…       2 Bug    Poison     0.6\n## 10         15 Mega… Bibor         1 Normal Poison…       2 Bug    Poison     1.4\n## # … with 504 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n```\n:::\n\n\nThis is less fun than using `dplyr` because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you're going. The real power of `dplyr` is in the collection of the full set of verbs with a consistent user interface; nothing done in `dplyr` is so special that it can't be done in base R as well.\n\n:::\n\n#### Sorting rows by variable values\n\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n::: panel-tabset\n\n##### R: `dplyr` {-}\n\n`arrange()` is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, `desc()`, which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to `arrange` to sort the data frame by multiple columns hierarchically; each column can be modified with `desc()` separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\narrange(poke, desc(total_points))\n## # A tibble: 1,028 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 1,018 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n```\n:::\n\n\n##### Python {-}\n\nIn pandas, we use the `sort_values` function, which has an argument `ascending`. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npoke.sort_values(['total_points'], ascending = False)\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 1027             890   Eternatus Eternamax  ...           1.0            1.0\n## 190              150         Mega Mewtwo Y  ...           1.0            1.0\n## 189              150         Mega Mewtwo X  ...           1.0            2.0\n## 458              384         Mega Rayquaza  ...           1.0            2.0\n## 456              383        Primal Groudon  ...           0.5            0.5\n## ...              ...                   ...  ...           ...            ...\n## 351              298               Azurill  ...           2.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## \n## [1028 rows x 49 columns]\n```\n:::\n\n\n\n##### Base R {-}\n\nThe `sort()` function in R can be used to sort a vector, but when sorting a data frame we usually want to use the `order()` function instead. This is because `sort()` orders the values of the argument directly, where `order()` returns a sorted index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n```\n:::\n\n\nWhen working with a data frame, we want to sort the entire data frame's rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke[order(poke$total_points, decreasing = T),]\n## # A tibble: 1,028 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 1,018 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n```\n:::\n\n:::\n\n#### Keep the top $n$ values of a variable\n\n::: panel-tabset\n\n##### R: `dplyr` {-}\n\n`slice_max()` will keep the top values of a specified variable. This is like a filter statement, but it's a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_max(poke, order_by = total_points, n = 5)\n## # A tibble: 6 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## 6         383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\n```\n:::\n\n\nBy default, `slice_max()` returns values tied with the nth value as well, which is why our result has 6 rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_max(poke, order_by = total_points, n = 5, with_ties = F) \n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\n```\n:::\n\n\nOf course, there is a similar `slice_min()` function as well:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_min(poke, order_by = total_points, n = 5)\n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n## 1         746 Wish… Lusard…       7 Normal Small …       1 Water  .          0.2\n## 2         191 Sunk… Sonnke…       2 Normal Seed P…       1 Grass  .          0.3\n## 3         824 Blip… .             8 Normal Larva …       1 Bug    .          0.4\n## 4         872 Snom  .             8 Normal Worm P…       2 Ice    Bug        0.3\n## 5         298 Azur… Azurill       3 Normal Polka …       2 Normal Fairy      0.2\n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\n```\n:::\n\n\n`slice_max` and `slice_min` also take a `prop` argument that gives you a certain proportion of the values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice_max(poke, order_by = total_points, prop = .01)\n## # A tibble: 10 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\n```\n:::\n\n\n##### Python {-}\n\nIn Python, `nlargest` and `nsmallest` work roughly the same as `dplyr`'s `slice_max` and `slice_min` for integer counts.\n\n\n::: {.cell}\n\n```{.python .cell-code}\npoke.nlargest(5, 'total_points')\n##       pokedex_number                 name  ... against_steel  against_fairy\n## 1027             890  Eternatus Eternamax  ...           1.0            1.0\n## 189              150        Mega Mewtwo X  ...           1.0            2.0\n## 190              150        Mega Mewtwo Y  ...           1.0            1.0\n## 458              384        Mega Rayquaza  ...           1.0            2.0\n## 454              382        Primal Kyogre  ...           0.5            1.0\n## \n## [5 rows x 49 columns]\npoke.nsmallest(5, 'total_points')\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 351              298               Azurill  ...           2.0            1.0\n## \n## [5 rows x 49 columns]\n```\n:::\n\n\nTo get proportions, though, we have to do some math:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npoke.nlargest(int(len(poke)*0.01), 'total_points')\n##       pokedex_number                      name  ... against_steel  against_fairy\n## 1027             890       Eternatus Eternamax  ...           1.0            1.0\n## 189              150             Mega Mewtwo X  ...           1.0            2.0\n## 190              150             Mega Mewtwo Y  ...           1.0            1.0\n## 458              384             Mega Rayquaza  ...           1.0            2.0\n## 454              382             Primal Kyogre  ...           0.5            1.0\n## 456              383            Primal Groudon  ...           0.5            0.5\n## 930              800            Ultra Necrozma  ...           1.0            2.0\n## 584              493                    Arceus  ...           1.0            1.0\n## 1022             888      Zacian Crowned Sword  ...           1.0            0.5\n## 1024             889  Zamazenta Crowned Shield  ...           0.5            1.0\n## \n## [10 rows x 49 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total_points')\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 351              298               Azurill  ...           2.0            1.0\n## 478              401             Kricketot  ...           1.0            1.0\n## 13                10              Caterpie  ...           1.0            1.0\n## 16                13                Weedle  ...           1.0            0.5\n## 317              265               Wurmple  ...           1.0            1.0\n## 332              280                 Ralts  ...           2.0            1.0\n## \n## [10 rows x 49 columns]\n```\n:::\n\n\n##### Base R {-}\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing.\nIn the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke[order(poke$total_points, decreasing = T)[1:5],]\n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\npoke[order(poke$total_points, decreasing = T)[1:round(nrow(poke)*.01)],]\n## # A tibble: 10 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 39 more variables: weight_kg <chr>, abilities_number <dbl>,\n## #   ability_1 <chr>, ability_2 <chr>, ability_hidden <chr>, total_points <dbl>,\n## #   hp <dbl>, attack <dbl>, defense <dbl>, sp_attack <dbl>, sp_defense <dbl>,\n## #   speed <dbl>, catch_rate <chr>, base_friendship <chr>,\n## #   base_experience <chr>, growth_rate <chr>, egg_type_number <dbl>,\n## #   egg_type_1 <chr>, egg_type_2 <chr>, percentage_male <chr>,\n## #   egg_cycles <chr>, against_normal <dbl>, against_fire <dbl>, …\n```\n:::\n\n\n:::\n\n::: {.callout-tip}\n### Try it out: Filtering\n\n::: panel-tabset\n\n#### Problem {-}\n\nUsing the Pokemon data, can you create a new data frame that has only water type Pokemon? Can you write a filter statement that looks for any Pokemon which has water type for either type1 or type2?\n\n#### R: `dplyr` {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\nfilter(poke, type_1 == \"Water\")\n## # A tibble: 134 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 124 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n\nfilter(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## # A tibble: 153 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 143 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n# The conditions have to be separated by |, which means \"or\"\n```\n:::\n\n\n#### Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\npoke.query(\"type_1=='Water'\")\n##       pokedex_number            name  ... against_steel  against_fairy\n## 9                  7        Squirtle  ...           0.5            1.0\n## 10                 8       Wartortle  ...           0.5            1.0\n## 11                 9       Blastoise  ...           0.5            1.0\n## 12                 9  Mega Blastoise  ...           0.5            1.0\n## 72                54         Psyduck  ...           0.5            1.0\n## ...              ...             ...  ...           ...            ...\n## 964              834         Drednaw  ...           1.0            1.0\n## 976              846        Arrokuda  ...           0.5            1.0\n## 977              847     Barraskewda  ...           0.5            1.0\n## 1016             882       Dracovish  ...           0.5            2.0\n## 1017             883       Arctovish  ...           1.0            1.0\n## \n## [134 rows x 49 columns]\npoke.query(\"type_1=='Water'|type_2=='Water'\")\n# The conditions have to be separated by |, which means \"or\"\n##       pokedex_number            name  ... against_steel  against_fairy\n## 9                  7        Squirtle  ...           0.5            1.0\n## 10                 8       Wartortle  ...           0.5            1.0\n## 11                 9       Blastoise  ...           0.5            1.0\n## 12                 9  Mega Blastoise  ...           0.5            1.0\n## 72                54         Psyduck  ...           0.5            1.0\n## ...              ...             ...  ...           ...            ...\n## 975              845       Cramorant  ...           0.5            1.0\n## 976              846        Arrokuda  ...           0.5            1.0\n## 977              847     Barraskewda  ...           0.5            1.0\n## 1016             882       Dracovish  ...           0.5            2.0\n## 1017             883       Arctovish  ...           1.0            1.0\n## \n## [153 rows x 49 columns]\n```\n:::\n\n\n#### Base R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoke <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\nsubset(poke, type_1 == \"Water\")\n## # A tibble: 134 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 124 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n\nsubset(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## # A tibble: 153 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         <dbl> <chr> <chr>     <dbl> <chr>  <chr>     <dbl> <chr>  <chr>    <dbl>\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 143 more rows, 39 more variables: weight_kg <chr>,\n## #   abilities_number <dbl>, ability_1 <chr>, ability_2 <chr>,\n## #   ability_hidden <chr>, total_points <dbl>, hp <dbl>, attack <dbl>,\n## #   defense <dbl>, sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   catch_rate <chr>, base_friendship <chr>, base_experience <chr>,\n## #   growth_rate <chr>, egg_type_number <dbl>, egg_type_1 <chr>,\n## #   egg_type_2 <chr>, percentage_male <chr>, egg_cycles <chr>, …\n# The conditions have to be separated by |, which means \"or\"\n```\n:::\n\n\n:::\n\n\n\n## Select: Pick columns\n\nSometimes, we don't want to work with a set of 50 variables when we're only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. `df[, c(1, 3, 5)]`), but this can get tedious.\n\n::: panel-tabset\n\n### R: `dplyr` {-}\n\nIn `dplyr`, the function to pick a few columns is `select()`. The syntax from the help file (`?select`) looks deceptively simple.\n\n> select(.data, ...)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. `...` means something along the lines of \"put in any additional arguments that make sense in context or might be passed on to other functions\".\n\nSo what can go in there?\n\n::: {.callout-collapse}\n\n#### An exhaustive(?) list of ways to select variables in `dplyr`\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).^[It accomplishes this through the magic of quasiquotation, which we will not cover in this course because it's basically witchcraft.]  \nMost `dplyr` commands work with \"bare\" variable names - you don't need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they're very explicitly exceptions.\n\n- `var3:var5`: `select(df, var3:var5)` will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n- `!(<set of variables>)` will give you any columns that aren't in the set of variables in parentheses\n    - `(<set of vars 1>) & (<set of vars 2>)` will give you any variables that are in both set 1 and set 2. `(<set of vars 1>) | (<set of vars 2>)` will give you any variables that are in either set 1 or set 2.\n    - `c()` combines sets of variables.\n\n`dplyr` also defines a lot of variable selection \"helpers\" that can be used inside `select()` statements. These statements work with bare column names (so you don't have to put quotes around the column names when you use them).\n\n- `everything()` matches all variables\n- `last_col()` matches the last variable. `last_col(offset = n)` selects the n-th to last variable.\n- `starts_with(\"xyz\")` will match any columns with names that start with xyz. Similarly, `ends_with()` does exactly what you'd expect as well.\n- `contains(\"xyz\")` will match any columns with names containing the literal string \"xyz\". Note, `contains` does not work with regular expressions (you don't need to know what that means right now).\n- `matches(regex)` takes a regular expression as an argument and returns all columns matching that expression.\n- `num_range(prefix, range)` selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n- `all_of(char)` matches all variable names in the character vector `char`. If one of the variables doesn't exist, this will return an error.\n- `any_of(char)` matches the contents of the character vector `char`, but does not throw an error if the variable doesn't exist in the data set.\n\nThere's one final selector -\n\n- `where()` applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n:::\n\nLet's try these selector functions out and see what we can accomplish!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n```\n:::\n\n\nWe'll start out with the `nycflights13` package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n::: {.callout-tip}\nYou might want to try out your EDA skills from the previous module to see what you can find out about the dataset, before seeing how `select()` works.\n:::\n\n\nWe could get a data frame of departure information for each flight:\n\n::: {.cell}\n\n```{.r .cell-code}\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n```\n:::\n\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\n::: {.cell}\n\n```{.r .cell-code}\nflights %>%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n```\n:::\n\n\nNote that `everything()` won't duplicate columns you've already added.\n\nExploring the difference between bare name selection and `all_of()`/`any_of()`\n\n::: {.cell}\n\n```{.r .cell-code}\nflights %>%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 × 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, 1 more variable: time_hour <dttm>, and abbreviated\n## #   variable names ¹​sched_arr_time, ²​air_time\n\nvarlist <- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %>%\n  select(all_of(varlist))\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nvarlist <- c(varlist, \"whoops\")\n\nflights %>%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `whoops` doesn't exist.\n\nflights %>%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n```\n:::\n\n\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with `filter`) and column-wise (with `select`).\n\n\n\n### Python {-}\n\nFirst, let's install the nycflights13 package[@chowNycflights13DataPackage2020] in python with `pip install nycflights13`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nycflights13 import flights\n```\n:::\n\n\nSelect operations are not as easy in python as they are using select() with helpers.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n```\n:::\n\n\n#### List Comprehensions\n\nIn Python, there are certain shorthands called \"list comprehensions\" [@pythonfoundationDataStructures2022] that can perform similar functions to e.g. the `matches()` function in dplyr. \n\nSuppose we want to get all columns containing the word 'time'. We could iterate through the list of columns (`flights.columns`) and add the column name any time we detect the word 'time' within. That is essentially what the following code does: \n\n\n::: {.cell}\n\n```{.python .cell-code}\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n```\n:::\n\n\n`for col in flights.columns` iterates through the list of columns; `if 'time' in col` detects the presence of the word 'time', and the `col` out front adds the time-containing `col` to the array of columns to keep.\n\n#### Selecting columns in Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\nselcols.extend([\"whoops\"])\nselcols\n\n# Subset the data frame\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\nflights.loc[:,selcols]\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['whoops'] not in index\"\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n```\n:::\n\n\n### Base R {-}\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. `matches` in `dplyr`.\n\n`grepl` is a shorthand function for `grep`, which searches for a pattern in a vector of strings. `grepl` returns a logical vector indicating whether the pattern (`\"dep\"`, in this case) was found in the vector (`names(flights)`, in this case). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndepcols <- names(flights)[grepl(\"dep\", names(flights))]\ncollist <- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n```\n:::\n\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_order <- names(flights)\nnew_order <- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n```\n:::\n\n\nThis is less convenient than `dplyr::everything` in part because it depends on us to get the column indexes right. \n\n:::\n\n\n::: callout-note callout-collapse\n### dplyr::relocate\n\nAnother handy `dplyr` function is `relocate`; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I'm covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n![relocate lets you rearrange columns (by Allison Horst)](https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/dplyr_relocate.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\n# move numeric variables to the front\nflights %>% relocate(where(is.numeric))\n## # A tibble: 336,776 × 19\n##     year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ flight\n##    <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl>  <int>\n##  1  2013     1     1      517         515       2     830     819      11   1545\n##  2  2013     1     1      533         529       4     850     830      20   1714\n##  3  2013     1     1      542         540       2     923     850      33   1141\n##  4  2013     1     1      544         545      -1    1004    1022     -18    725\n##  5  2013     1     1      554         600      -6     812     837     -25    461\n##  6  2013     1     1      554         558      -4     740     728      12   1696\n##  7  2013     1     1      555         600      -5     913     854      19    507\n##  8  2013     1     1      557         600      -3     709     723     -14   5708\n##  9  2013     1     1      557         600      -3     838     846      -8     79\n## 10  2013     1     1      558         600      -2     753     745       8    301\n## # … with 336,766 more rows, 9 more variables: air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, carrier <chr>, tailnum <chr>, origin <chr>,\n## #   dest <chr>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n```\n:::\n\n\n:::\n\n## Mutate: Add and transform variables\n\nUp to this point, we've been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is `mutate`, but in base R and python, we'll simply use assignment to add columns to our data frames.\n\n\n![Mutate (by Allison Horst)](https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/dplyr_mutate.png)\n\nLets use the [police violence data](https://github.com/srvanderplas/unl-stat850/raw/main/data/police_violence.xlsx) to demonstrate. \nThere is an entry labeled \"Unknown race\" for victim's race, but this is essentially equivalent to `NA` (missing data). So we should replace the `Unknown race` entry with `NA` so that those data points are handled consistently. \n\n::: panel-tabset\n\n### Base R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\n\n# There are two categories for \"unknown race\"\ntable(police_violence$`Victim's race`, useNA = 'ifany')\n## \n##            Asian            Black         Hispanic  Native American \n##              142             2498             1777              139 \n## Pacific Islander     Unknown race            White \n##               60              976             4350\n\n# First, copy the original data into a race variable that is easy to access (no spaces)\npolice_violence$race <- police_violence$`Victim's race`\n# Then, overwrite \"Unknown race\" with NA\npolice_violence$race[police_violence$`Victim's race` == \"Unknown race\"] <- NA\n\n# Fixed!\ntable(police_violence$race, useNA = 'ifany')\n## \n##            Asian            Black         Hispanic  Native American \n##              142             2498             1777              139 \n## Pacific Islander            White             <NA> \n##               60             4350              976\n```\n:::\n\n\nNotice that we had to type the name of the dataset at least 4 times to perform the operation we were looking for. I could reduce that a bit to 2x with the `ifelse` function, but it's still a lot of typing.\n\n### R: `dplyr` {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\n\nlibrary(dplyr)\npolice_violence %>%\n  mutate(race = ifelse(`Victim's race` == \"Unknown race\", NA, `Victim's race`)) %>%\n  select(`Victim's race`, race) %>%\n  table(useNA = 'ifany')\n##                   race\n## Victim's race      Asian Black Hispanic Native American Pacific Islander White\n##   Asian              142     0        0               0                0     0\n##   Black                0  2498        0               0                0     0\n##   Hispanic             0     0     1777               0                0     0\n##   Native American      0     0        0             139                0     0\n##   Pacific Islander     0     0        0               0               60     0\n##   Unknown race         0     0        0               0                0     0\n##   White                0     0        0               0                0  4350\n##                   race\n## Victim's race      <NA>\n##   Asian               0\n##   Black               0\n##   Hispanic            0\n##   Native American     0\n##   Pacific Islander    0\n##   Unknown race      976\n##   White               0\n```\n:::\n\nThe last 2 rows are just to organize the output - we keep only the two variables we're working with, and get a crosstab.\n\n\n### Python {-}\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with). \n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence[\"race\"] = police_violence[\"Victim's race\"].replace(\"Unknown race\", pd.NA)\npolice_violence.race\n\n# This doesn't actually work :(\n## 0           <NA>\n## 1           <NA>\n## 2           <NA>\n## 3           <NA>\n## 4           <NA>\n##           ...   \n## 9937       Black\n## 9938    Hispanic\n## 9939    Hispanic\n## 9940       White\n## 9941       White\n## Name: race, Length: 9942, dtype: object\npolice_violence.groupby('race', dropna=False)['race'].count()\n## race\n## Asian                142\n## Black               2498\n## Hispanic            1777\n## Native American      139\n## Pacific Islander      60\n## White               4350\n## NaN                    0\n## Name: race, dtype: int64\n```\n:::\n\n\nUnfortunately, for some reason, I can't seem to get pandas to count the NAs once they're in the data frame. \n\nAnother function that may be useful is the `assign` function, which can be used to create new variables if you don't want to use the `[\"new_col\"]` notation. In some circumstances, `.assign(var = ...)` is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I'd like to [not get into right now](https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy) for simplicity's sake). \n\n:::\n\nThe learning curve here isn't actually knowing how to assign new variables (though that's important). The challenge comes when you want to do something *new* and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. \n\n\n:::{.callout-note collapse=true}\n\n### Mutate and new challenges\n\nI'm not going to be able to teach you how to handle every mutate statement task you'll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to _read documentation_ and _google things intelligently_, and to _understand what you're reading_ enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\n\nGoogle and StackOverflow are very common and important programming skills!\n\n![[Source](https://twitter.com/madsbrodt/status/1339127984670773251)](images/data-transformations/twitter-google-stackoverflow.png){fig-alt=\"A screenshot of a tweet from @madsbrodt. Tweet text: Googling and StackOverflow'ing is a natural part of programming. Don't think of it as cheating. Knowing what to search for, and which results will fit your given situation is an important skill (flame emoji).\"}\n\n![[Source](https://twitter.com/cszhu/status/1230954186520461312)](images/data-transformations/twitter-happy-debugging.png){fig-alt=\"A screenshot of a tweet from @cszhu. Tweet text: if you're not happy single, you won't be happy in a relationship. true happiness comes from closing 100 chrome tabs after solving an obscure programming bug, not from someone else.\"}\n\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line. \n\nPart of the goal of this textbook is to help you **learn how to read through a package description and evaluate whether the package will do what you want**. \nWe're going to try to build some of those skills starting now. \nIt would be relatively easy to teach you how to do a set list of tasks, but you'll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n![Apologies for the noninclusive language, but the sentiment is real. [Source](https://twitter.com/abt_programming/status/459414524303785984)](images/data-transformations/twitter-teach-program.png){fig-alt=\"A screenshot of a tweet from @abt_programming. Tweet text: Give a man a program, frustrate him for a day.Teach a man to program, frustrate him for a lifetime - Muhammad Waseem.\"}\n\n:::\n\n\n\n::: callout-note\nHere is a quick table of places to look in R and python to solve some of the more common problems. \n\nProblem | R | Python\n--- | ----- | -----\nDates and Times | `lubridate` package (esp. `ymd_hms()` and variants, `decimal_date()`, and other convenience functions) | `pandas` has some date time support by default; see the [`datetime` module](https://docs.python.org/3/library/datetime.html) for more functionality. |\nString manipulation | `stringr` package | Quick Tips [@chinguyenTipsStringManipulation2021], Whirlwind Tour of Python chapter [@jacobvanderplasStringManipulationRegular2016]\n\n:::\n\n\n## Summarize\n\nThe next verb is one that we've already implicitly seen in action: `summarize` takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we've used e.g. `group_by` + `count` in Python to perform certain tasks as well. \n\nHere (in a trivial example), I compute the overall average age of a victim of police violence, and then also compute the average number of characters in their name. Admittedly, that last computation is a bit silly, but it's mostly for demonstration purposes.\n\n::: panel-tabset\n### R: `dplyr`{-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\npolice_violence %>%\n  mutate(age = as.numeric(`Victim's age`),\n         name_length = nchar(`Victim's name`)) %>%\n  summarize(age = mean(age, na.rm = T), name_length = mean(name_length))\n## # A tibble: 1 × 2\n##     age name_length\n##   <dbl>       <dbl>\n## 1  36.8        16.7\n```\n:::\n\n\n### Python {-}\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as `mean`. You can also build custom summary functions [@whortonApplyingCustomFunctions2021], or use the `agg()` function to define multiple summary variables.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence = police_violence.assign(\n  age = pd.to_numeric(police_violence[\"Victim's age\"], errors='coerce'),\n  name_length = police_violence[\"Victim's name\"].str.len()\n)\n\npolice_violence[[\"age\", \"name_length\"]].mean()\n## age            36.846476\n## name_length    16.659022\n## dtype: float64\npolice_violence[[\"age\", \"name_length\"]].agg(['mean', 'min'])\n##             age  name_length\n## mean  36.846476    16.659022\n## min    1.000000     7.000000\n```\n:::\n\n\n:::\n\nThe real power of summarize, though, is in combination with Group By. We'll see more summarize examples, but it's easier to make good examples when you have all the tools - it's hard to demonstrate how to use a hammer if you don't also have a nail. \n\n## Group By + (?) = Power!\n\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\n1. split my dataset into smaller datasets - one for each day\n2. compute summary values for each smaller dataset\n3. put my summarized data back together into a single dataset\n\nThis is known as the `split-apply-combine` [@wickhamSplitapplycombineStrategyData2011,@GroupSplitapplycombine2022] or sometimes, `map-reduce` [@deanMapReduceSimplifiedData2008] strategy (though map-reduce is usually on specifically large datasets and performed in parallel). \n\nIn tidy parlance, `group_by` is the verb that accomplishes the first task. `summarize` accomplishes the second task and implicitly accomplishes the third as well.\n\nLet's see how things change when we calculate the average age and name length of victims of police violence by their recorded race.\n\n::: panel-tabset\n### R: `dplyr`{-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolice_violence <- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\npolice_violence %>%\n  mutate(age = as.numeric(`Victim's age`),\n         name_length = nchar(`Victim's name`)) %>%\n  group_by(`Victim's race`) %>%\n  summarize(age = mean(age, na.rm = T), name_length = mean(name_length))\n## # A tibble: 7 × 3\n##   `Victim's race`    age name_length\n##   <chr>            <dbl>       <dbl>\n## 1 Asian             36.1        14.0\n## 2 Black             32.4        16.3\n## 3 Hispanic          33.6        17.2\n## 4 Native American   31.8        17.3\n## 5 Pacific Islander  33.6        15.8\n## 6 Unknown race      43.1        17.1\n## 7 White             39.7        16.6\n```\n:::\n\n\n### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence = police_violence.assign(\n  age = pd.to_numeric(police_violence[\"Victim's age\"], errors='coerce'),\n  name_length = police_violence[\"Victim's name\"].str.len()\n)\n\npolice_violence.groupby(\"Victim's race\")[[\"age\", \"name_length\"]].agg(\"mean\")\n##                         age  name_length\n## Victim's race                           \n## Asian             36.107143    13.978873\n## Black             32.415209    16.345076\n## Hispanic          33.604389    17.214406\n## Native American   31.789474    17.266187\n## Pacific Islander  33.650000    15.816667\n## Unknown race      43.073292    17.081967\n## White             39.733943    16.597241\n```\n:::\n\n\n:::\n\n\nWhen you `group_by` a variable, your result carries this grouping with it. In R, `summarize` will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the `ungroup()` command. In Python, you should consider using `reset_index` or `grouped_thing.obj()` to access the original information[@danchoAnswerThereUngroup2021].\n\n![The ungroup() command is just as important as the group_by() command! (by Allison Horst)](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-blanks/ungroup_blank.png)\n\n\n::: callout-caution\n### Storms Example \n\nLet's try a non-trivial example, using the `storms` dataset that is part of the `dplyr` package.\n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 11,859 × 13\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Amy    1975     6    27     0  27.5 -79   tropical dep… -1         25    1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical dep… -1         25    1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical dep… -1         25    1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical dep… -1         25    1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical dep… -1         25    1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical dep… -1         25    1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical dep… -1         25    1011\n##  8 Amy    1975     6    28    18  34   -77   tropical dep… -1         30    1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical sto… 0          35    1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical sto… 0          40    1002\n## # … with 11,849 more rows, 2 more variables:\n## #   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n```\n:::\n\n\n#### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n```\n:::\n\n\n:::\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes). \n\nOne thing we might want to know is at what point each storm was the strongest. Let's define strongest in the following way: \n\n1. The points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause). \n2. If there's a tie, we might want to know when the maximum wind speed occurred. \n3. If that still doesn't get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred. \n\nLet's start by translating these criteria into basic operations. I'll use dplyr function names here, but I'll also specify what I mean when there's a conflict (e.g. filter in dplyr means something different than filter in python). \n\nInitial attempt:\n1. **For each storm** (`group_by`), \n2. we need the point where the storm has lowest atmospheric pressure. (`filter` - pick the row with the lowest pressure). \n\nThen we read the next part: \"If there is a tie, pick the maximum wind speed.\"\n\n1. `group_by`\n2. `arrange` by ascending pressure and descending wind speed\n3. `filter` - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\n1. `group_by`\n2. `arrange` by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n3. `filter` - pick the row(s) which have the lowest pressure and highest wind speed\n4. `summarize` - compute the average time and category (if there are multiple rows)\n\nLet's write the code, now that we have the order of operations straight!\n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmax_power_storm <- storms %>%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %>%\n  filter(pressure == min(pressure, na.rm = T)) %>%\n  filter(wind == max(wind, na.rm = T)) %>%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %>%\n  arrange(time) %>%\n  ungroup()\nmax_power_storm\n## # A tibble: 512 × 7\n##    name      year pressure  wind category status         time               \n##    <chr>    <dbl>    <dbl> <dbl> <ord>    <chr>          <dttm>             \n##  1 Amy       1975      981    60 0        tropical storm 1975-07-02 12:00:00\n##  2 Caroline  1975      963   100 3        hurricane      1975-08-31 06:00:00\n##  3 Doris     1975      965    95 2        hurricane      1975-09-02 21:00:00\n##  4 Belle     1976      957   105 3        hurricane      1976-08-09 00:00:00\n##  5 Gloria    1976      970    80 1        hurricane      1976-09-30 00:00:00\n##  6 Anita     1977      926   150 5        hurricane      1977-09-02 06:00:00\n##  7 Clara     1977      993    65 1        hurricane      1977-09-08 12:00:00\n##  8 Evelyn    1977      994    65 1        hurricane      1977-10-15 00:00:00\n##  9 Amelia    1978     1005    45 0        tropical storm 1978-07-31 00:00:00\n## 10 Bess      1978     1005    40 0        tropical storm 1978-08-07 12:00:00\n## # … with 502 more rows\n```\n:::\n\n\n#### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n```\n:::\n\n\n:::\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm. \n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/ggplot-storms-1.png){width=2100}\n:::\n:::\n\n\n#### Python {-}\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## <ggplot: (8779747295193)>\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/ggplot-storms-py-1.png){width=614}\n:::\n:::\n\n:::\n\nWe could also look to see whether there has been any change over time in pressure.\n\n::: panel-tabset\n\n#### R {-}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/ggplot-power-pressure-r-3.png){width=2100}\n:::\n:::\n\n\n#### Python {-}\n\n::: {.cell}\n\n```{.python .cell-code}\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## <ggplot: (8779881229412)>\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/ggplot-power-pressure-py-1.png){width=614}\n:::\n:::\n\n\n:::\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic (see [this coverage map from 1995](image/weather_radar_graph_1995.gif)). \n\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstorm_strength_duration <- storms %>%\n  group_by(name, year) %>%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %>%\n  ungroup() %>%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %>%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/duration-storm-3.png){width=2100}\n:::\n:::\n\n\n#### Python {-}\n\n::: {.cell}\n\n```{.python .cell-code}\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## <ggplot: (8779881158301)>\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/duration-storm-py-1.png){width=614}\n:::\n:::\n\n\n:::\n\nYou don't need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with. \n\nWe could also look to see how a storm's diameter evolves over time, from when the storm is first identified (group_by + mutate) \n\nDiameter measurements don't exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (`?storms` documents the dataset and its variables). \n\n\n::: panel-tabset\n\n#### R {-}\n\nNote the use of `as.numeric(as.character(max(category)))` to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstorm_evolution <- storms %>%\n  filter(!is.na(hurricane_force_diameter)) %>%\n  group_by(name, year) %>%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %>%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/storm-evo-3.png){width=2100}\n:::\n:::\n\n\n#### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## <ggplot: (8779878839881)>\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/facets/facet.py:390: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/storm-evo-py-1.png){width=614}\n:::\n:::\n\n\n:::\n\n\nFor this plot, I've added `facet_wrap(~year)` to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms. \n\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstorm_evolution %>%\n  filter(year == 2008) %>%\n  arrange(desc(time_since_start))\n## # A tibble: 327 × 15\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Ike    2008     9    14     6  35.5 -93.7 tropical sto… 0          35     985\n##  2 Ike    2008     9    14     0  33.5 -94.9 tropical sto… 0          35     980\n##  3 Ike    2008     9    13    18  31.7 -95.3 tropical sto… 0          50     974\n##  4 Ike    2008     9    13    12  30.3 -95.2 hurricane     2          85     959\n##  5 Ike    2008     9    13     7  29.3 -94.7 hurricane     2          95     950\n##  6 Ike    2008     9    13     6  29.1 -94.6 hurricane     2          95     951\n##  7 Ike    2008     9    13     0  28.3 -94   hurricane     2          95     952\n##  8 Fay    2008     8    27     0  35   -85.8 tropical dep… -1         15    1005\n##  9 Ike    2008     9    12    18  27.5 -93.2 hurricane     2          95     954\n## 10 Fay    2008     8    26    18  34.6 -86.5 tropical dep… -1         20    1004\n## # … with 317 more rows, 4 more variables: tropicalstorm_force_diameter <int>,\n## #   hurricane_force_diameter <int>, time <dttm>, time_since_start <drtn>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorm_evolution %>% filter(name == \"Ike\") %>%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/storm-evo-year-3.png){width=2100}\n:::\n:::\n\n\n#### Python {-}\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n##      name  year  ...                time              age\n## 8000  Ike  2008  ... 2008-09-14 06:00:00 13 days 00:00:00\n## 7999  Ike  2008  ... 2008-09-14 00:00:00 12 days 18:00:00\n## 7998  Ike  2008  ... 2008-09-13 18:00:00 12 days 12:00:00\n## 7997  Ike  2008  ... 2008-09-13 12:00:00 12 days 06:00:00\n## 7996  Ike  2008  ... 2008-09-13 07:00:00 12 days 01:00:00\n## \n## [5 rows x 12 columns]\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## <ggplot: (8779746984771)>\n```\n\n::: {.cell-output-display}\n![](data-cleaning-verbs_files/figure-html/storm-evo-year-py-1.png){width=614}\n:::\n:::\n\n\n:::\n\n:::\n\n### Summarizing Across Multiple Variables\n\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. `across()` is particularly useful - it allows you to make a modification to several columns at the same time.\n\n![dplyr's across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)](https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/dplyr_across.png)\n\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don't want to write out all of the summarize statements individually, so we use across() instead. \n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity <- storms %>%\n  filter(status == \"hurricane\") %>%\n  group_by(name) %>%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %>%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %>%\n  arrange(desc(wind)) %>% \n  # get top 10\n  filter(row_number() <= 10) %>%\n  knitr::kable() # Make into a pretty table\n```\n\n::: {.cell-output-display}\n|name    | year|     month|     wind| pressure| tropicalstorm_force_diameter| hurricane_force_diameter|\n|:-------|----:|---------:|--------:|--------:|----------------------------:|------------------------:|\n|Andrew  | 1992|  8.000000| 118.2609| 946.6522|                          NaN|                      NaN|\n|Mitch   | 1998| 10.000000| 115.9091| 945.3182|                          NaN|                      NaN|\n|Rita    | 2005|  9.000000| 114.7368| 931.6316|                     265.2941|                 97.05882|\n|Isabel  | 2003|  9.000000| 112.1875| 946.5417|                          NaN|                      NaN|\n|Gilbert | 1988|  9.000000| 110.8929| 945.4286|                          NaN|                      NaN|\n|Luis    | 1995|  8.928571| 110.5952| 948.6190|                          NaN|                      NaN|\n|Wilma   | 2005| 10.000000| 110.3030| 939.4242|                     349.8333|                118.33333|\n|Matthew | 2016|  9.880952| 109.5238| 952.1190|                     263.5714|                 62.02381|\n|Hugo    | 1989|  9.000000| 106.5789| 950.9211|                          NaN|                      NaN|\n|David   | 1979|  8.457143| 105.1429| 956.1429|                          NaN|                      NaN|\n:::\n:::\n\n\n#### Python {-}\n\n[Stackoverflow reference](https://stackoverflow.com/questions/63200530/python-pandas-equivalent-to-dplyr-1-0-0-summarizeacross)\n\nIn the interests of using the same example, I've exported dplyr's `storms` data to [CSV](https://github.com/srvanderplas/unl-stat850/raw/main/data/storms.csv).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n```\n:::\n\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a **lambda function** - basically, a \"dummy\" function that has some arguments but not all of the arguments. Here, our lambda function is a function of `x`, and we calculate `x.mean(skipna=True)` for each x passed in (so, for each column). \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n```\n:::\n\n\n:::\n\n\n## Try it out \n\nYou can [read about the gapminder project here](https://www.gapminder.org/data/documentation/). \n\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in `gapminder` (in R) contain data about every 5 year period between 1952 and 2007, the country's life expectancy at birth, population, and per capita GDP (in US \\$, inflation adjusted). In the `gapminder_unfiltered` table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don't have complete data. The `gapminder` package in python (install with `pip install gapminder`) is a port of the R package, but doesn't contain the unfiltered data, so we'll instead use a CSV export. \n\n::: callout-tip\n### Read in the Data\n::: panel-tabset\n\n#### R {-}\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 3,303 more rows\n```\n:::\n\n\n#### Python {-}\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/gapminder_unfiltered.csv\")\n```\n:::\n\n:::\n\n### Task 1: How Bad is It?\n\n::: panel-tabset\n\n#### Problem {-}\nUsing your EDA skills, determine *how bad* the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information. \n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngapminder_unfiltered %>% \n  group_by(country) %>% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %>%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 × 5\n##    country        n missinglifeExp missingpop missingGDP\n##    <fct>      <int>          <int>      <int>      <int>\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # … with 73 more rows\n```\n:::\n\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. \n\n#### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n```\n:::\n\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. \n\n:::\n\n### Task 2: Exclude any data which isn't at 5-year increments\nStart in 1952 (so 1952, 1957, 1962, ..., 2007). \n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 2,003 more rows\n```\n:::\n\n\n#### Python {-}\n\n[Reminder about python list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)\n\n[Explanation of the query @ statement](https://stackoverflow.com/questions/62914335/python-pandas-query-for-values-in-list)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n```\n:::\n\n\n:::\n\n\n### Task 3: Exclude any countries that don't have a full set of observations\n\n::: panel-tabset\n\n#### R {-}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5)) %>%\n  group_by(country) %>%\n  mutate(nobs = n()) %>% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %>%\n  select(-nobs)\n## # A tibble: 1,704 × 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,694 more rows\n```\n:::\n\n\n#### Python {-}\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]\n```\n:::\n\n\n\n:::\n:::\n\n::: callout-note\n\n## Additional Resources \n- [Introduction to dplyr](https://stat545.com/dplyr-intro.html) and [Single Table dplyr functions](https://stat545.com/dplyr-single.html)\n\n- R for Data Science: [Data Transformations](https://r4ds.had.co.nz/transform.html)\n\n- Additional practice exercises: [Intro to the tidyverse](https://stat579-at-isu.github.io/materials/03_tidyverse/01_dplyr.html#19), [group_by + summarize examples](https://stat579-at-isu.github.io/materials//03_tidyverse/02_dplyr-examples.html), [group_by + mutate examples](https://stat579-at-isu.github.io/materials//03_tidyverse/03_dplyr-examples.html#1) (from a similar class at Iowa State)\n\n- [Base R data manipulation](https://vknight.org/SAS-R/Content/R-Chapter-03/)\n\n- [Videos of analysis of new data from Tidy Tuesday](https://www.youtube.com/playlist?list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8) - may include use of other packages, but almost definitely includes use of dplyr as well. \n  - [TidyTuesday Python github repo](https://github.com/waiyanps/TidyTuesday-Python) - replicating Tidy Tuesday analyses in Python with Pandas\n\n:::\n",
    "supporting": [
      "data-cleaning-verbs_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}