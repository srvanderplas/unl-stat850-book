# Data Transformations {#sec-data-transformations}

## Module Objectives {#module6-objectives .unnumbered}

Broadly, your objective while reading this chapter is to be able to identify datasets which have "messy" formats and determine a sequence of operations to transition the data into "tidy" format. To do this, you should be master the following concepts:

-   Determine what data format is necessary to generate a desired plot or statistical model
-   Join and split data columns using string operations
-   Understand the differences between "wide" and "long" format data and how to transition between the two structures
-   Understand relational data formats and how to use data joins to assemble data from multiple tables into a single table.

> Happy families are all alike; every unhappy family is unhappy in its own way. - Leo Tolstoy

> Tidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham

## Tidy and Messy Data

### Motivating Example

Consider the spreadsheet screenshot in @fig-human-readable.

![Spreadsheet intended for human consumption, from @mccallumBadDataHandbook2013 (Chapter 3)](images/data-transformations/Data-human-consumption.png){#fig-human-readable}

This spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. @fig-machine-readable shows a restructured version of this data in a more compact rectangular format.

![Spreadsheet reorganized for data analysis](images/data-transformations/Data-machine-consumption.png){#fig-machine-readable}

In @fig-machine-readable, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as "tidy" data: it has a single column for each variable and a single row for each observation.

### Defining Tidy data

The illustrations below are lifted from an [excellent blog post](https://www.openscapes.org/blog/2020/10/12/tidy-data/) [@lowndesTidyDataEfficiency2020] about tidy data; they're reproduced here because

1.  they're beautiful and licensed as CCA-4.0-by, and
2.  they might be more memorable than the equivalent paragraphs of text without illustration.

Most of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:

-   Tables may be laid out for easy data entry, so that there are multiple observations in a single row
-   It may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison

When we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.

The structure of **tidy data** reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.

![Tidy data format, illustrated.](https://www.openscapes.org/img/blog/tidydata/tidydata_1.jpg){#fig-tidy-data-definition}

![An illustration of the principle that every messy dataset is messy in its own way.](https://www.openscapes.org/img/blog/tidydata/tidydata_2.jpg)

The preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).

![Tidy data is easier to manage because the same tools and approaches apply to multiple datasets.](https://www.openscapes.org/img/blog/tidydata/tidydata_3.jpg)

In addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator's analysis and easily apply them to your own dataset.

::: {#fig-tidy-data-advantages layout-ncol=2}

![Collaboration with tidy data.](https://www.openscapes.org/img/blog/tidydata/tidydata_4.jpg){fig-alt="Two tidyverse monsters collaborating on a tidy dataset using similarly-shaped tools."}

![Tidy data enables standardized workflows.](https://www.openscapes.org/img/blog/tidydata/tidydata_5.jpg){fig-alt="A tidy data analysis conveyor belt, with new data loaded onto the conveyor that is sequentially wrangled, visualized, modeled, and leaves the conveyor belt as a complete analysis."}

Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.
:::

::: callout-warning

### Examples: Messy Data {-}
```{r tidypkgs, message = F, include = F}
library(dplyr) # Data wrangling
library(tidyr) # Data rearranging
library(tibble) # data table
```


These datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.

::: panel-tabset

#### Table 1 {-}

```{r tidy1, echo = F}
knitr::kable(table1, caption = "Table 1")
```

Here, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column). 

#### 2 {-}

```{r tidy2, echo = F}
knitr::kable(table2, caption = "Table 2")
```

Here, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1. 

#### 3 {-}

```{r tidy3, echo = F}
knitr::kable(table3, caption = "Table 3")
```

This form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can't do *anything* with this format as it stands, because we can't do math on data stored as characters. However, this form might be easier to read and record for a human being. 

#### 4 {-}

```{r tidy4, echo = F}
knitr::kable(table4a, caption = "Table 4a")
knitr::kable(table4b, caption = "Table 4b")
```

In this form, we have two tables - one for population, and one for cases. Each year's observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we'll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year. 

#### 5 {-}

```{r tidy5, echo = F}
knitr::kable(table5, caption = "Table 5")
```

Table 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns  (or date and time in separate columns), often to deal with the fact that spreadsheets don't always handle dates the way you'd hope they would. 

:::

:::

::: callout-tip
### Try it out: Classifying Messy Data

::: panel-tabset

#### Problem

For each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates. 

What would you have to do in order to compute a standardized TB infection rate per 100,000 people?

#### Table 1 {-}


```{r tidy1-2, echo = F}
knitr::kable(table1, caption = "Table 1")
```

This is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.

#### 2 {-}

```{r tidy2-2, echo = F}
knitr::kable(table2, caption = "Table 2")
```

Each variable does not have its own column (so a single year's observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.

#### 3 {-}

```{r tidy3-2, echo = F}
knitr::kable(table3, caption = "Table 3")
```

Each value does not have its own cell (and each variable does not have its own column). In Table 3, you'd have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1. 


#### 4 {-}

```{r tidy4-2, echo = F}
knitr::kable(table4a, caption = "Table 4a")
knitr::kable(table4b, caption = "Table 4b")
```


There are multiple observations in each row because there is not a column for year. To compute the rate, you'd need to "stack" the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1. 

#### 5 {-}


```{r tidy5-2, echo = F}
knitr::kable(table5, caption = "Table 5")
```


Each variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren't actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you'd need to combine the two year columns together first). 

:::

:::

It is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be: 

1. Each dataset goes into its own table (or tibble, if you are using R)
2. Each variable gets its own column

::: callout-note
## Additional reading

@internationalbusinessmachinesRisksUsingSpreadsheets2018 - IBM SPSS ad that talks about the perils of spreadsheets

@obeirneHorrorStories2020 - assembled news stories involving spreadsheet mishaps
:::


By the end of this chapter, you will have the skills needed to wrangle the most common "messy" data sets into "tidy" form.

## String operations

Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different "levels" of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated). 

> Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski

![Alternately, the xkcd version of the above quote](https://imgs.xkcd.com/comics/perl_problems.png)


### Common, simple string tasks

#### Find and replace

#### Parsing numbers


### Separating multi-variable columns

### Joining columns

### Regular Expressions
Matching exact strings is easy - it's just like using find and replace.

```{r regular-expressions}
human_talk <- "blah, blah, blah. Do you want to go for a walk?"
dog_hears <- str_extract(human_talk, "walk")
dog_hears
```

But, if you can master even a small amount of regular expression notation, you'll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you're creative, but often they're much simpler. 

You may find it helpful to follow along with this section using this [web  app](https://spannbaueradam.shinyapps.io/r_regex_tester/) built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found [here](https://regex101.com/). The subset of regular expression syntax we're going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but [you can find regular expressions to do just about anything string-related](https://stackoverflow.com/questions/tagged/regex?tab=Votes). As with any tool, there are situations where it's useful, and situations where you should not use a regular expression, no matter how much you want to. 

Here are the basics of regular expressions:

- `[]` enclose sets of characters    
Ex: `[abc]` will match any single character `a`, `b`, `c`
  - `-` specifies a range of characters (`A-z` matches all upper and lower case letters)
  - to match `-` exactly, precede with a backslash (outside of `[]`) or put the `-` last (inside `[]`)
- `.` matches any character (except a newline)
- To match special characters, escape them using `\` (in most languages) or `\\` (in R). So `\\.` will match a literal `.`, `\\$` will match a literal `$`. 

::: panel-tabset

#### R {-}
```{r phone-strings}
num_string <- "phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789"

ssn <- str_extract(num_string, "[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]")
ssn
```

#### Python {-}

XXX TODO XXX

:::

Listing out all of those numbers can get repetitive, though. How do we specify repetition?

- `*` means repeat between 0 and inf times
- `+` means 1 or more times
- `?` means 0 or 1 times -- most useful when you're looking for something optional
- `{a, b}` means repeat between `a` and `b` times, where `a` and `b` are integers. `b` can be blank. So `[abc]{3,}` will match `abc`, `aaaa`, `cbbaa`, but not `ab`, `bb`, or `a`. For a single number of repeated characters, you can use `{a}`. So `{3, }` means "3 or more times" and `{3}` means "exactly 3 times"

::: panel-tabset

#### R {-}

```{r regular-expression-banana}
str_extract("banana", "[a-z]{1,}") # match any sequence of lowercase characters
str_extract("banana", "[ab]{1,}") # Match any sequence of a and b characters
str_extract_all("banana", "(..)") # Match any two characters
str_extract("banana", "(..)\\1") # Match a repeated thing
```

```{r regular-expression-phone-ssn}
num_string <- "phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23"

ssn <- str_extract(num_string, "[0-9]{3}-[0-9]{2}-[0-9]{4}")
ssn
phone <- str_extract(num_string, "[0-9]{3}.[0-9]{3}.[0-9]{4}")
phone
nuid <- str_extract(num_string, "[0-9]{8}")
nuid
bank_balance <- str_extract(num_string, "\\$[0-9,]+\\.[0-9]{2}")
bank_balance
```

#### Python {-}

XXX TODO XXX

:::


There are also ways to "anchor" a pattern to a part of the string (e.g. the beginning or the end)

- `^` has multiple meanings:
  - if it's the first character in a pattern, `^` matches the beginning of a string
  - if it follows `[`, e.g. `[^abc]`, `^` means "not" - for instance, "the collection of all characters that aren't a, b, or c". 
- `$` means the end of a string

Combined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.


::: panel-tabset

#### R {-}

```{r regular-expression-addresses}
address <- "1600 Pennsylvania Ave NW, Washington D.C., 20500"

house_num <- str_extract(address, "^[0-9]{1,}")

 # Match everything alphanumeric up to the comma
street <- str_extract(address, "[A-z0-9 ]{1,}")
street <- str_remove(street, house_num) %>% str_trim() # remove house number

city <- str_extract(address, ",.*,") %>% str_remove_all(",") %>% str_trim()

zip <- str_extract(address, "[0-9-]{5,10}$") # match 5 and 9 digit zip codes
```


#### Python {-}

XXX TODO XXX

:::



- `()` are used to capture information. So `([0-9]{4})` captures any 4-digit number
- `a|b` will select a or b. 

If you've captured information using (), you can reference that information using backreferences. In most languages, those look like this: `\1` for the first reference, `\9` for the ninth. 

::: panel-tabset

#### R {-}

In R, the `\` character is special, so you have to escape it. So in R, `\\1` is the first reference, and `\\2` is the second, and so on. 

```{r -regular-expression-fancier}
phone_num_variants <- c("(123) 456-7980", "123.456.7890", "+1 123-456-7890")
phone_regex <- "\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})"
# \\( and \\) match literal parentheses if they exist
# ([0-9]{3})? captures the area code, if it exists
# .? matches any character
# ([0-9]{3}) captures the exchange code
# ([0-9]{4}) captures the 4-digit individual code

str_extract(phone_num_variants, phone_regex)
str_replace(phone_num_variants, phone_regex, "\\1\\2\\3")
# We didn't capture the country code, so it remained in the string

human_talk <- "blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. "
dog_hears <- str_extract_all(human_talk, "walk|treat")
dog_hears
```


#### Python {-}

XXX TODO XXX

:::



Putting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:

::: panel-tabset

#### R {-}
```{r regex-r-version-of-sas}
strings <- c("abcdefghijklmnopqrstuvwxyzABAB",
"banana orange strawberry apple",
"ana went to montana to eat a banana",
"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.",
"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23",
"1600 Pennsylvania Ave NW, Washington D.C., 20500")

phone_regex <- "\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})"
dog_regex <- "(walk|treat)"
addr_regex <- "([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), ([0-9]{5})"
abab_regex <- "(..)\\1"

tibble(
  text = strings,
  phone = str_detect(strings, phone_regex),
  dog = str_detect(strings, dog_regex),
  addr = str_detect(strings, addr_regex),
  abab = str_detect(strings, abab_regex))
```


#### Python {-}

XXX TODO XXX

:::


::: callout-note
## Other resources

@doughertyCleanMessyData2021 - very nice task-oriented chapter that's below the level addressed in this course but still useful

:::

## References
