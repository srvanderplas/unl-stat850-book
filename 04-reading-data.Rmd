---
output: 
  html_document: 
    keep_md: yes
---
# External Data {#reading-data}

## Module Objectives {-}

- Import data from text and excel files into R and SAS for analysis
- Conduct exploratory data analysis to determine 
    - how the data is structured, 
    - what cleaning must be done, and 
    - what (if any) interesting artifacts are in the dataset

## External Data Formats

In order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we'll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:

- as raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable. 

- as a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.
    - R, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as `foreign` in R will let you read data from other programs, and packages such as `haven` in R will let you write data into binary formats used by other programs. To read data from R into SAS, the easiest way is probably to [call R from PROC IML](http://proc-x.com/2015/05/import-rdata-to-sas-along-with-labels/). 
    - [Here](https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/) is a very thorough explanation of why binary file formats exist, and why they're not necessarily optimal.


- in a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they're also not raw text files either. They're a hybrid. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet. There is a collection of horror stories [here](https://github.com/jennybc/scary-excel-stories) and a series of even more horrifying tweets [here](https://twitter.com/JennyBryan/status/722954354198597632)

[![](https://imgs.xkcd.com/comics/algorithms.png)](https://xkcd.com/1667/)

- in a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with. 

There are, of course, many other non-tabular data formats -- some open and easy to work with, some inpenetrable. A few which may be more common:

- Web related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.

- Spatial files: Shapefiles are by far the most common version of spatial files^[though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times]. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. We'll explore these a bit more when we talk about maps. 


To be minimally functional in R and SAS, let's start with figuring out how to read text, spreadsheet, and SQLite database formats into both programs. 

## Reading and Writing text data

There are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What's the difference, you say?

### Fixed-width files

In a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don't have to read the whole thing into memory). 

```
Col1    Col2    Col3
 3.4     4.2     5.4
27.3    -2.4    15.9
```

In base R (no extra packages), you can read fwf files in using `read.fwf`, but you must specify the column breaks yourself.
<details>
```{r, error = T, eval = -1, echo = -2}
url <- "https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/"
url <- "data/mesodata.txt"
read.fwf(url, 
         skip = 3, # Skip the first 2 lines (useless) + header line
         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, 
7, 8, 8, 8)) # There is a row with the column names specified
```
</details>

You can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you. 
<details>
```{r}

# I like to cheat a bit....
# Read the first few lines in
tmp <- readLines(url, n = 20)[-c(1:2)]

# split each line into a series of single characters
tmp_chars <- strsplit(tmp, '') 

# Bind the lines together into a character matrix
# do.call applies a function to an entire list - so instead of doing 18 rbinds, 
# one command will put all 18 rows together
tmp_chars <- do.call("rbind", tmp_chars) # (it's ok if you don't get this line)

# Make into a logical matrix where T = space, F = not space
tmp_chars_space <- tmp_chars == " "

# Add up the number of rows where there is a non-space character
# space columns would have 0s/FALSE
tmp_space <- colSums(!tmp_chars_space)

# We need a nonzero column followed by a zero column
breaks <- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)

# Then, we need to get the widths between the columns
widths <- diff(c(0, breaks))

# Now we're ready to go
mesodata <- read.fwf(url, skip = 3, widths = widths, header = F)
# read header separately - if you use header = T, it errors for some reason.
# It's easier just to work around the error than to fix it :)
mesodata_names <- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, 
                           stringsAsFactors = F)
names(mesodata) <- as.character(mesodata_names)

mesodata
```
</details>

But, there's an even simpler way...

The `readr` package creates data-frame like objects called tibbles (really, they're a souped-up data frame), but it is *much* friendlier to use. Tibbles also do not have the problems with factors (see the [introduction to factors](#factors)) - they will always read characters in as characters. 

```{r, message = F}
library(readr) # Better data importing in R

read_table(url, skip = 2) # Gosh, that was much easier!
```

You can also write fixed-width files if you *really* want to: 

```{r, message = F}
if (!"gdata" %in% installed.packages()) install.packages("gdata")

library(gdata)

write.fwf(mtcars, file = "data/04_mtcars-fixed-width.txt")
```


In SAS, it's a bit more complicated, but not that much - the biggest difference is that you generally have to specify the column names for SAS. For complicated data, as in R, you may also have to specify the column widths. 

<details>  
```{sashtml}
/* This downloads the file to my machine */
/* x "curl https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/ 
> ~/Projects/Class/unl-stat850/2020-stat850/data/mesodata.txt" */
/* only run this once */

data mesodata;
infile  "~/Projects/Class/unl-stat850/2020-stat850/data/mesodata.txt" firstobs = 4; 
/* Skip the first 3 rows */
  length STID $ 4; /* define ID length */
  input STID $ STNM TIME RELH TAIR 
        WSPD WVEC WDIR WDSD WSSD WMAX 
        RAIN PRES SRAD TA9M WS2M TS10 
        TB10 TS05 TS25 TS60 TR05 TR25 TR60;
run;

proc print data=mesodata (obs=10); /* print the first 10 observations */
  run;
```
</details>

In SAS data statements, you generally need to specify the data names explicitly. 


<!-- ```{sashtml} -->

<!-- proc import datafile = "~/Projects/Class/unl-stat850/2020-stat850/data/mesodata.txt" out=mesodata  -->
<!--   DBMS = DLM; /* delimited file */ -->
<!--     datarow = 4; -->
<!--     GETNAMES = YES; -->
<!-- run; -->

<!-- proc print data=mesodata (obs=10); /* print the first 10 observations */ -->
<!--   run; -->

<!-- ``` -->

In theory you can also get SAS to write out a fixed-width file, but it's much easier to just... not. You can generally use a CSV or format of your choice -- and you should definitely do that, because delimited files are much easier to work with. 

### Delimited Text Files

Delimited text files are files where fields are separated by a specific character, such as " ", ",", tab, etc. Often, delimited text files will have the column names as the first row in the file. 

As long as you know the delimiter, it's pretty easy to read in data from these files in R using the `readr` package. 
<details>
```{r}
url <- "https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv"

pokemon_info <- read_csv(url)
pokemon_info[1:6, 1:6] # Show only the first 6 lines & cols

# a file delimited with |

url <- "https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt"
nebraska_locations <- read_delim(url, delim = "|")
nebraska_locations[1:6, 1:6]
```
</details>

You can also read in the same files using read.csv and read.delim, which are the equivalent base R functions. 
<details>
```{r}
url <- "https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv"

pokemon_info <- read.csv(url, header = T, stringsAsFactors = F)
pokemon_info[1:6, 1:6] # Show only the first 6 lines & cols


# a file delimited with |

url <- "https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt"
nebraska_locations <- read.delim(url, sep = "|", header = T)
nebraska_locations[1:6, 1:6]
```
</details>

SAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters). Don't know what UTF-8 is? [Watch this excellent YouTube video explaining the history of file encoding!](https://www.youtube.com/watch?v=MijmeoH9LT4)

While writing this code, I got an error of "Invalid logical name" because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS. 

<details>
#### Note {- .note}
Note: Due to UTF-8 issues, this code chunk runs (with complaints) when I paste it into SAS, but does not run in bookdown. The complaints are because there are column names that are not valid SAS names (presumably, SAS doesn't allow UTF-8 characters as column names).

#### {-}

````
/* x "curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv > ~/Projects/Class/unl-stat850/2020-stat850/data/pokemon.csv";
only run this once... */

filename pokeloc '~/Projects/Class/unl-stat850/2020-stat850/data/pokemon.csv' encoding="utf-8";

proc import datafile = pokeloc out=poke
  DBMS = csv; /* comma delimited file */
  GETNAMES = YES
  ;
proc print data=poke (obs=10); /* print the first 10 observations */
  run;
````

The only abnormal thing is that on my computer, the japanese characters don't render.
[Here is the output from SAS running the above code interactively](other/04Pokemon_output.html)


Alternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.

<details>
If I fix the file in R (because I know how to fix it there... another option is to fix it manually), 
```{r}
library(readr)
library(dplyr)
tmp <- read_csv("~/Projects/Class/unl-stat850/2020-stat850/data/pokemon.csv")[,-1]
# You'll learn how to do this later
tmp <- select(tmp, -japanese_name) %>%
  mutate_all(iconv, from="UTF-8", to = "ASCII//TRANSLIT")
write_csv(tmp, "data/pokemon_ascii.csv", na='.')
```

Then, reading in the new file allows us to actually see the output.
```{sashtml pokemon}
filename pokeloc  "~/Projects/Class/unl-stat850/2020-stat850/data/pokemon_ascii.csv";

proc import datafile = pokeloc out=poke
  DBMS = csv /* comma delimited file */
  replace;
  GETNAMES = YES;
  GUESSINGROWS = 1028 /* use all data for guessing the variable type */
  ;
proc print data=poke (obs=10); /* print the first 10 observations */
  run; 
```

</details>
</details>


To read in a pipe delimited file (the '|' character), we have to make some changes. Here is the proc import code. Note that I am reading in a version of the file that I've converted to ASCII (see details below) because while the import works with the original file, it causes the SAS -> R pipeline that the book is built on to break. 
<details>

```{r}
tmp <- readLines("data/NE_Features_20200501.txt")
tmp_ascii <- iconv(tmp, to = "ASCII//TRANSLIT")
writeLines(tmp_ascii, "data/NE_Features_ascii.txt")
```
</details>

```{sashtml}
proc import datafile = "~/Projects/Class/unl-stat850/2020-stat850/data/NE_Features_ascii.txt" out=nefeatures
  DBMS = DLM /* delimited file */
  replace;
  GETNAMES = YES;
  DELIMITER = '|';
  GUESSINGROWS = 31582;
run;

proc print data=nefeatures (obs=10); /* print the first 10 observations */
  run;

```
<details>
Under the hood, proc import is just writing code for a data step. So when proc import doesn't work, we can just write the code ourselves. It requires a bit more work (specifying column names, for example) but it also doesn't fail nearly as often. 
```{sashtml, warning = T, error = T}
/* x "curl https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt
> ~/Projects/Class/unl-stat850/2020-stat850/data/NE_Features_20200501.txt";
only run this once... */

data nefeatures;
/*infile "~/Projects/Class/unl-stat850/2020-stat850/data/NE_Features_20200501.txt"*/
infile "~/Projects/Class/unl-stat850/2020-stat850/data/NE_Features_ascii.txt"
dlm='|' /* specify delimiter */
  encoding="utf-8" /* specify encoding */
  DSD /* delimiter sensitive data */
  missover /* keep going if missing obs encountered */
  firstobs=2; /* skip header row */
  input FEATURE_ID $
  FEATURE_NAME $
  FEATURE_CLASS $
  STATE_ALPHA $
  STATE_NUMERIC
COUNTY_NAME $
  COUNTY_NUMERIC $
  PRIMARY_LAT_DMS $
  PRIM_LONG_DMS $
  PRIM_LAT_DEC
PRIM_LONG_DEC
SOURCE_LAT_DMS $
  SOURCE_LONG_DMS $
  SOURCE_LAT_DEC
SOURCE_LONG_DEC
ELEV_IN_M
ELEV_IN_FT
MAP_NAME $
  DATE_CREATED $
  DATE_EDITED $
  ;
run;

proc print data=nefeatures (obs=10); /* print the first 10 observations */
  run;
```
</details>


## Binary Files

Both R and SAS have binary data files that store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. 

Luckily, it is possible to read the binary data files in both programs.

Let's read in the data from the 2009 National Household Travel Survey:
```{sashtml}
libname classdat "~/Projects/Class/unl-stat850/2020-stat850/data";
/* this tells SAS where to look for (a bunch of) data files */

proc contents data=classdat.cen10pub; /* This tells sas to access the specific file */
run;
```

We can read the same file into R using the `sas7bdat` library:
```{r}
if (!"sas7bdat" %in% installed.packages()) install.packages("sas7bdat")

library(sas7bdat)
data <- read.sas7bdat("https://github.com/srvanderplas/unl-stat850/raw/master/data/cen10pub.sas7bdat")
head(data)
```
If you are curious about what this data means, then by all means, take a look at the [codebook](https://github.com/srvanderplas/unl-stat850/raw/master/data/cen10_codebook.xlsx) (XLSX file). For now, it's enough that we can see roughly how it's structured.

There are [theoretically ways to read R data into SAS via the R subsystem](https://blogs.sas.com/content/iml/2011/05/13/calling-r-from-sasiml-software.html). Feel free to do that on your own machine.^[I tried, and it crashed SAS on my machine.]





## Spreadsheets

In R, the easiest way to read Excel data in is to use the `readxl` package. There are many other packages with different features, however - I have used `openxlsx` in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in Excel or LibreOffice first to make sure the formatting is as you expected it to be.

```{r, warning = F}
if (!"readxl" %in% installed.packages()) install.packages("readxl")
library(readxl)
path <- "~/Projects/Class/unl-stat850/2020-stat850/data/police_violence.xlsx"
if (!file.exists(path)) download.file("https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx", path, mode = "wb")

police_violence <- read_xlsx("data/police_violence.xlsx", sheet = 1)
police_violence[1:10, 1:6]
```


In SAS, PROC IMPORT is one easy way to read in xlsx files

```{sashtml}
PROC IMPORT OUT=WORK.police 
    DATAFILE="~/Projects/Class/unl-stat850/2020-stat850/data/police_violence.xlsx" 
    DBMS=xlsx /* Tell SAS what type of file it's reading */ 
    REPLACE; /* replace the dataset if it already exists */
  SHEET="2013-2019 Police Killings"; /* SAS reads the first sheet by default */
  GETNAMES=yes;
RUN;

PROC PRINT DATA=WORK.police (obs=10); /* print the first 10 observations */
  VAR Victim_s_name Victim_s_age Victim_s_gender Victim_s_race VAR6;
RUN;
```

[Here](https://stats.idre.ucla.edu/sas/faq/how-do-i-readwrite-excel-files-in-sas/) is some additional information about reading and writing Excel files in SAS. In general, it is better to avoid working in Excel, as it is not easy to reproduce the results. Saving your data in more reproducible formats will make writing reproducible code much easier. 

## Databases

There are many different database formats. Some of the most common databases are SQL* related formats and Microsoft Access files. 

### Microsoft Access

In R, we can read in MS Access files using the `Hmisc` package, as long as the mdbtools library is available on your computer. For this demo, we'll be using the [Scottish Witchcraft Database](http://witches.shca.ed.ac.uk/index.cfm?fuseaction=home.register), which you can download from their website, or acquire from the course data folder. 

```{r}
if (!"Hmisc" %in% installed.packages()) install.packages("Hmisc")
library(Hmisc)
db_loc <- "data/Witchcraftsurvey_download.mdb"

mdb.get(db_loc, tables = TRUE) # get table list
mdb.get(db_loc, tables = "WDB_Trial") # get table of trials

```
Many databases have multiple tables with **keys** that connect information in each table. We'll spend more time on databases later in the semester - for now, it's enough to be able to get data out of one. 


Unfortunately, it appears that SAS on Linux doesn't allow you to read in Access files. So I can't demonstrate that for you. But, since you know how to do it in R, worst case you can open up R and export all of the tables to separate CSV files, then read those into SAS. `emo::ji("sad")`

### SQLite

SQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. Let's try working with a sqlite file that has only one table: 

```{r}
if (!"RSQLite" %in% installed.packages()) install.packages("RSQLite")
if (!"DBI" %in% installed.packages()) install.packages("DBI")
library(RSQLite)
library(DBI)
con <- dbConnect(RSQLite::SQLite(), "data/ssa-babynames-for-2015.sqlite")
dbListTables(con) # List all the tables
babyname <- dbReadTable(con, "babynames")
babyname
```

You can of course write formal queries using the DBI package, but for many databases, it's easier to do the querying in R. We'll cover both options later - the R version will be in the next module.

In SAS, you can theoretically connect to SQLite databases, but there are very specific instructions for how to do that for each operating system (and it's not clear there's Linux support, in any case, so I'm SOL for demonstrating it. Sorry 'bout that.)


## Exploratory Data Analysis

- tables
- summary statistics
- basic plots?
- unique values


## References

- [Reading JSON in SAS](https://support.sas.com/resources/papers/proceedings17/0856-2017.pdf) -- You know SAS documentation is getting weird when they advertise a method as "the sexiest way to import JSON data into SAS". 
- [Reading Rdata files in SAS](http://proc-x.com/2015/05/import-rdata-to-sas-along-with-labels/)
- [Common problems with SAS data files](https://blogs.sas.com/content/sgf/2015/04/17/turning-text-files-into-sas-data-sets-6-common-problems-and-their-solutions/)
- U.S. Department of Transportation, Federal Highway Administration, 2009 National Household 
Travel Survey. URL: http://nhts.ornl.gov. Data acquired from data.world. 
- [RSQLite vignette](https://cran.r-project.org/web/packages/RSQLite/vignettes/RSQLite.html)





