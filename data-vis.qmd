# Data Visualization {#sec-data-vis}

## Module Objectives  {- #module11-objectives}

<!-- ::: column-margin -->
<!-- ![Mouseover text: I would describe my personal alignment as "lawful heterozygous silty liquid."](https://imgs.xkcd.com/comics/alignment_chart_alignment_chart.png) -->

<!-- IMO, Randall missed the opportunity to put a pie chart as Neutral Evil.  -->
<!-- ::: -->

- Create charts designed to communicate specific aspects of the data
- Describe charts using the grammar of graphics 
- Create layered graphics that highlight multiple aspects of the data
- Evaluate existing charts and develop new versions that improve accessibility and readability


There are a lot of different types of charts, and equally many ways  to categorize and describe the different types of charts. I'm going to be opinionated on this one - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics specifically. 


::: column-margin
Visualization and statistical graphics are also my research area, so I'm more passionate about this material, which means there's going to be more to read. Sorry about that in advance. I'll do my best to indicate which content is actually mission-critical and which content you can skip if you're not that interested.
:::

This is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That's why I'm showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.


::: {.callout-note collapse=true}
### Guides and Resources

Graph galleries contain sample code to create many different types of charts. Similar galaries are available in [R](https://r-graph-gallery.com/) and [Python](https://www.python-graph-gallery.com/).

Cheat Sheets:

- [Python](https://www.python-graph-gallery.com/cheat-sheets/)
- [Ggplot2](https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf)
- [Base R](https://r-graph-gallery.com/134-general-plot-parameters-reminder.html)

:::


## Why do we create graphics?

::: column-margin
> The greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. ("Why, that scatter diagram has a hole in the middle!") -- John W. Tukey [@tukeyDataBasedGraphicsVisual1990]
:::

Charts are easier to understand than raw data. 

When you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the "real world" into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is "tangible" in some way -- if not physical and literally touch-able, at least something we can view and "wrap our heads around". 

Consider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:

1. Print out the data set
2. Create some summary statistics of each variable and perhaps the covariance between the two variables
3. Draw a scatter plot of the two variables

Which one would you rather use? Why?

Our brains are very good at processing large amounts of visual information quickly. In very basic evolutionary terms, it's important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we're not nearly as equipped to process quickly (and in parallel). 

It's a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out.

## General approaches to creating graphics

There are two general approaches to generating statistical graphics computationally: 
  
1. Manually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.    
Base R, matplotlib, old-style SAS graphics

2. Describe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.    
ggplot2, plotnine, seaborn (sort of)


::: column-margin
There is a difference between *low-level* plotting libraries (base R, matplotlib) and *high-level* plotting libraries (ggplot2, plotnine, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it's probably a low level library.
:::

In the introduction to The Grammar of Graphics [@wilkinsonGrammarGraphics1999], Leland Wilkinson suggests that the first approach is what we would call "charts" - pie charts, line charts, bar charts - objects that are "instances of much more general objects". His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the "grammar of graphics". 

<!-- There are other graphics systems (namely, `lattice` in R, `seaborn` in Python, and some web-based rendering engines like `Observable` or `d3`) that you could explore, but it's far more important that you know how to functionally create plots in R and/or Python. I don't recommend you try to become proficient in all of them. **Pick one (two at most) and get familiar with those libraries, then google for the rest.** -->

Before we delve into the grammar of graphics, let's motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we've known it for 100 years[@croxtonBarChartsCircle1927a], so in the interests of showing that we know that pie charts are awful, we'll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We'll talk about what makes pie charts terrible at the end of this module in [Creating Good charts](@sec-creating-good-charts).

::: {.callout-caution}
### Example: Generations of Pokemon

::: column-margin
![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/98/International_Pok%C3%A9mon_logo.svg/1280px-International_Pok%C3%A9mon_logo.svg.png){fig-alt="Pokemon logo"}
:::

Suppose we want to explore Pokemon. There's not just the original 150 (gotta catch 'em all!) - now there are over 1000! 
Let's start out by looking at the proportion of Pokemon added in each of the 8 generations.

::: panel-tabset

#### R setup {-}
```{r poke-read-data-r}
#| message: false
library(readr)
library(dplyr)
library(tidyr)
library(stringr)

# Setup the data
poke <- read_csv("data/pokemon_ascii.csv", na = '.') %>%
  mutate(generation = factor(generation))
```

#### Python setup {-}
```{python poke-read-data-py}
import pandas as pd
poke = pd.read_csv("data/pokemon_ascii.csv")
poke['generation'] = pd.Categorical(poke.generation)
```
:::

Once the data is read in, we can start plotting:

::: panel-tabset

#### ggplot2 {-}
In ggplot2, we start by specifying which variables we want to be mapped to which features of the data. 

In a pie or stacked bar chart, we don't care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single "stack". So it's easiest to specify our x variable as a constant, "". We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable. 

Then, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want -- that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use `geom_bar`, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, "dodge").

```{r gg-bar}
library(ggplot2)

ggplot(aes(x = "", fill = generation), data = poke) + 
  geom_bar(position = "stack") 
```

If we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the "y" coordinate, and that we want to start at $\theta = 0$. 

```{r gg-pie, message = F}
ggplot(aes(x = "", fill = generation), data = poke) + 
  geom_bar(position = "stack") + 
  coord_polar("y", start = 0)
```

Notice how the syntax and arguments to the functions didn't change much between the bar chart and the pie chart? That's because the `ggplot` package uses what's called the **grammar of graphics**, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it's very easy to switch between plot types by changing one word or adding one transformation.

#### Base R {-}

Let's start with what we want: for each generation, we want the total number of pokemon. 

To get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.

```{r base-pie}
# Create summary of pokemon by type
tmp <- poke %>%
  group_by(generation) %>%
  count() 

pie(tmp$n, labels = tmp$generation)
```

We could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.

```{r base-barplot}
# Create summary of pokemon by type
tmp <- poke %>%
  group_by(generation) %>%
  count() 

# Matrix is necessary for a stacked bar chart
matrix(tmp$n, nrow = 8, ncol = 1, dimnames = list(tmp$generation)) %>%
barplot(beside = F, legend.text = T, main = "Generations of Pokemon")
```

There's not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.


#### Matplotlib {-}

Let's start with what we want: for each generation, we want the total number of pokemon. 

To get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.

```{python plt-pie}
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

# Create summary of pokemon by type
labels = list(set(poke.generation)) # create labels by getting unique values
sizes = poke.generation.value_counts(normalize=True)*100

# Draw the plot
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)
ax1.axis('equal')
plt.show()
```


We could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.

```{python plt-bar}
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

# Create summary of pokemon by type
labels = list(set(poke.generation)) # create labels by getting unique values
sizes = poke.generation.value_counts()
sizes = sizes.sort_index()

# Find location of bottom of the bar for each bar
cumulative_sizes = sizes.cumsum() - sizes
width = 1

fig, ax = plt.subplots()

for i in sizes.index:
  ax.bar("Generation", sizes[i], width, label=i, bottom = cumulative_sizes[i])

ax.set_ylabel('# Pokemon')
ax.set_title('Pokemon Distribution by Generation')
ax.legend()

plt.show()
```

#### Plotnine {-}

As of September 2022, pie charts are [still not supported in plotnine](https://github.com/has2k1/plotnine/issues/10). So this demo will fall a bit flat. 

```{python gg-bar-py}
from plotnine import *
plt.cla() # clear out matplotlib buffer

ggplot(aes(x = "1", fill = "generation"), data = poke) + geom_bar(position = "stack")
```

<!-- #### Seaborn {-} -->

<!-- As with Plotnine, seaborn does not support pie charts due to the same underlying issue. The best option is to create a pie chart in matplotlib and use seaborn colors [@zachHowCreatePie2021].  -->

<!-- ```{python seaborn-poke-pie} -->
<!-- # Load the seaborn package -->
<!-- # the alias "sns" stands for Samuel Norman Seaborn  -->
<!-- # from "The West Wing" television show -->
<!-- import seaborn as sns -->
<!-- import matplotlib.pyplot as plt -->

<!-- # Initialize seaborn styling; context -->
<!-- sns.set_style('white') -->
<!-- sns.set_context('notebook') -->

<!-- plt.cla() # clear out matplotlib buffer -->

<!-- # Create summary of pokemon by type -->
<!-- labels = list(set(poke.generation)) # create labels by getting unique values -->
<!-- sizes = poke.generation.value_counts(normalize=True)*100 -->

<!-- #define Seaborn color palette to use -->
<!-- colors = sns.color_palette('pastel')[0:5] -->

<!-- # Draw the plot -->
<!-- fig1, ax1 = plt.subplots() -->
<!-- ax1.pie(sizes, labels = labels, colors = colors, autopct='%1.1f%%', startangle = 90) -->
<!-- ax1.axis('equal') -->
<!-- plt.show() -->
<!-- ``` -->

<!-- Seaborn doesn't have the `autopct` option that matplotlib uses, so we have to aggregate the data ourselves before creating a barchart. -->

<!-- ```{python seaborn-poke-pie} -->
<!-- poke['generation'] = pd.Categorical(poke['generation']) -->

<!-- plt.cla() # clear out matplotlib buffer -->
<!-- sns.countplot(data = poke, y = 'status') -->
<!-- plt.show() -->
<!-- ``` -->
:::

:::

We'll talk first about the general idea behind the grammar of graphics. For each concept, I'll provide you first with the ggplot grammar of graphics code, and then, where it is possible to replicate the chart easily in base R or Python graphics, I will provide code for that as well - so that you can compare the approaches, but also so that you get a sense for what is easy and what is possible in each plotting system. 



## The Grammar of Graphics

In the grammar of graphics, a plot consists of several mostly independent specifications:

1. **aesthetics** - links between data variables and graphical features (position, color, shape, size)
2. **layers** - geometric elements (points, lines, rectangles, text, ...) 
3. **transformations** - transformations specify a functional link between the data and the displayed information (identity, count, bins, density, regression). Transformations act on the variables. 
4. **scales** - scales map values in data space to values in the aesthetic space. Scales change the coordinate space of an aesthetic, but don't change the underlying value (so the change is at the visual level, not the mathematical level). 
5. **coordinate system** - e.g. polar or Cartesian
6. **faceting** - facets allow you to split plots by other variables to produce many sub-plots.
7. **theme** - formatting items, such as background color, fonts, margins...

We can contrast this with other plotting systems (e.g. Base R, matplotlib, seaborn), where transformations and scales must be handled manually (or are handled differently in each type of plot), there may be separate plotting systems for different coordinate systems, etc. 

::: callout-note
Working with a full data frame can sometimes be a pain, because you may end up with labels that are repeated many, many times. As with any system, you should ensure you're formatting your data in a way that is consistent and compatible with the underlying philosophy.
:::

Functionally, the biggest difference between the two systems is that in the grammar of graphics system (as implemented in ggplot2), we work with a full tabular data set. As with the rest of the tidyverse, ggplot2 will allow you to reference bare column names as if they were variables, so long as you've passed in the data set to the `data =` argument.

::: column-margin
![Building a masterpiece, by Allison Horst](https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/ggplot2_masterpiece.png){fig-alt="A fuzzy monster in a beret and scarf, critiquing their own column graph on a canvas in front of them while other assistant monsters (also in berets) carry over boxes full of elements that can be used to customize a graph (like themes and geometric shapes). In the background is a wall with framed data visualizations. Stylized text reads “ggplot2: build a data masterpiece.”"}
:::

::: panel-tabset
We'll use the [`palmerpenguins` package](https://allisonhorst.github.io/palmerpenguins/) in R to do this visualization demo [@palmerpenguins]. Palmerpenguins also exists as a python package which is installable using pip.

### R data setup
```{r graphics-data-setup-r}
if (!"palmerpenguins" %in% installed.packages())
  install.packages("palmerpenguins")
data(penguins, package = "palmerpenguins")
head(penguins)
```

### Python data setup
Remember to run `pip3 install palmerpenguins` if you haven't already.

```{python graphics-data-setup-py}
from palmerpenguins import load_penguins
penguins = load_penguins()
penguins.head
```
:::


### Basic Plot Components - Axes and Points

Let's start out with a basic scatterplot: we want to use x and y locations to show two variables, and we want to use points to indicate the space where the x location and y location meet in the Cartesian plane.

::: panel-tabset

#### ggplot2

Let's start out with a basic scatterplot: we want to use x and y locations to show two variables, and we want to use points to indicate the space where the x location and y location meet in the Cartesian plane. 

```{r aes-basic-gg}
library(ggplot2)
# This defines a blank coordinate plane
ggplot(data = penguins) + 
  aes(x = bill_length_mm, y = body_mass_g)

# This plot actually has points!
ggplot(data = penguins) + 
  aes(x = bill_length_mm, y = body_mass_g) + 
  geom_point() # add points
```

The `aes()` statement can also go inside of the `ggplot()` statement or inside of `geom_point(aes(...))`. It's useful to show it outside of the `ggplot()` statement to show you exactly how the plot is built up, but most people write the code as `ggplot(aes(x = ..., y = ...), data = ...) + geom_point()` by convention.

#### plotnine

```{python aes-basic-gg-py}
from plotnine import *
# Define a blank coordinate plane
ggplot(data = penguins) + aes(x = "bill_length_mm", y = "body_mass_g")

# Add a points layer
ggplot(data = penguins) + aes(x = "bill_length_mm", y = "body_mass_g") + geom_point()
```

#### Base R

In base R, we have to use `df$var` notation to reference the variables. 

```{r aes-basic-baser}
plot(x = penguins$bill_length_mm, y = penguins$body_mass_g)

# We can also use formula notation, which allows a data = ... argument
plot(body_mass_g ~ bill_length_mm, data = penguins)
```

#### Matplotlib

```{python aes-basic-plt}
import numpy as np
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

plt.scatter(penguins.bill_length_mm, penguins.body_mass_g)
plt.show()
```

:::


### Adding Labels and Titles


::: panel-tabset

#### ggplot2

```{r aes-label-gg}
library(ggplot2)
# This defines a blank coordinate plane
ggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + 
  # add points
  geom_point() + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
```

You can even add labels that have math symbols if you are careful about how you do it (or if you use the [`latex2exp` package](https://www.stefanom.io/latex2exp/) [@latex2exp]).

#### plotnine

```{python aes-label-gg-py}
from plotnine import *

(ggplot(aes(x = "bill_length_mm", y = "body_mass_g"), data = penguins) + 
  geom_point() + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
)

```

#### Base R

```{r aes-label-baser}
plot(x = penguins$bill_length_mm, y = penguins$body_mass_g,
     main = "Penguin Bill Length and Body Mass",
     xlab = "Bill Length (mm)", ylab = "Body Mass (g)")
```

#### Matplotlib

```{python aes-label-plt}
import numpy as np
import matplotlib.pyplot as plt

plt.title("Penguin Bill Length and Body Mass")
plt.scatter(penguins.bill_length_mm, penguins.body_mass_g)
plt.xlabel("Bill Length (mm)")
plt.ylabel("Body Mass (g)")
plt.show()
```

:::

### Mapping vs. Constant Aesthetics

When considering other aesthetics, such as the color or size of plotted objects, the shape of points, line types, and alpha blending, it is important to differentiate between mapping these quantities to **dataset variables** and setting **constant values**. 

Mapping quantities to a variable in the dataset requires considerably more work under the hood, as we have to set up a scale from the original values that maps to the aesthetic values, we have to generate the corresponding aesthetic values for each data point, and we usually want to create a legend. 

When we set constant values, as in @sec-gog-constant-values, we just... set the value and we're done. Constant values are usually set to adhere to style guides, to enhance visual appeal, and sometimes to "backwards-engineer" a scale (but this is only necessary in limited circumstances).

### Mapping Categorical Variables

::: column-margin
For those of you used to British spellings, in ggplot2, all variations on both `colour` and `color` work for any parameter.
:::

If the goal is to map an aesthetic to a variable in the dataset, then we need to ensure that each geometric object has an appropriately computed mapping (and each object is plotted with that mapping). This is more computationally complex than just setting a constant value for all objects plotted.

In grammar of graphics terminology, this is an **aesthetic mapping**, and in `ggplot2` and `plotnine`, aesthetic mappings go inside `aes()` statements. 

Here, I'll demonstrate mapping variables to color, shape, and size aesthetics, but the process is similar for other aesthetics, such as linetype, fill, etc.


::: column-margin
Note that `color` is the outside border of a shape, `fill` is the inside. So if you're working on a bar plot, you probably want to map things to fill instead of color. If you're working on a scatterplot, color is probably what you want instead of fill.
:::

::: panel-tabset

#### ggplot2

```{r aes-mapping-gg}
library(ggplot2)
# This defines a blank coordinate plane
ggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + 
  # add points
  geom_point(aes(shape = species, color = species)) + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
```

Notice that we haven't had to specify any sort of categorical mapping here - ggplot picks the shapes and colors we're using based on the number of categories we have. If we want to customize these default mappings, we can use `scale_color_discrete`, `scale_color_manual`, `scale_shape_discrete`, and so on. Only if we want to override the default mapping do we have to specify that we're working with a discrete variable.

#### plotnine

```{python aes-mapping-gg-py}
from plotnine import *

(ggplot(aes(x = "bill_length_mm", y = "body_mass_g"), data = penguins) + 
  geom_point(aes(shape = "species", color = "species")) + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
)

```

#### Base R

I've used code from Jenny Bryan's Stat 545 at UBC website [@bryanPuttingColorsWork] and modified it, since I'm not an expert in base graphics. Regardless, the whole thing feels very clunky to me relative to the grammar-of-graphics approach in ggplot2/plotnine.

```{r aes-mapping-baser}
library(RColorBrewer)
# Create color and point mapping between data and plotted values
aes_mapping <- data.frame(
  species = unique(penguins$species), 
  color = brewer.pal(nlevels(penguins$species), name = 'Dark2'),
  shape = 1:3)

plot(x = penguins$bill_length_mm, y = penguins$body_mass_g,
     col = aes_mapping$color[match(penguins$species, aes_mapping$species)],
     pch = aes_mapping$shape[match(penguins$species, aes_mapping$species)],
     main = "Penguin Bill Length and Body Mass",
     xlab = "Bill Length (mm)", ylab = "Body Mass (g)")
# Manually create legend
legend(x = 'bottomright', 
       legend = as.character(aes_mapping$species),
       col = aes_mapping$color, pch = aes_mapping$shape, bty = 'n', xjust = 1)
```

#### Matplotlib

```{python aes-mapping-plt}
import numpy as np
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

penguins['species'] = pd.Categorical(penguins.species)

# Create categorical mapping to marker type
marker_types = {'Adelie':'o', 'Chinstrap':'*', 'Gentoo':'+'}
marker_color = {'Adelie':'orange', 'Chinstrap':'green', 'Gentoo':'purple'}

groups = penguins.groupby('species')
for name, group in groups:
  plt.plot(group.bill_length_mm, group.body_mass_g, marker = marker_types[name], color = marker_color[name], linestyle = '', label = name)

plt.xlabel("Bill Length (mm)")
plt.ylabel("Body Mass (g)")
plt.legend()
plt.show()
```

Python's dict type helps a lot here: we can create the mapping between species and shape/color a bit more naturally. But it's still a lot of details to think about and customize, where ggplot2 tries very hard to give you sensible default color/shape mappings that you don't have to customize unless the defaults aren't what you want.

:::

### Mapping Continuous Variables

Above, we demonstrated mapping to categorical variables, where we had to select an appropriate color for each unique value in the variable. With continuous mappings, however, we have to instead specify a range of output values (e.g. color, size) and map those values to the values in the dataset. This is understandably more complicated: 

- We need some sort of one-to-one function from our variable's values to our color space, but it doesn't have to be linear (and in many cases, we may want to use a transformation).
- We need a way to interpolate between a vector of color values to get a continuous color space. Color spaces are [complicated](https://en.wikipedia.org/wiki/Color_space) [@wiki-color-space].


::: panel-tabset

#### ggplot2

```{r aes-mapping2-gg}
library(ggplot2)
# This defines a blank coordinate plane
ggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + 
  # add points
  geom_point(aes(color = bill_depth_mm)) + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
```


#### plotnine

```{python aes-mapping2-gg-py}
from plotnine import *

(ggplot(aes(x = "bill_length_mm", y = "body_mass_g"), data = penguins) + 
  geom_point(aes(color = "bill_depth_mm")) + 
  ggtitle("Penguin Bill Length and Body Mass") + 
  xlab("Bill Length (mm)") + 
  ylab("Body Mass (g)")
)

```

#### Base R

I found one solution at [StackOverflow](https://stackoverflow.com/questions/18827214/one-colour-gradient-according-to-value-in-column-scatterplot-in-r), but gave up getting it to work in the interests of finishing the rest of this chapter. 
<!-- XXX TODO XXX -->

```{r aes-mapping2-baser, include = F}
library(RColorBrewer)
cols <- brewer.pal(9, "Blues") # get colors
pal <- colorRampPalette(cols) # Define color palette

# Create interval of color levels corresponding to penguin bill depth
bill_depth_intervals <- seq(min(penguins$bill_depth_mm, na.rm = T), 
                            max(penguins$bill_depth_mm, na.rm = T), 
                            length.out = 100)
# Match values to bill depth intervals
value_match <- findInterval(penguins$bill_depth_mm, bill_depth_intervals)
# Translate value into color
bill_depth_color <- pal(100)[value_match]
# Color NAs
bill_depth_color[is.na(bill_depth_color)] <- "#FF0000" # red

# Do the same thing, but for the legend
leg_intervals <- seq(min(penguins$bill_depth_mm, na.rm = T),
                     max(penguins$bill_depth_mm, na.rm = T),
                     length.out = 8)
leg_color <- pal(8)

plot(x = penguins$bill_length_mm, y = penguins$body_mass_g,
     col = bill_depth_color,
     main = "Penguin Bill Length and Body Mass",
     xlab = "Bill Length (mm)", ylab = "Body Mass (g)")
legend(x = 31, 
       y = 6500, 
       legend = leg_intervals,
       col = leg_color, 
       pch = 16, 
       bty = "o",
       title = "Bill Depth (mm)",
       xpd = TRUE)
```

#### Matplotlib

```{python aes-mapping2-plt}
import numpy as np
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

fig, ax = plt.subplots()

scatter = ax.scatter(penguins.bill_length_mm, penguins.body_mass_g, c = penguins.bill_depth_mm, cmap = 'Greens')

# Produce a legend for the ranking (colors).
legend1 = ax.legend(*scatter.legend_elements(num=8),
                    loc="upper left", title="Bill Depth")
ax.add_artist(legend1)

plt.show()
```
:::

::: callout-tip
#### Try it out

::: panel-tabset

##### Problem

Use the Pokemon data to see if there is a relationship between a Pokemon's attack and special attack points (`attack` and `sp_attack`, respectively). Can you map a Pokemon's weight in kg to the point opacity (`alpha`) so that light Pokemon show up as semi-transparent?

##### R solution
```{r poke-tryitout-r}
library(ggplot2)

poke <- read_csv("data/pokemon_ascii.csv", na = '.') 
ggplot(data = poke, aes(x = attack, y = sp_attack, alpha = weight_kg)) + geom_point()
```

##### Python solution
```{python poke-tryitout-py}
from plotnine import *

poke = pd.read_csv("data/pokemon_ascii.csv")
poke['weight_kg'] = pd.to_numeric(poke.weight_kg, errors='coerce')

# Get rid of NA points - they mess up the scales
poke_sub = poke.dropna(axis = 0, subset=['weight_kg', 'attack', 'sp_attack'])
ggplot(data = poke_sub) + geom_point(aes(x = "attack", y = "sp_attack", alpha = "weight_kg"))
```
:::
:::

### Customizing Appearance with Constant Values {#sec-gog-constant-values}

Here I'm focusing on characteristics like color and alpha value, but you can customize all sorts of different parameters, depending on the plot type - line size and type, point size, and more.

::: panel-tabset

#### ggplot2

```{r gg-adjust-aes-const}
ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(color = "blue", alpha = .5, size = 4, shape = 6)
```

#### plotnine

```{python gg-adjust-aes-const-py, message = F}
(
  ggplot(penguins, aes(x = "bill_length_mm", y = "body_mass_g")) +
  geom_point(color = "blue", alpha = .5, size = 4, shape = 6)
)
```

#### Base R
Base R doesn't support alpha blending by default, so we have to load the `scales` package in order to get that functionality.
```{r base-adj-aes-const}
library(scales)
# Using constant alpha:
plot(body_mass_g ~ bill_length_mm, 
     col = alpha("blue", .5),
     pch = 6,
     cex = 3,
     data = penguins) 
```

#### Matplotlib

```{python plt-adj-aes-const}
import numpy as np
import matplotlib.pyplot as plt
plt.cla() # clear out matplotlib buffer

plt.scatter(penguins.bill_length_mm, penguins.body_mass_g, c = 'blue', marker = '^', alpha = .5, s = 200)
plt.show()
```

:::


::: callout-tip
### Try it out: Debugging plots

::: panel-tabset

#### Problem

Can you fix the following plot so that it has blue points? 

What mistake was made, and why did the plot end up having pink points?

```{r debugging-plots}
penguins %>%
ggplot(aes(x = bill_length_mm, y = body_mass_g, color = "blue")) +
  geom_point()
```

#### Solution

```{r debugging-plots-solution}
ggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + geom_point(color = "blue")
```

In the original code, `color = "blue"` was inside the AES statement, which set the color aesthetic to a string "blue". ggplot2 maps this string to a default value (the reddish-pink color). If we move the `color = "blue"` statement into `geom_point()`, then this specifies that we want all points to be a constant blue color, instead of mapping color to a variable that has constant value "blue" (which isn't interpreted by ggplot2 to indicate a color).

:::
:::

### Layers


One of the main advantages of ggplot2 is that the syntax is basically consistent across very different types of plots. In base R and matpotlib, this is not the case - you have to look up the available options for each different plot type. In ggplot2, I might have to look up what the aesthetic names are for a specific geom, but I can guess most of the time. So let's look a bit more into what ggplot2's approach to graph specification is and what it allows us to do.

You're fairly used to the syntax of the pipe by now; but ggplot works on a slightly different (but similar) concept that we've used implicitly up until this point. There is the initial plot statement, `ggplot()`, and successive layers are added using `+`. 

You can specify a data set and aesthetic variables in the `ggplot()` statement (which is what we'll usually do), but you can also have a completely blank `ggplot()` statement and specify your aesthetic mappings and data sets for each layer separately. This approach is more useful when you start creating complex plots, because you may need to plot summary information and raw data, or e.g. separate tables with city information, geographic boundaries, and rivers, all of which need to be represented in the same map. Technically, you can even specify the `aes()` statement outside of the `ggplot()` statement or a `geom_...()` layer call... but this is not typically done because auto complete then doesn't work.

In this extended example, we'll examine the different features we need to make a map and how to add new layers to a map. We'll also look at some new geoms: `geom_polygon` and `geom_path`.

#### Data Setup

Let's use some data from the state of Nebraska to generate a state map with important geographic features: state parks, railroads, and counties.

- [State parks](https://www.nebraskamap.gov/datasets/outdoornebraska::state-park-areas-1/explore?location=41.485819%2C-99.571578%2C7.17)  (I downloaded the GeoJSON file)
- [Railroads](https://www.nebraskamap.gov/datasets/railroads/explore?location=41.929514%2C-102.243474%2C6.05) (I downloaded the GeoJSON file)
- [County boundaries](https://www.nebraskamap.gov/datasets/county-boundaries/explore?location=41.369872%2C-99.634627%2C6.00)

::: panel-tabset
##### R

```{r read-in-neb-map-data-r}
if(!"sf" %in% installed.packages()) install.packages("sf")
library(sf)
library(ggplot2)

ne <- map_data("state", "nebraska") # state outline, built into R
ne_parks <- read_sf("data/State_Park_Areas.geojson")
ne_railroads <- read_sf("data/Railroads.geojson")
ne_counties <- read_sf("data/County_Boundaries.geojson")
```

##### Python
```{python read-in-neb-map-data-py}
# This installs the package in the current python environment
# %pip install geopandas
# (uncomment the line above to use)
import geopandas as gp

ne = r.ne # state outline (steal from R)
ne_parks = gp.read_file("data/State_Park_Areas.geojson")
ne_railroads = gp.read_file("data/Railroads.geojson")
ne_counties = gp.read_file("data/County_Boundaries.geojson")
```

:::

#### Data Exploration

We're working with GIS data (geographical information systems), which has a different format than most of the data we've worked with thus far. Geographic data tends to be nested, so that e.g. a railroad segment will contain a set of lat/long coordinates that define the segment, but there may be many different segments in a continuous track. 

Let's see how each of these data types are represented by doing a bit of EDA. 

First, let's look at the `ne` object, which contains the coordinates for the border of the state.

::: panel-tabset

##### R
```{r explore-map-data-r1}
head(ne)
```

##### Python
```{python explore-map-data-py}
ne.describe
```
:::

We have a series of latitude and longitude values, with a group variable and an order. That is, `group` defines which piece of the state we're working with (this is especially important in the case of states with disjoint regions, such as Michigan or Hawaii), and `order` defines the order in which the points are to be connected. This is important - it matters which order we connect the points.

The `ne` object is a bit lower-level than the objects containing the spatial information about parks, railroads, and counties. We've used the `sf` and `geopandas` packages, which store all of the data from the geojson file in a table-like structure, but under the hood, these objects are a bit more complicated. We're going to completely ignore these complexities for the moment.

::: {.panel-tabset}
##### R
```{r explore-map-data-r2, collapse = F}
head(ne_parks)
head(ne_railroads)
head(ne_counties)
```

##### Python
```{python explore-map-data-py2, collapse = F}
ne_parks.describe
ne_railroads.describe
ne_counties.describe
```
:::

#### Plotting Regions

States are a good example of polygons - they're a set of points which are connected and define the border of a region. Now, Nebraska is pretty boring, because we don't have any islands (ok, you could probably fill in a lot of things instead of islands in that sentence). But, in general, it's important to make sure that when dealing with generic polygons, we only connect points from the same polygon. 

If you neglect point order and grouping, you may end up with a map like this:

```{r fail-connected-polygons-r, echo = F}
#| fig-width: 5
#| fig-height: 4
#| out-width: "50%"
#| fig-align: center
usa <- map_data("state", region = ".")
ggplot() + 
  geom_polygon(data = usa, aes(x = long, y = lat), fill = "white", color = "black") + 
  coord_map()
```


::: panel-tabset
##### R
```{r polygon-r-state}
#| out-width: "50%"
library(ggplot2)
library(mapproj)

ggplot() + 
  geom_polygon(data = ne, aes(x = long, y = lat, group = group), fill = "white", color = "black") + 
  coord_map() # This forces the right aspect ratio
```

##### Python
```{python polygon-py-state}
#| out-width: "50%"
from plotnine import *

ggplot() + geom_polygon(aes(x = "long", y = "lat", group = "group"), data = ne, fill = "white", color = "black")
# coord_map isn't implemented in plotnine :(
```
:::

#### Plotting counties

::: panel-tabset
##### R
```{r polygon-r-county}
library(ggplot2)
library(mapproj)

ggplot() + 
  # geom_sf automatically uses lat/long and the type of data to choose the right geom
  geom_sf(data = ne_counties) + 
  coord_sf() # This forces the right aspect ratio
```

We could even add annotations to the counties showing what the county names are, if we want to do so - this requires a call to `geom_sf_text`, and an additional aesthetic mapping to label (the value of the text we're printing). The geographic centroid operation is handled automatically behind the scenes.

```{r polygon-r-county2}
ggplot() + 
  # geom_sf automatically uses lat/long and the type of data to choose the right geom
  geom_sf(data = ne_counties) + 
  geom_sf_text(data = ne_counties, aes(label = Cnty_Name), size = 2) + 
  coord_sf() # This forces the right aspect ratio
```

##### Python
```{python polygon-py-county}
from plotnine import *
import geopandas as gp

ggplot() + geom_map(data = ne_counties)
```

In python, it requires a bit more work to handle county names - we have to define a function to give us the centroid of each region. 

```{python polygon-py-county2}
from plotnine import *
import geopandas as gp

# from plotnine docs: https://plotnine.readthedocs.io/en/stable/generated/plotnine.geoms.geom_map.html?highlight=coord_map
def calculate_center(df):
  """
  Calculate the centre of a geometry

  This method first converts to a planar crs, gets the centroid
  then converts back to the original crs. This gives a more
  accurate
  """
  original_crs = df.crs
  planar_crs = 'EPSG:3857'
  return df['geometry'].to_crs(planar_crs).centroid.to_crs(original_crs)

ne_counties['center'] = calculate_center(ne_counties)

ggplot() + geom_map(data = ne_counties, fill = "#f0f0f0") +\
geom_text(ne_counties, aes("center.x", "center.y", label = 'Cnty_Name'), size = 5)

```
:::


#### Adding Polygons

Next, let's add some park information to our map. 

::: panel-tabset
##### R
```{r polygon-r-park}
library(ggplot2)
library(mapproj)

ggplot() + 
  # geom_sf automatically uses lat/long and the type of data to choose the right geom
  geom_sf(data = ne_counties) + 
  geom_sf(data = ne_parks, fill = "green", color = "darkgreen") + 
  coord_sf() # This forces the right aspect ratio
```


##### Python
```{python polygon-py-park}
from plotnine import *
import geopandas as gp

ggplot() +\
geom_map(data = ne_counties, fill = "#fafafa") +\
geom_map(data = ne_parks, color = "darkgreen", fill = "green")
```
:::

#### Adding Lines

So far, everything we've plotted on our map has been related to polygons. Not all geometric objects are polygons, though - railroads and rivers are (usually) lines, and points of interest may be actual points and not spatial regions. We can add this additional information by including extra layers on our plot.


::: panel-tabset
##### R
```{r polygon-r-railroad}
library(ggplot2)
library(mapproj)

ggplot() + 
  # geom_sf automatically uses lat/long and the type of data to choose the right geom
  geom_sf(data = ne_counties, fill = "white", color = "grey80") + 
  geom_sf(data = ne_parks, fill = "green", color = "darkgreen") + 
  geom_sf(data = ne_railroads, color = "black") + 
  coord_sf() # This forces the right aspect ratio
```


##### Python
```{python polygon-py-railroad}
from plotnine import *
import geopandas as gp

ggplot() +\
geom_map(data = ne_counties, fill = "#ffffff", color = "#f0f0f0") +\
geom_map(data = ne_parks, color = "darkgreen", fill = "green") +\
geom_map(data = ne_railroads, color = "black")
```
:::


::: {.callout-tip collapse=true}
### Try it out: Maps of Middle Earth

Dedicated fans have re-created middle earth in digital format using ArcGIS files. These map file formats, called shape files, can be read into R and plotted.

You may need to install a few spatial packages first ([Mac and Windows](http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS1_SpatialDataTypes_part0_setup.html), [Linux](https://philmikejones.me/tutorials/2014-07-14-installing-rgdal-in-r-on-linux/))

The `sf` package in R contains a special geom, `geom_sf`, which will plot map objects with an appropriate geom, whether they are points, lines, or polygons. In complicated maps with many layers, this is a really awesome feature. Similarly, if you have `geopandas` in python, `plotnine`'s `geom_map` function will work with geopandas tables. 

I've provided some code to get you started, but there are many other shapefiles in the dataset. Pick some layers which you think are interesting, and plot them with appropriate geoms to make a map of Middle Earth. 

Unfortunately, in this map there is not an underlying polygon (the coastline is a series of shorter segments). To resolve this, I have provided a theme statement that will have a white background, so that you can add useful layers without the grey grid background. 

```{r middle-earth-map}
library(ggplot2)
library(ggthemes)
library(sf)

if (!dir.exists("data")) mkdir("data")

if (!file.exists("data/MiddleEarthMap.zip")) {
  download.file("https://github.com/jvangeld/ME-GIS/archive/master.zip", "data/MiddleEarthMap.zip", mode = "wb")
}
if (!dir.exists("data/ME-GIS-master/")) {
  unzip("data/MiddleEarthMap.zip", exdir = "data/")
}

coastline <- read_sf("data/ME-GIS-master/Coastline2.shp")
cities <- read_sf("data/ME-GIS-master/Cities.shp")
forests <- read_sf("data/ME-GIS-master/Forests.shp")
lakes <- read_sf("data/ME-GIS-master/Lakes.shp")
rivers <- read_sf("data/ME-GIS-master/Rivers.shp")
roads <- read_sf("data/ME-GIS-master/Roads.shp")

ggplot() + 
  geom_sf(data = coastline) + 
  geom_sf(data = forests, color = NA, fill = "darkgreen", alpha = .2) + 
  geom_sf(data = rivers, color = "blue", alpha = .1) + 
  geom_sf(data = lakes, fill = "blue", color = NA, alpha = .2) + 
  theme_map()
```

:::



### Statistics and Plot Types

At this point, we've primarily looked at charts which have two continuous variables - scatter plots and line plots. There are a number of situations where these types of charts are inadequate. For one thing, we might want to only look at the distribution of a single variable. Or, we might want to look at how a continuous response variable changes when the level of a categorical variable changes. In this section, we'll hit the most common types of plots, but there are almost infinite variations. Sites like [the Data Viz Catalogue](https://datavizcatalogue.com/search.html) can be useful if you're trying to accomplish a specific task and want to know what type of plot to use. 

In all of the plots which we discuss in this section, there is a statistical function applied to the data before plotting. So while you may specify e.g. `x = var1`,  what is plotted is `f(var1)`, where `f()` might be the mean/median/quartiles, a binned count, or a computed kernel density. 

In ggplot2, you can formally specify a statistic by using `stat_xxx` functions, but many geoms implicitly call these same functions. 


::: {.callout-note collapse=true}
### Introducing Seaborn: higher level Python plotting
It's at this point in the chapter that I gave up trying to do everything in `matplotlib`. `Matplotlib` is the low-level plotting library, and when computing statistics and creating these types of plots, it's ... not ideal. I will instead demonstrate `plotnine`, which is grammar-of-graphics style, and [`seaborn`](https://seaborn.pydata.org/) [@waskomSeaborn2022], which is another plotting library in python that isn't strictly grammar-of-graphics but is at least somewhat higher level than `matplotlib`.

While standard `seaborn` isn't a grammar-of-graphics interface, it appears that the next generation of the package will be much more grammar-of-graphics like. [@waskomNextgenerationSeabornInterface2022]

Install seaborn with `pip3 install seaborn` in your terminal.

```{python}
# Load the seaborn package
# the alias "sns" stands for Samuel Norman Seaborn 
# from "The West Wing" television show
import seaborn as sns
import matplotlib.pyplot as plt

# Initialize seaborn styling; context
sns.set_style('white')
sns.set_context('notebook')
```
:::


### Visualizing Distributions

#### Box plots

A box plot can show some summary information about the distribution of a single continuous variable, and usually is used to show differences in the level of a response variable at different levels of a categorical variable. 


Let's look at the distribution of penguin weight by species and sex. 
We'll return to this plot later when discussing how to create good charts.

::: panel-tabset

##### ggplot2
```{r boxplot-ggplot-r}
library(ggplot2)
library(palmerpenguins)
library(dplyr)

penguins %>%
  filter(!is.na(sex)) %>% # Remove unknown sex penguins
  ggplot(aes(x = species, y = body_mass_g, color = sex)) + 
  geom_boxplot(outlier.shape = NA) + 
  # We can add another layer overlaying the actual points, 
  # with some random noise added to reduce overplotting
  geom_jitter(position = position_jitterdodge(), shape = 1) + 
  ylab("Body Mass (g)") + xlab("Species") + ggtitle("Penguin Weight by Species and Sex")
```


##### plotnine
```{python boxplot-plotnine-py}
import pandas as pd
from plotnine import *
from palmerpenguins import load_penguins
penguins = load_penguins()

# Remove NAs
tmp = penguins.dropna(axis = 0, subset = 'sex')

ggplot(aes(x = "species", y = "body_mass_g", color = "sex"), data = tmp) +\
geom_boxplot(outlier_shape='') +\
geom_jitter(position = position_jitterdodge(), shape = 'o') +\
ggtitle("Penguin Weight by Species and Sex") +\
ylab("Body Mass (g)") +\
xlab("Species")

```

##### Base R

Because base R doesn't have a default way to handle jittering or putting boxplots side-by-side, it's not easy to add points on top of these boxplots.

```{r boxplot-base-r}
boxplot(body_mass_g ~ species + sex, data = penguins, 
        # Manually color boxplots
        col = rep(c("red", "blue"), times = 3))
```

##### Seaborn

```{python boxplot-seaborn}
plt.cla() # clear out matplotlib buffer
sns.boxplot(data=penguins, x = 'species', y = 'body_mass_g', hue = 'sex')
plt.show()
```

:::

#### Histograms and Density Plots

Box plots aren't the only way to show distributions. If we want to, we can show distributions using histograms or density plots. A **histogram** is created by binning the variable, then counting the number of observations that fall within each specified range.
Usually, these ranges have constant width (but not always). 

Changing the binwidth can radically change the appearance and usefulness of the histogram.


::: panel-tabset
##### ggplot2
```{r gg-hist-r}
penguins %>%
  ggplot(aes(x = body_mass_g)) + 
  geom_histogram(color = "black", fill = "grey")

penguins %>%
  ggplot(aes(x = body_mass_g)) + 
  geom_histogram(color = "black", fill = "grey", binwidth = 60)

penguins %>%
  ggplot(aes(x = body_mass_g)) + 
  geom_histogram(color = "black", fill = "grey", binwidth = 300)
```
##### plotnine
```{python plotnine-hist-py}
ggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = "body_mass_g")) + geom_histogram(color = "black", fill = "grey")
  
ggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = "body_mass_g")) + geom_histogram(color = "black", fill = "grey", binwidth = 60)
  
ggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = "body_mass_g")) + geom_histogram(color = "black", fill = "grey", binwidth = 300)
```
##### Base R

```{r base-hist-r}
hist(penguins$body_mass_g)

hist(penguins$body_mass_g, breaks = 30) # number of bins

hist(penguins$body_mass_g, 
     seq(min(penguins$body_mass_g, na.rm = T), 
         max(penguins$body_mass_g, na.rm = T), 
         by = 60)) # specific bin width
```

##### Seaborn
```{python seaborn-hist-py}
plt.cla() # clear out matplotlib buffer
sns.histplot(data = penguins, x = "body_mass_g")
plt.show()

plt.cla() # clear out matplotlib buffer
sns.histplot(data = penguins, x = "body_mass_g", binwidth = 60)
plt.show()

plt.cla() # clear out matplotlib buffer
sns.histplot(data = penguins, x = "body_mass_g", binwidth = 300)
plt.show()
```
:::

#### Violin Plots

One interesting hybrid between a boxplot and a histogram is a **violin plot**, which shows the full density of the data, reflected (usually in y) to form an object which could potentially look like a violin for e.g. bimodal data.

::: column-margin
![](https://imgs.xkcd.com/comics/violin_plots.png){fig-alt="XKCD comic showing a chart of suggestiveness of visualization types (from not very suggestive to suggestive). Violin charts are shown for pie charts (not very suggestive), line graphs (slightly more sugestive), Georgia O'Keeffe paintings (very suggestive) and violin plots (even more suggestive)"}
[Source](https://xkcd.com/1967/)
:::

Seaborn has options to split the violin plots by sex - this is  a bit trickier in ggplot2 but it [can be done](https://stackoverflow.com/questions/35717353/split-violin-plot-with-ggplot2) with the help of the `introdataviz` package available on GitHub.

::: panel-tabset

##### ggplot2
```{r violinplot-ggplot-r}
library(ggplot2)
library(palmerpenguins)
library(dplyr)

penguins %>%
  filter(!is.na(sex)) %>% # Remove unknown sex penguins
  ggplot(aes(x = species, y = body_mass_g, color = sex)) + 
  geom_violin(outlier.shape = NA) + 
  # We can add another layer overlaying the actual points, 
  # with some random noise added to reduce overplotting
  geom_jitter(position = position_jitterdodge(), shape = 1) + 
  ylab("Body Mass (g)") + xlab("Species") + ggtitle("Penguin Weight by Species and Sex")
```
```{r violinplot-ggplot-r-split}
# devtools::install_github("psyteachr/introdataviz")
library(introdataviz)

penguins %>%
  filter(!is.na(sex)) %>% # Remove unknown sex penguins
  ggplot(aes(x = species, y = body_mass_g, color = sex)) + 
  geom_split_violin() + 
  # We can add another layer overlaying the actual points, 
  # with some random noise added to reduce overplotting
  geom_jitter(position = position_jitterdodge(), shape = 1) + 
  ylab("Body Mass (g)") + xlab("Species") + ggtitle("Penguin Weight by Species and Sex")
```

##### Seaborn

```{python violinplot-seaborn}
plt.cla() # clear out matplotlib buffer
sns.violinplot(data = penguins, x = 'species', y = 'body_mass_g', hue = 'sex', split = True)
plt.show()
```



:::


#### Bar Charts

A bar chart is a plot with a categorical variable on one axis and a summary statistic on the other (usually, this is a count).
Note that a bar chart is NOT the same as a histogram (a histogram looks very similar, but has a binned numeric variable on one axis and counts on the other). Geometrically, bar charts are rectangles; typically each rectangle will have equal width and variable height.

::: panel-tabset
##### ggplot2
```{r gg-bar-r}
#| out-width: "50%"
penguins %>%
  ggplot(aes(x = species, fill = species)) + 
  geom_bar()
penguins %>%
  ggplot(aes(x = species, fill = sex)) + 
  geom_bar(position = "dodge")
penguins %>%
  ggplot(aes(x = species, fill = sex)) + 
  geom_bar(position = "stack")
```
##### plotnine

```{python plotnine-bar-py}
#| out-width: "50%"
ggplot(penguins.dropna(axis = 0, subset='species'), aes(x = "species", fill = "species")) + geom_bar()
ggplot(penguins.dropna(axis = 0, subset=['species', 'sex']), aes(x = "species", fill = "sex")) + geom_bar()
ggplot(penguins.dropna(axis = 0, subset=['species', 'sex']), aes(x = "species", fill = "sex")) + geom_bar(position = "dodge")
```
##### Base R
```{r base-bar-r}
#| out-width: "50%"
count_species <- table(penguins$species)
barplot(count_species)
count_species2 <- table(penguins$sex, penguins$species, useNA = 'ifany')
barplot(count_species2, beside = T)
```

##### Seaborn

`countplot` is a canonical barchart in seaborn. `barplot` shows the aggregated value of a y variable, with a confidence interval, which is not what is typically meant by a bar plot.

```{python seaborn-bar-py}
#| out-width: "50%"
plt.cla() # clear out matplotlib buffer
sns.countplot(data = penguins, x = "species")
plt.show()
plt.cla() # clear out matplotlib buffer
sns.countplot(data = penguins, x = "species", hue = 'sex')
plt.show()
```
:::


### Beyond Points: Lines, Rectangles, and Other Geoms

There are many situations where points aren't the best way to display data. If we have several series of data connected over e.g. time, then we might want to join our individual observations with lines that suggest continuity. Or, we may want to display a range of values over each time point, at which point we might be better off with a ribbon-like area enclosing the maximum and minimum values over time. 

In the grammar of graphics, we need to select a geometric object and then provide variable mappings for each required spatial dimension. When working with non-grammar approaches, however, these different plots sometimes use very different syntax. 

#### Lines

Let's consider the [lego set data](data/lego_sets.csv) and examine the number of sets over time.

::: panel-tabset
##### R data setup
```{r}
legos_time <- readr::read_csv("data/lego_sets.csv") %>%
  group_by(year) %>%
  count()
```

##### Python data setup
```{python}
legos_time = pd.read_csv("data/lego_sets.csv").\
  groupby("year")[["theme_id"]].\
  agg("count").\
  reset_index().\
  rename(columns = {"theme_id": "count"})
```
:::

::: panel-tabset
##### ggplot2
```{r ggplot2-line}
ggplot(legos_time, aes(x = year, y = n)) + geom_line()
```
##### plotnine
```{python plotnine-line}
ggplot(legos_time, aes(x = "year", y = "count")) + geom_line()

```
##### Base R
```{r base-line}
plot(legos_time$year, legos_time$n, type = 'l')
```

##### Seaborn

```{python seaborn-line}
plt.cla()
sns.lineplot(data = legos_time, x = "year", y = "count")
plt.show()
```

:::

#### Statistics and the Grammar of Graphics

Let's explore the different ways we can represent data using the grammar of graphics and statistics. This section will be written in R, but of course, you can also do much the same thing in plotnine.

Let's consider the case where we want to understand the joint distribution between penguin flipper length and bill depth: 
```{r penguin-joint-scatter}
#| out-width: "50%"
#| fig-align: center
ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_point()
```

We're already getting some overplotting in this data (points on top of each other), which interferes with our ability to actually see how many points there are. We could instead create a grid for the data and fill in the grid based on how many points are in each cell. 
We would be using fill/color/hue as a third "dimension", if we wanted to go that route. We lose some numerical accuracy, as we can't map points directly to colors as accurately as we do location, but we might have a better intuitive feeling for the data distribution than we get from seeing the points (and this approach scales to data sets that are bigger than the penguins data).


```{r penguin-joint-grid}
#| out-width: "50%"
#| fig-align: center
ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_bin2d()
```

While the points (or counts of points) are great, we might want to add some sort of density statistic to the mix. We can use `geom_density2d` to do this calculation. Note that we have to tell ggplot to wait until after the statistic is computed to calculate the fill (and what statistic to use). 

```{r penguin-joint-2d-dens-fill}
ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + 
  stat_density2d(aes(fill = after_stat(density)), 
                 geom = "tile", 
                 contour = F)
```

If you want to preserve some numerical precision, we could instead work on contour lines -- like elevation maps, contour plots show lines connecting densities of the same numerical value.


```{r penguin-joint-2d-dens-lines}
ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + 
  geom_density2d()
```

We can also combine fill and contour lines and plot filled polygons:

```{r penguin-joint-2d-dens-polygon}
ggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + 
  stat_density2d_filled()
```

Obviously, there's a lot of ways to generate 3D-like charts in R, but can we actually generate 3D charts? Of course! We'll use the `rayshader` package [@rayshader]. 

::: column-margin
![The rayshader package lets you create 3d maps and graphs (image by Allison Horst)](https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/rayshader.png)
:::

```{r penguin-joint-rayshader, eval = F}
library(rayshader)

my_penguin_plot <- penguins %>% 
  ggplot(aes(x = flipper_length_mm, y = bill_depth_mm)) + 
  stat_density2d(aes(fill = after_stat(density)), 
                 geom = "tile", 
                 contour = F)

plot_gg(my_penguin_plot) # Running this is very memory intensive
```

If you happen to want to get a 3D printed version of your plot, you can do that too using `rayshader`'s `save_3dprint()` function.

## Creating Good Charts {#sec-creating-good-charts}

A chart is *good* if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate. 

Unfortunately, while our visual system is *amazing*, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms. 

::: column-margin
I'm going to provide a broad overview of considerations in this section, but if you have more questions, please contact me. This is my research area, and I love to talk about it.
:::

::: {.callout-note collapse=true}
### How do we know a chart is good?

We do experiments on people! (*evil cackle*) 

- A review of graphical testing in statistics  [@vanderplasTestingStatisticalCharts2020]
- A really cool paper about testing competing graphical designs to figure out polar coordinates suck [@hofmannGraphicalTestsPower2012a]

If you're still curious after reading those, set up an appointment and let's talk!
:::

### General guidelines for accuracy

There are certain tasks which are easier for us relative to other, similar tasks. 
When making a judgement corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level. @clevelandGraphicalPerceptionTheory1984 is the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged. 

::: column-margin
```{r accuracy-guidelines-08, echo = F}
#| fig-cap: "Which of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels."
#| fig-width: 4
#| fig-height: 2
segs <- bind_rows(
  tibble(x = 1:3, y1 = 0, y2 = c(2.5, 2.75, 2.25), type = "Aligned scale"),
  tibble(x = 1:3, y1 = c(1, 0, .5), y2 = c(1, 0, .5) + c(2.5, 2.75, 2.25), type = "Unaligned scale")
)
ggplot(segs, aes(x = x, xend = x, y = y1, yend = y2)) + 
  geom_segment() + facet_wrap(~type) + coord_fixed() + 
  expand_limits(x = c(.5, 3.5)) + 
  theme_void() + 
  theme(panel.border = element_rect(color = "grey", fill = "transparent"), strip.background = element_rect(fill = "grey"))
```
:::

1. Position (common scale)
2. Position (non-aligned scale)
3. Length, Direction, Angle, Slope
4. Area
5. Volume, Density, Curvature
6. Shading, Color Saturation, Color Hue

If we compare a pie chart and a stacked bar chart, the bar chart asks readers to assess position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not recommended -- they make it harder on the reader to accurately read information off of the charts 

When creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.

There is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area. 

::: callout-caution
#### Example: Misleading Charts

```{r disproportionate-pixels, include = F}
imgs <- list.files("images/graphics", "3d_pie", full.names = T)
tmp <- tibble(portfolio = c("Cash", "Bond", "Stocks"), 
              pct = c(.05, .35, .5),
              img = purrr::map(imgs, ~png::readPNG(source = .)),
              px = purrr::map_dbl(img, ~sum(.[,,4] != 0 ))) %>%
  mutate(px_pct = px/sum(px))
tmp
```

![Here, the area and height both encode the same variable, leading to a far disproportionate number of pixels allocated to "Stocks" than "Cash Investments" [@fungWhenPieChart2020]. In the first chart, stocks make up 60% of the portfolio, but have 67.5% of the pixels; Cash makes up 5% of the portfolio but those investments represent 2.3% of the pixels.](images/graphics/3d_graphs_suck.jpg){fig-alt="A three-dimensional pie chart cylinder, where both height and angle are mapped to portfolio percent. As a result, the visual region of the chart dedicated to each category is inconsistent with the data."}

@hickey27WorstCharts2013 has some more great examples of misleading charts.

:::

::: column-margin
Here's a great Ted Ed talk about spotting misleading charts:

<iframe width="100%" height="auto" src="https://www.youtube.com/embed/E91bGT9BjYk" title="How to spot a misleading graph - Lea Gaslowitz" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
:::


Extra dimensions and other annotations are sometimes called "chartjunk" and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration). 

### Perceptual and Cognitive Factors

#### Short Term Memory

We have a limited amount of memory that we can instantaneously utilize. This mental space, called **short-term memory**, holds information for active use, but only for a limited amount of time. Without rehearsing information, short term memory lasts a few seconds. 

::: {.callout-tip}
##### Try it out

<details><summary>
Click here, read the information, and then click to hide 
</summary>
1 4 2 2 3 9 8 0 7 8</details>

<details><summary>Wait a few seconds, then expand this section</summary>
What was the third number?
</details>
:::

Without rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 "bits" of information. 

In charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.

Where possible, try to keep your legends to 6 or 7 characteristics. 


::: callout-warning
##### Important Takeaways

- Limit the number of categories in your legends to minimize the short term memory demands on your reader.
    - When using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude. 
    
- Use colors and symbols which have implicit meaning to minimize the need to refer to the legend.

- Add annotations on the plot, where possible, to reduce the need to re-read captions.
:::


#### Color
Our eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. We evolved to be able to distinguish different shades of green because it impacts our ability to find food. There aren't that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.    

::: column-margin
![Sensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)](https://upload.wikimedia.org/wikipedia/commons/c/c0/Eyesensitivity.svg)
:::

Not everyone perceives color in the same way. Some individuals are [colorblind or color deficient](https://en.wikipedia.org/wiki/Color_blindness) [@ColorBlindness2022]. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, <1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances. 


::: margin-column
::: {.callout-note collapse=true}
##### Colorblindness Test & Info
You can take a test designed to screen for colorblindness [here](https://www.eyeque.com/color-blind-test/test/)

Your monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. 

A different test is available [here](https://www.color-blindness.com/farnsworth-munsell-100-hue-color-vision-test/). 

<!-- ![My results on one monitor](images/graphics/08_colorblindness_monitorLG.png) -->
<!-- ![My results on a monitor that has a different tech and supposedly higher color fidelity](images/graphics/08_colorblindness_monitorDell.png) -->

<!-- ![The Munsell colorblindness test](images/graphics/08_colorblind_munsell.png) -->
<!-- In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.  -->

:::
:::

::: {.callout-note collapse=true}
##### Simulating Color Deficiency
Color "blindness" can arise from either a mutation in one of the three cones (causing the cone to function less optimally), or when the gene for a cone is deleted or mutated to the point where it is not functional. We can simulate what these conditions look like when a rainbow color scheme is used:

<table width="100%"><tr><td rowspan = 2 width="34%">
![Original image using a rainbow color scale](images/graphics/RainbowScaleOrig.jpg)</td><td width="22%">![Red deficient](images/graphics/RainbowScaleOrig-protanomaly.jpg)</td><td width="22%">![Green deficient](images/graphics/RainbowScaleOrig-deuteranomaly.jpg)</td><td width="22%">![Blue deficient](images/graphics/RainbowScaleOrig-tritanomaly.jpg)</td></tr><tr><td width="22%">![Red absent](images/graphics/RainbowScaleOrig-protanopia.jpg)</td><td width="22%">![Green absent](images/graphics/RainbowScaleOrig-deuteranopia.jpg)</td><td width="22%">![Blue absent](images/graphics/RainbowScaleOrig-tritanopia.jpg)</td></tr></table>
:::

In addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context. 

::: {#fig-color-constancy layout-ncol=2}

![](images/graphics/CheckerShadow.png){fig-alt="A checkered 5x5 square with a green cylinder casting a shadow on the checkered surface. A point outside the shadowed region on the dark square is labeled 'A', and a point inside the shadowed region on a light square is labeled 'B'. The two squares appear to be very different colors."}

![](images/graphics/CheckerShadow2.png){fig-alt="In this figure, the checkered surface is removed and only the labeled squares and the cylinder remain. Without the contextual information about the checkered surface and the cylinder's shadow, the two squares labeled 'A' and 'B' appear the same color."}

The color constancy illusion. The squares marked A and B are actually the same color
:::

Our brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly "remove" the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we're correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade. 

::: callout-warning
##### Important Takeaways

- **Do not use rainbow color gradient schemes**
    - Because of the unequal perception of different wavelengths, these schemes are _misleading_ - the color distance does not match the perceptual distance. 
    - Palettes such as [Viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) and those on [colorcet.com](https://colorcet.com/) are better alternatives because they are perceptually uniform [@nunezOptimizingColormapsConsideration2018,@kovesiGoodColourMaps2015,@kovesiCETPerceptuallyUniform2021].

- Avoid **green-yellow-red signaling**    
While these schemes are intuitive (go, caution, stop), they are very difficult for those with common types of colorblindness to read.

- Be conscious of **implied meaning**    
This can be used to your advantage, but you also want to avoid adding context that isn't supported by the data.
    - Common associations can make it easier for viewers    
    (but may be culturally specific)
        - blue for cold, orange/red for hot is a natural scale, 
        - red = Republican and blue = Democrat in the US
        - white -> blue gradients for rainfall totals
    - Some colors can can provoke emotional responses that may not be desirable (red = blood, black = death, yellow = happy).
    - Some color schemes have social baggage:
        - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing
        - Instead, consider using a dark color (blue or purple) for men and a light color (yellow, orange, lighter green) for women [@rostWhatConsiderWhen2018]. 

::: column-margin
When the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information. [@fieldMappingCoronavirusResponsibly2020] 
:::
:::

::: {.callout-note collapse=true}
##### Color Palette Options

- Packages such as `RColorBrewer` and `dichromat` have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (`dichromat` is better for that than `RColorBrewer`). 

- Other fun color palette generation methods [@muthYourFriendlyGuide2016,@taylorHavingTroublePicking2021] may also be useful. 
  - Artistic sources:
      - Taylor Swift album colors [@taylorswift]
      - Wes Anderson and Studio Ghibli films [@wesanderson,@ghibli]
      - famous paintings and other images [@paletter] 
  - Natural palettes: 
      - The `earthtones` package generates palettes for elevation maps based on the actual location you're showing. [@earthtones]
:::

::: {.callout-note collapse=true}
##### Colorblind Friendly Graphics
- If you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!

- **double encoding** - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out. This also makes your plot more readable [@vanderplasClustersBeatTrend2017] and helps your mapping stand out more.

- If you are using a color gradient, use a **monochromatic** color scheme where possible. This is perceived as light -> dark by colorblind people, so it will be correctly perceived no matter what color you use. 

- If you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is **purple - white - orange**.    
In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.

:::


#### Grouping and Sense-making: Imposing order on visual chaos
Take a look at each of the examples in the panels below.

::: panel-tabset

##### Ambiguous Image

![Is it a rabbit, or a duck?](images/graphics/DuckRabbitIllusion.jpg){fig-alt="An ambiguous image that could be either a duck or a rabbit head."}

When faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options. 

##### Illusory Contours

![Consider this image - what do you see?](images/graphics/IllusoryContour.png){fig-alt="An image which appears to consist of three layers, listed from the bottom to the top: three circles (top left, top right, bottom middle), a triangle pointing up between the three circles with a black border, and a white triangle (no border) of the same size which overlays both of the previous layers. As a result, there are three pac-man shapes and three angles visible, but the viewer has no trouble perceiving the three layers even though they do not directly exist."
width="40%"}

Did you see something like "3 circles, a triangle with a black outline, and a white triangle on top of that"? In reality, there are 3 angles and 3 pac-man shapes. But, it's much more likely that we're seeing layers of information, where some of the information is obscured (like the "mouth" of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience. 

##### Figure-background

Now, look at the logo for the Pittsburgh Zoo. 

![What do you see?](https://upload.wikimedia.org/wikipedia/en/5/5b/Pittsburgh_Zoo_%26_PPG_Aquarium_logo.svg){width="60%"
fig-alt="The logo for the Pittsburgh Zoo & Aquarium, which features a tree and birds; the negative space around the tree shows a gorilla and a lionness."}

Do you see the gorilla and lionness? Or do you see a tree? Here, we're not entirely sure which part of the image is the figure and which is the background. 

:::

The ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs. 

The catchphrase of Gestalt psychology is 

> The whole is greater than the sum of the parts

That is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene. 

![The Gestalt Heuristics help us to impose order on ambiguous visual stimuli](images/graphics/gestalt.jpg){fig-alt="A conceptual rendering of the word GESTALT, where G shows the principle of closure (a white bar over the G occludes part of the letter, but it is still perceived as a single unit), E shows proximity (A set of gridded squares colored black and gray are seen as a whole E), S shows continuation (a bar through the S occludes part of the letter but we perceive both the bar and the S as continuous), T shows similarity (both T's are shown in the same striped pattern), A and L show figure/ground (there is a tree within the stylized A)."}

You can read about the gestalt rules in more depth if you want to, but they are also demonstrated in the figure above [@PrinciplesGrouping2022].

In graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving "with" the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders. 

::: {.callout-caution collapse=true}
##### Example: Grouping in Graphics

```{r chart-emphasis-bar-08}
#| fig.height: 6
#| fig.width: 4
#| layout-ncol: 3
#| echo: false
#| fig-subcap:
#|   - "A bar chart"
#|   - "A line chart"
#|   - "A box plot"
#| fig-cap: Which chart best demonstrates that in every state and region, the murder rate decreased?

if (!"classdata" %in% installed.packages()) devtools::install_github("heike/classdata") 
# A package of data sets which are useful for class demonstrations
data(fbi, package = "classdata")

fbiwide <- fbi %>%
  select(-Violent.crime) %>%
  pivot_wider(names_from = Type, values_from = Count) %>%
  # Rename variables
  rename(Murder = Murder.and.nonnegligent.Manslaughter, 
         Assault = Aggravated.assault, 
         Larceny = Larceny.theft, 
         Auto.theft = Motor.vehicle.theft) %>%
  mutate(Rape = as.numeric(Rape))

fbiwide %>% 
  filter(Year %in% c(1980, 2010)) %>%
  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) + 
  geom_col(position = "dodge") +
  coord_flip() + 
  ylab("Murders per 100,000 residents") + 
  theme(legend.position = c(1,1), legend.justification = c(1,1))

fbiwide %>% 
  filter(Year %in% c(1980, 2010)) %>%
  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) + 
  geom_line() + 
  ylab("Murders per 100,000 residents")

fbiwide %>% 
  filter(Year %in% c(1980, 2010)) %>%
  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) + 
  geom_boxplot() + 
  ylab("Murders per 100,000 residents")
```

The line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn't allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can't see the individual relationships. 

:::

::: callout-warning
##### Important Takeaways
The aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) come to when examining those plots [@vanderplasClustersBeatTrend2017].
:::

## Useful Tricks

### Saving figures

::: panel-tabset
#### ggplot2
```{r save-ggplot, eval = F}
ggplot(data = penguins,
       aes(x = bill_length_mm, y = body_mass_g, color = species)) + 
  geom_point()
ggsave(filename = "images/graphics/test-penguin-ggplot2.png", 
       width = 8, height = 6, dpi = 300)
```

#### Base R
```{r save-base, eval = F}
png("images/graphics/test-penguin-baseR.png", width = 8, height = 6, units = "in", res = 300)
plot(penguins$bill_length_mm, penguins$body_mass_g) 
dev.off()
```

There are other functions available, such as `pdf`, `cairo_pdf` (which may have better unicode support), and `jpeg`. Some other R packages, such as `ragg`, provide additional ways to save R plots to files.

#### Plotnine
In plotnine, we store the plot as an object and then use the `.save()` method, which has the same arguments as `ggsave()`.

```{python}
from plotnine import *

myplot = ggplot(penguins, aes(x = "bill_length_mm", y = "body_mass_g", color = "species")) + geom_point()
myplot.save("images/graphics/test-penguin-plotnine.png", width = 8, height = 6, dpi = 300, units = "in")
```

#### Matplotlib

```{python}
import matplotlib as plt

plt.cla() # clear out matplotlib buffer
plt.figure(figsize=[8,6], dpi = 300) # set figure size
plt.scatter(penguins.bill_length_mm, penguins.body_mass_g)
plt.savefig("images/graphics/test-penguin-matplotlib.png")
```

#### Seaborn

Seaborn is built on top of matplotlib, so it works the same way.

```{python}
import matplotlib as plt
import seaborn as sns

plt.cla() # clear out matplotlib buffer
plt.figure(figsize=[8,6], dpi = 300) # set figure size
sns.scatterplot(data = penguins, x = "bill_length_mm", y = "body_mass_g", hue = "species")
plt.savefig("images/graphics/test-penguin-seaborn.png")
```

:::

### Combining Figures

We often want to combine multiple figures (or pieces of figures) together. While this topic is outside the scope of this book, I do want to at least point you in the right direction.

::: column-margin
![The patchwork package is a great way to combine figures](https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/patchwork_1.jpg)
:::

::: panel-tabset
#### R

- [patchwork](https://patchwork.data-imaginist.com/) [@patchwork]
- [cowplot](https://wilkelab.org/cowplot/index.html) [@cowplot]

#### Python

- [patchwork](https://github.com/ponnhide/patchworklib) (works with matplotlib, plotnine, and seaborn) [@moriPatchworklib2022]
- the `subplot_mosaic` function in `matplotlib`
:::

### Interactive Plots

We'll get to interactive plots later, but for now, check out 

- [plotly](https://plotly.com/) ([R](https://plotly.com/r/) and [python](https://plotly.com/python/))
- [animint2](https://github.com/tdhock/animint2) (Takes ggplot2 graphics and creates interactive d3 charts)
- [Shiny](https://shiny.rstudio.com/) (R originally, and as of July 2022, for [python](https://shiny.rstudio.com/py/) too!)


## References
