[["index.html", "Stat 850: Computing Tools for Statisticians Introduction Forward Course Description Course Goals Course Objectives How to Use This Book", " Stat 850: Computing Tools for Statisticians Susan Vanderplas Last Updated: 2021-05-10 Introduction Forward This textbook is intended to be a substitute for hours and hours of video lectures wherein I read code to you. That’s awful, and boring1, and honestly, as painful for me to have to listen to myself while editing the things as it is for you to watch them. Instead, I hope that you’ll be able to work through this book, week by week, over the course of Stat 850. I’ve included comics, snark, gifs, YouTube videos, and more, with the goal of making this a collection of the best information I could find on learning R and SAS and statistical programming. Course Description Introductions to statistical computing packages and document preparation software. Topics include: graphical techniques, data management, Monte Carlo simulation, dynamic document preparation, presentation software. Course Goals (Broad, amorphous conceptual things that are hard to measure) Learn how to use R and SAS for data analysis, data processing, and data visualization. Become familiar with the process, techniques, and goals of exploratory data analysis. Create, assess, and debug code effectively. Use online resources to find software to perform a task, comparing approaches taken by competing programs. Read error messages, find related problems in online forums, and isolate the conditions necessary to generate the error. Generate minimum working examples or reproducible examples of errors in order to ask for help effectively. Communicate statistical results using reproducible, dynamic tools. Understand the importance of reproducibility in scientific computation. Course Objectives (what you should be able to do at the end of this course) A. Clean and format the data appropriately for the intended analysis or visualization method. (Goals: 1) B. Explore a data set using numerical and visual summaries, developing questions which can be answered using statistics. (Goals: 1, 2) C. Evaluate methods or software to assess relevance to a problem. Compare similar options to determine which are more appropriate for a given application (Goals: 1, 3) D. Test and debug software, using the following sequence: (Goals: 3, 4) Reproduce the error in a new environment, Create a minimal reproducible example, Research the error message and evaluate online resources for relevance, Ask for help, describing the error or problem appropriately. E. Document the data, methods, and results of an analysis using reproducible methods. (Goals: 1, 2, 4) How to Use This Book I’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class. Header Buttons/Links The header contains a number of buttons which should help you use, customize, and navigate the textbook Open/close the sidebar TOC Search the text Set font size and white/sepia/dark mode Suggest edits if you find a typo. This will help you automatically submit a pull request in Github, and I can review and approve it easily. Download the entire (very long) Rmarkdown source code for the textbook See keyboard shortcuts to navigate the book Reference the book in a tweet, if you really want to. Tag me if you do (@srvanderplas) Find the book repository on GitHub quickly if you want to see code/data/etc. linked in the book Special Sections Try It Out Try it out sections contain activities you should do to reinforce the things you’ve just read. Watch Out Watch out sections contain things you may want to look out for - common errors, etc. Learn More Learn More sections contain other references that may be useful on a specific topic. Suggestions are welcome (use the edit link at the top to suggest a new reference that I should add), as there’s no way for one person to catalog all of the helpful R resources on the internet! Note Note sections contain clarification points (anywhere I would normally say \"note that ….) My Opinion These sections contain things you should definitely not consider as fact and should just take with a grain of salt. Expandable Sections These are expandable sections, with additional information when you click on the line This additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output. If you want boring, or have insomnia, check out this Microsoft Word tutorial from 1989. Youtube says it’s the most boring video ever made.↩︎ "],["tools.html", "Module 1 Tools for Statistical Computing Module Objectives 1.1 Setting Up Software 1.2 Looking Around Your Environment 1.3 Some Computer Basics 1.4 Using Version Control (with RStudio) 1.5 Topic Sequencing 1.6 References", " Module 1 Tools for Statistical Computing The goal of this class is to expose you to basic computing skills in R and SAS, which are two of the more common languages for statistical computing (python is the 3rd most common, and is particularly popular in data science and machine learning, but will not be explicitly taught in this class.) Since we’ll be learning how to use a bunch of different software, the first step is to get everything set up on your machine: git SAS 9.4 (or later) R (4.0 or higher) RStudio LaTeX You will also need to sign up for a GitHub account Module Objectives Set up R, Rstudio, SAS, LaTeX, and git on personal machines Detect and resolve problems related to file systems, working directories, and system paths when troubleshooting software installation Use version control to track changes to a document (git add, commit, push, pull) 1.1 Setting Up Software First, lets cover the basics of setting up the software you’re going to be using in this course. You don’t at this point have to understand what the different software DOES, but you should at least get it installed and working – this may be tricky, so we’ll take it slow. 1.1.1 SAS For SAS installation, see Steve Westerholt. He manages UNL SAS installations. You can pay a small fee to get the full SAS installation, or you can use SAS community edition (but if you take this route, it may be more difficult to manage your paths, etc., and you may not be able to use SAS Markdown on your machine). You can also use the HCC environment (we will talk more about this in week 2 or thereabouts) set up for you to be able to do your homework. As long as you can run SAS code on a machine you have regular access to, I’m fine. 1.1.2 R Note that the tutorial videos use versions of R that are old. You should be installing at least R 4.0 (if you have an older version, please update.) The basic installation process is the same, though, so the videos are still useful. R on Windows installation R on Mac installation R on Linux installation On Linux, instead of a YouTube video, you get a text-based tutorial. One of the Debian maintainers, Dirk Eddelbuettel, is also on R core, which means that R tends to work extremely well with Debian-based distributions, like Ubuntu and Linux Mint. R does work on RPM based distros, and you can of course also compile it from source for Arch/Gentoo, but I’ve stuck with Deb-based distributions for approximately 7 years because it seems to be a bit less hassle. Additional troubleshooting can be found here. Once you have R installed, try to run the following code, which will install (most of) the packages you need to run the code in this book. # Read in a list of all packages that are required pkgs &lt;- readLines(&quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/packages&quot;) # Remove packages only available on github pkgs &lt;- setdiff(pkgs, c(&quot;nycsquirrels18&quot;, &quot;emo&quot;, &quot;tweetrmd&quot;, &quot;classdata&quot;)) # Remove any already installed packages from the list pkgs &lt;- setdiff(pkgs, installed.packages()) # The following code will not make a lot of sense... yet. Come back to it in a # few weeks and see how much you understand (or can decode) # This ensures that if the installation fails, the code will keep running try_install_pkg &lt;- function(...) try(install.packages(..., dependencies = T)) lapply(pkgs, try_install_pkg) pkgs &lt;- setdiff(pkgs, installed.packages()) if (length(pkgs) &gt; 0) { paste(&quot;The following packages did not install: \\n&quot;, paste(pkgs, collapse = &quot;\\n&quot;)) } # Try installing github packages devtools::install_github(c(&quot;mine-cetinkaya-rundel/nycsquirrels18&quot;, &quot;hadley/emo&quot;, &quot;gadenbuie/tweetrmd&quot;, &quot;heike/classdata&quot;)) If some packages did not install, feel free to post in Yellowdig with the error message(s) received and the list of packages you’re having trouble installing (or contact me and we’ll set up a time to debug). 1.1.3 RStudio You can find RStudio at https://rstudio.com/products/rstudio/download/. You want the open source edition of RStudio Desktop. If you’re feeling brave, you can install the preview release - this may have a few bugs, but tends to be relatively stable and has the latest features. Please install RStudio 1.3.9+ for this class. If you’re on Linux, go ahead and import RStudio’s public key so that software validation works. (All of the commands are provided in the linked page) 1.1.4 git The git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better. Go read Chapter 1. 1.1.4.1 Getting set up: GitHub See the instructions here 1.1.4.2 Getting set up: git To install git, see the instructions here Write down, or keep track of, the git installation path. This will make your life much easier. There is a troubleshooting guide that has some common problems which occur during git installation. 1.1.4.3 Introduce yourself to git You need to tell git what your name and email address are, because every “commit” you make will be signed. Follow the instructions here, or run the lines below: user_name &lt;- readline(prompt = &quot;Your full name: &quot;) user_email &lt;- readline(prompt = &quot;The address associated w your github account: &quot;) if (!&quot;usethis&quot; %in% installed.packages()) install.packages(&quot;usethis&quot;) library(usethis) use_git_config(user.name = user_name, user.email = user_email, scope = &quot;user&quot;) # Tell git to ignore all files that are OS-dependent and don&#39;t have useful data. git_vaccinate() # Create a ssh key if one doesn&#39;t already exist if (!file.exists(git2r::ssh_path(&quot;id_rsa.pub&quot;))) { system(&quot;ssh-keygen -t rsa -b 4096&quot;) # Create an ssh key system(&quot;eval $(ssh-agent -s)&quot;) system(&quot;ssh-add ~/.ssh/id_rsa&quot;) } Then, in RStudio, go to Tools &gt; Global Options &gt; Git/SVN. View your public key, and copy it to the clipboard. Then, proceed to github. Make sure you’re signed into GitHub. Click on your profile pic in upper right corner and go Settings, then SSH and GPG keys. Click “New SSH key.” Paste your public key in the “Key” box. Give it an informative title. For example, you might use 2018-mbp to record the year and computer. Click “Add SSH key.” 1.1.4.4 Optional: Install a git client Instructions I don’t personally use a git client other than RStudio, but you may prefer to have a client, especially if you anticipate doing lots of work in SAS. 1.1.5 LaTeX You will need to follow operating-system instructions to install LaTeX. If you are using R/RStudio/SAS on Parallels, install LaTeX in parallels using the windows instructions. Windows: https://miktex.org/download Mac: http://www.tug.org/mactex/ Linux: https://www.tug.org/texlive/ You may find TeXLive in your package manager, but it’s usually easier to install it from source so that it can be easily updated. Just make sure you select the option to add LaTeX to your path! 1.2 Looking Around Your Environment Now that you have the software installed, it’s useful to take a look at each program and learn just a little bit about each one. Where there is interesting history and commentary, I’ve added that in an expandable details section that you can read if you are interested. 1.2.1 R and RStudio R is an open-source statistical programming language. It is a domain-specific language – that is, it is optimized for doing statistics, and is NOT a general-purpose programming language like C or python. This means that R is very good for doing data-related tasks, but if you want to implement something that is not data related (or even not tabular data related), it may be harder than you expect. A short “history of R” R is a statistical computing language which originated as an open-source clone of Bell labs S computing language. S was inspired by Scheme, but also has features which are similar to Lisp. It is a scripting language (you don’t have to compile the code before it runs) and is natively accessed using a command-line prompt. One feature of R that is relatively unique is that it uses vector-based math, which means that mathematical operations on vectors occur for the entire vector without having to use loops to iterate through the vector line-by-line (this feature is more common in languages designed for data manipulation, like Matlab and Julia; it is rare in more general-purpose computing languages). R is optimized for working on data: unlike more general programming languages such as Python, R was built with the idea of facilitating data analysis. As a result, data structures in R tend to be more natural for statistical work than similar structures in Python or C, which can feel unwieldy. From a computer science perspective, though, R seems like an extremely odd language because the design choices that make data analysis easier are unconventional for more general-purpose languages. For the most part, in this class, we will use R within RStudio. RStudio is an integrated development environment(IDE) for R. Basically, it adds a pretty graphical layer on top of R, providing an easy way to develop R code, evaluate that code, and keep track of all of the variables which are available in the computing environment. RStudio contains integrations which provide syntax highlighting, code folding, basic checks (missing parentheses, etc.), debugging tools, and many other features. RStudio was designed around the idea of making R easier to use and making it easy to develop statistical software reproducibly. RStudio (the company) is responsible for adding many features to the R ecosystem which facilitate running statistical analyses and presenting the results in user-friendly ways. RStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question. Another useful explanation of R and RStudio can be found in Section 1 of ModernDive’s book The RStudio window will look something like this. In the top-left pane is the text editor. This is where you’ll do most of your work. In the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections. On the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab. On the bottom right, there are a set of tabs: - files (to give you an idea of where you are working, and what files are present), - plots (which will be self-explanatory), - packages (which extensions to R are installed and loaded), - the help window (where documentation will show up), and - the viewer window, which is used for interactive graphics or previewing HTML documents. Try It Out To get started, type 2+2 into the console window and hit enter. Now, type 2+2 into the text editor and press the run button that is on the pane’s shortcut bar (or, you can hit Ctrl-Enter/CMD-Enter to send a single line to the console). If both of those things worked, you’re probably set up correctly! Next, try typing this into the text editor, then run the line. Look in the environment tab and see if you can see what has changed. a &lt;- 3 # store 3 in the variable a Your environment window should now look something like this (the .Last.value entry may not be there, and that’s ok) You can use the environment window to preview your data, check on the status of variables, and more. Note that while R is running, the window doesn’t update (so you can’t check on the status of a loop while the loop is running using the window). 1.2.2 SAS SAS is another extensively used statistical programming language. It is primarily used for mixed models and in the biostatistics community (for e.g. drug trials). Note SAS looks different on different machines. On Linux, SAS looks like you took a time machine back to the early 1990s. Screenshots from SAS will likely look very different on your machine than on mine. I will try to integrate screenshots from other OS’s where it matters.^[As I write this in late July 2020, I do not yet have access to a Windows or Mac machine. Sigh. XXX TODO: Get Windows/Mac screenshots XXX. I still don’t have access to a Windows or Mac machine, and it’s now April 2021. Sigh.] 1.2.2.1 SAS Windows/Panes The SAS toolbox has options to create new files, submit code, and more (but I mostly use keyboard shortcuts in the individual windows). This is probably one of the biggest things that’s different on Linux… The SAS Log gives you lots of detailed information about how your code ran – look here for errors and warnings. The SAS program editor is where you’ll be writing your code. If you want, you can write code in a more advanced text editor and copy/paste it into the log when you’re ready to run it. There are two different places your output may end up: if you’re using the old output system, you’ll get text output in the output pane. The old output system output pane. The new output system uses HTML and will output to your browser. You can navigate through your results using the results window The explorer window lets you access documentation, data, and more If you click on libraries, you get to this list: This area of the SAS explorer shows all of the libraries you have access to. Clicking on any one library will show you a list of datasets in that library You can then click on a dataset and you will get a spreadsheet-like view. 1.2.2.2 SAS Modules SAS is a very large set of programs. In this class, we’re primarily working with base SAS (the underlying language and interpreter), SAS/STAT (the statistical procedures, like PROC GLM), and SAS/IML, which is SAS’s version of a scripting language. IML allows you to implement your own procedures “from scratch.” Initially, we’ll primarily focus on SAS/IML, because it contains information parallel to what you need to know to start programming in R. It’s easier to teach general programming concepts at the same time, even if your typical SAS course would probably introduce you to the DATA step and simple PROC statements first. 1.2.3 Version Control with Git This section discusses what version control and git are. For information on how to actually USE git, see this subsection. Git is a program whose primary purpose is version control. Git tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history. When writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently overwrite your files, so long as you remember to commit frequently. The git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better. Go read Chapter 1, if you haven’t already. 1.2.3.1 Git vs Github Slightly crude (but memorable) analogy (don’t click if you’re offended by PG13/R rated stuff) Git is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories. Git is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git. If you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else. Remember: any data you don’t have in 3 different places is data you don’t care about.2 1.2.4 LaTeX LaTeX is a typesetting program, which makes it different from most other document creation software, such as MS Word, which is “WYSIWYG” - what you see is what you get. In LaTeX, you’ll type in code to create a document, and LaTeX will compile the document into something pretty. The goal is that you have to think less about formatting and what goes on which page - LaTeX will handle that part for you - so that you can think more about the content. Why use LaTeX? (shamelessly stolen from FB, which apparently stole it from Tumblr) Latex is meant for you to focus on the content and not worry about the formatting - the computer optimizes figure placement according to directives you’ve given. In practice, it doesn’t usually work out like that, so there are programs like markdown which aim to simplify document creation even more to free you from the formatting that LaTeX requires. LaTeX is often used for typesetting statistical and mathematical papers because its equation editor is top notch. (It was actually written by Donald Knuth because he got so annoyed trying to write his dissertation that he took some time off to write TeX first, and then used it to write his dissertation).3 If you want to try to make a simple LaTeX document in RStudio, try out this LaTeX tutorial. 1.2.5 Markdown We’ll work with LaTeX later in the semester, but for now, we’ll be primarily working with Markdown, which is much simpler. Here’s a quick cheatsheet. When you use markdown within RStudio, you don’t have to worry about where pandoc lives on your computer (e.g. the path). You can just click the button at the top of the file that says “Knit” or “Preview” to see what your file looks like. If you want to try out some simple Markdown formatting, you can create a new Rmarkdown document by going to File &gt; New File &gt; Rmarkdown and testing out the simple document it starts you out with. 1.3 Some Computer Basics 1.3.1 File Systems File systems control how data is stored and accessed on your computer. You might be familiar with acronyms like NTFS, FAT32, etc. - basically, these systems help make sense of the storage on your computer, tracking where files start and end, how much space is left, directory structures, and so on. For this class, it will probably be important to distinguish between local file storage (your C:/ drive on , /user/your-name/ on , or /home/your-name/ on ) and network/virtual file systems, such as OneDrive and iCloud. You want to save your files in this class to your physical hard drive. This will save you a lot of troubleshooting time. 1.3.2 System Paths When you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don’t need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu, and you find your programs there. Unfortunately, that isn’t sufficient when you’re programming, because you may need to know where a program is in order to reference that program – for instance, if you need to pop open a browser window as part of your program, you’re going to have to tell your computer where that browser executable file lives. To simplify this process, operating systems have what’s known as a “system path” or “user path” - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available. This explanation of how to set system paths may be useful if it comes up; an additional (slightly different) explanation here may also help. Additional examples are available here, here, and here. If you run across an error that says something along the lines of could not locate xxx.exe The system cannot find the path specified Command Not Found you might start thinking about whether your system path is set correctly for what you’re trying to do. If you want to locate where an executable is found (in this example, we’ll use git), you can run where git on windows, or which git on OSX/Linux. Some programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools &gt; Global Options &gt; Git/SVN, you can set the path to git. 1.3.3 Working Directories When you launch a program, that program starts up with a specific directory as its “location” - the place where it will look for files. Most of the time, we don’t think too much about this when using graphical programs, but it’s much more important to get this right when programming things ourselves. In RStudio, you can set your working directory in several ways: Work in an RStudio Project (File &gt; New Project), which will make it easy to access your project folder, and will set your working directory to the right folder automatically. I’ve created a project in /home/susan/Projects/Class/unl-stat850/test-project Use setwd() to set your working directory. If I’m not working in the RStudio project described above, I can set my working directory to that folder using: setwd(\"/home/susan/Projects/Class/unl-stat850/test-project\") Use the files tab (bottom right) to navigate to your preferred folder. Then, click More &gt; Set As Working Directory. In SAS, you can set your working directory in the Program Editor, Log, or Explorer windows by clicking on Tools &gt; Options &gt; Change Directory, navigating to your preferred directory, and clicking ok. In this class, we’re going to focus on reproducibility - making sure your code runs on any other computer with the right project setup. So with that in mind: Work in projects Store your data in your project folder Use local paths - if you’re in your project folder and trying to reference “data.xls,” then you would just use “data.xls” as the path. If your data is in a “code” folder, you would use “code/data.xls” as the path. This will ensure that your code should work on any other machine, as long as your data is in your git repository and your code uses local file paths. In SAS, you’ll want to store your .sas code file in the RStudio project folder as well, and I believe that should be sufficient to set your working directory for any SAS code you may write as part of e.g. a homework assignment. 1.4 Using Version Control (with RStudio) If that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything. I’ve set this class up so that you’ll be using version control from the very beginning. Not only will this help you to learn good habits, it will also give you a platform for collaboration, hosting your work online, and more. In this class, we’ll be using Github Classroom. Basically, this allows me to set up a template repository for each assignment. You’ll accept the assignment, which will create a copy of the repository on your GitHub account, and then your work will be saved to your repository using a fairly standard workflow which will be discussed below. When you submit your assignment, you’ll copy the link to the commit you want to be graded, and upload that to Canvas. I will clone your repository, compile your files (I’ll change the SAS path if necessary), and grade the compiled result. So, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is primarily my higher-level understanding of git - I do not have any idea how it works on the backend, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion. 1.4.1 Create a Repository Repositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. To create a repository, you can start with your local computer first, or you can start with the online repository first. Both methods are relatively simple, but the options you choose depend on which method you’re using, so be careful not to get them confused. 1.4.1.1 Local repository first Let’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it. If you want, you can use the following R code to set this up: dir &lt;- &quot;./hello-world-1&quot; if (!dir.exists(dir)) { dir.create(dir) } file &lt;- file.path(dir, &quot;README&quot;) if (!file.exists(file)) { writeLines(&quot;hello world&quot;, con = file) } To create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in git init Alternately, if you prefer a GUI (graphical user interface) approach, that will work too: Open Rstudio Project (upper right corner) -&gt; New Project -&gt; Existing Directory. Navigate to the directory. (In your new project) Tools -&gt; Project options -&gt; Git/SVN -&gt; select git from the dropdown, initialize new repository. RStudio will need to restart. Navigate to your new Git tab on the top right. Screen capture video of the steps described above to create a git repository in RStudio The next step is to add our file to the repository. Using the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\". Using the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button. Screen capture video of the steps described above to add and commit a file in RStudio The final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you’re logged in. Create a new repository using the “New” button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time). You’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go. Connect your local git repository to GitHub 1.4.1.2 GitHub repository first In the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally). GUI method: Log into GitHub and create a new repository Initialize your repository with a README Copy the repository location by clicking on the “Code” button on the repo homepage Open RStudio -&gt; Project -&gt; New Project -&gt; From version control. Paste your repository URL into the box. Hit enter. Make a change to the README file Click commit, then push your changes Check that the remote repository (Github) updated Video of the GUI method for creating a repository starting with GitHub Command line method: Log into GitHub and create a new repository Initialize your repository with a README Copy the repository location by clicking on the “Code” button on the repo homepage Navigate to the location you want your repository to live on your machine. Clone the repository by using the git shell or terminal: git clone &lt;your repo url here&gt;. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git Make a change to your README file and save the change Commit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking). Push your changes to the remote (GitHub) repository and check that the repo has updated: git push 1.4.2 Adding files git add tells git that you want it to track a particular file. git add diagram: add tells git to add the file to the index of files git monitors. You don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about,” and what it thinks belongs in each commit. If you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (sort-of, kind-of) equivalent to clicking the check box. 1.4.2.1 What files should I add to git? Git is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB4. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free. In general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository). You should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good. I highly recommend that you make a point to only add and commit files which you consciously want to track. 1.4.3 Staging your changes In RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file. More formally, staging is saying “I’d like these changes to be added to the current version, I think.” Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out. Individually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do). 1.4.4 Committing your changes A git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache. Here, we commit the red line as a change to our file. In general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things: You’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing) When you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred. 1.4.5 Pushing and Pulling When you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document. If you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document. For this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible. Pulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up). Pushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine. In general, your workflow will be Clone the project or create a new repository Make some changes Stage the changes with git add Commit the changes with git commit Pull any changes from the remote repository Resolve any merge conflicts Push the changes (and merged files) with git push If you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways. 1.5 Topic Sequencing In several places in this class, you’ll have to use material that you haven’t been formally taught yet. I will do my absolute best to provide thorough instructions, help you along as much as I can, and generally provide enough support that you can muddle through. But it’s going to be hard to teach you everything you need to e.g. analyze some data, before providing you the opportunity to SEE that data using visualization packages. And it’s silly to teach you plotting before you know how to read data in. But to teach you how to read data in, you need to be able to take a look at the data, and plots are the best way to do that. To do any of this stuff, you need to know about functions, but it can be easier to figure out how to run a function than to write a function. You see my problem. So instead, what I’m going to do is to leave you lots of comments as to what a piece of code does when I’m using things you haven’t been formally shown yet. Then, you can copy/paste/modify those pieces of code, and if they break, you can ask why and we’ll dig into it (breaking code is usually a good thing, because it means you’re learning how to program). For each chapter, focus on learning how to write code that accomplishes that chapter’s objectives. If you understand some of the code you’re modifying that covers other topics not in that chapter, so much the better. But it’s not an expectation or a requirement. If you’re confused, please post on the class message boards so that those who have seen this material before can help you out. SAS quick start guide R quick start guide 1.6 References Happy Git and GitHub for the useR - Guide to using git, R, and RStudio together. Git “Hello World” Tutorial on GitHub Crash course on git (30 minute YouTube video) Git and GitHub for poets YouTube playlist (this is supposed to be the best introduction to Git out there…) More advanced git concepts, in comic form, by Erika Heidi Yes, I’m aware that this sounds paranoid. It’s been a very rare occasion that I’ve needed to restore something from another backup. You don’t want to take chances. I knew a guy who had to retype his entire masters thesis from the printed out version the night before it was due because he had stored it on a network drive that was decommissioned. You don’t want to be that person.↩︎ Amusingly, knitr was written in much the same manner. Yihui Xie had to substitute-teach ISU’s version of 850 on the day we covered Sweave (a predecessor to knitr). He got so frustrated teaching the class that he went home and started writing knitr. Later, he developed Rmarkdown, bookdown, blogdown, and several other packages aimed at making writing documents + code easier to handle. Moral of the story - if you get frustrated with the tools you have, you’re in good company. Use it as fuel to make better tools.↩︎ Yes, I’m seriously pushing it with this book; several of the datasets are ~30 MB↩︎ "],["intro-prog.html", "Module 2 Introduction to Statistical Programming Module Objectives Definitions Statistical Programming Languages 2.1 Breaking Problems Down 2.2 Variable types 2.3 Data structures 2.4 Control structures 2.5 Overgrown Calculators References and Links", " Module 2 Introduction to Statistical Programming The only way to learn how to program effectively is to take something that works, break it, and then fix it again. There’s plenty of theory and you should definitely learn that, but fundamentally, if you are not regularly breaking code, you’re probably not programming. Figure 2.1: This is basically the class summarized. The goal for this chapter (and several chapters to come) is that you can modify example code and adapt it to the problem at hand. This is the best way to learn how to program, but it means you may break the code and not know how to fix it. If that happens, please try the following steps: Google the error and see if you can understand why it happened. Consult with a classmate to see if they can understand where things broke. Post to the discussion board and see if anyone in the class can understand where things broke. (When you do this, post all of the code relevant to the problem, and the error you’re getting, so that your classmates can replicate the problem) If you do not hopelessly break code during this chapter, then please do your best to help others who may not have previously programmed (or previously programmed in these languages). While writing this chapter, I came across about 10 errors in SAS that I’d never encountered before. If all else fails, while you’re waiting for someone to help you figure out what an error message means… try this approach. Module Objectives Create a program flow map by breaking a problem down into smaller steps Write basic scripts to solve mathematical problems with variables, control structures, and scalar/matrix algebra More informally, the goal is to get familiar with the basics of each programming language, and to show you where to find references for how to use each command – because (at least) half of programming is knowing where to look something up. Definitions Many programming resources talk about 3, or 5, or 10 core concepts in any programming language. In this module, we’re going to discuss the generic concepts, and then how these concepts are implemented in R and SAS. Interestingly, the “core concepts” aren’t necessarily the same across lists. Here is a consensus list of concepts which are generic across languages and usually important Variables - a symbolic name or reference to some kind of information. In the expression a + b &gt; a, both a and b are variables. Variables may have a specific type (what data can be stored in the variable), scope (where the variable can be accessed), location (in memory). Here is a nice explanation of the difference between variables in programming and variables in math. Conditional statements (if statements) - These statements allow the program to handle information adaptively - if a statement is true, one set of instructions will be used, and if the statement is false, a different set of instructions will be used. Looping and iteration - An iteration is any time a sequence of steps is executed. Most languages have several different types of loops or iteration: for loops, which allow for the sequence of steps to be executed a specific number of times, while loops, which allow for the sequence of steps to be executed while a conditional statement is true, recursion, where a block of code calls itself. Data types and data structures - these concepts determine what information a variable can hold. Data types are lower-level, simple objects (floating-point numbers, integers, boolean T/F, characters, strings). Data structures may include lists (sequences of many objects) and vectors (sequences of many objects of the same type), dictionaries (a list of key-value pairs), objects (data structures which may hold multiple related pieces of information). Functions, or self-contained modules of code that accomplish a particular task. Syntax, the set of rules that define which combinations of symbols consist of correctly structured and interpretable commands in the language. Tools, the set of external programs which may help with development and writing code. Some common tools are IDEs (Integrated Development Environments), which may correct syntax and typos, organize files for you, allow you to keep track of which variables you have defined, and assist you with code organization and navigation. Other tools include compilers (which take human-written code and translate it into efficient machine code), version control systems (which help you track changes to code over time), debuggers, and documentation generators. Not all of these tools are necessary for all languages - scripting languages such as python and R do not require compilers by default, for instance. Sequence of commands: It’s important to have the right commands in the right order. Some recipes, like bread dough, are flexible, and you can add the ingredients in almost any order, but in other recipes, the order matters as much as the correct quantity of ingredients (try putting the cheese powder in before the noodles are boiled when making macaroni and cheese. Yuck.). Programming tends to be like these less flexible recipes. Statistical Programming Languages Having established the generic definitions of the concepts which apply to almost any programming language, we now must examine how R and SAS implement these concepts. R and SAS are both statistical programming languages - they are specifically designed to work with data, which means that they make compromises that other languages do not in order to make it easier to write code where the data (rather than the functions, classes, methods, or objects) are the primary concern. With that said, there are a few notes which are worth mentioning – Historical view Both R and SAS have long histories. SAS in particular dates back to the 1960s, and has syntax which is unique compared to more modern languages such as C, python, Java, and R. R’s predecessor, S, dates back to 1976 and was designed for internal use at Bell Labs. The histories of both languages are useful in understanding why they are optimized for their respective tasks, but are not essential for this course (so read them at your leisure). Difference between R and SAS The biggest difference between R and SAS (at a fundamental level) is that R is a functional language - it consists mainly of functions, which can (and do) manipulate objects, including other functions. SAS, on the other hand, is a procedural language - most SAS programs follow a specific series of steps, known as “proc”s. Procs are essentially functions (or compositions of multiple functions), but in SAS, it is simpler to think of an analysis as a series of procedural steps; in R, there are steps, but they may be implemented in a more flexible way (depending on the analysis). Another interesting feature of SAS is that it’s really several languages - some commands work in PROC IML (interactive matrix language) but not in a DATA step. When looking for help in SAS, make sure you’re referencing the correct part of the language documentation. I’m teaching SAS very differently I’m definitely teaching SAS differently than it is normally taught. This is so that we don’t have to do half the semester in SAS and half in R - I’d rather teach the concepts and show you how they’re implemented than split them up by language. BUT, this means that some of the things we’re doing first in SAS are things you wouldn’t normally do until you were already proficient in SAS. It also means that SAS is probably going to seem even more oddly organized when taught this way than it actually is (and it is oddly organized, in my opinion). We’re going to start with SAS IML (programming concepts) and then talk about the DATA step. We’ll use some procedures implicitly along the way, but hopefully that will make sense in context. Then, we’ll work on the PROCs (SQL, Transpose, and graphing) - in greater detail. If you’ve used R or SAS a lot in the past Note: If you’ve programmed before, this chapter is going to seem very … boring. Sorry, there’s no help for that. Some of your classmates haven’t ever so much as written “Hello World,” and we have to get them up to speed. If you’re bored, or feel like you know this material, skim through it anyways just to confirm (and if I’m doing something that’s really out there, or there’s an easier way to do it, tell me!). Then you can either find something in the references that you don’t know already (the book Advanced R is always a great place to start if you want to be quickly confused), or help your classmates that are less experienced. Basic Syntax and Cheatsheets SAS Cheatsheet (from another class like this) SAS Cheatsheet (by SAS) R Cheatsheet - this is a simplified cheat sheet offered by RStudio. R Cheatsheet (classic) SAS Programming for R Users (free book) I kept the classic R reference card by my computer for about 5 years, and referenced it at least once or twice a day for that entire period. There will be other cheat sheets and reference cards scattered through this book because if you can’t remember something’s name, you might be able to remember where it is on the reference card (or at least, that’s how I learned R). 2.1 Breaking Problems Down The most fundamental part of programming that you will need to learn in this class is how to break down a big problem into smaller (hopefully solvable) problems. This post is a great example of the process of breaking things down for programming, but the same concept applies outside of programming too! Breaking problems down - remodeling edition My spouse and I recently decided to replace our shower curtain with glass doors because the curtain didn’t really prevent water from getting all over the floor. We went to the store and picked out the parts, and the installation instructions broke the steps down like this: Install the base of the track Install the top of the track Hang shower doors Add hardware to shower doors So we started in on the instructions, only to find out that when our house was built, our shower wasn’t leveled properly. The instructions had a solution - we could send off for a $300 custom part that would level our floor, but we’d have to wait at least 4-6 weeks for them to make and ship the part to us. We’re both programmer-adjacent, so we started thinking through how we could deal with our problem a different way. We considered ignoring the instructions – our shower was about 1/8\" off of level, surely that couldn’t be so important, right? My spouse is a bit more … detail oriented … than I am, so he wasn’t good with that suggestion. We considered adding a ton of caulk or plaster to try to level the shower out. But we figured that 1) probably wouldn’t work, and 2) would look awful. Finally, I suggested that my spouse 3D print sections of track-leveler using our 3D printer. Now, this isn’t an option for most people, but it is for us - we have a small 3D printer, and spouse knows how to use OpenSCAD to create very accurate, custom dimension 3D printer files. He tested things out a few times, and printed up a series of 12 ~5\" sections that when assembled were equivalent to the $300 custom part we could have ordered. Then, he proceeded with the rest of the installation as the instructions listed. Essentially, because we had a list of subproblems (steps for installation), we could focus our efforts on debugging the one problem we had (not level shower ledge) and we didn’t have to get bogged down in “it’s impossible to get this job done” - we knew that if we could solve the little problem, we’d be able to get the bigger job done. Programming is just like this - if you can break your problem down into steps (and not steps with code), you can think through how to solve a single step of the problem before you worry about the next step. One tool that is often used to help break a problem down is a flowchart. Try it out The biggest advantage to breaking problems down into smaller steps is that it allows you to focus on solving a small, approachable problem. Let’s think through an example: suppose I want you to write a program to print out a pyramid of stars, 10 lines high. Yes, I remember, I haven’t taught you how to write any code yet. Don’t worry about code right now - let’s just think about how we might create a pyramid of stars. Start by writing some instructions to yourself. Solution First, we have to think about what we would need to make a pyramid of stars. So let’s make a miniature one by hand (I’m using - for spaces here to make things visible): ---*--- --***-- -*****- To make my miniature star pyramid, I started out by adding space on the first line, then a star, then more space. When I moved to the next line, I added space, but one less space than I’d added before, and then 3 stars, and then more space. So we can break our problem down into two components: How much space? (one side) How many stars? (redundant piece) How much space (the other side). Thinking my way through how I created my manual pyramid, I realized that I was adding \\(n\\) spaces (where \\(n\\) is the total number of rows) on the first line, and then \\(n - i\\) spaces on subsequent lines, if we start with i=0. But I am an R programmer, so we start with \\(i=1\\), which means I need to have \\(n - i + 1\\) spaces on each row first. Then, for \\(i=1\\) the first row, we have \\(2*i - 1\\) stars - i = 1, stars = 1, then i = 2, stars = 3, then i = 3, stars = 5…. you can do the regression if you want to, but it’s pretty easy to see the relationship. Finally, we have to (in theory) add the same amount of space on the other side – strictly speaking, this is optional, but it makes the lines the same length, so it is nice. So we’ve thought through the problem step by step, and now we just need to summarize it: If we want a pyramid that is \\(n\\) rows high, we might think of creating it by using the following line-by-line formula, where \\(i\\) is our current line: \\(n - i +1\\) spaces, \\(2i - 1\\) stars, \\(n - i + 1\\) spaces Working this out in a small example helped me come up with that formula; now, I can write a “loop”: line 1: i = 1, n = 10, 10 spaces, 1 star, 10 spaces line 2: i = 2, n = 10, 9 spaces, 3 stars, 9 spaces line 3: i = 3, n = 10, 8 spaces, 5 stars, 8 spaces line 4: i = 4, n = 10, 7 spaces, 7 stars, 7 spaces … line n: i = n, n = 10, 1 space, 19 stars, 1 space Our program flow map would look something like this: 2.2 Variable types Variable types are sufficiently different in R and SAS that we will cover R first, then SAS. For a general overview, though, this video, titled ‘Why TRUE + TRUE = 2’ is an excellent introduction. 2.2.1 R In R, there are 4 commonly-used types: Type Description character holds text-based information: “abcd” or “3.24a” are examples of values which would be stored as characters in R logical holds binary information: 0/1, or FALSE/TRUE. Logical variables are stored as single bit information (e.g. either a 0 or 1), but display as TRUE and FALSE (which are reserved words and constants). integer holds (as you might expect) integers. Note that integers are handled differently than doubles (floating point numbers), but in general, R will implicitly convert integers to doubles to avoid common pitfalls with integer divison (which does not allow for decimals). double holds floating point numbers. By default, most numeric variables in R are doubles. You can test to see whether a variable holds a value of a specific type using the is.xxx() functions, which are demonstrated below. You can convert a variable of one type to another with as.xxx() functions. You can test what type a variable is using typeof(). Note that &lt;- is used for assigning a value to a variable. So x &lt;- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’.” Character variables x &lt;- &quot;R is awesome&quot; typeof(x) ## [1] &quot;character&quot; is.character(x) ## [1] TRUE is.logical(x) ## [1] FALSE is.integer(x) ## [1] FALSE is.double(x) ## [1] FALSE Logical Variables x &lt;- FALSE typeof(x) ## [1] &quot;logical&quot; is.character(x) ## [1] FALSE is.logical(x) ## [1] TRUE is.integer(x) ## [1] FALSE is.double(x) ## [1] FALSE It is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false.5 Integer Variables x &lt;- 2 typeof(x) ## [1] &quot;double&quot; is.character(x) ## [1] FALSE is.logical(x) ## [1] FALSE is.integer(x) ## [1] FALSE is.double(x) ## [1] TRUE Wait, 2 is an integer, right? 2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically. x &lt;- 2L # The L immediately after the 2 indicates that it is an integer. typeof(x) ## [1] &quot;integer&quot; is.character(x) ## [1] FALSE is.logical(x) ## [1] FALSE is.integer(x) ## [1] TRUE is.double(x) ## [1] FALSE is.numeric(x) ## [1] TRUE Double Variables x &lt;- 2.45 typeof(x) ## [1] &quot;double&quot; is.character(x) ## [1] FALSE is.logical(x) ## [1] FALSE is.integer(x) ## [1] FALSE is.double(x) ## [1] TRUE is.numeric(x) ## [1] TRUE Numeric Variables A fifth common “type”6, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing. The numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work. The general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double. Type Conversions R will generally work hard to seamlessly convert variables to different types. So, for instance, TRUE + 2 ## [1] 3 2L + 3.1415 ## [1] 5.1415 &quot;abcd&quot; + 3 ## Error in &quot;abcd&quot; + 3: non-numeric argument to binary operator This conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops. When you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be: as.double(TRUE) + 2 ## [1] 3 as.double(2L) + 3.1415 ## [1] 5.1415 as.numeric(&quot;abcd&quot;) + 3 ## Warning: NAs introduced by coercion ## [1] NA When we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value. There’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA. If you are unsure what the type of a variable is, use the typeof() function to find out. w &lt;- &quot;a string&quot; x &lt;- 3L y &lt;- 3.1415 z &lt;- FALSE typeof(w) ## [1] &quot;character&quot; typeof(x) ## [1] &quot;integer&quot; typeof(y) ## [1] &quot;double&quot; typeof(z) ## [1] &quot;logical&quot; Factors In R, there is one other type of variable to know about, and that is a factor. Factors are basically labeled integers. Instead of storing the data as a string or character, R instead stores the data as a series of integers, and then stores a separate table mapping the integers to labels. This is technically more efficient (which was important when computers had extremely limited memory), but it is also a pain in the rear (that’s a technical term). Factors are the default way to store characters for most base R functions. Or rather, they were. In R 4.0, the default way to read data in will change from stringsAsFactors = T to stringsAsFactors = F. You can read about why factors aren’t ideal here, which helps explain why this change was made. Depending on what version of R you have installed, you may run into errors related to factors, or not. Because R 4.0 is so new (released in May 2020) most of the tutorials online will probably have behavior that isn’t matched by your R installation. I’m new enough to R 4.0 that I’m not sure when factor related errors will pop up. Other reasons to learn factors besides for debugging purposes: They allow you to control the order of things in graphs, tables, and models They allow you to easily change category labels without having to sort through an entire data table … I’m sure there are more, but I’m drawing a blank at the moment In this example, we’ll use a data.frame, which you can think of as a spreadsheet-type table. We’ll work with data frames later in much more detail, but for now, I’m mostly trying to show you a real-life situation that happens ALL the time, with the hopes that you’ll recognize the error when/if you encounter it. The data frame isn’t the important part. Factors example Let’s look at the names of the months: month.name ## [1] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; &quot;May&quot; &quot;June&quot; ## [7] &quot;July&quot; &quot;August&quot; &quot;September&quot; &quot;October&quot; &quot;November&quot; &quot;December&quot; df &lt;- data.frame(num = 1:12, name = month.name, stringsAsFactors = T) # I&#39;m putting the argument in so that this is still relevant when everyone # switches to R 4.0. Even with stringsAsFactors = F, factors are still useful # and we still need to work with them. # # Any time you create a data frame in base R, you should be watchful for errors # that are based on strings being converted to factors. str(df) ## &#39;data.frame&#39;: 12 obs. of 2 variables: ## $ num : int 1 2 3 4 5 6 7 8 9 10 ... ## $ name: Factor w/ 12 levels &quot;April&quot;,&quot;August&quot;,..: 5 4 8 1 9 7 6 2 12 11 ... Notice that as soon as we make that data.frame, the months are converted into a factor variable? The other big problem is that the order of the factor levels is … not what we’d normally want. We don’t want alphabetical ordering of month names - they have a different, implicit, and natural order. We could get this same behavior without the data.frame, but this is where it shows up most often. month_fct &lt;- factor(month.name) # the order is still not exactly what we&#39;d want it to be To fix this, we can explicitly specify that we’re dealing with a factor, and what we want the levels to be. If you specify the levels manually (instead of letting R do the work for you) then you get to determine the order. month_fct &lt;- factor(month.name, levels = month.name) str(month_fct) ## Factor w/ 12 levels &quot;January&quot;,&quot;February&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... We can even be more explicit: month_fct &lt;- factor(month.name, levels = month.name, ordered = T) str(month_fct) ## Ord.factor w/ 12 levels &quot;January&quot;&lt;&quot;February&quot;&lt;..: 1 2 3 4 5 6 7 8 9 10 ... Making the factor ordered lets us explicitly say which levels are less than other levels. Factors are technically integers, with labels that are stored as an attribute. That doesn’t mean you can do math with them, though. month_fct[1] + month_fct[2] ## Warning in Ops.ordered(month_fct[1], month_fct[2]): &#39;+&#39; is not meaningful for ## ordered factors ## [1] NA Often, years or dates or other numeric-like information will end up as factor variables. When this happens, you need to be a little bit careful. # This works pretty naturally for months, right? as.numeric(month_fct) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 yfact &lt;- factor(2000:2020, levels = 2000:2020) yfact ## [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 ## [16] 2015 2016 2017 2018 2019 2020 ## 21 Levels: 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 ... 2020 # But, this does not... as.numeric(yfact) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 as.character(yfact) # gets the labels ## [1] &quot;2000&quot; &quot;2001&quot; &quot;2002&quot; &quot;2003&quot; &quot;2004&quot; &quot;2005&quot; &quot;2006&quot; &quot;2007&quot; &quot;2008&quot; &quot;2009&quot; ## [11] &quot;2010&quot; &quot;2011&quot; &quot;2012&quot; &quot;2013&quot; &quot;2014&quot; &quot;2015&quot; &quot;2016&quot; &quot;2017&quot; &quot;2018&quot; &quot;2019&quot; ## [21] &quot;2020&quot; as.numeric(as.character(yfact)) # gets the info we want ## [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 ## [16] 2015 2016 2017 2018 2019 2020 When converting factors with numeric labels, you need to first convert the factor to a character, and then to a numeric variable. That will get the information you actually want back out. Try It Out Create variables string, integer, decimal, and logical, with types that match the relevant variable names. string &lt;- integer &lt;- decimal &lt;- logical &lt;- Can you get rid of the error that occurs when this chunk is run? logical + decimal integer + decimal string + integer What happens when you add string to string? logical to logical? Solutions string &lt;- &quot;hi, I&#39;m a string&quot; integer &lt;- 4L decimal &lt;- 5.412 logical &lt;- TRUE logical + decimal ## [1] 6.412 integer + decimal ## [1] 9.412 as.numeric(string) + integer ## Warning: NAs introduced by coercion ## [1] NA &quot;abcd&quot; + &quot;defg&quot; ## Error in &quot;abcd&quot; + &quot;defg&quot;: non-numeric argument to binary operator TRUE + TRUE ## [1] 2 In R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value. 2.2.2 SAS In SAS, there are two basic variable types: numeric and character variables. SAS does not differentiate between integers and floats and doubles. Functionally, though, the same basic operations can be performed in SAS. As with R, SAS does attempt to implicitly convert variable types, and will notify you that the conversion has taken place in the log file. Type Conversions SAS will attempt to implicitly convert variables when: a character value is assigned to a previously defined numeric variable a character value is used in arithmetic operations a character value is compared to a numeric value using a comparison operator (&lt;, &gt;, &lt;=, &gt;=) a character value is specified in a function that takes numeric arguments Implicit conversion does not occur in WHERE statements. (This will make more sense later, but is here for reference) Manual type conversions If you want to manually convert a value, use the INPUT statement. Unlike in R, the INPUT statement has the ability to read numbers which are formatted differently. For instance data set1; x = 3; y = &#39;3.1415&#39;; z = x * y; put z; run; data set2; x = 3; y = &#39;3.1415&#39;; z = x * y; put z; /* print to log */ x = &#39;3.14159&#39;; /* x previously had a number in it, so it will be converted to a number here */ put x; /* print to log */ zz = y &lt;= 2; /* comparison operator: y will be converted */ put zz; /* print to log */ run; Notice that in SAS, zz, which is the result of the logical statement y&lt;=2, is a numeric variable. The value 0 signifies that the comparison was false. SAS does not have a logical data type, it uses the numeric variable with 0:=FALSE, 1:=TRUE. Try it out Create variables string1 and string2 that each have text/character values. “Bob” and “Jane” might be good options. How does logical operation work with actual character values? What happens if you use string1 and add 3 to it? Solutions 6 data set1; 7 string1 = &#39;Bob&#39;; 8 string2 = &#39;Jane&#39;; 9 x = string1 &lt; string2; 10 put x=; /* This prints the result to the log */ 11 run; x=1 NOTE: The data set WORK.SET1 has 1 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 SAS will actually compare strings based on the first letter: Bob comes before Jane, so Bob &lt; Jane. 6 data set2; 7 string1 = &#39;Bob&#39;; 8 y = string1 + 3; 9 put y=; 10 run; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 8:7 NOTE: Invalid numeric data, string1=&#39;Bob&#39; , at line 8 column 7. y=. string1=Bob y=. _ERROR_=1 _N_=1 NOTE: Missing values were generated as a result of performing an operation on missing values. Each place is given by: (Number of times) at (Line):(Column). 1 at 8:15 NOTE: The data set WORK.SET2 has 1 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds The . in SAS is a missing value (like NA in R). So SAS is behaving basically like R does: it complains about the fact that you asked it to add a string to a number, and then it stores the result as a missing value. 2.3 Data structures Data structures are more complex arrangements of information than single variables. Of primary interest in statistical programming are the following types of structures:   Homogeneous Heterogeneous 1d vector list 2d matrix data frame (R) or data set (SAS) nd array (R) In the table above, homogeneous means that all entries in the structure must be of the same type. Heterogeneous means that the entries are allowed to be of different types. Figuring out what to call these types with two languages is hard - in SAS, an array is a group of columns of a data set, but in R, it’s a multi-dimensional matrix. In this section, we’ll discuss the generic concepts relevant to both languages. The differences between the two languages will be discussed as appropriate. As there are more similarities than differences, it’s easier to do this in a single section rather than duplicating half of the content. 2.3.1 Homogeneous data structures (R and SAS) R does not have scalar types - even single-value variables are technically vectors of length 1. SAS does have scalar types. If we try to create a heterogeneous vector in R, using the concatenate function, c(), which combines scalar entries into a vector, what happens? c(1, 2, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) ## [1] &quot;1&quot; &quot;2&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; Because there were 3 character entries, the entire vector is now a character vector. R’s vector-by-default approach can cause some errors - for instance, R does not read in numeric data formatted with commas as numeric data. You may thus get the result x &lt;- c(356, 452, &quot;1,325&quot;) mean(x) ## Warning in mean.default(x): argument is not numeric or logical: returning NA ## [1] NA If you are reading in data from a file, this will cause some issues - the whole column of data will be formatted as characters. Keep an eye out for errors of this type. One fix for this is to read things in as a character and use the parse_number function from the readr package – we’ll talk about the readr package in Module 4 Jenny Bryan has an excellent set of images to demonstrate R data types as legos. She’s released them under an open license, so I am shamelessly stealing them. Logical vector Factor vector Integer and Numeric vectors These correspond to the 1-dimensional homogeneous data structures. Similarly, models can be made for 2-dimensional and 3-dimensional homogeneous data structures7: Vector (1D) Matrix (2D) Array (3D) 2.3.2 Heterogeneous data structures The heterogeneous data types are not much harder to grasp, as they’re mostly different ways to combine various homogeneous data types. 2.3.2.1 Lists A list is, well, a list - a sequence of potentially different-typed values. Unlike when concatenating values, the list() command in R allows each value to keep its natural type. You can access elements of a list using [] (this will extract a subset of the list items) or [[]], which will extract a single item from the list. (there will be more on this in the Indexing section below) Basic List Syntax in R x &lt;- list(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, 1, 2, 3) x ## [[1]] ## [1] &quot;a&quot; ## ## [[2]] ## [1] &quot;b&quot; ## ## [[3]] ## [1] &quot;c&quot; ## ## [[4]] ## [1] 1 ## ## [[5]] ## [1] 2 ## ## [[6]] ## [1] 3 x[[3]] ## [1] &quot;c&quot; x[[4]] + x[[5]] ## [1] 3 x[1:2] # This will work ## [[1]] ## [1] &quot;a&quot; ## ## [[2]] ## [1] &quot;b&quot; x[[1:2]] # This won&#39;t work ## Error in x[[1:2]]: subscript out of bounds The lego version of a list looks like this: Figure 2.2: A list of 4 vectors. Even though the vectors in the list are all the same size in this case, they don’t have to be, because they’re not organized in any sort of cohesive rectangle shape. A data frame (see below) is essentially a list where all of the components are vectors or lists of the same length. Indexing in Lists Some lists (and data frames) consist of named variables. These list components can be accessed either by index (as above) or by name, using the $ operator. Names which have spaces or special characters must be enclosed in backticks (next to the 1 on the keyboard). Named components can also be accessed using the [[ ]] operator. dog &lt;- list(name = &quot;Edison Vanderplas&quot;, age = 8, breed = &quot;Jack Russell Terrorist&quot;, `favorite toy` = &quot;a blue and orange stuffed duck. Or rawhide.&quot;, `link(video)` = &quot;https://youtu.be/zVeoQTOTIuQ&quot;) dog ## $name ## [1] &quot;Edison Vanderplas&quot; ## ## $age ## [1] 8 ## ## $breed ## [1] &quot;Jack Russell Terrorist&quot; ## ## $`favorite toy` ## [1] &quot;a blue and orange stuffed duck. Or rawhide.&quot; ## ## $`link(video)` ## [1] &quot;https://youtu.be/zVeoQTOTIuQ&quot; dog$name ## [1] &quot;Edison Vanderplas&quot; dog$breed ## [1] &quot;Jack Russell Terrorist&quot; dog$`favorite toy` ## [1] &quot;a blue and orange stuffed duck. Or rawhide.&quot; dog[[&quot;link(video)&quot;]] ## [1] &quot;https://youtu.be/zVeoQTOTIuQ&quot; You can get a sense of the structure of a list (or any other object) in R using the str() command. str(dog) ## List of 5 ## $ name : chr &quot;Edison Vanderplas&quot; ## $ age : num 8 ## $ breed : chr &quot;Jack Russell Terrorist&quot; ## $ favorite toy: chr &quot;a blue and orange stuffed duck. Or rawhide.&quot; ## $ link(video) : chr &quot;https://youtu.be/zVeoQTOTIuQ&quot; Recursive lists Lists can also contain other lists. When accessing a list-within-a-list, just add another index or name reference (see below). grocery_list &lt;- list( dairy = list(&quot;asiago&quot;, &quot;fontina&quot;, &quot;mozzarella&quot;, &quot;blue cheese&quot;), baking = list(&quot;flour&quot;, &quot;yeast&quot;, &quot;salt&quot;), canned_goods = list(&quot;pepperoni&quot;, &quot;pizza sauce&quot;, &quot;olives&quot;), meat = list(&quot;bacon&quot;, &quot;sausage&quot;, &quot;anchovies&quot;), veggies = list(&quot;bell pepper&quot;, &quot;onion&quot;, &quot;scallions&quot;, &quot;tomatoes&quot;, &quot;basil&quot;) ) ick &lt;- c(grocery_list[[4]][2:3], grocery_list$canned_goods[[3]]) ick ## [[1]] ## [1] &quot;sausage&quot; ## ## [[2]] ## [1] &quot;anchovies&quot; ## ## [[3]] ## [1] &quot;olives&quot; crust_ingredients &lt;- c(grocery_list$baking, &quot;water&quot;) crust_ingredients ## [[1]] ## [1] &quot;flour&quot; ## ## [[2]] ## [1] &quot;yeast&quot; ## ## [[3]] ## [1] &quot;salt&quot; ## ## [[4]] ## [1] &quot;water&quot; essential_toppings &lt;- c(grocery_list$dairy[3], grocery_list$canned_goods[2]) essential_toppings ## [[1]] ## [1] &quot;mozzarella&quot; ## ## [[2]] ## [1] &quot;pizza sauce&quot; yummy_toppings &lt;- c(grocery_list$dairy[c(1, 2, 4)], grocery_list$meat[1], grocery_list[[5]][c(3, 5)]) yummy_toppings ## [[1]] ## [1] &quot;asiago&quot; ## ## [[2]] ## [1] &quot;fontina&quot; ## ## [[3]] ## [1] &quot;blue cheese&quot; ## ## [[4]] ## [1] &quot;bacon&quot; ## ## [[5]] ## [1] &quot;scallions&quot; ## ## [[6]] ## [1] &quot;basil&quot; Basic List Syntax in SAS There are also lists in SAS IML which function similarly to lists in R. To create a named object in a list, precede the name with #. In SAS, the $ operator can be used to get items from a list, using either name or numeric references. proc iml; grocery_list = [ #dairy = [&quot;asiago&quot;, &quot;fontina&quot;, &quot;mozzarella&quot;, &quot;blue cheese&quot;], #baking = [&quot;flour&quot;, &quot;yeast&quot;, &quot;salt&quot;], #canned = [&quot;pepperoni&quot;, &quot;pizza sauce&quot;, &quot;olives&quot;], #meat = [&quot;bacon&quot;, &quot;sausage&quot;, &quot;anchovies&quot;], #veggies= [&quot;bell pepper&quot;, &quot;onion&quot;, &quot;scallions&quot;, &quot;tomatoes&quot;, &quot;basil&quot;] ]; /* print only works on matrices and vectors */ /* so we&#39;ll cheat and load another library to print lists */ package load ListUtil; /* run ListPrint(grocery_list); */ /* This would print the thing, but it&#39;s long */ ick = [grocery_list$&quot;canned&quot;$3, grocery_list$4$2, grocery_list$4$3]; crust = grocery_list$&quot;baking&quot;; call ListAddItem(crust, &quot;water&quot;); /* add an item to a list */ essential_toppings = [grocery_list$&quot;dairy&quot;$3, grocery_list$&quot;canned&quot;$2]; yummy_toppings = [grocery_list$&quot;dairy&quot;$1, grocery_list$&quot;dairy&quot;$2, grocery_list$&quot;dairy&quot;$4, grocery_list$&quot;meat&quot;$1, grocery_list$5$3] ; /* The || is a concatenation operator, like c(). */ /* It is inefficient for large data sets */ run ListPrint(ick); run ListPrint(crust); run ListPrint(yummy_toppings); quit; ——— List = ick——— Item 1 olives Item 2 sausage Item 3 anchovies ——— List = crust——— Item 1 flour Item 2 yeast Item 3 salt Item 4 water ——— List = yummy_toppings——— Item 1 asiago Item 2 fontina Item 3 blue cheese Item 4 bacon Item 5 scallions Try it out Using the list of pizza toppings above as a starting point, make your own list of pizza toppings organized by grocery store section (approximately). Create your own vectors of yummy, essential, and ick toppings, using R and SAS. 2.3.2.2 Data frames (R only) A data frame is a special type of list - one in which each element in the list is a vector of the same length. If you put these vectors side-by-side, you get a table of data that looks like a spreadsheet. The lego version of a data frame looks like this: Figure 2.3: A data frame with data frame 4 columns. A data frame is essentially a list where all of the components are vectors or lists, and are constrained to have the same length. Basic Data Frame Syntax When you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head() command shows the first 6 rows of a data frame (enough to see what’s there, not enough to overflow your screen). head(mtcars) ## A data frame included in base R ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... You can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. mtcars$gpm &lt;- 1/mtcars$mpg # gpm is sometimes used to assess efficiency summary(mtcars$gpm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615 summary(mtcars$mpg) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.40 15.43 19.20 20.09 22.80 33.90 Often, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary(). summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb gpm ## Min. :0.0000 Min. :3.000 Min. :1.000 Min. :0.02950 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 1st Qu.:0.04386 ## Median :0.0000 Median :4.000 Median :2.000 Median :0.05208 ## Mean :0.4062 Mean :3.688 Mean :2.812 Mean :0.05423 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:0.06483 ## Max. :1.0000 Max. :5.000 Max. :8.000 Max. :0.09615 dim(mtcars) ## [1] 32 12 nrow(mtcars) ## [1] 32 ncol(mtcars) ## [1] 12 Missing variables in an R data frame are indicated with NA. Creating an R data frame math_and_lsd &lt;- data.frame(lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41), test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97)) math_and_lsd ## lsd_conc test_score ## 1 1.17 78.93 ## 2 2.97 58.20 ## 3 3.26 67.47 ## 4 4.69 37.47 ## 5 5.83 45.65 ## 6 6.00 32.92 ## 7 6.41 29.97 # add a column - character vector math_and_lsd$subjective &lt;- c(&quot;finally coming back&quot;, &quot;getting better&quot;, &quot;it&#39;s totally better&quot;, &quot;really tripping out&quot;, &quot;is it over?&quot;, &quot;whoa, man&quot;, &quot;I can taste color, but I can&#39;t do math&quot;) math_and_lsd ## lsd_conc test_score subjective ## 1 1.17 78.93 finally coming back ## 2 2.97 58.20 getting better ## 3 3.26 67.47 it&#39;s totally better ## 4 4.69 37.47 really tripping out ## 5 5.83 45.65 is it over? ## 6 6.00 32.92 whoa, man ## 7 6.41 29.97 I can taste color, but I can&#39;t do math Try it out The dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below. data(state) state_facts &lt;- data.frame(state.x77) state_facts &lt;- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) # State names were stored as row labels # Store them in a variable instead, and add it to the data frame row.names(state_facts) &lt;- NULL # get rid of row names head(state_facts) ## state Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area ## 1 Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## 2 Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## 3 Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## 4 Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## 5 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## 6 Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 How many rows and columns does it have? Can you find at least 3 ways to get that information? The Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands. Calculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density? Solutions # 3 ways to get rows and columns str(state_facts) ## &#39;data.frame&#39;: 50 obs. of 9 variables: ## $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... ## $ Population: num 3615 365 2212 2110 21198 ... ## $ Income : num 3624 6315 4530 3378 5114 ... ## $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... ## $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... ## $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... ## $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... ## $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... ## $ Area : num 50708 566432 113417 51945 156361 ... dim(state_facts) ## [1] 50 9 nrow(state_facts) ## [1] 50 ncol(state_facts) ## [1] 9 # Illiteracy state_facts$TotalNumIlliterate &lt;- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) # Population Density state_facts$PopDensity &lt;- state_facts$Population * 1e3/state_facts$Area # in people per square mile # minimum population state_facts$state[which.min(state_facts$PopDensity)] ## [1] &quot;Alaska&quot; Advanced Data Frames: Tibbles and List-columns If at this point you’re bored because you’ve seen this material before, keep reading to find out about tibbles, list columns and other ways to make data frames even more powerful. A tibble is a fancy data frame that is optimized to work with the tidyverse, which is a collection of R packages that make data wrangling (getting the data clean and ready for analysis) easier. You can read about tibbles here. You like data frames? Lists? Let’s put some lists inside a data frame! (All about list columns) Let’s start with the lego picture: (The full explanation is available in slide form here). A list is just another object that could be stored in a data frame! It is a “generalized vector” in that each entry in a list can be thought of as another list - so a list is really a vector of lists. List-columns make it possible to store e.g. whole data sets in a nested, organized way. Another useful feature is that each entry in a list-column doesn’t have to be the same length, which makes it easier to store “ragged” data. You can see a couple of examples here (but they assume that you know things that you’ll only learn in a few modules). It is worth coming back to this link later in the book. I will try to remind you. Data Sets (SAS) The SAS data set structure is similar to a R data frame. In SAS, missing values are indicated with . SAS datasets also come with a description which is attached to the table. The descriptor portion of the data set records names of variables (and attributes), numbers of observations, and date/time stamps of creation and updates. Creating a SAS data set In the next code chunk, we’ll create a data set using a SAS Data step. We’ll talk more about the anatomy of a SAS command later, but for now, notice that I’m specifying some metadata (the title), telling SAS what the variable names are (Drugs, Score), and then providing some data (indicated by the datalines statement). data mathLSD; title &#39;Average math test scores under the influence of LSD&#39;; input Drugs Score; datalines; 1.17 78.93 2.97 58.20 3.26 67.47 4.69 37.47 5.83 45.65 6.00 32.92 6.41 29.97 ; /* Describe the dataset */ proc datasets; contents data = mathLSD; run; proc print data = mathLSD; run; Average math test scores under the influence of LSD Directory Libref WORK Engine V9 Physical Name /tmp/SAS_work4B2E00004B80_yeti Filename /tmp/SAS_work4B2E00004B80_yeti Inode Number 41697413 Access Permission rwx—— Owner Name susan File Size 4KB File Size (bytes) 4096 # Name Member Type File Size Last Modified 1 MATHLSD DATA 128KB 05/09/2021 10:25:28 2 SASMAC3 CATALOG 20KB 05/09/2021 10:25:28 3 SET1 DATA 128KB 05/09/2021 10:25:28 4 SET2 DATA 128KB 05/09/2021 10:25:28 Average math test scores under the influence of LSD Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 05/09/2021 10:25:28 Observation Length 16 Last Modified 05/09/2021 10:25:28 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding utf-8 Unicode (UTF-8) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_work4B2E00004B80_yeti/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 41686769 Access Permission rw-rw-r– Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 The last two blocks are SAS procedures (PROCs). In the first block, I’m asking SAS to describe the contents of the mathMJ dataset. In the second block, I’m telling SAS to print the whole mathMJ dataset out. 2.3.3 Indexing The 1, 2, and multi-dimensional homogeneous data types should be familiar from e.g. linear algebra and calculus. Single elements of a vector can be extracted using single square brackets, e.g. x[1] will get the first element of the vector x. In a matrix, elements are indexed as row, column, so to get the (2, 2) entry of a matrix x, you would use x[2,2]. This is extended for multi-dimensional arrays in R, with each dimension added, e.g. x[3,1,2] or x[4, 3, 2, 1]. To get a full row or column from a matrix (in both SAS and R) you would use x[1,] (get the first row) or x[,3] (get the 3rd column). To select multiple rows or columns from a matrix, you would use x[, c(1, 3)] in R or x[,{1 3}] in SAS - both options get the first and third column of the matrix, with all rows of data included. In both R and SAS, a:b where a and b are numbers will form a sequence from a to b by 1s. So 1:4 is 1, 2, 3, 4. This is often used to get a set of rows or columns: x[3:4, 1:2]. R matrix example x &lt;- matrix(1:20, nrow = 5, byrow = T) # Create a matrix with values 1 to 20, 5 rows, and fill by row x ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 ## [5,] 17 18 19 20 x[3:4, 1:2] ## [,1] [,2] ## [1,] 9 10 ## [2,] 13 14 # Gets a submatrix SAS matrix example In SAS, the same basic code works (though matrix definition is a bit more manual). proc iml; /* Interactive Matrix Language */ x = {1 2 3 4 5, 6 7 8 9 10, 11 12 13 14 15, 16 17 18 19 20}; y = x[3:4, 1:2]; print x; /* Here, print is used instead of put */ print y; quit; /* exit proc IML */ x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Both R and SAS are 1-indexed languages, so the elements of a list or vector are indexed as 1, 2, 3, 4, …8 As R has logical vectors, it is possible to index a vector using a logical vector of the same length. Try it out (From project Euler) If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000. Hint: The modulo operator, %%, gives the integer remainder of one number divided by another. So a %% b gives the integer remainder when dividing a by b. Modular division is often used to find multiples of a number. R solution x &lt;- 1:999 # all nums below 1000 m3 &lt;- (x %% 3) == 0 # multiple of 3 m5 &lt;- (x %% 5) == 0 # multiple of 5 m3or5 &lt;- m3 | m5 sum(x[m3or5]) ## [1] 233168 SAS solution 6 data tmp; 7 do x = 1 to 999; 8 output; 9 end; 10 run; NOTE: The data set WORK.TMP has 999 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 11 12 proc summary data=tmp; /* Summarize data */ 13 where (mod(x, 3) = 0) | (mod(x, 5) = 0); 14 /* Keep only obs where x is divisible by 3 or 5 */ 15 16 var x; /* what variable we want the summary for */ 17 18 output out=sum_x sum=; /* output sum_x to a new dataset */ 19 run; NOTE: There were 466 observations read from the data set WORK.TMP. WHERE (MOD(x, 3)=0) or (MOD(x, 5)=0); NOTE: The data set WORK.SUM_X has 1 observations and 3 variables. NOTE: PROCEDURE SUMMARY used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 20 21 proc print data = sum_x; /* print our sum_x dataset */ 22 run; NOTE: There were 1 observations read from the data set WORK.SUM_X. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Obs TYPE FREQ x 1 0 466 233168 Note on the SAS code: where statements allow you to select part of the data for further processing. There was a note earlier about the fact that type conversion doesn’t happen in where clauses… this is one of those clauses. We’ll get into where clauses in more detail later, in module 5. Most complicated structures in R are actually lists underneath. You should be able to access any of the pieces of a list using a combination of named references and indexing. If you have trouble distinguishing between $, [, and [[, you’re not alone. The R for Data Science book has an excellent illustration, which I will summarize for you here in abbreviated form (pictures directly lifted from the book). R4DS indexing illustration x x[1] x[[1]] x[[1]][[1]] 2.4 Control structures 2.4.1 If statements If statements are just about as simple in programming as they are in real life. Figure 2.4: Source. I’ve actually met some programmers who talk like this in real life. General structure of an if statement In general, the structure of an if statement is if (condition) then { # do something here } If the condition is true, the inner code will be executed. Otherwise, nothing happens. You can add an else statement that will execute if the condition is not true if (condition) then { # do something } else { # do a different thing } And in some languages, you can even have many sets of if statements: if (condition) { # do something } else if (condition 2) { # do something else } else { # do a third thing } Note that this could also be written (perhaps more clearly) as: if (condition) { # do something } else { if (condition 2) { # do something else } else { # do a third thing } } That is, condition 2 is only checked once it is known that condition is false. Often, programmers use logic flow maps, like the one shown below, to map out a logical sequence and ensure that every possible value is handled appropriately. Example: If/then logic in SAS and R The syntax for conditional statements using if/then logic is shown below using an example where Santa must determine which members of a household will receive a toy for Christmas and which members will receive coal.9 In R tmp &lt;- data.frame(name = c(&quot;Alex&quot;, &quot;Edison&quot;, &quot;Susan&quot;, &quot;Ryan&quot;), status = c(&quot;naughty&quot;, &quot;nice&quot;, NA, &quot;neutral&quot;), stringsAsFactors = F) # Santa&#39;s decision process if (tmp$status == &quot;naughty&quot;) { tmp$present &lt;- &quot;coal&quot; } else { tmp$present &lt;- &quot;toy&quot; } ## Warning in if (tmp$status == &quot;naughty&quot;) {: the condition has length &gt; 1 and only ## the first element will be used tmp ## name status present ## 1 Alex naughty coal ## 2 Edison nice coal ## 3 Susan &lt;NA&gt; coal ## 4 Ryan neutral coal What happened? When evaluating if statements, R does not evaluate each entry in the vector tmp$status separately. Instead, it takes the first value and issues a warning message. One option would be to use a loop, and examine each row in the data set separately. We’ll talk about loops in the next subsection. Another option is to use the ifelse() function, which is ifelse(condition, thing to do if condition is true, thing to do if condition is false) tmp$present &lt;- ifelse(tmp$status == &quot;naughty&quot;, &quot;coal&quot;, &quot;toy&quot;) tmp ## name status present ## 1 Alex naughty coal ## 2 Edison nice toy ## 3 Susan &lt;NA&gt; &lt;NA&gt; ## 4 Ryan neutral toy When R evaluates a missing value, (so ? NA == “naughty”), the result is NA. This is fine for us - if we don’t have data on whether someone is naughty or nice, maybe we don’t need to give them a present at all. But “neutral” is evaluated as getting a toy. Do we want that to happen? Maybe not. We might have to nest ifelse statements to solve this issue… tmp$present &lt;- ifelse(tmp$status == &quot;naughty&quot;, &quot;coal&quot;, ifelse(tmp$status == &quot;nice&quot;, &quot;toy&quot;, NA)) tmp ## name status present ## 1 Alex naughty coal ## 2 Edison nice toy ## 3 Susan &lt;NA&gt; &lt;NA&gt; ## 4 Ryan neutral &lt;NA&gt; In SAS In a data step: data santa; input name $ status $; datalines; Edison nice Alex naughty Susan . Ryan neutral ; /* Modify santa_list and make a new dataset, present_list */ data presents; set santa; if status = &quot;naughty&quot; then present = &quot;coal&quot;; else present = &quot;toy&quot;; run; /* must end with run if no datalines option */ proc print data=presents; run; Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy Note that ., or missing data is handled the same as ‘nice.’ That might not be what we wanted… this is the natural thing to do, right? 6 data santa; 7 input name $ status $; 8 datalines; NOTE: The data set WORK.SANTA has 4 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 13 ; 14 15 /* Modify santa_list and make a new dataset, present_list */ 16 data presents; 17 set santa; 18 if status = &quot;naughty&quot; then present = &quot;coal&quot;; 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ______ 22 19 ! .); ERROR: Undeclared array referenced: else. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 388 19 ! .); ERROR 22-322: Syntax error, expecting one of the following: !, !!, &amp;, (, *, **, +, &#39;,&#39;, -, /, &lt;, &lt;=, &lt;&gt;, =, &gt;, &gt;&lt;, &gt;=, AND, EQ, GE, GT, IN, LE, LT, MAX, MIN, NE, NG, NL, NOTIN, OR, [, ^=, {, |, ||, ~=. ERROR 388-185: Expecting an arithmetic operator. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 202 19 ! .); ERROR 202-322: The option or parameter is not recognized and will be ignored. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 388 19 ! .); ERROR 388-185: Expecting an arithmetic operator. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 202 19 ! .); ERROR 202-322: The option or parameter is not recognized and will be ignored. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = 19 ! .); _ 22 ERROR 22-322: Syntax error, expecting one of the following: +, =. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = 19 ! .); _ 76 ERROR 76-322: Syntax error, statement will be ignored. 20 run; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 19:54 NOTE: The SAS System stopped processing this step because of errors. NOTE: Due to ERROR(s) above, SAS set option OBS=0, enabling syntax check mode. This prevents execution of subsequent data modification statements. WARNING: The data set WORK.PRESENTS may be incomplete. When this step was stopped there were 0 observations and 4 variables. WARNING: Data set WORK.PRESENTS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ! /* must end with run if no datalines option */ 21 22 proc print data=presents; 23 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 4. SAS doesn’t handle nested if statements very well - they can be ambiguous. Instead, SAS documentation suggests using do; and end; to denote the start and end points of each if statement (like the {} in R). data santa; input name $ status $; datalines; Edison nice Alex naughty Susan . Ryan neutral ; data presents; set santa; if status = &quot;naughty&quot; then do; present = &quot;coal&quot;; end; else if status = &quot;nice&quot; then do; present = &quot;toy&quot;; end; else do; present = .; end; run; proc print data=presents; run; Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan . 4 Ryan neutral . Interestingly, if you set a character variable to be missing, SAS converts it to ‘.’ So, if we actually want to have the value be missing, we can set it to an empty string. data santa; input name $ status $; datalines; Edison nice Alex naughty Susan . Ryan neutral ; data presents; set santa; if status = &quot;naughty&quot; then do; present = &quot;coal&quot;; end; else if status = &quot;nice&quot; then do; present = &quot;toy&quot;; end; else do; present = &#39;&#39;; end; run; proc print data=presents; run; Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan 4 Ryan neutral Now things work the way we expected them to work. There are more complicated if-statement like control structures, such as switch statements, which can save time and typing. In the interests of simplicity, we will skip these for now, as any conditional can be implemented with sequences of if statements in the proper order. If you would like to read about switch statements, here are links to SAS case statement documentation and base R switch statement explanation and documentation. Try it out The sample() function selects a random sample of entries from a vector. Suppose we sample a random vector \\(x\\) with 10 entries. Write one or more if statements to fulfill the following conditions if \\(x\\) is divisible by 2, \\(y\\) should be positive; otherwise, it should be negative. if \\(x\\) is divisible by 3, \\(y\\) should have a magnitude of 2; otherwise, it should have a magnitude of 1. It may be helpful to define separate variables y_mag and y_sign and then multiply them afterwards. Once you have found the value of \\(y\\) compute \\(\\text{sum}(x * y)\\). You may use the following R and SAS code skeletons to set the problem up. set.seed(342502837) x &lt;- sample(1:50, size = 20, replace = F) # Conditional statements go here sum(x * y) ## [1] 1567.609 proc iml; call randseed(342502837); x = sample(1:50, 20)`; create sampledata from x [colname = &quot;x&quot;]; append from x; close; quit; data xy; set sampledata; /* Conditional statements go here */ /* Leave this so that the code below works */ res = x * y; run; proc summary data=xy; /* Summarize data */ var res; /* what variable we want the summary for */ output out=tmpsum sum=; /* output tmpsum to a new dataset */ run; proc print data = xy; /* print our original dataset to check result */ var x y res; sum res; run; proc print data = tmpsum; /* print our tmpsum dataset */ run; R Solution set.seed(342502837) x &lt;- sample(1:50, size = 20, replace = F) y_sign &lt;- ifelse(x %% 2 == 0, 1, -1) y_mag &lt;- ifelse(x %% 3 == 0, 2, 1) y &lt;- y_sign * y_mag sum(x * y) ## [1] 157 SAS Solution 6 proc iml; NOTE: IML Ready 7 call randseed(342502837); 8 x = sample(1:50, 20)`; 9 create sampledata from x [colname = &quot;x&quot;]; 10 append from x; 11 close; NOTE: Closing WORK.SAMPLEDATA NOTE: The data set WORK.SAMPLEDATA has 20 observations and 1 variables. 12 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.01 seconds cpu time 0.00 seconds 13 14 data xy; 15 set sampledata; 16 17 y_sign = 0 * x; 18 y_mag = 0 * x; 19 20 /* Conditional statements go here */ 21 if MOD(x, 2) = 0 then y_sign = 1; 22 else y_sign = -1; 23 if MOD(x, 3) = 0 then y_mag = 2; 24 else y_mag = 1; 25 26 y = y_sign * y_mag; 27 res = x * y; 28 run; NOTE: There were 20 observations read from the data set WORK.SAMPLEDATA. NOTE: The data set WORK.XY has 20 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 proc summary data=xy; /* Summarize data */ 31 var res; /* what variable we want the summary for */ 32 33 output out=tmpsum sum=; /* output tmpsum to a new dataset */ 34 run; NOTE: There were 20 observations read from the data set WORK.XY. NOTE: The data set WORK.TMPSUM has 1 observations and 3 variables. NOTE: PROCEDURE SUMMARY used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 35 36 37 proc print data = xy; /* print our original dataset to check 37 ! result */ 38 var x y res; 39 sum res; 40 run; NOTE: There were 20 observations read from the data set WORK.XY. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 41 42 proc print data = tmpsum; /* print our tmpsum dataset */ 43 run; NOTE: There were 1 observations read from the data set WORK.TMPSUM. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Obs x y res 1 44 1 44 2 42 2 84 3 1 -1 -1 4 7 -1 -7 5 42 2 84 6 13 -1 -13 7 27 -2 -54 8 19 -1 -19 9 41 -1 -41 10 16 1 16 11 6 2 12 12 44 1 44 13 26 1 26 14 48 2 96 15 22 1 22 16 15 -2 -30 17 40 1 40 18 1 -1 -1 19 27 -2 -54 20 13 -1 -13 235 Obs TYPE FREQ res 1 0 20 235 See this to understand how the print statement works and how to add column summary values. 2.4.2 Loops Often, we need to do a single task many times - for instance, we may need to calculate the average data value for each week, using daily data. Rather than typing out 52 different iterations of the same code, it is likely easier to type out one single block of code which contains the steps necessary to complete one instance of the task, and then leverage variables to ensure that each task is completed the correct number of times, using the correct inputs. Let us start with the most generic loop written in pseudocode (code that won’t work, but provides the general idea of the steps which are taken) loop_invocation(iteration variable, exit condition) { # Steps to repeat } We use the loop_invocation function to indicate what type of loop we use. We have at least one iteration variable that indicates where in the looping process we currently are. This may be an index (if we want to do something 500 times, it would take values from 1 to 500), or it may take a more complicated sequence of values (for instance, if we are testing convergence, we might put some sort of delta variable as the iteration variable). Most loops also have an explicit exit condition that is part of the loop invocation; more rarely, a loop may depend on break statements that cause the control flow of the code to exit. Without some sort of exit condition, our program would run forever, which is… not optimal. 2.4.2.1 Count controlled loops (FOR loops) In a for loop, the steps in the loop body repeat a specified number of times. That is, for each value in a sequence, the steps within the loop are repeated. Another explanation of for loops is available at Khan Academy. Example: Santa and if/else + loops in R (plus some debugging strategies) For instance, suppose we want to revisit our R Santa example from the previous section. The original if/else code we wrote in R didn’t work, because R evaluates if statements using a single (scalar or vector of length 1) condition. If we add a loop around that code, we can evaluate only one row at a time. We need to check every row, so we’ll iterate over 1:nrow(tmp) - it’s better to get the upper bound from the data frame, rather than just using 4 - if we add another entry, the code will still work if we’re using nrow(tmp) to define how many iterations we need. We start by defining our data frame: tmp &lt;- data.frame(name = c(&quot;Alex&quot;, &quot;Edison&quot;, &quot;Susan&quot;, &quot;Ryan&quot;), status = c(&quot;naughty&quot;, &quot;nice&quot;, NA, &quot;neutral&quot;), stringsAsFactors = F) And then we add the basic loop syntax: for (i in 1:nrow(tmp)) { } For some reason, i is often used as the iteration variable (with j and k for nested loops). What this loop says is that i will first take on the value 1, then 2, then 3, then 4. On each iteration, i will advance to the next value in the vector of options we have provided. Now we need to add the middle part by adapting the conditional statement we used before so that it looks at only the ith row. I’ve also added the catch-all else condition that assigns NA for any value that isn’t “naughty” or “nice.” It’s good practice to initialize your variable (create a column for it) ahead of time and set the variable to a default value. tmp$present &lt;- NA # Initialize column and set to NA by default for (i in 1:nrow(tmp)) { # Santa&#39;s decision process if (tmp$status[i] == &quot;naughty&quot;) { tmp$present[i] &lt;- &quot;coal&quot; } else if (tmp$status[i] == &quot;nice&quot;) { tmp$present[i] &lt;- &quot;toy&quot; } else { tmp$present[i] &lt;- NA_character_ # use a special NA value that has # character type to avoid any issues } } ## Error in if (tmp$status[i] == &quot;naughty&quot;) {: missing value where TRUE/FALSE needed Well, that didn’t work! We can see that the loop stopped at i = 3 by printing out the value of i - because the loop failed, i will still contain the value which caused the loop to stop. i ## [1] 3 tmp[i,] # print tmp at that point ## name status present ## 3 Susan &lt;NA&gt; &lt;NA&gt; Combining this information with the error above, we can guess that R stopped evaluating the loop because the if statement returned NA (missing) instead of TRUE or FALSE. if/else statements in R can’t evaluate to NA, so we need to restructure our conditional statement - first, we’ll test for NA values, then, we can test for naughty and nice, and we’ll keep the catch-all statement at the bottom. We’ll test for an NA value using the function is.na(). tmp$present &lt;- NA # Initialize column and set to NA by default for (i in 1:nrow(tmp)) { # Santa&#39;s decision process if (is.na(tmp$status[i])) { tmp$present[i] &lt;- NA_character_ } else if (tmp$status[i] == &quot;naughty&quot;) { tmp$present[i] &lt;- &quot;coal&quot; } else if (tmp$status[i] == &quot;nice&quot;) { tmp$present[i] &lt;- &quot;toy&quot; } else { tmp$present[i] &lt;- NA_character_ } } tmp ## name status present ## 1 Alex naughty coal ## 2 Edison nice toy ## 3 Susan &lt;NA&gt; &lt;NA&gt; ## 4 Ryan neutral &lt;NA&gt; Now the if/else logic works exactly as intended. This is longer than the version using ifelse(), but it is perhaps more readable. In most cases in R and SAS, it is possible to write code without needing loops at all, because both languages are vector-based - they will often use vectorized functions which implicitly loop over each row without having to write a loop to do so. ifelse() is a vectorized version of if() {} else {}. Here is an example of the most basic for loop logic - printing the numbers 1 through 10 - in both R and SAS. SAS code is provided for both PROC IML and DATA steps. For loops in R # R Example loop for (i in 1:10) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 “For loops” in SAS IML (using do) 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready 8 do i = 1 to 10; 9 print i; 10 end; 10 ! /* This ends the loop definition */ 11 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.02 seconds cpu time 0.02 seconds i 1 i 2 i 3 i 4 i 5 i 6 i 7 i 8 i 9 i 10 “For loops” in a SAS DATA step 6 data A; 7 do i = 1 to 10; 8 put i=; 9 end; /* This ends the loop definition */ 10 run; i=1 i=2 i=3 i=4 i=5 i=6 i=7 i=8 i=9 i=10 NOTE: The data set WORK.A has 1 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds While the most straighforward (and common) case of for-loop use in practice is to count from 1 to N, both R and SAS allow for loops to use other sequence structures. Other sequences in loops in R R allows loops to occur over any vector… even randomly generated numbers, or nonnumeric vectors (say, a character vector of URLs). x &lt;- rnorm(5) # Generate 5 normal (0,1) samples for (i in x) { print(i^2) } ## [1] 4.500349 ## [1] 3.820737 ## [1] 2.656216 ## [1] 1.120775 ## [1] 2.10086 We can also iterate by non-integer values using seq(from = , to = , by = ) # This loop counts down in 1/2 units from 5 to 0 for (i in seq(5, 0, -.5)) { # do nothing } Other sequence structures in SAS for loops We can iterate by non-integer values: 6 data A; 7 y = 0; 8 do i = 5 to 0 by -0.5; 9 put i=; 10 end; 11 run; i=5 i=4.5 i=4 i=3.5 i=3 i=2.5 i=2 i=1.5 i=1 i=0.5 i=0 NOTE: The data set WORK.A has 1 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds We can even add additional conditions: 6 data A; 7 y = 0; 8 do i = 5 to 0 by -0.5 while (i**2 &gt; 1); 9 put i=; 10 end; 11 run; i=5 i=4.5 i=4 i=3.5 i=3 i=2.5 i=2 i=1.5 NOTE: The data set WORK.A has 1 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Try it out (in R) The beepr package plays sounds in R to alert you when your code has finished running (or just to annoy your friends and classmates). (Documentation) We’ll learn more about packages in the next chapter, but for now, just go with it. You can install the package using the following command: install.packages(&quot;beepr&quot;) (if you are using Linux you will also need to make sure one of paplay, aplay, or vlc is installed) Load the library and write a for loop which plays the 10 different sounds corresponding to integers 1 through 10. library(beepr) # load the beepr library beep(sound = 1) # sound is any integer between 1 and 10. It may be helpful to add the command Sys.sleep(5) into your loop to space out the noises so that they can be heard individually. Solution library(beepr) for (i in 1:10) { beep(sound = i) Sys.sleep(5) } Try it out (in SAS) Write a for loop which will output the first 30 fibbonacci numbers. You can use the following code as a starting point: 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready 8 current = 1; 9 prev = 0; 10 11 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 Solution 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready 8 current = 1; 9 prev = 0; 10 11 do i = 1 to 30; 12 new = current + prev; 13 prev = current; 14 current = new; 15 print current; 16 end; 16 ! /* This ends the loop definition */ 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.03 seconds cpu time 0.04 seconds 18 current 1 current 2 current 3 current 5 current 8 current 13 current 21 current 34 current 55 current 89 current 144 current 233 current 377 current 610 current 987 current 1597 current 2584 current 4181 current 6765 current 10946 current 17711 current 28657 current 46368 current 75025 current 121393 current 196418 current 317811 current 514229 current 832040 current 1346269 2.4.2.2 Condition-controlled loops (WHILE, DO WHILE) Frequently, we do not know how many times a loop will need to execute a priori. We might be converging on a value, and want to repeat the calculation until the new value is within an acceptably epsilon of the previous iteration. In these cases, it can be helpful to use a WHILE loop, which loops while the condition is true (another variant, the do-while loop, is similar, except that a do-while loop will always execute once, and checks the condition at the end of the iteration). If a WHILE loop condition is never falsified, the loop will continue forever. Thus, it is usually wise to include a loop counter as well, and a condition to terminate the loop if the counter value is greater than a certain threshold. Another explanation of while loops is available at Khan Academy. Example: The Basel Problem Let’s solve the Basel problem in R and SAS using WHILE loops - we’ll repeat the calculation until the value changes by less than 0.000001. The Basel problem is the problem of calculating the precise infinite summation \\[\\sum_{n=1}^\\infty \\frac{1}{n^2}\\] We’ll stick to calculating it computationally. In R # Start out by defining your starting values outside of the loop i &lt;- 1 basel_value &lt;- 0 # initial guess prev_basel_value &lt;- -Inf # previous value while (abs(basel_value - prev_basel_value) &gt; 0.000001) { prev_basel_value &lt;- basel_value # update condition basel_value &lt;- basel_value + 1/i^2 i &lt;- i + 1 # Prevent infinite loops if (i &gt; 1e6) { break } # Monitor the loop to know that it&#39;s behaving if (i %% 200 == 0) { print(c(&#39;i = &#39; = i, &#39;prev&#39; = prev_basel_value, &#39;current&#39; = basel_value, diff = basel_value - prev_basel_value)) } } ## i = prev current diff ## 2.000000e+02 1.639896e+00 1.639922e+00 2.525189e-05 ## i = prev current diff ## 4.000000e+02 1.642425e+00 1.642431e+00 6.281368e-06 ## i = prev current diff ## 6.000000e+02 1.643263e+00 1.643266e+00 2.787060e-06 ## i = prev current diff ## 8.000000e+02 1.643682e+00 1.643683e+00 1.566414e-06 ## i = prev current diff ## 1.000000e+03 1.643933e+00 1.643934e+00 1.002003e-06 i ## [1] 1001 basel_value ## [1] 1.643935 prev_basel_value ## [1] 1.643934 In SAS 6 proc iml; NOTE: IML Ready 7 i = 1; 8 basel = 0; 9 prev = -1; 10 do while((basel - prev) &gt; 1e-6); 11 prev = basel; 12 basel = basel + 1/i**2; 12 ! /* ** is the exponent operator */ 13 i = i + 1; 14 15 if i &gt; 1e6 then 16 do; 17 leave; 18 end; 19 20 if MOD(i, 200) = 0 then 21 do; 22 print i, prev, basel; 23 end; 24 end; 25 26 print i, basel; 27 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 28 i 200 prev 1.6398963 basel 1.6399215 i 400 prev 1.6424247 basel 1.6424309 i 600 prev 1.6432632 basel 1.643266 i 800 prev 1.6436817 basel 1.6436833 i 1000 prev 1.6439326 basel 1.6439336 i 1001 basel 1.6439346 Try it out Write a while loop in R and in SAS to calculate \\(\\displaystyle \\lim_{x \\rightarrow 4} \\frac{2 - \\sqrt{x}}{4-x}\\) by starting at 3 and halving the distance to 4 with each iteration. Exit the loop when you are within 1e-6 of the value computed on the previous iteration, or when you are within 1e-6 from 4. Which exit condition did you hit first? How do you know? Solutions x &lt;- 3 dist &lt;- 4 - x current_value &lt;- 0 prev_value &lt;- -Inf while (abs(current_value - prev_value) &gt; 1e-6 &amp; dist &gt; 1e-6) { prev_value &lt;- current_value dist &lt;- dist/2 x &lt;- 4 - dist current_value &lt;- (2 - sqrt(x))/(4-x) } c(x = x, dist = dist, current_value = current_value, d_value = abs(current_value - prev_value)) ## x dist current_value d_value ## 3.999939e+00 6.103516e-05 2.500010e-01 9.536961e-07 Before \\(x\\) got to 4 - 1e-6, the change in f(x) became less than 1e-6. 6 proc iml; NOTE: IML Ready 7 x = 3; 8 dist = 4 - x; 9 fx = 0; 10 prev_fx = 1; 11 dfx = abs(fx - prev_fx); 12 do while(dfx &gt; 1e-6 &amp; dist &gt; 1e-6); 13 prev_fx = fx; 14 dist = dist/2; 15 x = 4 - dist; 16 fx = (2 - sqrt(x))/(4 - x); 17 dfx = abs(fx - prev_fx); 18 end; 19 20 print x, dist, fx, dfx; 21 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 22 x 3.999939 dist 0.000061 fx 0.250001 dfx 9.537E-7 2.4.2.3 Other Loops and Interative Structures There are many different ways to implement iteration in any language, including very low-level controls like repeat (in R). Higher level iteration may include a FOREACH loop, where a series of commands is applied to a list or vector (the *apply commands in R are examples of this). An additional method of iteration that requires functions is the recursion (where a function calls itself). In every case, these alternative loop structures can be translated to for or while loops. 2.5 Overgrown Calculators While R and SAS are both extremely powerful statistical programming languages, the core of both languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in R and SAS. In this section, we will essentially be using both R and SAS as overgrown calculators. Operation R SAS Addition + + Subtraction - - Elementwise Multiplication * # Matrix/Vector Multiplication %*% * Division \\ \\ Elementwise Exponentiation ^ ## Matrix Exponentiation ^ ** Matrix Transpose t(A) `A\\`` R basic mathematical operators # transpose these to make row vectors to match SAS x &lt;- t(1:10) y &lt;- t(seq(3, 30, by = 3)) x + y ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 4 8 12 16 20 24 28 32 36 40 x - y ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -2 -4 -6 -8 -10 -12 -14 -16 -18 -20 x * y ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 3 12 27 48 75 108 147 192 243 300 x / y ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 ## [,8] [,9] [,10] ## [1,] 0.3333333 0.3333333 0.3333333 x^2 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 4 9 16 25 36 49 64 81 100 t(x) %*% y ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 3 6 9 12 15 18 21 24 27 30 ## [2,] 6 12 18 24 30 36 42 48 54 60 ## [3,] 9 18 27 36 45 54 63 72 81 90 ## [4,] 12 24 36 48 60 72 84 96 108 120 ## [5,] 15 30 45 60 75 90 105 120 135 150 ## [6,] 18 36 54 72 90 108 126 144 162 180 ## [7,] 21 42 63 84 105 126 147 168 189 210 ## [8,] 24 48 72 96 120 144 168 192 216 240 ## [9,] 27 54 81 108 135 162 189 216 243 270 ## [10,] 30 60 90 120 150 180 210 240 270 300 SAS basic mathematical operators By default, SAS creates row vectors with do(a, b, by = c) syntax. The transpose operator (a single backtick) can be used to transform A into A`. 6 proc iml; NOTE: IML Ready 7 x = do(1, 10, 1); 8 y = do(3, 30, 3); 9 10 z = x + y; 11 z2 = x - y; 12 z3 = x # y; 13 z4 = x/y; 14 z5 = x##2; 15 z6 = x` * y; 16 print z, z2, z3, z4, z5, z6; 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.07 seconds cpu time 0.07 seconds z COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 4 8 12 16 20 24 28 32 36 40 z2 COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 -2 -4 -6 -8 -10 -12 -14 -16 -18 -20 z3 COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 3 12 27 48 75 108 147 192 243 300 z4 COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 z5 COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 1 4 9 16 25 36 49 64 81 100 z6 COL1 COL2 COL3 COL4 COL5 COL6 COL7 COL8 COL9 COL10 ROW1 3 6 9 12 15 18 21 24 27 30 ROW2 6 12 18 24 30 36 42 48 54 60 ROW3 9 18 27 36 45 54 63 72 81 90 ROW4 12 24 36 48 60 72 84 96 108 120 ROW5 15 30 45 60 75 90 105 120 135 150 ROW6 18 36 54 72 90 108 126 144 162 180 ROW7 21 42 63 84 105 126 147 168 189 210 ROW8 24 48 72 96 120 144 168 192 216 240 ROW9 27 54 81 108 135 162 189 216 243 270 ROW10 30 60 90 120 150 180 210 240 270 300 Other matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy: R matrix operations mat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T) t(mat) # transpose ## [,1] [,2] [,3] ## [1,] 1 6 7 ## [2,] 2 4 8 ## [3,] 3 5 9 det(mat) # get the determinant ## [1] 18 diag(mat) # get the diagonal ## [1] 1 4 9 diag(diag(mat)) # get a square matrix with off-diag 0s ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 4 0 ## [3,] 0 0 9 diag(1:3) # diag() also will create a diagonal matrix if given a vector ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 2 0 ## [3,] 0 0 3 SAS matrix operations 6 proc iml; NOTE: IML Ready 7 mat = {1 2 3, 6 4 5, 7 8 9}; 8 tmat = mat`; 8 ! /* transpose */ 9 determinant = det(mat); 9 ! /* get the determinant */ 10 diagonal_vector = vecdiag(mat); 10 ! /* get the diagonal as a 10 ! vector */ 11 diagonal_mat = diag(mat); 11 ! /* get the diagonal as a square 11 ! matrix */ 12 /* with 0 on off-diagonal entries */ 13 14 dm = diag({1 2 3}); 14 ! /* make a square matrix with vector as the 14 ! diagonal */ 15 16 print tmat, determinant, diagonal_vector, diagonal_mat, dm; 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.01 seconds cpu time 0.01 seconds tmat 1 6 7 2 4 8 3 5 9 determinant 18 diagonal_vector 1 4 9 diagonal_mat 1 0 0 0 4 0 0 0 9 dm 1 0 0 0 2 0 0 0 3 The other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, in both languages, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. Invert a matrix in R mat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T) minv &lt;- solve(mat) # get the inverse minv ## [,1] [,2] [,3] ## [1,] -0.2222222 0.3333333 -0.1111111 ## [2,] -1.0555556 -0.6666667 0.7222222 ## [3,] 1.1111111 0.3333333 -0.4444444 mat %*% minv ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Invert a matrix in SAS Documentation 6 proc iml; NOTE: IML Ready 7 mat = {1 2 3, 6 4 5, 7 8 9}; 8 9 mat_inv = solve(mat, diag({1 1 1})); 9 ! /* get the inverse */ 10 mat_inv2 = inv(mat); 10 ! /* less efficient and less accurate */ 11 print mat_inv, mat_inv2; 12 13 id = mat * mat_inv; 14 id2 = mat * mat_inv2; 15 print id, id2; 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.02 seconds cpu time 0.02 seconds mat_inv -0.222222 0.3333333 -0.111111 -1.055556 -0.666667 0.7222222 1.1111111 0.3333333 -0.444444 mat_inv2 -0.222222 0.3333333 -0.111111 -1.055556 -0.666667 0.7222222 1.1111111 0.3333333 -0.444444 id 1 -2.22E-16 0 0 1 0 0 -8.88E-16 1 id2 1 -2.22E-16 0 0 1 0 0 -8.88E-16 1 References and Links Non-exhaustive list of general R and SAS references used in this chapter: SAS Matrix reference SAS Data set documentation Creating SAS Data Sets from IML (also this friendly guide and this blog post) SAS Data Step options SAS Mathematical Operators Lists and Data Structures in SAS Loops in SAS and SAS documentation for DO WHILE loops Random number generation in SAS SAS and R compared (by SAS) Repeatable random number generation in R Data structures in Advanced R 2.5.1 Cheat Sheets and Reference Cards SAS Cheatsheet (from another class like this) SAS Cheatsheet (by SAS) R Cheatsheet - this is a simplified cheat sheet offered by RStudio. R Cheatsheet (classic) SAS Programming for R Users (free book) and github site with training materials R programming for SAS users - site for the book, plus link to a free early version of the book (the book is now published) 2.5.2 SAS (as taught in other places) Introduction to SAS - Undergraduate course at Penn State Intermediate SAS - Undergraduate course at Penn State Advanced SAS - Undergraduate course at Penn State 2.5.3 R courses (as taught elsewhere) and Textbooks Stat 579 at Iowa State (as taught by Heike Hofmann) Stat 545 at Univ. British Columbia (developed by Jenny Bryan) R for Data Science - R textbook (free) by Hadley Wickham and Garret Grolemund Advanced R by Hadley Wickham 2.5.4 Combination of R and SAS courses Stat 850 as taught by Chris Bilder at UNL 2.5.5 Non-Exhaustive List of Sources used to aggregate “core programming concepts”: https://blog.upperlinecode.com/computer-language-fundamentals-five-core-concepts-1aa43e929f40 https://howtoprogramwithjava.com/programming-101-the-5-basic-concepts-of-any-programming-language/ https://dev.to/lucpattyn/basic-programming-concepts-for-beginners-2o73 http://livecode.byu.edu/programmingconcepts/ControlStruct.php http://holowczak.com/programming-concepts-tutorial-programmers/ There is also an R package dedicated to pure evil that will set F and T randomly on startup. Use this information wisely.↩︎ numeric is not really a type, it’s a mode. Run ?mode for more information.↩︎ While there are both Duplo and Lego in my house, my toddler is a lot more willing to share than my husband, so Duplo will have to do.↩︎ Most languages are 0-indexed languages: C, C++, python, Java, javascript. Vectors in these languages are indexed as 0, 1, 2, 3. Other 1-indexed languages include FORTRAN, Matlab, Julia, Mathematica, and Lua, many of which were intended for mathematical processing or data analysis.↩︎ Traditionally, naughty children get coal, while nice children get toys or candy.↩︎ "],["organization.html", "Module 3 Organization: Packages, Functions, Scripts, and Documents Module Objectives 3.1 Reproducibility and Markdown 3.2 Functions and Modules 3.3 Procs and Data steps 3.4 Scripts 3.5 Packages References and Links", " Module 3 Organization: Packages, Functions, Scripts, and Documents Module Objectives Compose markdown documents with code and text Write functions and scripts in R and SAS to solve basic problems Analyze additional software available from package and software repositories to determine whether it is suitable for the problem 3.1 Reproducibility and Markdown The concepts of replication and reproducibility are central to science - we do not trust studies whose results cannot be replicated by additional repetitions of the experiment, and we do not trust statistical analyses whose results are not backed up by valid methods that can be re-run to verify the reported results. While replication covers the lab methods, experimental design, and data collection procedures, reproducibility is concerned with the code and the data from an experiment which has already been run. Specifically, the idea is that the research paper is basically an advertisement - by exposing the code and data used in the analysis, readers can engage with the core of the analysis methods, leading to better peer feedback as well as easier adoption of the research for future work. Reproducibility has several advantages: It allows you to show the correctness of your results Trying to reproduce an analysis from the data and the description in the journal article is… challenging, if not impossible. By providing the raw data and code to take the data from raw form to analysis results, readers can verify the legitimacy of each step in the analysis. This allows researchers to review each others’ methods, finding mistakes due to bugs in the software used or due to implementation errors. In one particularly prominent failure of reproducibility, a study used to support macroeconomic theories that shaped the response to the 2008-2009 recession negatively correlating national debt with gdp growth was found to be flawed due to an excel indexing mistake. Use of GUI-based (graphical user interface) statistical analysis software may make it harder to identify these mistakes, because the formulas and code are not visually displayed. It allows others to use your results more easily By sharing your code and raw data, you provide the wider scientific community with the ability to use your results to build new scientific studies. This increases the relevance of your work, the number of citations your papers get, and you also benefit from the community adopting a culture of openness and reproducibility. It allows you to use your results and code more easily In 2 years, when you need to find the code you used for that analysis in XXX paper, you’ll be able to find the code (and the data) to see how it worked and what you did. The code may or may not run as-is (depending on software versioning, package updates, etc.), but you will have the methods clearly documented along with the data (so it’s easy to replicate the data format needed, etc.) There are other advantages (personal and public) described in an issue of Biostatistics dedicated to reproducibility. David Donoho’s response is particularly useful. As you might expect, there are many different types of reproducibility. Code reproducibility - allows replication of the computing aspects of the study. Includes code, software, hardware, and implementation details. Data reproducibility - allows replication of the non-computational parts of the study (e.g. experiment and data collection). This may include making protocols and data available. Statistical reproducibility - allows replication of the statistical methods. Includes details about model parameters, thresholds, etc. and may also include study pre-registration to prevent p-hacking. There are also many levels of reproducibility. Much of the computer code written in the 1960s is no longer runnable today, because the computer architecture it was written for is not available anymore. Code which depends on URLs is vulnerable to website rearrangements or the content no longer being hosted. Archiving projects on GitHub is nice, but what happens if GitHub goes down? Even small package version updates can break code so that it no longer runs as it once did. It’s important to decide what type of reproducibility is important for a particular project, and then design the project’s workflow around that process. For most of my projects, I don’t worry about software versioning too much.10 As much as possible, I keep the code and the data (if it’s small) on GitHub in a public repository for people to access, along with any manuscripts or presentations related to the project. Manuscripts are written in knitr or Rmarkdown, so that the code is documented by the context of the project, and every image in the article generated by R has corresponding code available. This ensures that my code (and data) is stored somewhere off-site (backed up in the cloud) my code is available if others want to use it I can track my contributions to a project relative to any collaborators I can undo changes that I make if something in the code breaks. I can undo changes my collaborators inadvertently make because all changes are recorded. I can reuse blocks of code easily (and find them easily on GitHub) In situations where I run experiments, I also make sure that any experimental stimuli or other code that would contribute to the execution and data collection part of the experiment are also included in the repository. This may involve archiving intermediate results that would not normally be archived so that exact stimuli can be regenerated “just in case.” The github reproducibility work flow is convenient - it allows for me to easily collaborate with others, without emailing versions of code and documents back and forth or dealing with Dropbox version conflicts. I can revert changes that are made that had unintentional effects fairly easily. I can sync my files across multiple machines effortlessly. And if necessary, I can look back at the changes I’ve made and see why I made them, or what I’ve already tried. Reproducibility References and Reading I highly recommend scanning these resources to get a good sense of the different ways the word “reproducibility” is used in the literature. Advanced R’s reproducibility guide A reproducible R workflow ROpenSci’s guide to reproducibility Roger Peng’s Biostatistics editorial on reproducibility The Biostatistics reproducibility issue w/ responses to the editorial and associated commentary 3.1.1 Markdown and R Rmarkdown is magic. (image by Allison Horst) In this class, we’re primarily going to use Rmarkdown to create dynamic documents. Markdown itself (without the R) is a special style of text that is intended to allow you to do basic formatting without having to pause to actually click the buttons (if you were writing in word). Some versions of markdown integrate equation functionality (so you can type mathematical equations using LaTeX syntax) and also allows for the use of templates (so you can write whole journal articles in a text editor). Markdown is also program agnostic - it will allow you to compile your work into HTML, word, or PDF form. Markdown documents must be compiled - a computer program runs and transforms the text file into a full document. RStudio has markdown functionality built-in, and also supports Rmarkdown, which is a markdown variant designed to make it easy to integrate R code and generated pictures with your text (this is called “literate programming”). There are other markdown packages in R which extend markdown’s functionality so that you can write a book (like this one), create presentations or posters, or maintain a blog using primarily Rmarkdown. Rmarkdown, despite the name, also allows you to integrate the results from code in other languages. As you saw in the last chapter, SAS code can be integrated into markdown as well. Other languages commonly used include python, julia, SQL, Bash, C++, and Stan. There is a full set of Rmarkdown tutorials from RStudio. There is also a handy cheatsheet. If you run into trouble or want to do something more complicated, there is an Rmarkdown cookbook which contains a number of useful tricks. Quick Anatomy of Rmarkdown documents Or, you can take the “jump right in” approach - open RStudio, File -&gt; New -&gt; Rmarkdown document. To compile it, click the knit button in the bar at the top of the text editor window. Make changes to the text and the R code, compile it, and see what happens. Voila! You’re a markdow expert! Rmarkdown documents may contain code used to support an analysis, but they are usually not the best way to develop an analysis method - they are better for documentation, writing tutorials, and other scenarios where you need both text explanations and code/analysis/results. There are other “containers” for code, though, including functions, scripts, and packages. Each has their own advantages and disadvantages, and most of them can be used together. 3.1.2 Rmarkdown with… SAS? You may have noticed that I’ve been including SAS chunks throughout this book, and even in your homework assignments. Here’s how that’s set up. SAS in Rmarkdown – guide. It’s really fairly easy (on Linux) and not too bad on Windows, which surprised me – I was expecting it to be a lot more involved to set SASmarkdown up. Of course, I don’t have a Mac, so I was surprised to learn that it is a pain in the … to get SASmarkdown working on Macs because of the way SAS runs. SASmarkdown works with the following setup(s): Windows + SAS (not Community Edition) + R Mac + Parallels + SAS + R (with SAS and RStudio both installed in parallels) Linux + SAS + R If you’re using SAS Community Edition, you cannot use SASMarkdown. Instead, you’ll have to submit SAS files separately from your Rmarkdown code, or use the server set up by HCC that has the correct configuration. If you are using SAS Community Edition, you are still expected to ensure your code is fully reproducible on other computers. 3.2 Functions and Modules A function (or a module, in SAS) is a block of code which is only run when it is called. It takes arguments (known as parameters) and returns data or some other value. You should write a function if you’re written similar code more than 2x - this will reduce the amount of code you have to wade through, and will make changing your code easier. There is some extensive material on this subject in R for Data Science on functions. If you aren’t familiar with functions, you should read that material before proceeding. Let’s look at the structure of a generic function in pseudocode (code that isn’t really part of any language, but describes the steps of a program): my_function_name = function(param1, param2 = 3) { step1 // do something step2 // do something else return step_1/step2 } The first part of a function declaration (storing information in a named object) is the function’s intended name, my_function_name. Then, we indicate that we are defining a function (with = function()), and what parameters our function requires. For param1, we do not provide a default value, but for param2, we indicate that the default value is 3. If we call the function (tell the program to run this function with certain arguments and provide the result), we could either say my_function_name(param1 = value1, param2 = value2) or my_function_name(param1 = value1) (which is equivalent to my_function_name(param1 = value1, param2 = 3)). In R, you can even say my_function_name(value1, value2) and the assumption is that you’ve supplied the parameters in the correct order.11 Inside the function block (indicated by {} here, but some languages may use do ... end;), we perform whatever steps we’ve decided to include in the function At the end of the function, we return a value - the function exits, and leaves behind some information. Inside the function, we refer to the values passed in using their function names - in this case, param1 and param2. Outside the function, these values probably are assigned to variables with other names – but inside the function, they have an alias that is only valid while we’re working within the function. In R, functions look like this: function_name &lt;- function( arglist ) { expr return(value) } and we would call our function by writing the code function_name(args). In SAS, functions are called modules and look like this: START function_name arglist GLOBAL arglist2; expr FINISH function_name; and we would call the function using RUN function_name args; Module documentation Statements That Define and Execute Modules Modules are used to create a user-defined subroutine or function. A module definition begins with a START statement, which has the following general form: START &lt;name&gt; &lt;( arguments )&gt; &lt;GLOBAL( arguments )&gt;; A module definition ends with a FINISH statement, which has the following general form: FINISH &lt;name&gt;; To execute a module, you can use either a RUN statement or a CALL statement. The general forms of these statements are as follows: `RUN &lt;name&gt; &lt;( arguments)&gt;; `CALL &lt;name&gt; &lt;( arguments)&gt;;` The only difference between the RUN and CALL statements is the order of resolution. Source: SAS function reference Let’s try functions out by writing a simple function that takes two numbers as arguments and adds them together. adder &lt;- function(a, b) { return(a + b) } adder(3, 4) ## [1] 7 6 proc IML; NOTE: IML Ready 7 8 start adder(a, b); 9 return(a + b); 10 finish; NOTE: Module ADDER defined. 11 12 /* In IML, you can use the function like this as well */ 13 c = adder(3, 4); 14 print &#39;3 + 4 = &#39; c; 15 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds c 3 + 4 = 7 Of course, it’s not just important to be able to write your own functions. It’s also helpful to be able to see how functions are written, both to explore how a method is implemented and for debugging purposes. In SAS, this is generally not an option, because SAS is closed source, but in R, you can see the code behind any function which is implemented in R (it is harder to see functions implemented in C or C++, but not impossible) by typing the function name (no parentheses) into the command prompt. Let’s examine how the colSums() function is implemented colSums ## function (x, na.rm = FALSE, dims = 1L) ## { ## if (is.data.frame(x)) ## x &lt;- as.matrix(x) ## if (!is.array(x) || length(dn &lt;- dim(x)) &lt; 2L) ## stop(&quot;&#39;x&#39; must be an array of at least two dimensions&quot;) ## if (dims &lt; 1L || dims &gt; length(dn) - 1L) ## stop(&quot;invalid &#39;dims&#39;&quot;) ## n &lt;- prod(dn[id &lt;- seq_len(dims)]) ## dn &lt;- dn[-id] ## z &lt;- if (is.complex(x)) ## .Internal(colSums(Re(x), n, prod(dn), na.rm)) + (0+1i) * ## .Internal(colSums(Im(x), n, prod(dn), na.rm)) ## else .Internal(colSums(x, n, prod(dn), na.rm)) ## if (length(dn) &gt; 1L) { ## dim(z) &lt;- dn ## dimnames(z) &lt;- dimnames(x)[-id] ## } ## else names(z) &lt;- dimnames(x)[[dims + 1L]] ## z ## } ## &lt;bytecode: 0x55d952ef4c88&gt; ## &lt;environment: namespace:base&gt; You can see that the first 3 steps in the function are if statements to test whether the inputs are acceptable - x must be a data frame, a matrix, or an array (with 2+ dimensions). The next couple of lines test to see whether there are additional “column” dimensions (don’t worry if you don’t understand what’s going on in this code - it’s highly optimized and a bit arcane). Then, the function checks to see if x is real-valued or complex, and if it’s complex, computes the real and imaginary sums separately. The .Internal(colSums(x...)) part is calling a C function - basically, functions written in C are faster than R because they’re compiled, so this speeds basic operations up in R. Then there are statements that transfer dimension names over to the summed object. At the end of the function, the last value computed is returned automatically (in this case, z). Try it out Write a function named circle_area which computes the area of a circle given the radius. Make sure to use reasonable parameter names! (Note: in R, pi is conveniently stored in the variable of the same name - it can be overwritten if you want to do so, but why would you want to do that? In SAS, you can get the value of pi using constant(\"pi\")) Solution circle_area &lt;- function(r) { r^2*pi # automatically returned as the last computed value } circle_area(5) ## [1] 78.53982 A more complete and robust answer might include a test for numeric r: circle_area &lt;- function(r) { if (!is.numeric(r)) { stop(&quot;Supplied radius must be numeric&quot;) # This issues an error } r^2*pi # automatically returned as the last computed value } circle_area(5) ## [1] 78.53982 6 proc IML; NOTE: IML Ready 7 8 start circle_area(r); 9 pi = constant(&quot;pi&quot;); 10 return(pi*r**2); 11 finish; NOTE: Module CIRCLE_AREA defined. 12 13 c = circle_area(5); 14 print c; 15 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 17 c 78.539816 One last trick to note: functions generally can only return one object. If you need to return more than one thing, put the objects into a list or another data structure, and return that - then you can take the list/structure apart outside the function to use the returned values separately. 3.2.1 Scope The scope of a variable is the space in the program where a variable is defined and can be accessed. A local variable is one which can only be accessed within a function or block of code - it does not exist outside of that code. A global variable is one which is available to the entire program. In R, every function is defined in a certain environment, and once it is defined, executed in a specific environment. Think of an environment as a space full of available variables, functions, and objects. Any defined object or variable that a function has access to is in scope. When you are inside of a function block, you have access to values defined within the function, plus any other values outside the function. When there are two variables with the same name, the object in the environment which is “closest” is used. Demonstration of scoping in R a &lt;- 3 myfun &lt;- function(a, b) { a + b + 2 } myfun(5, 6) # a is 5 inside the function, so that overrides the ## [1] 13 # a defined outside the function myfun(a, 3) # this references the a outside the function ## [1] 8 Scope diagram. When myfun() is called, the calling environment contains the two parameters a and b. a &lt;- 3 myfun2 &lt;- function(d) { myfun(a, d) } myfun2(3) # the only a in scope inside fun2 is the a defined at the top of the chunk ## [1] 8 Scope diagram. When myfun2() is called, the calling environment contains only a parameter \\(d\\). \\(a\\) is pulled from the global environment, as there is no parameter \\(a\\) in the myfun2 calling environment. a &lt;- 3 myfun3 &lt;- function(a, d) { b &lt;- a; # make a copy of the value a &lt;- 250; myfun(b, d) } a ## [1] 3 myfun3(5, 3) # now, a is defined inside fun3 as a = 5, so there is an a in ## [1] 10 # fun3&#39;s scope that isn&#39;t in the global environment. a # value of a hasn&#39;t changed ## [1] 3 Scope diagram. When myfun3() is called, the calling environment contains parameters a and d, which are then copied into the calling environment of myfun as \\(a\\) and \\(b\\). The variable \\(a\\) in the global environment is ignored. If you want to avoid too many issues with scoping (because scoping rules are complicated), the simplest way is to not reuse variable names inside of a function if you’ve already used those names outside the function (this holds for all languages, really). R does have global variables and a global assignment operator, &lt;&lt;-, but the use of global variables is strongly discouraged, and global variables are not permitted in e.g. CRAN packages. In SAS, scoping rules are more like those in other programming languages - you have to keep track of how arguments are made available to the function. Demonstration of scoping in SAS. Environments do not inherit variables from the calling environment. 2 PROC IML; NOTE: IML Ready 3 a = 3; 4 5 start myfun(a, b); 6 return a + b + 2; 7 finish; NOTE: Module MYFUN defined. 8 9 r1 = myfun(5, 6); 10 /* a is 5 inside the function, so that overrides the 11 a defined outside the function */ 12 13 r2 = myfun(a, 3); 14 /* this references the a outside the function */ 15 16 print r1 r2; 17 quit; NOTE: Exiting IML. NOTE: The PROCEDURE IML printed page 3. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 18 ## The SAS System 1 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 3 + 4 = 7 ## The SAS System 2 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 78.539816 ## ## ## ## r1 r2 ## ## 13 8 In SAS, the equivalent version of the program used to demonstrate lexical scoping in R produces an error. In SAS, you cannot assume the function has access to values defined outside of that function that are not passed into the function as arguments. 2 PROC IML; NOTE: IML Ready 3 a = 3; 4 5 start myfun(a, b); 6 return a + b + 2; 7 finish; NOTE: Module MYFUN defined. 8 9 start myfun2(d); 10 return myfun(a, d); 10 ! /* SAS complains because a is not defined 10 ! */ 11 finish; NOTE: Module MYFUN2 defined. 12 13 r1 = myfun2(3); ERROR: (execution) Matrix has not been set to a value. operation : + at line 6 column 12 operands : a, b a 0 row 0 col (type ?, size 0) b 1 row 1 col (numeric) 3 statement : RETURN at line 6 column 3 traceback : module MYFUN at line 6 column 3 module MYFUN2 at line 10 column 3 NOTE: Paused in module MYFUN. 14 15 print r1; ERROR: Matrix r1 has not been set to a value. statement : PRINT at line 15 column 1 16 quit; NOTE: Exiting IML. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 2. ## The SAS System 1 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 3 + 4 = 7 ## The SAS System 2 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 78.539816 ## The SAS System 3 ## Sunday, May 9, 2021 10:29:00 AM ## ## r1 r2 ## ## 13 8 First, let’s consider the similarities: like R, functions have a local scope, and changing a similarly named value inside the function doesn’t change the value outside the function. 2 PROC IML; NOTE: IML Ready 3 start myfun(x); 4 y = 2 * x; 5 print y[label=&quot;y inside function (local)&quot;]; 6 return 1; 7 finish; NOTE: Module MYFUN defined. 8 9 y = 0; 10 x = 1:5; 11 res = myfun(x); 12 print y[label=&quot;y outside function&quot;]; 13 14 quit; NOTE: Exiting IML. NOTE: The PROCEDURE IML printed page 4. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.02 seconds 15 ERROR: Errors printed on page 2. ## The SAS System 1 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 3 + 4 = 7 ## The SAS System 2 ## Sunday, May 9, 2021 10:29:00 AM ## ## c ## ## 78.539816 ## The SAS System 3 ## Sunday, May 9, 2021 10:29:00 AM ## ## r1 r2 ## ## 13 8 ## ## ## ## y inside function (local) ## ## 2 4 6 8 10 ## ## ## y outside function ## ## 0 However, R’s environment feature and lexical scoping is not common to many other programming languages. When values are passed into the function as arguments, the behavior in SAS deviates from the equivalent behavior in R. In SAS, arguments to functions are passed by reference. An argument that is passed by value makes a copy of the value (to a new memory location) for the local function scope (this is what R does). When an argument is passed by reference, the address of the argument is passed in instead12. This is faster and more efficient (because you aren’t making a new copy of the data), but it does mean that changes inside the function persist outside that function. This can cause problems when reusing modules. Argument passing in SAS If we make a slight modification to myfun(a, b), though, we see some interesting behavior in SAS that we wouldn’t see in the equivalent R program. 2 PROC IML; NOTE: IML Ready 3 a = 3; 4 5 start myfun(a, b); 6 a = a + 2; 7 return a + b; 8 finish; NOTE: Module MYFUN defined. 9 10 b = a; 11 12 r1 = myfun(5, 6); 13 14 c = a; 15 16 r2 = myfun(a, 3); 17 18 d = a; 19 20 print b[label=&quot;a before r1&quot;] r1 c[label=&quot;a after r1&quot;] r2 20 ! d[label=&quot;a after r2&quot;]; 21 quit; NOTE: Exiting IML. NOTE: The PROCEDURE IML printed page 1. NOTE: PROCEDURE IML used (Total process time): real time 0.03 seconds cpu time 0.03 seconds ## a before r1 r1 a after r1 r2 a after r2 ## ## 3 13 3 8 5 In SAS, be sure that if you are using a logical operator AND, &amp;, you put spaces around it (e.g. &amp;) - &amp;var2 is a way to specify that you are passing an argument (var2) by reference, and SAS can get confused and issue weird warnings if you actually intended this to be a logical statement of var1 &amp;var2. I ran across this once and it thoroughly confused me until I remembered that &amp;var can mean “pass by reference” in some languages. Another example of SAS passing arguments by reference (and the unexpected effects that can have on a program’s state) 2 PROC IML; NOTE: IML Ready 3 start myfun(x); 4 call sort(x, 1); 4 ! /* sort the values */ 5 return (cusum(x)); 5 ! /* cusum() = cumulative sum */ 6 finish; NOTE: Module MYFUN defined. 7 8 y = {3, 1, 4, 1, 5, 9, 2, 6, 5, 4}; 9 z = y; 10 11 print y[label=&quot;y before function is called&quot;]; 12 13 cs = myfun(y); 14 15 print z[label=&quot;original y&quot;] 16 y[label=&quot;y after function is called&quot;] 17 cs[label=&quot;cumulative sum of sorted y&quot;]; 18 19 quit; NOTE: Exiting IML. NOTE: The PROCEDURE IML printed page 2. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ## The SAS System 1 ## Sunday, May 9, 2021 10:29:00 AM ## ## a before r1 r1 a after r1 r2 a after r2 ## ## 3 13 3 8 5 ## ## ## ## y before function is called ## ## 3 ## 1 ## 4 ## 1 ## 5 ## 9 ## 2 ## 6 ## 5 ## 4 ## ## ## original y y after function is called cumulative sum of sorted y ## ## 3 1 1 ## 1 1 2 ## 4 2 4 ## 1 3 7 ## 5 4 11 ## 9 4 15 ## 2 5 20 ## 6 5 25 ## 5 6 31 ## 4 9 40 Because arguments in SAS are passed by reference, you can “trick” a function into returning multiple values by passing the variables in as arguments to the function, changing their values in the function, and returning. This is not necessarily a good practice - it can make code very difficult to debug, and may lead to non-obvious dependencies - but for short, simple programs, you can probably get away with it. 2 PROC IML; NOTE: IML Ready 3 start getDescriptive(Mean, SD, /* output args */ 4 x /* input arg */); 5 Mean = x[:]; 5 ! /* this is shorthand for compute the mean of 5 ! the column */ 6 SD = sqrt( ssq(x - Mean)/(nrow(x) - 1)); 7 finish; NOTE: Module GETDESCRIPTIVE defined. 8 9 m = 0; 10 s = 0; 11 y = {3, 1, 4, 1, 5, 9, 2, 6, 5, 4}; 12 13 run GetDescriptive(m, s, y); 14 print m s; 15 16 17 quit; NOTE: Exiting IML. NOTE: The PROCEDURE IML printed page 3. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 ## The SAS System 1 ## Sunday, May 9, 2021 10:29:00 AM ## ## a before r1 r1 a after r1 r2 a after r2 ## ## 3 13 3 8 5 ## The SAS System 2 ## Sunday, May 9, 2021 10:29:00 AM ## ## y before function is called ## ## 3 ## 1 ## 4 ## 1 ## 5 ## 9 ## 2 ## 6 ## 5 ## 4 ## ## ## original y y after function is called cumulative sum of sorted y ## ## 3 1 1 ## 1 1 2 ## 4 2 4 ## 1 3 7 ## 5 4 11 ## 9 4 15 ## 2 5 20 ## 6 5 25 ## 5 6 31 ## 4 9 40 ## ## ## ## m s ## ## 4 2.4494897 If you want to avoid any of these side-effects of SAS’s pass-by-reference behavior, you can very easily do so: just don’t write any modules that modify input arguments. Always modify a copy of the variable instead. Argument passing in R In R, variables inside a function don’t modify variables which are outside a function (generally speaking). In SAS, this is not necessarily the case. In the first call to myfun(), we pass in two numerical arguments, and we see that even though the value of a changes inside the function, that change doesn’t affect the variable defined outside of the function. In the second call to myfun(), we pass a variable in as an argument, and we see that the variable changes after the function’s execution! In the R chunk (below), you can see that the behavior of what is essentially the same code is different. a &lt;- 3 myfun &lt;- function(a, b) { a &lt;- a + 2 a + b } a ## [1] 3 myfun(5, 6) ## [1] 13 a ## [1] 3 myfun(a, 3) ## [1] 8 a ## [1] 3 Try it out Can you predict what the output of this chunk will be? f &lt;- function(x) { f &lt;- function(x) { f &lt;- function(x) { x ^ 2 } f(x) + 1 } f(x) * 2 } f(10) Run it - were you right? What happens when you run a similar program in SAS? (I only nested two functions this time, but you get the idea) proc IML; start f(x); start f(x); return x**x; finish; return f(x)+1; finish; quit; Solution Working through the R program: This gets much less confusing if you rename the functions following R’s scoping rules. f &lt;- function(x) { f1 &lt;- function(x) { # because inside of f(), the new definition will dominate f2 &lt;- function(x) { # because inside of f1(), the new definition will dominate x ^ 2 } f2(x) + 1 } f1(x) * 2 } f(10) Once this has been renamed, it is relatively easy to write out as a series of mathematical substitutions: \\[f(10) = f1(10) * 2 = (f2(10) + 1) * 2 = (10^2 + 1)*2 = 202\\] Running the similar program in SAS results in SAS complaining about a recursive function definition. What will this SAS program output? 6 proc IML; NOTE: IML Ready 7 8 start funwithSAS(x, y); 9 a = x; 10 x = x + 3; 11 return x*y; 12 finish; NOTE: Module FUNWITHSAS defined. 13 14 a = 0; 15 res1 = funwithSAS(a, 5); 16 a1 = a; 17 18 x = 3; 19 res2 = funwithSAS(x, 5); 20 a2 = a; 21 x2 = x; 22 23 y = 3; 24 res3 = funwithSAS(x, y); 25 26 /* print res1 a1 res2 a2 x2 res3 a x y; */ /* uncomment this 26 ! when you&#39;re ready */ 27 28 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 Solution This is easiest if we step through the program and list what is passed around inside and outside of the function evaluation. a = 0 res1 = ...? (inside funwithsas res1) x = (location of a), y = 5 (inside funwithsas res1) a = x = (location of a), x = (location of a) + 3 = 3, so (location of a) = 3 and x = 3. res1 returns x = 3 * y = 5 = 15 a1 = 3 res2 = ...? (inside funwithsas res2) x = (location of x), y = 5 (inside funwithsas res2) a = (location of x), x = (location of x) + 3 = 6 (a outside the function is unaffected because a is not a parameter) res2 returns x = 6 * y = 5 = 30 a2 = 3 x2 = 6 y = 3 res3 = ...? (inside funwithsas res3) x = (location of x), y = (location of y) (inside funwithsas res3) a = (location of x), x = (location of x) + 3 = 9 (a outside the function is unaffected; y is unchanged) res3 returns x = 9 * y = 3 = 27 at this point, x = 9 and y = 3. So the output is res1 = 15 a1 = 3 res2 = 30 a2 = 3 x2 = 6 res3 = 27 a = 3 x = 9 y = 3 The trick to this problem is to realize that inside of the function, when a variable is passed in (by reference) the thing that is assigned is the pointer to the value in memory where the original variable lives. So when a is passed in as x, x is assigned (location of a), which is then assigned to the local copy of a. So both variables now point at the outside (a), and any changes to x also affect both a and the local copy of a. 3.2.2 The Pipe You do not need to understand the collapsed section below. If you’re feeling comfortable with the material, go ahead and read it now. I’ve left it in the functions section, because it’s a topic relating to functions, but it’s not essential to understand to use R (even at an advanced level)13. Infix operators Most functions take arguments written after the function name. Can you think of any functions which work differently? Infix functions are functions that take arguments on both sides of the function name. Say, a + b: technically, a and b are arguments to the function +, so we could think of this as +(a, b). R has a number of infix functions, but you can also create your own. User-defined infix functions start and end with %. So %&gt;%, %in%, %dosomething% would all be valid infix operators. To define one, you need to enclose the name in back-ticks (`). `%dosomething%` &lt;- function(a, b) { a^b - a + b } 3 %dosomething% 4 ## [1] 82 You can also call default infix operators using this syntax: `+`(3, 4) = 7 One of the most useful infix operators is the pipe, %&gt;%, which is a part of the magrittr library and is commonly included in other packages, such as dplyr, and tidyr (we will talk about packages later in this chapter, but pipes are useful to discuss now, so roll with it for a few minutes). The one part of this section that is important to at least be able to use is the pipe: %&gt;%. There is an entire chapter dedicated to discussing the pipe in R4DS, including a discussion of when not to use the pipe. library(magrittr) 3 %&gt;% exp() ## [1] 20.08554 exp(3) ## [1] 20.08554 The pipe takes the left hand side and a function, and puts the left hand side as an argument to the function on the right hand side. It doesn’t sound very impressive, but it allows you to do a very cool thing: “chain” operations. Consider 3 functions: f1 &lt;- function(x, y = 3) { x * y } f2 &lt;- function(z) { z^2 } f3 &lt;- function(z2) { log10(z2) } Normally, you’d write f3(f2(f1(4))), which you have to read from the inside to the outside if you want to describe what this function call is doing. With pipes, you can write the same operation as: 4 %&gt;% f1() %&gt;% f2() %&gt;% f3() ## [1] 2.158362 This is much simpler to read - it’s like a recipe. “Take 4, do f1, then f2, then f3.” You don’t need to know how to define your own infix operators, but you will want to become familiar with the pipe. It’s a central component of writing “tidy” R code. Incidentally, SAS also has a use for the pipe: leveraging the operating system to work with files. In SAS, you just use the keyword pipe, which is hopefully pretty obvious. 3.3 Procs and Data steps In R, the primary unit of code is a function, and nearly every operation in R is a function call, but the functions themselves are malleable and can be easily re-written. SAS is not that flexible - its procedures have been checked and double-checked and maintained for 30 years (in many cases). That level of validation explains its popularity in e.g. the pharmaceutical industry or government, but it does lead to a certain rigidity in the “flow” of a SAS data analysis. Personally, I find that using SAS generally doesn’t require much thought, but I quickly get frustrated when it’s not possible to do the exact thing I want to do in a direct or relatively easy way. The primary units of code in SAS are “steps,” and come in two main flavors: data steps, and proc steps. Data steps are written by the user and customized extensively to the dataset you’re creating or reading in. Proc steps, on the other hand, execute mainly pre-defined procedures that are built into SAS. Thus far, you’ve seen DATA steps and PROC PRINT and IML. In the next module, you’ll see PROC IMPORT, PROC MEANS, PROC CORR, PROC SUMMARY, PROC FREQ, and PROC UNIVARIATE. There is a handy overview of the SAS language that may be useful in understanding the code that you’ve only been reading and copying up to this point. The bare SAS procedure with no options looks like this: PROC MEANS; RUN; By default, SAS will use the last dataset created to run this procedure. SAS procedures can also be run with options that modify the statement. 6 PROC PRINT DATA=SASHELP.CARS (obs=10); 7 RUN; NOTE: There were 10 observations read from the data set SASHELP.CARS. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.02 seconds 8 9 PROC MEANS DATA=SASHELP.CARS; 10 RUN; NOTE: There were 428 observations read from the data set SASHELP.CARS. NOTE: PROCEDURE MEANS used (Total process time): real time 0.02 seconds cpu time 0.03 seconds Obs Make Model Type Origin DriveTrain MSRP Invoice EngineSize Cylinders Horsepower MPG_City MPG_Highway Weight Wheelbase Length 1 Acura MDX SUV Asia All $36,945 $33,337 3.5 6 265 17 23 4451 106 189 2 Acura RSX Type S 2dr Sedan Asia Front $23,820 $21,761 2.0 4 200 24 31 2778 101 172 3 Acura TSX 4dr Sedan Asia Front $26,990 $24,647 2.4 4 200 22 29 3230 105 183 4 Acura TL 4dr Sedan Asia Front $33,195 $30,299 3.2 6 270 20 28 3575 108 186 5 Acura 3.5 RL 4dr Sedan Asia Front $43,755 $39,014 3.5 6 225 18 24 3880 115 197 6 Acura 3.5 RL w/Navigation 4dr Sedan Asia Front $46,100 $41,100 3.5 6 225 18 24 3893 115 197 7 Acura NSX coupe 2dr manual S Sports Asia Rear $89,765 $79,978 3.2 6 290 17 24 3153 100 174 8 Audi A4 1.8T 4dr Sedan Europe Front $25,940 $23,508 1.8 4 170 22 31 3252 104 179 9 Audi A41.8T convertible 2dr Sedan Europe Front $35,940 $32,506 1.8 4 170 23 30 3638 105 180 10 Audi A4 3.0 4dr Sedan Europe Front $31,840 $28,846 3.0 6 220 20 28 3462 104 179 Variable Label N Mean Std Dev Minimum Maximum MSRP Invoice EngineSize Cylinders Horsepower MPG_City MPG_Highway Weight Wheelbase Length Engine Size (L) MPG (City) MPG (Highway) Weight (LBS) Wheelbase (IN) Length (IN) 428 428 428 426 428 428 428 428 428 428 32774.86 30014.70 3.1967290 5.8075117 215.8855140 20.0607477 26.8434579 3577.95 108.1542056 186.3621495 19431.72 17642.12 1.1085947 1.5584426 71.8360316 5.2382176 5.7412007 758.9832146 8.3118130 14.3579913 10280.00 9875.00 1.3000000 3.0000000 73.0000000 10.0000000 12.0000000 1850.00 89.0000000 143.0000000 192465.00 173560.00 8.3000000 12.0000000 500.0000000 60.0000000 66.0000000 7190.00 144.0000000 238.0000000 In the code above, both DATA=SASHELP.CARS and (obs=10) are options that modify their respective statements. Some procs also support additional statements: for instance, we can use the VAR statement to tell SAS which variables we want to work with. 6 PROC MEANS DATA=SASHELP.CARS; 7 VAR msrp cylinders; 8 RUN; NOTE: There were 428 observations read from the data set SASHELP.CARS. NOTE: PROCEDURE MEANS used (Total process time): real time 0.01 seconds cpu time 0.01 seconds Variable N Mean Std Dev Minimum Maximum MSRP Cylinders 428 426 32774.86 5.8075117 19431.72 1.5584426 10280.00 3.0000000 192465.00 12.0000000 Another statement, CLASS, tells SAS to run the procedure for each level of a named categorical variable. 6 PROC MEANS DATA=SASHELP.CARS; 7 CLASS type; 8 VAR msrp cylinders; 9 RUN; NOTE: There were 428 observations read from the data set SASHELP.CARS. NOTE: PROCEDURE MEANS used (Total process time): real time 0.01 seconds cpu time 0.03 seconds Type N Obs Variable N Mean Std Dev Minimum Maximum Hybrid 3 MSRP Cylinders 3 3 19920.00 3.6666667 725.4653679 0.5773503 19110.00 3.0000000 20510.00 4.0000000 SUV 60 MSRP Cylinders 60 60 34790.25 6.5666667 13598.63 1.3822932 17163.00 4.0000000 76870.00 10.0000000 Sedan 262 MSRP Cylinders 262 262 29773.62 5.5801527 15584.59 1.4749723 10280.00 4.0000000 128420.00 12.0000000 Sports 49 MSRP Cylinders 49 47 53387.06 6.3404255 33779.63 1.7849199 18345.00 4.0000000 192465.00 12.0000000 Truck 24 MSRP Cylinders 24 24 24941.38 6.2500000 9871.97 1.5948286 12800.00 4.0000000 52975.00 8.0000000 Wagon 30 MSRP Cylinders 30 30 28840.53 5.3000000 11834.00 1.4178663 11905.00 4.0000000 60670.00 8.0000000 Some statements may have additional options that further modify the statement. The SAS documentation contains full lists of all statements (and all options for those statements). Try It Out Take a look at the Dictionary of SAS DATA step statements. Find the RENAME statement and read up on its syntax. Can you rename the variable msrp in the SASHELP.CARS dataset? DATA tmp; /* Create a temporary dataset */ SET SASHELP.CARS; /* use the CARS data */ /* Your RENAME statement goes here */ RUN; There is an overview of many common SAS procedures here. 3.4 Scripts Up until this point, you may have been writing code in an R Studio or SAS text editor window, or, you may have been typing commands into the command line without preserving them in a separate file. You might even have been working in R markdown documents, where you had code and non-code chunks of the file. A script is a file which contains only code and comments. It is intended to run from start to finish, and usually completes one or more tasks - for instance, cleaning your data, or loading a series of custom functions into your R environment. Scripts are useful because they preserve code so that it can be re-run… and in some cases, they can even be re-run autonomously - I have several scripts which automatically run at specific times every day to complete various tasks (scraping data off the internet, mostly). I find that when doing data analysis, it is often easier to write a script as opposed to working in R markdown or typing commands into the console. Scripts are a record of what I’ve done, and ensure that commands are executed in the right order. As with any tool, it is important to know where to use the tool and where the tool is usually not the best option. You can source (run) an R script using the source() command with the file path of the script as the argument. Scripts in R end in .r or .R, while scripts in SAS end in .sas. Scripts can be run in one of (at least) two modes: batch mode, or interactive mode. In batch mode, the entire script is run without human intervention or monitoring. This is useful for repetitive jobs – for instance, to record the weather at 6h intervals throughout the day. In interactive mode, scripts may be run line-by-line or block-wise, with small tweaks made to the code as you proceed through the file. I find that in some cases, what starts out as an interactive mode script can become a batch script as I work the kinks out. If you ever need to use the high-performance computing resources on campus, you will need to write code to run in batch mode, because these jobs are generally not friendly to interactive programming. Try it out I maintain a list of packages that I find to be useful so that when I install R on a new machine (or update R), I don’t have to spend 3 weeks realizing that I need to install X package. Instead of many repeated 5 minute pauses for package installation, I can just let this script run once and walk away. I’ve pared down my list of packages a bit for this class (you don’t need the packages for analysis of 3D bullet scan data, for instance), but this step should help populate your R installation with a few new packages. Read the script (located here) and try to understand what it is doing. Once you think you understand what it is doing, run the following command to run the script and install the packages on your machine. Were you right? url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/code/03_setup.R&quot; source(url) Try it out (SAS) Use this blog post to create a SAS script that draws a Koch Snowflake in SAS. Save it as “Snowflake.SAS” and open the file in SAS to run it. Snowflake.SAS PROC IML; start KochDivide(A, E); /* (x,y) coords of endpoints */ segs = j(5, 2, .); /* matrix to hold 4 shorter segs */ v = (E-A) / 3; /* vector 1/3 as long as orig */ segs[{1 2 4 5}, ] = A + v @ T(0:3); /* endpoints of new segs */ /* Now compute middle point. Use ATAN2 to find direction angle. */ theta = -constant(&quot;pi&quot;)/3 + atan2(v[2], v[1]); /* change angle by pi/3 */ w = cos(theta) || sin(theta); /* vector to middle point */ segs[3,] = segs[2,] + norm(v)*w; return segs; finish; /* create Koch Snowflake from an equilateral triangle */ start KochPoly(P0, iters=5); P = P0; do j=1 to iters; N = nrow(P) - 1; /* old number of segments */ newP = j(4*N+1, 2); /* new number of segments + 1 */ do i=1 to N; /* for each segment... */ idx = (4*i-3):(4*i+1); /* rows for 4 new segments */ newP[idx, ] = KochDivide(P[i,], P[i+1,]); /* generate new segments */ end; P = newP; /* update polygon and iterate */ end; return P; finish; /* create equilateral triangle as base for snowflake */ pi = constant(&quot;pi&quot;); angles = -pi/6 // pi/2 // 7*pi/6; /* angles for equilateral triangle */ P = cos(angles) || sin(angles); /* vertices of equilateral triangle */ P = P // P[1,]; /* append first vertex to close triangle */ K = KochPoly(P); /* create the Koch snowflake */ S = j(nrow(K), 3, 1); /* add ID variable with constant value 1 */ S[ ,1:2] = K; create Koch from S[colname={X Y ID}]; append from S; close; /* test KochDivide on line segment from (0,0) to (1,0) */ s = KochDivide({0 0}, {1 0}); title &quot;Fundamental Step in the Koch Snowflake Construction&quot;; ods graphics / width=600 height=300; call series(s[,1], s[,2]) procopt=&quot;aspect=0.333&quot;; QUIT; ods graphics / width=400px height=400px; footnote justify=Center &quot;Koch Snowflake&quot;; proc sgplot data=Koch; styleattrs wallcolor=CXD6EAF8; /* light blue */ polygon x=x y=y ID=ID / outline fill fillattrs=(color=white); xaxis display=none; yaxis display=none; run; 3.5 Packages Both SAS and R have systems for extending the base system/language with additional functionality. In R, these extensions are called packages. In SAS, language extensions are called modules (for things sold by SAS), macros (for functions distributed by users), and in packages (which exist but are rarely used).14 SAS packages In SAS, there are packages of code that encapsulate scripts. However, unlike R (and many other languages), there is no centralized repository for SAS packages. Papers may include SAS packages to demonstrate new methods, and other packages may be found on GitHub or various SAS forums. A more common way of distributing code in SAS seems to be through the use of single-file macros. (Even longtime SAS users don’t necessarily know about the package system). These macros often need to be slightly customized to your environment (or you need to customize your environment to match the assumptions made in the macro, which can be harder). R packages In R, there are two main sources for packages: CRAN (the Comprehensive R Archive Network) (and related archives, such as MRAN, which is Microsoft’s version of CRAN) and github. R packages published on CRAN go through a basic verification process that makes sure that the package meets certain standards (for instance, packages must have proper dependencies specified, cannot conflict with previous package names, must have a software license, and cannot contain malicious code). Note that CRAN does not check the packages for statistical correctness! On github, packages go through less verification. Many packages use a system where the version in development is on github and available for installation, but the version on CRAN is considered “stable.” Sometimes, packages are never put on CRAN: I contribute to several packages which are too large for CRAN, and it’s not worth the hassle to get an exception or figure out a workaround. Currently, the CRAN package repository features 1.7548^{4} available packages. How do you navigate them to find the one you need? Sometimes, the CRAN Task Views may be helpful - for instance, if you want to see all of the packages which are useful for Meta-Analysis, Finance, Bayesian statistics, etc. Other times, it’s useful to let Google help you navigate: searching for “R CRAN” + “what you want the package to do” can often narrow things down (I recommend adding CRAN in because Google results for “R” are not particularly useful.) Once you find and install a package (install.packages() for CRAN packages, devtools::install_github() for GitHub packages), you have to figure out how to use it. Many R packages come with vignettes, which are short articles that demonstrate how a package is used. You can browse the available vignettes using browseVignettes() (if you provide a package name as an argument, you will get only vignettes from that package). Another way to get help on a package is to use ?&lt;package name&gt;, e.g. ?ggplot2. That will take you to the main package description page, and there are often links to documentation. At the bottom of the description page, you can click on a link to get to the Index, which is a list of all functions that are provided in that package. From there, you can find the documentation for each function in the package. Try it out Install the R package tibble if it is not already installed. Pull up the vignettes for the tibble package, and read about tibbles. What is the equivalent base R object? How do tibbles differ from that object? Get to the package index for the tibble package, either by navigating through the Packages tab in RStudio, or using ?tibble. Access the documentation for the tribble function, and try loading the package and creating your own tibble using tribble. Solution ## install.packages(&quot;tibble&quot;) library(tibble) ## ?tibble ## ?tribble tribble(~var1, ~var2, 1, 2, 3, 4, 5, 6) ## # A tibble: 3 x 2 ## var1 var2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 3 4 ## 3 5 6 References and Links A more advanced take on functions in R can be found here (Advanced R chapter) There is also a handy cheat sheet style summary of PROC IML in SAS here and some useful demonstrations of simple tasks in IML here. A helpful blog post for scoping in SAS is here Here is another explanation of pass-by-value vs. pass-by-reference (intended for C programming) A comparison of R, SAS, and python - source of information about the SAS package distribution system. Another explanation of pipes (lots of examples) I may archive my sessionInfo() so that package versions are documented↩︎ It’s a good idea to specify your parameter names when you’re using functions you’re unfamiliar with, which at this point, is probably all of them.↩︎ if you’re familiar with C, the argument passed in is just a pointer to the original memory↩︎ I had been using R for ~8 years before I ever heard the term “infix operator” - and I’d been using infix operators for a long time at that point, just without thinking about what they were.↩︎ Note that the various components of SAS can be extremely confusing to separate. I found this guide to be somewhat helpful.↩︎ "],["data-programming-basics.html", "Module 4 Basics Module Objectives 4.1 Introduction 4.2 Example 1: Art", " Module 4 Basics Module Objectives Write basic functions and procedures to create simple plots and data summaries Apply syntax knowledge to reference variables and observations in common data structures Create new variables and columns or reformat existing columns in provided data structures 4.1 Introduction At this point in the course, you’ve learned how to write functions. You know the basics of how to create new variables, how data frames and lists work, and how to use markdown. And yet… these are skills that take some practice when applied to new data. So this week, we’re going to focus on working with datasets, but we’re not going to learn much in the way of new material (though I’ll provide sample code for tasks like basic plots and tables). This week is about reinforcing the skills you’ve already learned, and helping you find your feet a bit as you work through a data analysis. It will also provide a preview of some of the packages we’re going to work with in the coming weeks (because I’m going to show you how to e.g. summarize a dataset and plot a few things, even without having covered that material). As you’ve probably guessed by now, this week’s reading will primarily be focused on examples. 4.2 Example 1: Art The Tate Art Museum assembled a collection of 70,000 artworks (last updated in 2014). They catalogued information including accession number, artwork dimensions, units, title, date, medium, inscription, and even URLs for images of the art. library(readr) library(skimr) artwork &lt;- read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv&#39;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## id = col_double(), ## artistId = col_double(), ## year = col_double(), ## acquisitionYear = col_double(), ## width = col_double(), ## height = col_double(), ## depth = col_double(), ## thumbnailCopyright = col_logical() ## ) ## ℹ Use `spec()` for the full column specifications. skim(artwork) Table 4.1: Data summary Name artwork Number of rows 69201 Number of columns 20 _______________________ Column type frequency: character 12 logical 1 numeric 7 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace accession_number 0 1.00 6 7 0 69201 0 artist 0 1.00 4 120 0 3336 0 artistRole 0 1.00 5 24 0 19 0 title 0 1.00 1 320 0 43529 0 dateText 0 1.00 4 75 0 2736 0 medium 6384 0.91 3 120 0 3401 0 creditLine 3 1.00 14 820 0 3209 0 dimensions 2433 0.96 4 248 0 25575 0 units 3341 0.95 2 2 0 1 0 inscription 62895 0.09 14 14 0 1 0 thumbnailUrl 10786 0.84 55 57 0 58415 0 url 0 1.00 48 134 0 69201 0 Variable type: logical skim_variable n_missing complete_rate mean count thumbnailCopyright 69201 0 NaN : Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist id 0 1.00 39148.03 25980.47 3 19096.00 37339 54712 129068 ▇▇▅▁▁ artistId 0 1.00 1201.06 2019.42 0 558.00 558 1137 19232 ▇▁▁▁▁ year 5397 0.92 1867.23 72.01 1545 1817.00 1831 1953 2012 ▁▁▇▆▆ acquisitionYear 45 1.00 1910.65 64.20 1823 1856.00 1856 1982 2013 ▇▁▁▁▅ width 3367 0.95 323.47 408.81 3 118.00 175 345 11960 ▇▁▁▁▁ height 3342 0.95 346.44 538.04 6 117.00 190 359 37500 ▇▁▁▁▁ depth 66687 0.04 479.20 1051.14 1 48.25 190 450 18290 ▇▁▁▁▁ When you first access a new dataset, it’s fun to explore it a bit. I’ve shown a summary of the variables (character variables summarized with completion rates and # unique values, numeric variables summarized with quantiles and mean/sd) generated using the skimr package (which we’ll talk about next week). First, let’s pull out the year for each piece of artwork in the dataset and see what we can do with it… head(artwork$year) ## [1] NA NA 1785 NA 1826 1826 We reference a column of the dataset by name using dataset_name$column_name, and since our data is stored in artwork, and we want the column named year, we use artwork$year to get access to the data we want. I’ve used the head command to show only the first few values (so that the output isn’t overwhelming). When we have output like this, it is useful to summarize the output in some way: summary(artwork$year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1545 1817 1831 1867 1953 2012 5397 That’s much less output, but we might want to instead make a chart: hist(artwork$year) Personally, I much prefer the graphical version. It’s informative (though it does leave out NA values) and shows that there are pieces going back to the 1500s, but that most pieces were made in the early 1800s or late 1900s. We might be interested in the aspect ratio of the artwork - let’s take a look at the input variables and define new variables related to aspect ratio(s) hist(artwork$width) hist(artwork$depth) hist(artwork$height) So all of our variables are skewed quite a bit, and we know from the existence of the units column that they may not be in the same unit, either… table(artwork$units) ## ## mm ## 65860 Except apparently they are, so … cool. That does make life easier. To define a new variable that exists on its own, we might do something like this: aspect_hw &lt;- artwork$height/artwork$width hist(aspect_hw) hist(log(aspect_hw)) Ok, interesting. Most things are pretty square-ish, but there are obviously quite a few exceptions in both directions. The one problem with how we’ve done this is that we now have a data frame with all of our data in it, and a separate variable aspect_hw, that is not attached to our data frame. That’s not ideal - it’s easy to lose track of the variable, it’s easy to accidentally “sort” the variable so that the row order isn’t the same as in the original data frame… there are all sorts of potential issues. So, the better way to define a new variable is to add a new column to the data frame: artwork$aspect_hw &lt;- artwork$height/artwork$width (We’ll learn an easier way to do this later, but this is functional, if not pretty, for now). The downside to this is that we have to write out artwork$aspect_hw each time we want to reference the variable. That is a pain, but one that’s relatively temporary (we’ll get to a better way to do this in a couple of weeks). A little bit of extra typing is definitely worth it if you don’t lose data you want to keep. One mistake I see people make frequently is to calculate artwork$height/artwork$width, but then not assign that value to a variable. If you’re not using &lt;- (or = or -&gt; if you’re a total heathen) then you’re not saving that information to be referenced later - you’re just calculating values temporarily. It’s important to keep track of where you’re putting the pieces you create during an analysis - just as important as keeping track of the different sub-components when you’re putting a lego set together or making a complex recipe in the kitchen. Forgetting to assign your calculation to a variable is like dumping your mixture down the sink or throwing that small lego component into the trash. "],["reading-data.html", "Module 5 External Data Module Objectives 5.1 External Data Formats 5.2 Text Files 5.3 Spreadsheets 5.4 Binary Files 5.5 Databases 5.6 Exploratory Data Analysis References and Links", " Module 5 External Data Module Objectives Apply syntax to read in data from common formats into R or SAS Manage basic exploratory data analysis: examine data formats identify necessary data cleaning steps describe artifacts of the data set 5.1 External Data Formats In order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways: as raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable. in a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet. There is a collection of horror stories here and a series of even more horrifying tweets here as a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected. R, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. To read data from R into SAS, the easiest way is probably to call R from PROC IML. Here is a very thorough explanation of why binary file formats exist, and why they’re not necessarily optimal. in a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with. There are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common: Web related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. Spatial files: Shapefiles are by far the most common version of spatial files15. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. We’ll explore these a bit more when we talk about maps. To be minimally functional in R and SAS, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly cover binary files and databases, but it is less critical to remember how to read these in without consulting one or more online references. 5.2 Text Files There are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say? 5.2.1 Fixed-width files In a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory). Col1 Col2 Col3 3.4 4.2 5.4 27.3 -2.4 15.9 In base R (no extra packages), you can read fwf files in using read.fwf, but you must specify the column breaks yourself. ## url &lt;- &quot;https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/&quot; data &lt;- read.fwf(url, skip = 3, # Skip the first 2 lines (useless) + header line widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8)) # There is a row with the column names specified ## Warning in readLines(file, n = thisblock): incomplete final line found on &#39;data/ ## mesodata.txt&#39; data[1:6,] # first 6 rows ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 ## 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 6.9 0 964.79 272 31.8 4.0 ## 2 ADAX 1 0 55 32.4 1.0 0.8 108 36.5 0.4 2.2 0 976.20 245 32.0 0.2 ## 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 11.5 0 960.94 296 34.7 6.8 ## 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 9.0 0 957.45 298 35.5 5.4 ## 5 ANT2 135 0 73 27.8 0.0 0.0 0 0.0 0.0 0.0 0 990.11 213 27.8 0.0 ## 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 7.9 0 959.54 277 31.9 4.6 ## V17 V18 V19 V20 V21 V22 V23 V24 ## 1 29.2 36.2 31.6 25.2 21.7 3.09 2.22 1.48 ## 2 28.8 38.2 29.6 26.8 -998.0 2.61 1.88 -998.00 ## 3 29.3 34.1 30.7 26.1 -998.0 3.39 2.47 -998.00 ## 4 24.7 34.7 25.6 22.6 -998.0 2.70 1.60 -998.00 ## 5 29.5 31.1 30.2 26.8 23.8 1.96 1.73 1.33 ## 6 30.4 35.2 34.7 28.2 22.8 1.79 1.53 1.78 You can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you. # I like to cheat a bit.... # Read the first few lines in tmp &lt;- readLines(url, n = 20)[-c(1:2)] # split each line into a series of single characters tmp_chars &lt;- strsplit(tmp, &#39;&#39;) # Bind the lines together into a character matrix # do.call applies a function to an entire list - so instead of doing 18 rbinds, # one command will put all 18 rows together tmp_chars &lt;- do.call(&quot;rbind&quot;, tmp_chars) # (it&#39;s ok if you don&#39;t get this line) # Make into a logical matrix where T = space, F = not space tmp_chars_space &lt;- tmp_chars == &quot; &quot; # Add up the number of rows where there is a non-space character # space columns would have 0s/FALSE tmp_space &lt;- colSums(!tmp_chars_space) # We need a nonzero column followed by a zero column breaks &lt;- which(tmp_space != 0 &amp; c(tmp_space[-1], 0) == 0) # Then, we need to get the widths between the columns widths &lt;- diff(c(0, breaks)) # Now we&#39;re ready to go mesodata &lt;- read.fwf(url, skip = 3, widths = widths, header = F) ## Warning in readLines(file, n = thisblock): incomplete final line found on &#39;data/ ## mesodata.txt&#39; # read header separately - if you use header = T, it errors for some reason. # It&#39;s easier just to work around the error than to fix it :) mesodata_names &lt;- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, stringsAsFactors = F) names(mesodata) &lt;- as.character(mesodata_names) mesodata[1:6,] # first 6 rows ## STID STNM TIME RELH TAIR WSPD WVEC WDIR WDSD WSSD ## 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 ## 2 ADAX 1 0 55 32.4 1.0 0.8 108 36.5 0.4 ## 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 ## 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 ## 5 ANT2 135 0 73 27.8 0.0 0.0 0 0.0 0.0 ## 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 ## WMAX RAIN PRES SRAD TA9M WS2M TS10 TB10 TS05 ## 1 6.9 0 964.79 272 31.8 4.0 29.2 36.2 31.6 ## 2 2.2 0 976.20 245 32.0 0.2 28.8 38.2 29.6 ## 3 11.5 0 960.94 296 34.7 6.8 29.3 34.1 30.7 ## 4 9.0 0 957.45 298 35.5 5.4 24.7 34.7 25.6 ## 5 0.0 0 990.11 213 27.8 0.0 29.5 31.1 30.2 ## 6 7.9 0 959.54 277 31.9 4.6 30.4 35.2 34.7 ## TS25 TS60 TR05 TR25 TR60 ## 1 25.2 21.7 3.09 2.22 1.48 ## 2 26.8 -998.0 2.61 1.88 -998.00 ## 3 26.1 -998.0 3.39 2.47 -998.00 ## 4 22.6 -998.0 2.70 1.60 -998.00 ## 5 26.8 23.8 1.96 1.73 1.33 ## 6 28.2 22.8 1.79 1.53 1.78 But, there’s an even simpler way… The readr package creates data-frame like objects called tibbles (really, they’re a souped-up data frame), but it is much friendlier to use. Tibbles also do not have the problems with factors (see the introduction to factors) - they will always read characters in as characters. library(readr) # Better data importing in R read_table(url, skip = 2) # Gosh, that was much easier! ## # A tibble: 121 x 24 ## STID STNM TIME RELH TAIR WSPD WVEC WDIR WDSD WSSD WMAX RAIN PRES ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 6.9 0 965. ## 2 ADAX 1 0 55 32.4 1 0.8 108 36.5 0.4 2.2 0 976. ## 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 11.5 0 961. ## 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 9 0 957. ## 5 ANT2 135 0 73 27.8 0 0 0 0 0 0 0 990. ## 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 7.9 0 960. ## 7 ARD2 126 0 46 32.9 2.6 2.5 150 11.8 0.5 3.6 0 979. ## 8 ARNE 6 0 28 33.5 6.5 6.3 163 11.9 1.5 10 0 927. ## 9 BEAV 8 0 23 34.9 11.2 11.1 165 7.3 1.4 15.2 0 921. ## 10 BESS 9 0 37 33.8 8.3 8.3 156 6.6 1.3 11.2 0 951. ## # … with 111 more rows, and 11 more variables: SRAD &lt;dbl&gt;, TA9M &lt;dbl&gt;, ## # WS2M &lt;dbl&gt;, TS10 &lt;dbl&gt;, TB10 &lt;dbl&gt;, TS05 &lt;dbl&gt;, TS25 &lt;dbl&gt;, TS60 &lt;dbl&gt;, ## # TR05 &lt;dbl&gt;, TR25 &lt;dbl&gt;, TR60 &lt;dbl&gt; You can also write fixed-width files if you really want to: if (!&quot;gdata&quot; %in% installed.packages()) install.packages(&quot;gdata&quot;) library(gdata) write.fwf(mtcars, file = &quot;data/04_mtcars-fixed-width.txt&quot;) In SAS, it’s a bit more complicated, but not that much - the biggest difference is that you generally have to specify the column names for SAS. For complicated data, as in R, you may also have to specify the column widths. 6 /* This downloads the file to my machine */ 7 /* x &quot;curl 7 ! https://www.mesonet.org/index.php/dataMdfMts/dataController/getF 7 ! ile/202006070000/mdf/TEXT/ 8 &gt; data/mesodata.txt&quot; */ 9 /* only run this once */ 10 11 /* Specifying WORK.mesodata means the dataset will cease to 11 ! exist after this chunk exits */ 12 data WORK.mesodata; 13 14 infile &quot;data/mesodata.txt&quot; firstobs = 4; 15 /* Skip the first 3 rows */ 16 length STID $ 4; /* define ID length */ 17 input STID $ STNM TIME RELH TAIR 18 WSPD WVEC WDIR WDSD WSSD WMAX 19 RAIN PRES SRAD TA9M WS2M TS10 20 TB10 TS05 TS25 TS60 TR05 TR25 TR60; 21 run; NOTE: The infile &quot;data/mesodata.txt&quot; is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/mes odata.txt, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=07Jun2020:16:59:37, File Size (bytes)=20580 NOTE: LOST CARD. STID=&lt;/pr STNM=. TIME=. RELH=. TAIR=. WSPD=. WVEC=. WDIR=. WDSD=. WSSD=. WMAX=. RAIN=. PRES=. SRAD=. TA9M=. WS2M=. TS10=. TB10=. TS05=. TS25=. TS60=. TR05=. TR25=. TR60=. _ERROR_=1 _N_=121 NOTE: 121 records were read from the infile &quot;data/mesodata.txt&quot;. The minimum record length was 6. The maximum record length was 168. NOTE: SAS went to a new line when INPUT statement reached past the end of a line. NOTE: The data set WORK.MESODATA has 120 observations and 24 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 22 23 proc print data=mesodata (obs=10); /* print the first 10 23 ! observations */ 24 run; NOTE: There were 10 observations read from the data set WORK.MESODATA. NOTE: PROCEDURE PRINT used (Total process time): real time 0.05 seconds cpu time 0.05 seconds Obs STID STNM TIME RELH TAIR WSPD WVEC WDIR WDSD WSSD WMAX RAIN PRES SRAD TA9M WS2M TS10 TB10 TS05 TS25 TS60 TR05 TR25 TR60 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 6.9 0 964.79 272 31.8 4.0 29.2 36.2 31.6 25.2 21.7 3.09 2.22 1.48 2 ADAX 1 0 55 32.4 1.0 0.8 108 36.5 0.4 2.2 0 976.20 245 32.0 0.2 28.8 38.2 29.6 26.8 -998.0 2.61 1.88 -998.00 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 11.5 0 960.94 296 34.7 6.8 29.3 34.1 30.7 26.1 -998.0 3.39 2.47 -998.00 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 9.0 0 957.45 298 35.5 5.4 24.7 34.7 25.6 22.6 -998.0 2.70 1.60 -998.00 5 ANT2 135 0 73 27.8 0.0 0.0 0 0.0 0.0 0.0 0 990.11 213 27.8 0.0 29.5 31.1 30.2 26.8 23.8 1.96 1.73 1.33 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 7.9 0 959.54 277 31.9 4.6 30.4 35.2 34.7 28.2 22.8 1.79 1.53 1.78 7 ARD2 126 0 46 32.9 2.6 2.5 150 11.8 0.5 3.6 0 979.23 256 32.6 2.1 30.2 37.1 30.1 26.2 23.5 2.59 1.41 1.42 8 ARNE 6 0 28 33.5 6.5 6.3 163 11.9 1.5 10.0 0 927.19 334 33.4 4.3 31.9 35.0 33.1 26.9 22.6 2.64 3.02 2.57 9 BEAV 8 0 23 34.9 11.2 11.1 165 7.3 1.4 15.2 0 921.20 335 34.6 8.9 32.2 30.6 34.2 27.1 22.8 3.53 2.43 2.11 10 BESS 9 0 37 33.8 8.3 8.3 156 6.6 1.3 11.2 0 950.82 306 33.4 6.4 32.5 35.5 33.5 26.2 -998.0 3.15 3.34 -998.00 In SAS data statements, you generally need to specify the data names explicitly. In theory you can also get SAS to write out a fixed-width file, but it’s much easier to just… not. You can generally use a CSV or format of your choice – and you should definitely do that, because delimited files are much easier to work with. 5.2.2 Delimited Text Files Delimited text files are files where fields are separated by a specific character, such as \" “,”\", tab, etc. Often, delimited text files will have the column names as the first row in the file. As long as you know the delimiter, it’s pretty easy to read in data from these files in R using the readr package. url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; pokemon_info &lt;- read_csv(url) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## name = col_character(), ## german_name = col_character(), ## japanese_name = col_character(), ## status = col_character(), ## species = col_character(), ## type_1 = col_character(), ## type_2 = col_character(), ## ability_1 = col_character(), ## ability_2 = col_character(), ## ability_hidden = col_character(), ## growth_rate = col_character(), ## egg_type_1 = col_character(), ## egg_type_2 = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. pokemon_info[1:6, 1:6] # Show only the first 6 lines &amp; cols ## # A tibble: 6 x 6 ## X1 pokedex_number name german_name japanese_name generation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0 1 Bulbasaur Bisasam フシギダネ (Fushigidane)… 1 ## 2 1 2 Ivysaur Bisaknosp フシギソウ (Fushigisou)… 1 ## 3 2 3 Venusaur Bisaflor フシギバナ (Fushigibana)… 1 ## 4 3 3 Mega Venusa… Bisaflor フシギバナ (Fushigibana)… 1 ## 5 4 4 Charmander Glumanda ヒトカゲ (Hitokage) 1 ## 6 5 5 Charmeleon Glutexo リザード (Lizardo) 1 # a file delimited with | url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt&quot; nebraska_locations &lt;- read_delim(url, delim = &quot;|&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## FEATURE_ID = col_double(), ## PRIM_LAT_DEC = col_double(), ## PRIM_LONG_DEC = col_double(), ## SOURCE_LAT_DEC = col_double(), ## SOURCE_LONG_DEC = col_double(), ## ELEV_IN_M = col_double(), ## ELEV_IN_FT = col_double() ## ) ## ℹ Use `spec()` for the full column specifications. nebraska_locations[1:6, 1:6] ## # A tibble: 6 x 6 ## FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 171013 Peetz Table Area CO 08 Logan ## 2 171029 Sidney Draw Valley NE 31 Cheyenne ## 3 182687 Highline Canal Canal CO 08 Sedgwick ## 4 182688 Cottonwood Cre… Stream CO 08 Sedgwick ## 5 182689 Sand Draw Valley CO 08 Sedgwick ## 6 182690 Sedgwick Draw Valley CO 08 Sedgwick You can also read in the same files using read.csv and read.delim, which are the equivalent base R functions. url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; pokemon_info &lt;- read.csv(url, header = T, stringsAsFactors = F) pokemon_info[1:6, 1:6] # Show only the first 6 lines &amp; cols ## X pokedex_number name german_name japanese_name ## 1 0 1 Bulbasaur Bisasam フシギダネ (Fushigidane) ## 2 1 2 Ivysaur Bisaknosp フシギソウ (Fushigisou) ## 3 2 3 Venusaur Bisaflor フシギバナ (Fushigibana) ## 4 3 3 Mega Venusaur Bisaflor フシギバナ (Fushigibana) ## 5 4 4 Charmander Glumanda ヒトカゲ (Hitokage) ## 6 5 5 Charmeleon Glutexo リザード (Lizardo) ## generation ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 # a file delimited with | url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt&quot; nebraska_locations &lt;- read.delim(url, sep = &quot;|&quot;, header = T) nebraska_locations[1:6, 1:6] ## FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC ## 1 171013 Peetz Table Area CO 8 ## 2 171029 Sidney Draw Valley NE 31 ## 3 182687 Highline Canal Canal CO 8 ## 4 182688 Cottonwood Creek Stream CO 8 ## 5 182689 Sand Draw Valley CO 8 ## 6 182690 Sedgwick Draw Valley CO 8 ## COUNTY_NAME ## 1 Logan ## 2 Cheyenne ## 3 Sedgwick ## 4 Sedgwick ## 5 Sedgwick ## 6 Sedgwick SAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters). Don’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding! While writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS. CSV Import in SAS /* x &quot;curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv &gt; data/pokemon.csv&quot;; only run this once to download the file... */ filename pokeloc &#39;data/pokemon.csv&#39; encoding=&quot;utf-8&quot;; proc import datafile = pokeloc out=poke DBMS = csv; /* comma delimited file */ GETNAMES = YES ; proc print data=poke (obs=10); /* print the first 10 observations */ run; Obs VAR1 pokedex_number name german_name japanese_name generation status species type_number type_1 type_2 height_m weight_kg abilities_number ability_1 ability_2 ability_hidden total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience growth_rate egg_type_number egg_type_1 egg_type_2 percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 1 0 1 Bulbasaur Bisasam フシギダネ (Fushigidane) 1 Normal Seed Pokémon 2 Grass Poison 0.7 6.9 2 Overgrow Chlorophyll 318 45 49 49 65 65 45 45 70 64 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 2 1 2 Ivysaur Bisaknosp フシギソウ (Fushigisou) 1 Normal Seed Pokémon 2 Grass Poison 1 13 2 Overgrow Chlorophyll 405 60 62 63 80 80 60 45 70 142 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 3 2 3 Venusaur Bisaflor フシギバナ (Fushigibana) 1 Normal Seed Pokémon 2 Grass Poison 2 100 2 Overgrow Chlorophyll 525 80 82 83 100 100 80 45 70 236 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 4 3 3 Mega Venusaur Bisaflor フシギバナ (Fushigibana) 1 Normal Seed Pokémon 2 Grass Poison 2.4 155.5 1 Thick Fat 625 80 100 123 122 120 80 45 70 281 Medium Slow 2 Grass Monster 87.5 20 1 1 0.5 0.5 0.25 1 0.5 1 1 2 2 1 1 1 1 1 1 0.5 5 4 4 Charmander Glumanda ヒトカゲ (Hitokage) 1 Normal Lizard Pokémon 1 Fire 0.6 8.5 2 Blaze Solar Power 309 39 52 43 60 50 65 45 70 62 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 1 0.5 0.5 1 1 2 1 1 0.5 2 1 1 1 0.5 0.5 6 5 5 Charmeleon Glutexo リザード (Lizardo) 1 Normal Flame Pokémon 1 Fire 1.1 19 2 Blaze Solar Power 405 58 64 58 80 65 80 45 70 142 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 1 0.5 0.5 1 1 2 1 1 0.5 2 1 1 1 0.5 0.5 7 6 6 Charizard Glurak リザードン (Lizardon) 1 Normal Flame Pokémon 2 Fire Flying 1.7 90.5 2 Blaze Solar Power 534 78 84 78 109 85 100 45 70 240 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 2 0.25 1 0.5 1 0 1 1 0.25 4 1 1 1 0.5 0.5 8 7 6 Mega Charizard X Glurak リザードン (Lizardon) 1 Normal Flame Pokémon 2 Fire Dragon 1.7 110.5 1 Tough Claws 634 78 130 111 130 85 100 45 70 285 Medium Slow 2 Dragon Monster 87.5 20 1 0.25 1 0.5 0.25 1 1 1 2 1 1 0.5 2 1 2 1 0.5 1 9 8 6 Mega Charizard Y Glurak リザードン (Lizardon) 1 Normal Flame Pokémon 2 Fire Flying 1.7 100.5 1 Drought 634 78 104 78 159 115 100 45 70 285 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 2 0.25 1 0.5 1 0 1 1 0.25 4 1 1 1 0.5 0.5 10 9 7 Squirtle Schiggy ゼニガメ (Zenigame) 1 Normal Tiny Turtle Pokémon 1 Water 0.5 9 2 Torrent Rain Dish 314 44 48 65 50 64 43 45 70 63 Medium Slow 2 Monster Water 1 87.5 20 1 0.5 0.5 2 2 0.5 1 1 1 1 1 1 1 1 1 1 0.5 1 The only abnormal thing is that on my computer, the japanese characters don’t render. Here is the output from SAS running the above code interactively Alternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in. CSVs in SAS (via R) If I fix the file in R (because I know how to fix it there… another option is to fix it manually), library(readr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:gdata&#39;: ## ## combine, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union tmp &lt;- read_csv(&quot;data/pokemon.csv&quot;)[,-1] ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## name = col_character(), ## german_name = col_character(), ## japanese_name = col_character(), ## status = col_character(), ## species = col_character(), ## type_1 = col_character(), ## type_2 = col_character(), ## ability_1 = col_character(), ## ability_2 = col_character(), ## ability_hidden = col_character(), ## growth_rate = col_character(), ## egg_type_1 = col_character(), ## egg_type_2 = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. # You&#39;ll learn how to do this later tmp &lt;- select(tmp, -japanese_name) %&gt;% mutate_all(iconv, from=&quot;UTF-8&quot;, to = &quot;ASCII//TRANSLIT&quot;) write_csv(tmp, &quot;data/pokemon_ascii.csv&quot;, na=&#39;.&#39;) Then, reading in the new file allows us to actually see the output. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 /* Create a library of class data */ 8 9 filename pokeloc &quot;data/pokemon_ascii.csv&quot;; 10 11 proc import datafile = pokeloc out=classdat.poke 12 DBMS = csv /* comma delimited file */ 13 replace; 14 GETNAMES = YES; 15 GUESSINGROWS = 1028 /* use all data for guessing the variable 15 ! type */ 16 ; 17 proc print data=classdat.poke (obs=10); /* print the first 10 17 ! observations */ 18 /************************************************************** 18 ! ******** 19 * PRODUCT: SAS 20 * VERSION: 9.4 21 * CREATOR: External File Interface 22 * DATE: 09MAY21 23 * DESC: Generated SAS Datastep Code 24 * TEMPLATE SOURCE: (None Specified.) 25 *************************************************************** 25 ! ********/ 26 data CLASSDAT.POKE ; 27 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 27 ! */ 28 infile POKELOC delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 29 informat pokedex_number best32. ; 30 informat name $33. ; 31 informat german_name $12. ; 32 informat generation best32. ; 33 informat status $13. ; 34 informat species $21. ; 35 informat type_number best32. ; 36 informat type_1 $8. ; 37 informat type_2 $8. ; 38 informat height_m best32. ; 39 informat weight_kg best32. ; 40 informat abilities_number best32. ; 41 informat ability_1 $16. ; 42 informat ability_2 $16. ; 43 informat ability_hidden $16. ; 44 informat total_points best32. ; 45 informat hp best32. ; 46 informat attack best32. ; 47 informat defense best32. ; 48 informat sp_attack best32. ; 49 informat sp_defense best32. ; 50 informat speed best32. ; 51 informat catch_rate best32. ; 52 informat base_friendship best32. ; 53 informat base_experience best32. ; 54 informat growth_rate $11. ; 55 informat egg_type_number best32. ; 56 informat egg_type_1 $12. ; 57 informat egg_type_2 $10. ; 58 informat percentage_male best32. ; 59 informat egg_cycles best32. ; 60 informat against_normal best32. ; 61 informat against_fire best32. ; 62 informat against_water best32. ; 63 informat against_electric best32. ; 64 informat against_grass best32. ; 65 informat against_ice best32. ; 66 informat against_fight best32. ; 67 informat against_poison best32. ; 68 informat against_ground best32. ; 69 informat against_flying best32. ; 70 informat against_psychic best32. ; 71 informat against_bug best32. ; 72 informat against_rock best32. ; 73 informat against_ghost best32. ; 74 informat against_dragon best32. ; 75 informat against_dark best32. ; 76 informat against_steel best32. ; 77 informat against_fairy best32. ; 78 format pokedex_number best12. ; 79 format name $33. ; 80 format german_name $12. ; 81 format generation best12. ; 82 format status $13. ; 83 format species $21. ; 84 format type_number best12. ; 85 format type_1 $8. ; 86 format type_2 $8. ; 87 format height_m best12. ; 88 format weight_kg best12. ; 89 format abilities_number best12. ; 90 format ability_1 $16. ; 91 format ability_2 $16. ; 92 format ability_hidden $16. ; 93 format total_points best12. ; 94 format hp best12. ; 95 format attack best12. ; 96 format defense best12. ; 97 format sp_attack best12. ; 98 format sp_defense best12. ; 99 format speed best12. ; 100 format catch_rate best12. ; 101 format base_friendship best12. ; 102 format base_experience best12. ; 103 format growth_rate $11. ; 104 format egg_type_number best12. ; 105 format egg_type_1 $12. ; 106 format egg_type_2 $10. ; 107 format percentage_male best12. ; 108 format egg_cycles best12. ; 109 format against_normal best12. ; 110 format against_fire best12. ; 111 format against_water best12. ; 112 format against_electric best12. ; 113 format against_grass best12. ; 114 format against_ice best12. ; 115 format against_fight best12. ; 116 format against_poison best12. ; 117 format against_ground best12. ; 118 format against_flying best12. ; 119 format against_psychic best12. ; 120 format against_bug best12. ; 121 format against_rock best12. ; 122 format against_ghost best12. ; 123 format against_dragon best12. ; 124 format against_dark best12. ; 125 format against_steel best12. ; 126 format against_fairy best12. ; 127 input 128 pokedex_number 129 name $ 130 german_name $ 131 generation 132 status $ 133 species $ 134 type_number 135 type_1 $ 136 type_2 $ 137 height_m 138 weight_kg 139 abilities_number 140 ability_1 $ 141 ability_2 $ 142 ability_hidden $ 143 total_points 144 hp 145 attack 146 defense 147 sp_attack 148 sp_defense 149 speed 150 catch_rate 151 base_friendship 152 base_experience 153 growth_rate $ 154 egg_type_number 155 egg_type_1 $ 156 egg_type_2 $ 157 percentage_male 158 egg_cycles 159 against_normal 160 against_fire 161 against_water 162 against_electric 163 against_grass 164 against_ice 165 against_fight 166 against_poison 167 against_ground 168 against_flying 169 against_psychic 170 against_bug 171 against_rock 172 against_ghost 173 against_dragon 174 against_dark 175 against_steel 176 against_fairy 177 ; 178 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 178 ! detection macro variable */ 179 run; NOTE: The infile POKELOC is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/pok emon_ascii.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=09May2021:10:33:17, File Size (bytes)=207032 NOTE: 1028 records were read from the infile POKELOC. The minimum record length was 164. The maximum record length was 236. NOTE: The data set CLASSDAT.POKE has 1028 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 1028 rows created in CLASSDAT.POKE from POKELOC. NOTE: CLASSDAT.POKE data set was successfully created. NOTE: The data set CLASSDAT.POKE has 1028 observations and 49 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 1.37 seconds cpu time 1.35 seconds 180 run; NOTE: There were 10 observations read from the data set CLASSDAT.POKE. NOTE: PROCEDURE PRINT used (Total process time): real time 0.03 seconds cpu time 0.02 seconds Obs pokedex_number name german_name generation status species type_number type_1 type_2 height_m weight_kg abilities_number ability_1 ability_2 ability_hidden total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience growth_rate egg_type_number egg_type_1 egg_type_2 percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 1 1 Bulbasaur Bisasam 1 Normal Seed Pokemon 2 Grass Poison 0.7 6.9 2 Overgrow Chlorophyll 318 45 49 49 65 65 45 45 70 64 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 2 2 Ivysaur Bisaknosp 1 Normal Seed Pokemon 2 Grass Poison 1 13 2 Overgrow Chlorophyll 405 60 62 63 80 80 60 45 70 142 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 3 3 Venusaur Bisaflor 1 Normal Seed Pokemon 2 Grass Poison 2 100 2 Overgrow Chlorophyll 525 80 82 83 100 100 80 45 70 236 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 4 3 Mega Venusaur Bisaflor 1 Normal Seed Pokemon 2 Grass Poison 2.4 155.5 1 Thick Fat 625 80 100 123 122 120 80 45 70 281 Medium Slow 2 Grass Monster 87.5 20 1 1 0.5 0.5 0.25 1 0.5 1 1 2 2 1 1 1 1 1 1 0.5 5 4 Charmander Glumanda 1 Normal Lizard Pokemon 1 Fire 0.6 8.5 2 Blaze Solar Power 309 39 52 43 60 50 65 45 70 62 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 1 0.5 0.5 1 1 2 1 1 0.5 2 1 1 1 0.5 0.5 6 5 Charmeleon Glutexo 1 Normal Flame Pokemon 1 Fire 1.1 19 2 Blaze Solar Power 405 58 64 58 80 65 80 45 70 142 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 1 0.5 0.5 1 1 2 1 1 0.5 2 1 1 1 0.5 0.5 7 6 Charizard Glurak 1 Normal Flame Pokemon 2 Fire Flying 1.7 90.5 2 Blaze Solar Power 534 78 84 78 109 85 100 45 70 240 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 2 0.25 1 0.5 1 0 1 1 0.25 4 1 1 1 0.5 0.5 8 6 Mega Charizard X Glurak 1 Normal Flame Pokemon 2 Fire Dragon 1.7 110.5 1 Tough Claws 634 78 130 111 130 85 100 45 70 285 Medium Slow 2 Dragon Monster 87.5 20 1 0.25 1 0.5 0.25 1 1 1 2 1 1 0.5 2 1 2 1 0.5 1 9 6 Mega Charizard Y Glurak 1 Normal Flame Pokemon 2 Fire Flying 1.7 100.5 1 Drought 634 78 104 78 159 115 100 45 70 285 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 2 0.25 1 0.5 1 0 1 1 0.25 4 1 1 1 0.5 0.5 10 7 Squirtle Schiggy 1 Normal Tiny Turtle Pokemon 1 Water 0.5 9 2 Torrent Rain Dish 314 44 48 65 50 64 43 45 70 63 Medium Slow 2 Monster Water 1 87.5 20 1 0.5 0.5 2 2 0.5 1 1 1 1 1 1 1 1 1 1 0.5 1 This trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate. Non-comma delimited files in SAS (via R) To read in a pipe delimited file (the ‘|’ character), we have to make some changes. Here is the proc import code. Note that I am reading in a version of the file that I’ve converted to ASCII (see details below) because while the import works with the original file, it causes the SAS -&gt; R pipeline that the book is built on to break. tmp &lt;- readLines(&quot;data/NE_Features_20200501.txt&quot;) tmp_ascii &lt;- iconv(tmp, to = &quot;ASCII//TRANSLIT&quot;) writeLines(tmp_ascii, &quot;data/NE_Features_ascii.txt&quot;) 6 /* Without specifying the library to store the data in, it is 6 ! stored in WORK */ 7 proc import datafile = &quot;data/NE_Features_ascii.txt&quot; 7 ! out=nefeatures 8 DBMS = DLM /* delimited file */ 9 replace; 10 GETNAMES = YES; 11 DELIMITER = &#39;|&#39;; 12 GUESSINGROWS = 31582; 13 run; 14 /************************************************************** 14 ! ******** 15 * PRODUCT: SAS 16 * VERSION: 9.4 17 * CREATOR: External File Interface 18 * DATE: 09MAY21 19 * DESC: Generated SAS Datastep Code 20 * TEMPLATE SOURCE: (None Specified.) 21 *************************************************************** 21 ! ********/ 22 data WORK.NEFEATURES ; 23 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 23 ! */ 24 infile &#39;data/NE_Features_ascii.txt&#39; delimiter = &#39;|&#39; MISSOVER 24 ! DSD lrecl=32767 firstobs=2 ; 25 informat FEATURE_ID best32. ; 26 informat FEATURE_NAME $81. ; 27 informat FEATURE_CLASS $15. ; 28 informat STATE_ALPHA $2. ; 29 informat STATE_NUMERIC best32. ; 30 informat COUNTY_NAME $13. ; 31 informat COUNTY_NUMERIC best32. ; 32 informat PRIMARY_LAT_DMS $7. ; 33 informat PRIM_LONG_DMS $8. ; 34 informat PRIM_LAT_DEC best32. ; 35 informat PRIM_LONG_DEC best32. ; 36 informat SOURCE_LAT_DMS $7. ; 37 informat SOURCE_LONG_DMS $8. ; 38 informat SOURCE_LAT_DEC best32. ; 39 informat SOURCE_LONG_DEC best32. ; 40 informat ELEV_IN_M best32. ; 41 informat ELEV_IN_FT best32. ; 42 informat MAP_NAME $26. ; 43 informat DATE_CREATED mmddyy10. ; 44 informat DATE_EDITED mmddyy10. ; 45 format FEATURE_ID best12. ; 46 format FEATURE_NAME $81. ; 47 format FEATURE_CLASS $15. ; 48 format STATE_ALPHA $2. ; 49 format STATE_NUMERIC best12. ; 50 format COUNTY_NAME $13. ; 51 format COUNTY_NUMERIC best12. ; 52 format PRIMARY_LAT_DMS $7. ; 53 format PRIM_LONG_DMS $8. ; 54 format PRIM_LAT_DEC best12. ; 55 format PRIM_LONG_DEC best12. ; 56 format SOURCE_LAT_DMS $7. ; 57 format SOURCE_LONG_DMS $8. ; 58 format SOURCE_LAT_DEC best12. ; 59 format SOURCE_LONG_DEC best12. ; 60 format ELEV_IN_M best12. ; 61 format ELEV_IN_FT best12. ; 62 format MAP_NAME $26. ; 63 format DATE_CREATED mmddyy10. ; 64 format DATE_EDITED mmddyy10. ; 65 input 66 FEATURE_ID 67 FEATURE_NAME $ 68 FEATURE_CLASS $ 69 STATE_ALPHA $ 70 STATE_NUMERIC 71 COUNTY_NAME $ 72 COUNTY_NUMERIC 73 PRIMARY_LAT_DMS $ 74 PRIM_LONG_DMS $ 75 PRIM_LAT_DEC 76 PRIM_LONG_DEC 77 SOURCE_LAT_DMS $ 78 SOURCE_LONG_DMS $ 79 SOURCE_LAT_DEC 80 SOURCE_LONG_DEC 81 ELEV_IN_M 82 ELEV_IN_FT 83 MAP_NAME $ 84 DATE_CREATED 85 DATE_EDITED 86 ; 87 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 87 ! detection macro variable */ 88 run; NOTE: The infile &#39;data/NE_Features_ascii.txt&#39; is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/NE_ Features_ascii.txt, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=09May2021:10:33:19, File Size (bytes)=4227269 NOTE: 31582 records were read from the infile &#39;data/NE_Features_ascii.txt&#39;. The minimum record length was 82. The maximum record length was 204. NOTE: The data set WORK.NEFEATURES has 31582 observations and 20 variables. NOTE: DATA statement used (Total process time): real time 0.06 seconds cpu time 0.07 seconds 31582 rows created in WORK.NEFEATURES from data/NE_Features_ascii.txt. NOTE: WORK.NEFEATURES data set was successfully created. NOTE: The data set WORK.NEFEATURES has 31582 observations and 20 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 18.77 seconds cpu time 18.76 seconds 89 90 proc print data=nefeatures (obs=10); /* print the first 10 90 ! observations */ 91 run; NOTE: There were 10 observations read from the data set WORK.NEFEATURES. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 92 Obs FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME COUNTY_NUMERIC PRIMARY_LAT_DMS PRIM_LONG_DMS PRIM_LAT_DEC PRIM_LONG_DEC SOURCE_LAT_DMS SOURCE_LONG_DMS SOURCE_LAT_DEC SOURCE_LONG_DEC ELEV_IN_M ELEV_IN_FT MAP_NAME DATE_CREATED DATE_EDITED 1 171013 Peetz Table Area CO 8 Logan 75 405840N 1030332W 40.9777645 -103.0588116 . . 1341 4400 Peetz 10/13/1978 . 2 171029 Sidney Draw Valley NE 31 Cheyenne 33 410816N 1030116W 41.1377213 -103.021044 405215N 1040353W 40.8709614 -104.0646558 1256 4121 Brownson 10/13/1978 03/08/2018 3 182687 Highline Canal Canal CO 8 Sedgwick 115 405810N 1023137W 40.9694351 -102.5268556 . . 1118 3668 Sedgwick 10/13/1978 . 4 182688 Cottonwood Creek Stream CO 8 Sedgwick 115 405511N 1023355W 40.9197132 -102.5651893 405850N 1030107W 40.9805426 -103.0185329 1096 3596 Sedgwick 10/13/1978 10/23/2009 5 182689 Sand Draw Valley CO 8 Sedgwick 115 405951N 1023040W 40.9974447 -102.5111958 410203N 1023313W 41.0342898 -102.5536 1137 3730 Sedgwick 10/13/1978 12/20/2017 6 182690 Sedgwick Draw Valley CO 8 Sedgwick 115 405749N 1023313W 40.9636507 -102.5534931 410227N 1023722W 41.0407909 -102.6227148 1113 3652 Sedgwick 10/13/1978 11/04/2017 7 182692 Peterson Ditch Canal CO 8 Sedgwick 115 405604N 1023053W 40.9345581 -102.514778 . . 1171 3842 Sedgwick 10/13/1978 10/15/2019 8 182727 Sand Creek Stream CO 8 Sedgwick 115 404938N 1021752W 40.8272161 -102.2976858 404857N 1022344W 40.8158264 -102.3954646 1151 3776 Julesburg SE 10/13/1978 . 9 182826 Wildhorse Creek Stream CO 8 Phillips 95 403620N 1020722W 40.6055504 -102.1226852 403734N 1025720W 40.6261012 -102.9554801 1101 3612 Amherst SE 10/13/1978 . 10 183028 North Fork Republican River Stream NE 31 Dundy 57 400111N 1015618W 40.0197222 -101.9383333 395958N 1022714W 39.9994444 -102.4538889 988 3241 Haigler 10/13/1978 07/26/2019 Under the hood, proc import is just writing code for a data step. So when proc import doesn’t work, we can just write the code ourselves. It requires a bit more work (specifying column names, for example) but it also doesn’t fail nearly as often. 6 /* x &quot;curl 6 ! https://raw.githubusercontent.com/srvanderplas/unl-stat850/maste 6 ! r/data/NE_Features_20200501.txt 7 &gt; data/NE_Features_20200501.txt&quot;; */ 8 /* only run this once... */ 9 10 data nefeatures; 11 /*infile &quot;data/NE_Features_20200501.txt&quot;*/ 12 infile &quot;data/NE_Features_ascii.txt&quot; 13 dlm=&#39;|&#39; /* specify delimiter */ 14 encoding=&quot;utf-8&quot; /* specify encoding */ 15 DSD /* delimiter sensitive data */ 16 missover /* keep going if missing obs encountered */ 17 firstobs=2; /* skip header row */ 18 input FEATURE_ID $ 19 FEATURE_NAME $ 20 FEATURE_CLASS $ 21 STATE_ALPHA $ 22 STATE_NUMERIC 23 COUNTY_NAME $ 24 COUNTY_NUMERIC $ 25 PRIMARY_LAT_DMS $ 26 PRIM_LONG_DMS $ 27 PRIM_LAT_DEC 28 PRIM_LONG_DEC 29 SOURCE_LAT_DMS $ 30 SOURCE_LONG_DMS $ 31 SOURCE_LAT_DEC 32 SOURCE_LONG_DEC 33 ELEV_IN_M 34 ELEV_IN_FT 35 MAP_NAME $ 36 DATE_CREATED $ 37 DATE_EDITED $ 38 ; 39 run; NOTE: The infile &quot;data/NE_Features_ascii.txt&quot; is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/NE_ Features_ascii.txt, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=09May2021:10:33:19, File Size (bytes)=4227269 NOTE: 31582 records were read from the infile &quot;data/NE_Features_ascii.txt&quot;. The minimum record length was 82. The maximum record length was 204. NOTE: The data set WORK.NEFEATURES has 31582 observations and 20 variables. NOTE: DATA statement used (Total process time): real time 0.13 seconds cpu time 0.15 seconds 40 41 proc print data=nefeatures (obs=10); /* print the first 10 41 ! observations */ 42 run; NOTE: There were 10 observations read from the data set WORK.NEFEATURES. NOTE: PROCEDURE PRINT used (Total process time): real time 0.04 seconds cpu time 0.04 seconds Obs FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME COUNTY_NUMERIC PRIMARY_LAT_DMS PRIM_LONG_DMS PRIM_LAT_DEC PRIM_LONG_DEC SOURCE_LAT_DMS SOURCE_LONG_DMS SOURCE_LAT_DEC SOURCE_LONG_DEC ELEV_IN_M ELEV_IN_FT MAP_NAME DATE_CREATED DATE_EDITED 1 171013 Peetz Ta Area CO 8 Logan 075 405840N 1030332W 40.9778 -103.059 . . 1341 4400 Peetz 10/13/19 2 171029 Sidney D Valley NE 31 Cheyenne 033 410816N 1030116W 41.1377 -103.021 405215N 1040353W 40.8710 -104.065 1256 4121 Brownson 10/13/19 03/08/20 3 182687 Highline Canal CO 8 Sedgwick 115 405810N 1023137W 40.9694 -102.527 . . 1118 3668 Sedgwick 10/13/19 4 182688 Cottonwo Stream CO 8 Sedgwick 115 405511N 1023355W 40.9197 -102.565 405850N 1030107W 40.9805 -103.019 1096 3596 Sedgwick 10/13/19 10/23/20 5 182689 Sand Dra Valley CO 8 Sedgwick 115 405951N 1023040W 40.9974 -102.511 410203N 1023313W 41.0343 -102.554 1137 3730 Sedgwick 10/13/19 12/20/20 6 182690 Sedgwick Valley CO 8 Sedgwick 115 405749N 1023313W 40.9637 -102.553 410227N 1023722W 41.0408 -102.623 1113 3652 Sedgwick 10/13/19 11/04/20 7 182692 Peterson Canal CO 8 Sedgwick 115 405604N 1023053W 40.9346 -102.515 . . 1171 3842 Sedgwick 10/13/19 10/15/20 8 182727 Sand Cre Stream CO 8 Sedgwick 115 404938N 1021752W 40.8272 -102.298 404857N 1022344W 40.8158 -102.395 1151 3776 Julesbur 10/13/19 9 182826 Wildhors Stream CO 8 Phillips 095 403620N 1020722W 40.6056 -102.123 403734N 1025720W 40.6261 -102.955 1101 3612 Amherst 10/13/19 10 183028 North Fo Stream NE 31 Dundy 057 400111N 1015618W 40.0197 -101.938 395958N 1022714W 39.9994 -102.454 988 3241 Haigler 10/13/19 07/26/20 Try it out Rebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveninently available at their download page. Because these datasets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV. The readr package can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv? Solution library(readr) legosets &lt;- read_csv(&quot;https://cdn.rebrickable.com/media/downloads/sets.csv.gz&quot;) write_csv(legosets, &quot;data/lego_sets.csv&quot;) In SAS, it is also possible to read gzip files directly; however, it is tricky to get PROC IMPORT to work with gzip files. The code below will 1) download the file (uncomment that part), 2) create a link to the gzip file, 3) create a link to where the unzipped file will go, and 4) unzip the file to the link specified in (3). Can you write two different statements (one using proc import on the unzipped file, one using a datastep on the zipped file) to read the data in? Note that you may have to specify the length of character fields in the data step version using length var_name $ 100; before the input statement to set variable var_name to have maximum length of 100 characters. /* x &quot;curl https://cdn.rebrickable.com/media/downloads/sets.csv.gz &gt; \\ data/lego_sets.csv.gz&quot;; only run this once... */ filename legofile ZIP &quot;data/lego_sets.csv.gz&quot; GZIP; filename target &quot;data/lego_sets.csv&quot;; data _null_; infile legofile; file target; input; put _infile_; run; Solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 /* Work with the library of class data */ 8 9 filename legofile ZIP &quot;data/lego_sets.csv.gz&quot; GZIP; 10 filename target &quot;data/lego_sets.csv&quot;; 11 12 data _null_; 13 infile legofile; 14 file target; 15 input; 16 put _infile_; 17 run; NOTE: The infile LEGOFILE is: Filename=data/lego_sets.csv.gz NOTE: The file TARGET is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/leg o_sets.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=09May2021:10:34:22 NOTE: 15425 records were read from the infile LEGOFILE. The minimum record length was 20. The maximum record length was 116. NOTE: 15425 records were written to the file TARGET. The minimum record length was 20. The maximum record length was 116. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 18 19 proc import datafile = target out=classdat.legoset DBMS=csv 19 ! replace; 20 GETNAMES=YES; 21 GUESSINGROWS=15424; 22 run; 23 /************************************************************** 23 ! ******** 24 * PRODUCT: SAS 25 * VERSION: 9.4 26 * CREATOR: External File Interface 27 * DATE: 09MAY21 28 * DESC: Generated SAS Datastep Code 29 * TEMPLATE SOURCE: (None Specified.) 30 *************************************************************** 30 ! ********/ 31 data CLASSDAT.LEGOSET ; 32 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 32 ! */ 33 infile TARGET delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 34 informat set_num $20. ; 35 informat name $95. ; 36 informat year best32. ; 37 informat theme_id best32. ; 38 informat num_parts best32. ; 39 format set_num $20. ; 40 format name $95. ; 41 format year best12. ; 42 format theme_id best12. ; 43 format num_parts best12. ; 44 input 45 set_num $ 46 name $ 47 year 48 theme_id 49 num_parts 50 ; 51 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 51 ! detection macro variable */ 52 run; NOTE: The infile TARGET is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/leg o_sets.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=09May2021:10:34:22, File Size (bytes)=633877 NOTE: 15424 records were read from the infile TARGET. The minimum record length was 20. The maximum record length was 116. NOTE: The data set CLASSDAT.LEGOSET has 15424 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 15424 rows created in CLASSDAT.LEGOSET from TARGET. NOTE: CLASSDAT.LEGOSET data set was successfully created. NOTE: The data set CLASSDAT.LEGOSET has 15424 observations and 5 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 3.40 seconds cpu time 3.40 seconds 53 54 /* This dataset will be stored in WORK */ 55 data legoset2; 56 infile legofile dsd firstobs=2 57 dlm=&quot;,&quot;; 58 length set_num $20; 59 length name $100; 60 input set_num $ name $ year theme_id num_parts; 61 run; NOTE: The infile LEGOFILE is: Filename=data/lego_sets.csv.gz NOTE: 15424 records were read from the infile LEGOFILE. The minimum record length was 20. The maximum record length was 116. NOTE: The data set WORK.LEGOSET2 has 15424 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 62 63 proc print data=classdat.legoset (obs=10); 64 run; NOTE: There were 10 observations read from the data set CLASSDAT.LEGOSET. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 65 66 proc print data=legoset2 (obs=10); 67 run; NOTE: There were 10 observations read from the data set WORK.LEGOSET2. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Obs set_num name year theme_id num_parts 1 001-1 Gears 1965 1 43 2 0011-2 Town Mini-Figures 1978 84 12 3 0011-3 Castle 2 for 1 Bonus Offer 1987 199 0 4 0012-1 Space Mini-Figures 1979 143 12 5 0013-1 Space Mini-Figures 1979 143 12 6 0014-1 Space Mini-Figures 1979 143 12 7 0015-1 Space Mini-Figures 1979 143 18 8 0016-1 Castle Mini Figures 1978 186 15 9 002-1 4.5V Samsonite Gears Motor Set 1965 1 3 10 003-1 Master Mechanic Set 1966 366 403 Obs set_num name year theme_id num_parts 1 001-1 Gears 1965 1 43 2 0011-2 Town Mini-Figures 1978 84 12 3 0011-3 Castle 2 for 1 Bonus Offer 1987 199 0 4 0012-1 Space Mini-Figures 1979 143 12 5 0013-1 Space Mini-Figures 1979 143 12 6 0014-1 Space Mini-Figures 1979 143 12 7 0015-1 Space Mini-Figures 1979 143 18 8 0016-1 Castle Mini Figures 1978 186 15 9 002-1 4.5V Samsonite Gears Motor Set 1965 1 3 10 003-1 Master Mechanic Set 1966 366 403 5.3 Spreadsheets In R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in Excel or LibreOffice first to make sure the formatting is as you expected it to be. if (!&quot;readxl&quot; %in% installed.packages()) install.packages(&quot;readxl&quot;) library(readxl) path &lt;- &quot;data/police_violence.xlsx&quot; if (!file.exists(path)) download.file(&quot;https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx&quot;, path, mode = &quot;wb&quot;) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1) police_violence[1:10, 1:6] ## # A tibble: 10 x 6 ## `Victim&#39;s name` `Victim&#39;s age` `Victim&#39;s gender` `Victim&#39;s race` ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Eric M. Tellez 28 Male White ## 2 Name withheld by police NA Male Unknown race ## 3 Terry Hudson 57 Male Black ## 4 Malik Williams 23 Male Black ## 5 Frederick Perkins 37 Male Black ## 6 Michael Vincent Davis 49 Male White ## 7 Brian Elkins 47 Male Unknown race ## 8 Debra D. Arbuckle 51 Female White ## 9 Name withheld by police NA Male Unknown race ## 10 Cody McCaulou 27 Male White ## # … with 2 more variables: URL of image of victim &lt;chr&gt;, ## # Date of Incident (month/day/year) &lt;dttm&gt; In SAS, PROC IMPORT is one easy way to read in xlsx files. In this code chunk, we have to handle the fact that one of the columns in the spreadsheet contains dates. SAS and Excel handle dates a bit differently, so we have to transform the date variable – and we may as well relabel it at the same time. To do this, we use a DATA statement that outputs to the same dataset it references. We define a new variable date, adjust the Excel dates so that they conform to SAS’s standard, and tell SAS how to format the date. (We’ll talk more about dates and times later) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC IMPORT OUT=classdat.police 9 DATAFILE=&quot;data/police_violence.xlsx&quot; 10 DBMS=xlsx /* Tell SAS what type of file it&#39;s reading */ 11 REPLACE; 11 ! /* replace the dataset if it already exists */ 12 SHEET=&quot;2013-2019 Police Killings&quot;; /* SAS reads the first 12 ! sheet by default */ 13 GETNAMES=yes; 14 informat VAR6 mmddyy10.; /* tell SAS what format the date is 14 ! in */ 15 RUN; NOTE: Variable Name Change. Victim&#39;s name -&gt; Victim_s_name NOTE: Variable Name Change. Victim&#39;s age -&gt; Victim_s_age NOTE: Variable Name Change. Victim&#39;s gender -&gt; Victim_s_gender NOTE: Variable Name Change. Victim&#39;s race -&gt; Victim_s_race NOTE: Variable Name Change. URL of image of victim -&gt; URL_of_image_of_victim NOTE: Variable Name Change. Date of Incident (month/day/year -&gt; VAR6 NOTE: Variable Name Change. Street Address of Incident -&gt; Street_Address_of_Incident NOTE: Variable Name Change. Agency responsible for death -&gt; Agency_responsible_for_death NOTE: Variable Name Change. Cause of death -&gt; Cause_of_death NOTE: Variable Name Change. A brief description of the circu -&gt; A_brief_description_of_the_circu NOTE: Variable Name Change. Official disposition of death (j -&gt; Official_disposition_of_death__j NOTE: Variable Name Change. Criminal Charges? -&gt; Criminal_Charges_ NOTE: Variable Name Change. Link to news article or photo of -&gt; Link_to_news_article_or_photo_of NOTE: Variable Name Change. Symptoms of mental illness? -&gt; Symptoms_of_mental_illness_ NOTE: Variable Name Change. Alleged Weapon (Source: WaPo) -&gt; VAR20 NOTE: Variable Name Change. Alleged Threat Level (Source: Wa -&gt; Alleged_Threat_Level__Source__Wa NOTE: Variable Name Change. Fleeing (Source: WaPo) -&gt; VAR22 NOTE: Variable Name Change. Body Camera (Source: WaPo) -&gt; VAR23 NOTE: Variable Name Change. WaPo ID (If included in WaPo dat -&gt; WaPo_ID__If_included_in_WaPo_dat NOTE: Variable Name Change. Off-Duty Killing? -&gt; Off_Duty_Killing_ NOTE: Variable Name Change. Geography (via Trulia methodolog -&gt; Geography__via_Trulia_methodolog NOTE: One or more variables were converted because the data type is not supported by the V9 engine. For more details, run with options MSGLEVEL=I. NOTE: The import data set has 7663 observations and 27 variables. NOTE: CLASSDAT.POLICE data set was successfully created. NOTE: PROCEDURE IMPORT used (Total process time): real time 3.54 seconds cpu time 3.53 seconds 16 17 DATA classdat.police; 18 SET classdat.police; /* modify the dataset and write back out 18 ! to it */ 19 20 date = VAR6 - 21916; /* Conversion to SAS date standard from 20 ! Excel */ 21 FORMAT date MMDDYY10.; /* Tell SAS how to format the data when 21 ! printing it */ 22 DROP VAR6; /* Get rid of the original data */ 23 24 num_age = INPUT(Victim_s_age, 3.); /* create numeric age 24 ! variable */ 25 26 DROP 27 A_brief_description_of_the_circu 28 URL_of_image_of_victim 29 Link_to_news_article_or_photo_of; 30 /* drop longer variable to save space so the file fits on 30 ! GitHub */ 31 /* Size went from 100 MB to 6.7 MB without these 3 vars */ 32 RUN; NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43465 Street_Address_of_Incident=13600 Vanowen St City=Van Nuys State=CA Zipcode=91405 County=Los Angeles Agency_responsible_for_death=Los Angeles Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Officers responded about 4:30 a.m. to a &quot;s creaming woman radio call.&quot; According to police, a person came out and got into a fight with the man involved. The man went back inside the building, and people directed police to his apartment. When police knocked on the doo r, the man opened the door, and he allegedly was armed with a knife. The of ficers ordered him to drop the knife, and when he failed to comply, he was shot and killed. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://abc7.com/knife-wielding-suspect-ki lled-in-lapd-shooting-in-van-nuys/4995348/ Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=4340 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6567 date=12/31/2018 num_age=. _ERROR_=1 _N_=1099 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43465 Street_Address_of_Incident=5345 Memorial Drive City=Stone Mountain State=GA Zipcode=30083 County=DeKalb Agency_responsible_for_death=Pine Lake Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=An armed unidentified man entered Big John &#39;s Package Store at about 11:02 p.m. Police said the man fired a shot and t old the store employees and customers to get on the floor. An off-duty Pine Lake Police Department officer was on security duty in the store and shot and killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://wgxa.tv/news/local/dekalb-co-new-y ears-eve-officer-involved-shooting?fbclid=IwAR1-8_-p_ZYMSw0h74SBChUiQGaIRGT hjXhfT0roE2Dp-UuiAL_i74a2rXk Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_=Off-Duty Geography__via_Trulia_methodolog=Suburban ID=6566 date=12/31/2018 num_age=. _ERROR_=1 _N_=1101 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Nathan Shepard Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43463 Street_Address_of_Incident=3373 Alice Hall Rd City=Golden State=MS Zipcode=38847 County=Itawamba Agency_responsible_for_death=Itawamba County Sheriff&#39;s Office, Mississippi Bureau of Investigation, Mississippi Highway Patrol Cause_of_death=Gunshot A_brief_description_of_the_circu=Nathan Shepard took Paul Blackburn and Bla ckburn&#39;s daughter hostage, killing Paul Blackburn before police arrived. Af ter a 32-hour standoff, Shepard was shot and killed. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://hottytoddy.com/2018/12/31/10-year- old-survives-23-hour-hostage-standoff-in-itawamba-county/ Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6559 date=12/29/2018 num_age=. _ERROR_=1 _N_=1108 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43454 Street_Address_of_Incident=Chaney St and Collier Ave City=Lake Elsinore State=CA Zipcode=92530 County=Riverside Agency_responsible_for_death=Riverside County Sheriff&#39;s Department Cause_of_death=Gunshot A_brief_description_of_the_circu=About 4 p.m., police tried to arrest a man wanted in connection with a double shooting. As they attempted to move in, he did not pull over and a brief chase took place. The pursuit ended in a crash at a parking lot. He got out of the vehicle and reportedly produced a gun, and he was shot and killed. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://abc7.com/chase-leads-to-fatal-offi cer-involved-shooting-in-lake-elsinore/4939109/ Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=4319 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6538 date=12/20/2018 num_age=. _ERROR_=1 _N_=1128 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43453 Street_Address_of_Incident=N 36th St &amp; E Monte Vista Rd City=Phoenix State=AZ Zipcode=85008 County=Maricopa Agency_responsible_for_death=Phoenix Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Officers responded just before 9 p.m. afte r receiving reports of someone throwing objects through the front window of a Circle K store. As officers went to question a man seen in a nearby neig hborhood, the man attacked one of the officers, and they reportedly began f ighting. As the two were fighting, another Phoenix officer shot and killed the man Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.azcentral.com/story/news/local /phoenix-breaking/2018/12/19/phoenix-police-scene-shooting-involving-office r/2372812002/?fbclid=IwAR15GU13PF5ZqTe1FKpv0FP_8knKW4zhT8K3iNdSYR8GjmgSSmVR w2MjlZI Symptoms_of_mental_illness_=Unknown Unarmed=Unclear VAR20=undetermined Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=4313 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6537 date=12/19/2018 num_age=. _ERROR_=1 _N_=1131 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic URL_of_image_of_victim= VAR6=43447 Street_Address_of_Incident=N Benson Ave &amp; W 11th St City=Upland State=CA Zipcode=91786 County=San Bernardino Agency_responsible_for_death=Upland Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Police responded to a call from a nearby r esident about a suspicious person. Officers spotted a man walking around 3: 30 a.m. and approached him. Goodman said when officers approached the man, he reached into his pocket and produced the replica, and the officer shot a nd killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.dailybulletin.com/2018/12/13/m an-hospitalized-after-being-shot-by-upland-police/ Symptoms_of_mental_illness_=No Unarmed=Unarmed VAR20=toy Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6515 date=12/13/2018 num_age=. _ERROR_=1 _N_=1155 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=White URL_of_image_of_victim= VAR6=43353 Street_Address_of_Incident= City=Spanaway State=WA Zipcode= County= Agency_responsible_for_death= Cause_of_death=Gunshot A_brief_description_of_the_circu= Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of= Symptoms_of_mental_illness_= Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Foot VAR23=No WaPo_ID__If_included_in_WaPo_dat=4019 Off_Duty_Killing_= Geography__via_Trulia_methodolog=#N/A ID=6235 date=09/10/2018 num_age=. _ERROR_=1 _N_=1431 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43339 Street_Address_of_Incident=10900 Neiderhouse Rd City=Perrysburg State=OH Zipcode=43551 County=Wood Agency_responsible_for_death=Perrysburg Township Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Police stopped a vehicle with three occupa nts on I-75 north of the U.S. 20 exit at about 2:30 p.m. One person was tak en into custody at that time. The vehicle fled with the two remaining peopl e. A second person was taken into custody while on foot after leaving the v ehicle, police said. The third person continued in the fleeing vehicle, whi ch crashed in a ditch. The man led officers on a brief foot pursuit before he allegedly shot at police and was killed by Lt. Matt Gazarek, 41, Sgt. Da vid Motler, 39, and Officer Danny Widmer, 36. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=http://www.toledoblade.com/Police-Fire/201 8/08/27/Perrysburg-Township-Police-investigating-possible-shooting/stories/ 20180827129 Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22=Foot VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6203 date=08/27/2018 num_age=. _ERROR_=1 _N_=1465 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43339 Street_Address_of_Incident=8801 Airline Dr City=Houston State=TX Zipcode=77037 County=Harris Agency_responsible_for_death=Harris County Sheriff&#39;s Office Cause_of_death=Gunshot A_brief_description_of_the_circu=A deputy was working an extra job at a nig htclub when he noticed some suspicious activity in the parking lot. The dep uty walked over to a truck in the parking lot and saw two men sitting insid e. The deputy was questioning the men when he noticed a gun on the console. The deputy ordered the men to step out of the truck when the passenger all egedly grabbed the gun and pointed it at the deputy who shot and killed him . Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://cw39.com/2018/08/27/hcso-man-fatal ly-shot-by-deputy-nightclub-parking-lot-after-altercation/ Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6198 date=08/27/2018 num_age=. _ERROR_=1 _N_=1466 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43335 Street_Address_of_Incident=202 E First Street City=Santa Ana State=CA Zipcode=92701 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=About 3:15 p.m., officers were looking for a gray-green Volkswagen Passat that was connected to an earlier homicide. They found it at the Santa Ana Express Car Wash with two people inside. Off icers shot and killed one, and arrested the other, although what precipitat ed the killing or the reason for arresting the second person were withheld by police. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://abc7.com/murder-suspect-killed-in- santa-ana-officer-involved-shooting/4041715/ Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6182 date=08/23/2018 num_age=. _ERROR_=1 _N_=1485 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic URL_of_image_of_victim= VAR6=43326 Street_Address_of_Incident=Kings Canyon Rd and Chestnut Ave City=Fresno State=CA Zipcode=93702 County=Fresno Agency_responsible_for_death=Fresno Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Around 11:30 a.m., a 9-1-1 call reported a man yelling, agitated and perhaps on drugs in the area. Another call came in saying a second person was fighting with the first. After about 15 to 20 minutes, the man set down his backpack and pulled a knife. Officers report edly began to back away and continued to try to calm the man. The man alleg edly reached back into the backpack and pulled out a handgun. He raised the gun toward the officers, and they shot and killed him with some less-letha l rounds and some less-less lethal rounds. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.yourcentralvalley.com/news/loc al-news/police-respond-to-possible-officer-involved-shooting-in-southeast-f resno/1368436573 Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun and knife Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=3967 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6162 date=08/14/2018 num_age=. _ERROR_=1 _N_=1505 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43324 Street_Address_of_Incident=21000 Neely Dr City=Flint State=TX Zipcode=75762 County=Smith Agency_responsible_for_death=Smith County Sheriff&#39;s Office Cause_of_death=Gunshot A_brief_description_of_the_circu=Police received a call for a welfare check on a person. Officers arrived around 1:06 p.m., but they were unable to ge t someone to answer the door. Around 1:30 p.m. they were able to make conta ct with a person through the window. The person was asked to open the front door and did not. About five minutes later ,the occupants daughter arrived with a key to the home, and deputies entered. As deputies were clearing th e house when the woman who lives in the home confronted the deputies with a pistol. One shot was fired by a deputy and killed the woman who was allege dly armed. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.cbs19.tv/article/news/local/up date-standoff-ends-after-woman-shoots-self/501-583362575 Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6154 date=08/12/2018 num_age=. _ERROR_=1 _N_=1512 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43318 Street_Address_of_Incident=SW 24th Ave &amp; SW 21st St City=Okeechobee State=FL Zipcode=34974 County=Okeechobee Agency_responsible_for_death=Okeechobee County Sheriff&#39;s Office, Okeechobee Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Deputies responded to a domestic violence situation where a man armed himself and fled. A low-speed chase ensued to a nother location, where the armed man held a gun to his own head. The man ap proached deputies while still armed, and deputies reportedly fired less-let hal rounds at him, attempting to disable him. Deputies then shot and killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.wptv.com/news/region-okeechobe e-county/officer-involved-shooting-investigated-in-okeechobee-county Symptoms_of_mental_illness_=Unknown Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6135 date=08/06/2018 num_age=. _ERROR_=1 _N_=1532 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43309 Street_Address_of_Incident=2400 W 65th Ave City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Aurora Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Aurora police were tracking a vehicle by h elicopter. The vehicle was found in Adams County. Aurora police responded t o that location, and the driver drove his vehicle at Aurora police officers and hit several of them. He was shot and killed. He was allegedly a suspec t in a shooting. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://denver.cbslocal.com/2018/07/29/pol ice-aurora-deadly-shooting-adams-county/ Symptoms_of_mental_illness_=No Unarmed=Vehicle VAR20=vehicle Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=3900 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6107 date=07/28/2018 num_age=. _ERROR_=1 _N_=1561 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43292 Street_Address_of_Incident=11900 Canal St City=Willis State=TX Zipcode=77318 County=Montgomery Agency_responsible_for_death=Montgomery County Sheriff&#39;s Office Cause_of_death=Gunshot A_brief_description_of_the_circu=Deputies were called at approximately 2:30 p.m. to a residence on reports of a trespasser in progress call. When depu ties arrived a woman was inside the residence that does not belong to her. She was apparently known to police--commenters on social media said she was mentally ill--and had been accused of breaking into other homes in the are a. Deputies ordered her to drop the knife, deputies then reportedly attempt ed to use less-lethal weapons that proved ineffective as she continued to a dvance toward deputies. They shot and killed her. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.chron.com/neighborhood/moco/ne ws/article/Woman-killed-in-officer-involved-shooting-in-13067765.php Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3852 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6044 date=07/11/2018 num_age=. _ERROR_=1 _N_=1621 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43289 Street_Address_of_Incident=Magnolia Street and Bolsa Avenue City=Westminster State=CA Zipcode=92683 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=A man was shot and killed after a police c hase in a vehicle and a crash where two bystanders were injured. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://ktla.com/2018/07/09/pursuit-driver -fatally-shot-by-santa-ana-police/ Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3829 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6037 date=07/08/2018 num_age=. _ERROR_=1 _N_=1629 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43288 Street_Address_of_Incident=W 70th Ave and Broadway City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Adams County Sheriff&#39;s Office Cause_of_death=Gunshot A_brief_description_of_the_circu=Deputies responded to a trespassing call a t a vacant home two times within a four-hour period; once around between 9: 30 p.m. and 10 p.m. Friday, and again at around 1:30 a.m. Saturday. When th ey arrived to the house on the second call, deputies noticed cars speeding away from the home. The deputies started following the group of cars away f rom the house. About a mile away from the home, one of the cars got in a cr ash. People inside of the vehicle got out of the crashed car, and a deputy started chasing one of them on foot. That person had allegedly pulled a wea pon when he was shot and killed by the pursuing deputy. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://www.thedenverchannel.com/news/loca l-news/one-dead-in-officer-involved-shooting-in-adams-county Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unknown weapon Alleged_Threat_Level__Source__Wa=attack VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3835 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6036 date=07/07/2018 num_age=. _ERROR_=1 _N_=1632 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43282 Street_Address_of_Incident=1020 West Civic Center Dr City=Santa Ana State=CA Zipcode=92703 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Officers with the Santa Ana Police Departm ent responded to a parking structure, where a man was attempting to break i nto cars with a long metal stake. When police arrived at the structure, whi ch is located right across the street from the department, a fight began. O fficers reportedly used a Taser to try to subdue the man, but eventually sh ot and killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=http://abc7.com/1-dead-in-santa-ana-office r-involved-shooting/3690595/ Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=metal object Alleged_Threat_Level__Source__Wa=undetermined VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3814 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6018 date=07/01/2018 num_age=. _ERROR_=1 _N_=1647 NOTE: Invalid argument to function INPUT at line 24 column 13. Victim_s_name=Jason Erik Washington Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Black URL_of_image_of_victim=https://www.fatalencounters.org/wp-content/uploads/2 018/07/Jason-Erik-Washington.jpg VAR6=43280 Street_Address_of_Incident=1939 SW 6th Ave City=Portland State=OR Zipcode=97201 County=Multnomah Agency_responsible_for_death=Portland State University Department of Public Safety Cause_of_death=Gunshot A_brief_description_of_the_circu=Jason Washington was shot by campus police during a confrontation outside a bar. Witnesses said he was attempting to break up a fight when the registered gun in his waistband holster fell on t he ground. He attempted to pick it up when officers shot and killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=https://nbc16.com/news/nation-world/portla nd-state-university-students-renew-push-to-disarm-psu-campus-police-rally-p rotest-pioneer-square Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=undetermined VAR22= VAR23=No WaPo_ID__If_included_in_WaPo_dat=3815 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6010 date=06/29/2018 num_age=. _ERROR_=1 _N_=1654 NOTE: Invalid argument to function INPUT at line 24 column 13. WARNING: Limit set by ERRORS= option reached. Further errors of this type will not be printed. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race URL_of_image_of_victim= VAR6=43274 Street_Address_of_Incident=4860 Rolando Ct City=San Diego State=CA Zipcode=92115 County=San Diego Agency_responsible_for_death=San Diego Police Department Cause_of_death=Gunshot A_brief_description_of_the_circu=Officers responded to a 9-1-1 report of a violent disturbance about 10:15 p.m. Three officers arrived and knocked on the door, but got no response. When they smelled what seemed to be smoke co ming from the first-floor unit, they called San Diego firefighters for back up. They then forced open the door and were met by gunfire. Two officers we re hit. The man was found dead later, although it wasn&#39;t immediately appare nt whose bullet killed him. Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Link_to_news_article_or_photo_of=http://www.sandiegouniontribune.com/g00/ne ws/public-safety/sd-me-officers-wounded-20180623-story.html?i10c.encReferre r=aHR0cDovL3d3dy5ndW52aW9sZW5jZWFyY2hpdmUub3JnL2luY2lkZW50LzExNDg0MjI%3D&amp;i1 0c.ua=1&amp;i10c.dv=14 Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3800 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=5994 date=06/23/2018 num_age=. _ERROR_=1 _N_=1668 NOTE: Mathematical operations could not be performed at the following places. The results of the operations have been set to missing values. Each place is given by: (Number of times) at (Line):(Column). 139 at 24:13 NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: The data set CLASSDAT.POLICE has 7663 observations and 25 variables. NOTE: DATA statement used (Total process time): real time 0.07 seconds cpu time 0.06 seconds 33 34 35 PROC PRINT DATA=classdat.police (obs=10); /* print the first 10 35 ! observations */ 36 VAR Victim_s_name Victim_s_age num_age Victim_s_gender 36 ! Victim_s_race date; 37 RUN; NOTE: There were 10 observations read from the data set CLASSDAT.POLICE. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Obs Victim_s_name Victim_s_age num_age Victim_s_gender Victim_s_race date 1 Eric M. Tellez 28 28 Male White 12/31/2019 2 Name withheld by police . Male Unknown race 12/31/2019 3 Terry Hudson 57 57 Male Black 12/31/2019 4 Malik Williams 23 23 Male Black 12/31/2019 5 Frederick Perkins 37 37 Male Black 12/31/2019 6 Michael Vincent Davis 49 49 Male White 12/31/2019 7 Brian Elkins 47 47 Male Unknown race 12/31/2019 8 Debra D. Arbuckle 51 51 Female White 12/30/2019 9 Name withheld by police . Male Unknown race 12/30/2019 10 Cody McCaulou 27 27 Male White 12/30/2019 Here is some additional information about reading and writing Excel files in SAS. In general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier. Try it out The Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link Read that data in using both R and SAS. Be sure to look at the structure of the excel file, so that you can read the data in properly! Solution url &lt;- &quot;https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/2019_Veh_Reg_by_Plate_Type.xlsx&quot; download.file(url, destfile = &quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot;, mode = &quot;wb&quot;) library(readxl) ne_plates &lt;- read_xlsx(path = &quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot;, skip = 1) ## New names: ## * County -&gt; County...1 ## * County -&gt; County...34 ne_plates[1:10,1:6] ## Warning in fansi::strwrap_ctl(x, width = max(width, 0), indent = indent, : ## Encountered a C0 control character, see `?unhandled_ctl`; you can use ## `warn=FALSE` to turn off these warnings. ## # A tibble: 10 x 6 ## County...1 `Amateur\\r\\nRadio` `Apport-\\r\\nioned` `Apport\\r\\nTrlr` AC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 C01 - DOUGLAS 148 0 0 10 ## 2 C02 - LANCASTER 244 0 0 2 ## 3 C03 - GAGE 10 0 0 1 ## 4 C04 - CUSTER 6 0 0 0 ## 5 C05 - DODGE 28 0 0 0 ## 6 C06 - SAUNDERS 18 0 0 0 ## 7 C07 - MADISON 19 0 0 0 ## 8 C08 - HALL 16 0 0 0 ## 9 C09 - BUFFALO 26 0 0 2 ## 10 C10 - PLATTE 12 0 0 2 ## # … with 1 more variable: Breast Cancer &lt;dbl&gt; 6 PROC IMPORT OUT=WORK.licplate 7 DATAFILE=&quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot; 8 DBMS=xlsx /* Tell SAS what type of file it&#39;s reading */ 9 REPLACE; NOTE: The previous statement has been deleted. 9 ! /* replace the dataset if it already exists */ 10 RANGE=&quot;&#39;Reg By Plate Type&#39;$A2:0&quot; 11 GETNAMES=yes; ________ 22 202 ERROR 22-322: Expecting ;. ERROR 202-322: The option or parameter is not recognized and will be ignored. 12 RUN; NOTE: Variable Name Change. Amateur Radio -&gt; Amateur__Radio NOTE: Variable Name Change. Apport- ioned -&gt; VAR3 NOTE: Variable Name Change. Apport Trlr -&gt; Apport__Trlr NOTE: Variable Name Change. Breast Cancer -&gt; Breast__Cancer NOTE: Variable Name Change. Choose Life -&gt; Choose__Life NOTE: Variable Name Change. County Gov -&gt; County__Gov NOTE: Variable Name Change. Comm Truck -&gt; Comm__Truck NOTE: Variable Name Change. Creighton University -&gt; Creighton__University NOTE: Variable Name Change. Corn Growers -&gt; Corn__Growers NOTE: Variable Name Change. Dlr Boat Trailer -&gt; Dlr_Boat___Trailer NOTE: Variable Name Change. Dlr MC -&gt; Dlr___MC NOTE: Variable Name Change. Dlr Pass -&gt; Dlr__Pass NOTE: Variable Name Change. Prsnl Use Dlr -&gt; Prsnl__Use_Dlr NOTE: Variable Name Change. Dlr Trlr -&gt; Dlr__Trlr NOTE: Variable Name Change. Ducks Unlimited -&gt; Ducks__Unlimited NOTE: Variable Name Change. Ex-POW -&gt; Ex_POW NOTE: Variable Name Change. Former Military -&gt; Former__Military NOTE: Variable Name Change. Farm Truck -&gt; Farm__Truck NOTE: Variable Name Change. Farm Trailer -&gt; Farm___Trailer NOTE: Variable Name Change. Gold Star -&gt; Gold___Star NOTE: Variable Name Change. Greater Omaha -&gt; Greater___Omaha NOTE: Variable Name Change. Henry Doorly Zoo -&gt; Henry___Doorly_Zoo NOTE: Variable Name Change. Local Truck -&gt; Local__Truck NOTE: Variable Name Change. Military Honor -&gt; Military___Honor NOTE: Variable Name Change. Mini- truck -&gt; VAR32 NOTE: Variable Name Change. Mountain Lion -&gt; Mountain__Lion NOTE: Variable Name Change. Mobile Home -&gt; Mobile___Home NOTE: Variable Name Change. Muni Gov -&gt; Muni__Gov NOTE: Variable Name Change. NE 150 -&gt; NE_150 NOTE: Variable Name Change. Nebraska Cattlemen -&gt; Nebraska___Cattlemen NOTE: Variable Name Change. Native American -&gt; Native___American NOTE: Variable Name Change. Public Power -&gt; Public___Power NOTE: Variable Name Change. PH Surv -&gt; PH__Surv NOTE: Variable Name Change. Planned Prnthd -&gt; Planned___Prnthd NOTE: Variable Name Change. Purple Heart -&gt; Purple___Heart NOTE: Variable Name Change. Sammy&#39;s Sprhr -&gt; VAR50 NOTE: Variable Name Change. School District -&gt; School___District NOTE: Variable Name Change. Soil &amp; Water -&gt; VAR52 NOTE: Variable Name Change. Special Equip -&gt; Special___Equip NOTE: Variable Name Change. Special Interest -&gt; Special___Interest NOTE: Variable Name Change. Serious Injury -&gt; Serious___Injury NOTE: Variable Name Change. State Gov -&gt; State___Gov NOTE: Variable Name Change. Tax Exempt -&gt; Tax___Exempt NOTE: Variable Name Change. Union Pacific -&gt; Union___Pacific NOTE: One or more variables were converted because the data type is not supported by the V9 engine. For more details, run with options MSGLEVEL=I. NOTE: The import data set has 95 observations and 64 variables. NOTE: WORK.LICPLATE data set was successfully created. NOTE: PROCEDURE IMPORT used (Total process time): real time 0.06 seconds cpu time 0.06 seconds 13 14 /* just a few columns... way too many to handle */ 15 PROC PRINT DATA=WORK.licplate (obs=10); /* print the first 10 15 ! observations */ 16 Var County Amateur__Radio Breast__Cancer Choose__Life 16 ! County__Gov Comm__Truck Passenger Total; 17 RUN; NOTE: There were 10 observations read from the data set WORK.LICPLATE. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on page 23. Obs County Amateur__Radio Breast__Cancer Choose__Life County__Gov Comm__Truck Passenger TOTAL 1 C01 - DOUGLAS 148 957 584 1432 71296 313694 467720 2 C02 - LANCASTER 244 802 540 649 44031 164391 275088 3 C03 - GAGE 10 47 17 135 6153 12839 32484 4 C04 - CUSTER 6 13 5 218 2684 6312 22154 5 C05 - DODGE 28 86 32 92 8907 21248 45040 6 C06 - SAUNDERS 18 65 55 225 6019 13372 34531 7 C07 - MADISON 19 88 47 197 9308 19776 44478 8 C08 - HALL 16 136 59 212 15177 35270 72244 9 C09 - BUFFALO 26 79 46 206 12354 27235 61208 10 C10 - PLATTE 12 77 62 186 9437 20137 47551 5.4 Binary Files Both R and SAS have binary data files that store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Luckily, it is possible to read the binary data files in both programs. Let’s read in the data from the 2009 National Household Travel Survey: 6 libname classdat &quot;data&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/data 7 /* this tells SAS where to look for (a bunch of) data files */ 8 9 proc contents data=classdat.cen10pub; /* This tells sas to 9 ! access the specific file */ NOTE: Data file CLASSDAT.CEN10PUB.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 10 run; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.01 seconds cpu time 0.02 seconds ERROR: Errors printed on page 23. Data Set Name CLASSDAT.CEN10PUB Observations 150147 Member Type DATA Variables 8 Engine V9 Indexes 0 Created 06/30/2014 12:35:56 Observation Length 25 Last Modified 06/30/2014 12:35:56 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_32 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 4096 Number of Data Set Pages 928 First Data Page 1 Max Obs per Page 162 Obs in First Data Page 75 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/2020-stat850/data/cen10pub.sas7bdat Release Created 9.0202M2 Host Created W32_VSPRO Inode Number 39985763 Access Permission rw-rw-r– Owner Name susan File Size 4MB File Size (bytes) 3802112 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat Label 5 CBSACAT10 Char 2 CBSA category for the HH home address 4 CBSASIZE10 Char 2 CBSA (2010) population size for the HH home address 2 HH_CBSA10 Char 5 \\(CHAR5.&lt;/td&gt; &lt;td class=&quot;l data&quot;&gt;\\)CHAR5. HH CBSA location, 2013 CBSA definitions based on 2010 Census 1 HOUSEID Char 8 $8. $8. HH eight-digit ID number 3 RAIL10 Char 2 CBSA (2010) heavy rail status for HH 6 URBAN10 Char 2 Home address in urbanized area 8 URBRUR10 Char 2 Household in urban/rural area (2010 Urban definition) 7 URBSIZE10 Char 2 Size of urban area in which home address is located We can read the same file into R using the sas7bdat library: if (!&quot;sas7bdat&quot; %in% installed.packages()) install.packages(&quot;sas7bdat&quot;) library(sas7bdat) data &lt;- read.sas7bdat(&quot;https://github.com/srvanderplas/unl-stat850/raw/master/data/cen10pub.sas7bdat&quot;) head(data) ## HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10 ## 1 20000017 XXXXX 02 02 03 04 06 02 ## 2 20000231 XXXXX 02 03 03 01 03 01 ## 3 20000521 XXXXX 02 03 03 01 03 01 ## 4 20001283 35620 01 05 01 01 05 01 ## 5 20001603 -1 02 06 04 04 06 02 ## 6 20001649 XXXXX 02 03 03 01 02 01 If you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured. There are theoretically ways to read R data into SAS via the R subsystem. Feel free to do that on your own machine.16 In R, there are a couple of different binary files. .Rdata is perhaps the most common, and can store several objects (along with their names) in the same file. legos &lt;- read_csv(&quot;data/lego_sets.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## set_num = col_character(), ## name = col_character(), ## year = col_double(), ## theme_id = col_double(), ## num_parts = col_double() ## ) my_var &lt;- &quot;This variable contains a string&quot; save(legos, my_var, file = &quot;data/R_binary.Rdata&quot;) If we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious. We can load the R binary file back in using the load() function. rm(legos, my_var) # clear the files out ls() # all objects in the working environment ## [1] &quot;breaks&quot; &quot;data&quot; &quot;mesodata&quot; ## [4] &quot;mesodata_names&quot; &quot;ne_plates&quot; &quot;nebraska_locations&quot; ## [7] &quot;path&quot; &quot;pokemon_info&quot; &quot;police_violence&quot; ## [10] &quot;sasexe&quot; &quot;sasopts&quot; &quot;tmp&quot; ## [13] &quot;tmp_ascii&quot; &quot;tmp_chars&quot; &quot;tmp_chars_space&quot; ## [16] &quot;tmp_space&quot; &quot;url&quot; &quot;widths&quot; load(&quot;data/R_binary.Rdata&quot;) ls() # all objects in the working environment ## [1] &quot;breaks&quot; &quot;data&quot; &quot;legos&quot; ## [4] &quot;mesodata&quot; &quot;mesodata_names&quot; &quot;my_var&quot; ## [7] &quot;ne_plates&quot; &quot;nebraska_locations&quot; &quot;path&quot; ## [10] &quot;pokemon_info&quot; &quot;police_violence&quot; &quot;sasexe&quot; ## [13] &quot;sasopts&quot; &quot;tmp&quot; &quot;tmp_ascii&quot; ## [16] &quot;tmp_chars&quot; &quot;tmp_chars_space&quot; &quot;tmp_space&quot; ## [19] &quot;url&quot; &quot;widths&quot; RDS format (another binary file format in R) Another (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents. As a result, when you read from an RDS file, you need to store the result of that function into a variable. saveRDS(legos, &quot;data/RDSlego.rds&quot;) other_lego &lt;- readRDS(&quot;data/RDSlego.rds&quot;) Because RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately. Try it out Read in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. In RStudio, go to Session -&gt; Clear Workspace. (This will clear your environment) Now, using your RDS files, load the objects back into R with different names. Finally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious) Solution library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1, guess_max = 7000) police_violence2 &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 2, guess_max = 7000) save(police_violence, police_violence2, file = &quot;data/04_Try_Binary.Rdata&quot;) saveRDS(police_violence, &quot;data/04_Try_Binary1.rds&quot;) saveRDS(police_violence2, &quot;data/04_Try_Binary2.rds&quot;) rm(police_violence, police_violence2) # Limited clearing of workspace... pv1 &lt;- readRDS(&quot;data/04_Try_Binary1.rds&quot;) pv2 &lt;- readRDS(&quot;data/04_Try_Binary2.rds&quot;) load(&quot;data/04_Try_Binary.Rdata&quot;) all.equal(police_violence, pv1) ## [1] TRUE all.equal(police_violence2, pv2) ## [1] TRUE 5.5 Databases There are many different database formats. Some of the most common databases are SQL* related formats and Microsoft Access files. Strictly speaking you can get through this class without this section. Feel free to skip it and come back when/if you need it. This excellent GitHub repo contains code to connect to multiple types of databases in R, python, PHP, Java, SAS, and VBA 5.5.1 Microsoft Access In R, we can read in MS Access files using the Hmisc package, as long as the mdbtools library is available on your computer17. For this demo, we’ll be using the Scottish Witchcraft Database, which you can download from their website, or acquire from the course data folder. if (!&quot;Hmisc&quot; %in% installed.packages()) install.packages(&quot;Hmisc&quot;) library(Hmisc) ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## src, summarize ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units db_loc &lt;- &quot;data/Witchcraftsurvey_download.mdb&quot; mdb.get(db_loc, tables = TRUE) # get table list ## [1] &quot;WDB_Accused&quot; &quot;WDB_Accused_family&quot; ## [3] &quot;WDB_Appeal&quot; &quot;WDB_CalendarCustom&quot; ## [5] &quot;WDB_Case&quot; &quot;WDB_Case_person&quot; ## [7] &quot;WDB_Commission&quot; &quot;WDB_Complaint&quot; ## [9] &quot;WDB_Confession&quot; &quot;WDB_CounterStrategy&quot; ## [11] &quot;WDB_DemonicPact&quot; &quot;WDB_Denunciation&quot; ## [13] &quot;WDB_DevilAppearance&quot; &quot;WDB_Elf_FairyElements&quot; ## [15] &quot;WDB_Imprisonment&quot; &quot;WDB_LinkedTrial&quot; ## [17] &quot;WDB_Malice&quot; &quot;WDB_MentionedAsWitch&quot; ## [19] &quot;WDB_MovestoHLA&quot; &quot;WDB_MusicalInstrument&quot; ## [21] &quot;WDB_Ordeal&quot; &quot;WDB_OtherCharges&quot; ## [23] &quot;WDB_OtherNamedwitch&quot; &quot;WDB_Person&quot; ## [25] &quot;WDB_PrevCommission&quot; &quot;WDB_PropertyDamage&quot; ## [27] &quot;WDB_Ref_Parish&quot; &quot;WDB_Reference&quot; ## [29] &quot;WDB_ReligiousMotif&quot; &quot;WDB_RitualObject&quot; ## [31] &quot;WDB_ShapeChanging&quot; &quot;WDB_Source&quot; ## [33] &quot;WDB_Torture&quot; &quot;WDB_Trial&quot; ## [35] &quot;WDB_Trial_Person&quot; &quot;WDB_WeatherModification&quot; ## [37] &quot;WDB_WhiteMagic&quot; &quot;WDB_WitchesMeetingPlace&quot; mdb.get(db_loc, tables = &quot;WDB_Trial&quot;)[1:6,1:10] # get table of trials, print first 6 rows and 10 cols ## Trialref TrialId TrialSystemId CaseRef TrialType Trial.settlement ## 1 T/JO/1 1 JO C/EGD/2120 2 ## 2 T/JO/100 100 JO C/JO/2669 2 ## 3 T/JO/1000 1000 JO C/EGD/1474 2 ## 4 T/JO/1001 1001 JO C/EGD/1558 2 ## 5 T/JO/1002 1002 JO C/EGD/1681 2 ## 6 T/JO/1003 1003 JO C/EGD/1680 2 ## Trial.parish Trial.presbytery Trial.county Trial.burgh ## 1 Aberdeen Aberdeen Aberdeen ## 2 ## 3 ## 4 ## 5 ## 6 Many databases have multiple tables with keys that connect information in each table. We’ll spend more time on databases later in the semester - for now, it’s enough to be able to get data out of one. Unfortunately, it appears that SAS on Linux doesn’t allow you to read in Access files. So I can’t demonstrate that for you. But, since you know how to do it in R, worst case you can open up R and export all of the tables to separate CSV files, then read those into SAS. 😿 5.5.2 SQLite SQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. Let’s try working with a sqlite file that has only one table: if (!&quot;RSQLite&quot; %in% installed.packages()) install.packages(&quot;RSQLite&quot;) if (!&quot;DBI&quot; %in% installed.packages()) install.packages(&quot;DBI&quot;) library(RSQLite) library(DBI) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/ssa-babynames-for-2015.sqlite&quot;) dbListTables(con) # List all the tables ## [1] &quot;babynames&quot; babyname &lt;- dbReadTable(con, &quot;babynames&quot;) head(babyname, 10) # show the first 10 obs ## state year name sex count rank_within_sex per_100k_within_sex ## 1 AK 2015 Olivia F 56 1 2367.9 ## 2 AK 2015 Liam M 53 1 1590.6 ## 3 AK 2015 Emma F 49 2 2071.9 ## 4 AK 2015 Noah M 46 2 1380.6 ## 5 AK 2015 Aurora F 46 3 1945.0 ## 6 AK 2015 James M 45 3 1350.5 ## 7 AK 2015 Amelia F 39 4 1649.0 ## 8 AK 2015 Ava F 39 4 1649.0 ## 9 AK 2015 William M 44 4 1320.5 ## 10 AK 2015 Oliver M 41 5 1230.5 You can of course write formal queries using the DBI package, but for many databases, it’s easier to do the querying in R. We’ll cover both options later - the R version will be in the next module. In SAS, you can theoretically connect to SQLite databases, but there are very specific instructions for how to do that for each operating system. You’ll need to acquire the SQLite ODBC Driver for your operating system. You may also need to set up a DSN (Data Source Name) (Windows, Mac and Linux).18 Here is my .odbc.ini file as I’ve configured it for my Ubuntu 18.04 machine. A similar file should work for any Mac or Linux machine. In windows, you’ll need to use the ODBC Data Source Administrator to set this up. [babyname] Description = 2015 SSA baby names Driver = SQLite3 Database = data/ssa-babynames-for-2015.sqlite 6 /* This code requires that I&#39;ve set up a DSN connecting the 6 ! sqlite file to */ 7 /* a specific driver on my computer. You&#39;ll have to set up your 7 ! machine to */ 8 /* have a configuration that is appropriate for your setup */ 9 10 libname mydata odbc complete = 10 ! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX; NOTE: Libref MYDATA was successfully assigned as follows: Engine: ODBC Physical Name: babyname 11 12 proc print data=mydata.babynames (obs=10); ERROR: File MYDATA.babynames.DATA does not exist. 13 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 23,25. 5.6 Exploratory Data Analysis Once your data has been read in, we can do some basic exploratory data analysis. EDA is important because it helps us to know what challenges a particular data set might bring. Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence. While in many classes you’ll be given cleaner data, you do need to know how to clean your own data up so that you can use more interesting datasets for projects (and for fun!). The EDA chapter in R for Data Science is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. It is so good that I am not going to try to completely reproduce it here. Both R and SAS make it relatively easy to get summary statistics from a dataset, but the “flow” of EDA is somewhat different between the two programs, so this section will cover SAS first, and then R. Major components of EDA: - tables - summary statistics - basic plots - unique values 5.6.1 SAS Proc Freq generates frequency tables for variables or interactions of variables. This can help you to see whether there is missing information. Using those frequency tables, you can create frequency plots and set up chi squared tests. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 ODS GRAPHICS ON; 9 PROC FREQ DATA=classdat.poke ORDER=FORMATTED; 10 TABLES generation / CHISQ PLOTS=freqplot(type=dotplot); 11 RUN; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: PROCEDURE FREQ used (Total process time): real time 3.32 seconds cpu time 0.12 seconds 12 PROC FREQ DATA=classdat.poke ORDER=FREQ; 13 TABLES type_1 status / MAXLEVELS=10 13 ! PLOTS=freqplot(type=dotplot scale=percent); 14 RUN; NOTE: MAXLEVELS=10 is greater than or equal to the total number of levels, 4. The table of status displays all levels. NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: PROCEDURE FREQ used (Total process time): real time 0.47 seconds cpu time 0.12 seconds 15 ODS GRAPHICS OFF; ERROR: Errors printed on pages 23,25. generation Frequency Percent Cumulative Frequency Cumulative Percent 1 192 18.68 192 18.68 2 107 10.41 299 29.09 3 165 16.05 464 45.14 4 121 11.77 585 56.91 5 171 16.63 756 73.54 6 85 8.27 841 81.81 7 99 9.63 940 91.44 8 88 8.56 1028 100.00 Chi-Square Testfor Equal Proportions Chi-Square 94.1012 DF 7 Pr &gt; ChiSq &lt;.0001 Sample Size = 1028 type_1 Frequency Percent Cumulative Frequency Cumulative Percent Water 134 13.04 134 13.04 Normal 115 11.19 249 24.22 Grass 91 8.85 340 33.07 Bug 81 7.88 421 40.95 Psychic 76 7.39 497 48.35 Fire 65 6.32 562 54.67 Electric 61 5.93 623 60.60 Rock 60 5.84 683 66.44 Dark 44 4.28 727 70.72 Ghost 41 3.99 768 74.71 The first 10 levels are displayed. status Frequency Percent Cumulative Frequency Cumulative Percent Normal 915 89.01 915 89.01 Sub Legendary 45 4.38 960 93.39 Legendary 39 3.79 999 97.18 Mythical 29 2.82 1028 100.00 Proc Means can be used to get more useful summary statistics for numeric variables. Note that the Class statement identifies a categorical variable; the summary statistics are computed for each level of this variable. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC MEANS DATA = classdat.poke; 9 run; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: PROCEDURE MEANS used (Total process time): real time 0.03 seconds cpu time 0.03 seconds 10 11 proc means data = classdat.poke; 12 class status; 13 run; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: PROCEDURE MEANS used (Total process time): real time 0.06 seconds cpu time 0.07 seconds ERROR: Errors printed on pages 23,25. Variable N Mean Std Dev Minimum Maximum pokedex_number generation type_number height_m weight_kg abilities_number total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience egg_type_number percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 1028 1028 1028 1028 1027 1028 1028 1028 1028 1028 1028 1028 1028 924 924 924 1028 792 1027 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 1028 437.7110895 4.0340467 1.5272374 1.3680934 69.7537488 2.2840467 437.5719844 69.5778210 80.1196498 74.4756809 72.7324903 72.1322957 68.5340467 93.1720779 64.1396104 153.8149351 1.2714008 55.0031566 30.3164557 0.8684339 1.1254864 1.0535019 1.0342899 1.0041342 1.1964981 1.0787938 0.9523346 1.0846304 1.1663424 0.9793288 0.9924611 1.2397860 1.0107004 0.9756809 1.0656615 0.9803016 1.0848735 259.3664801 2.2349373 0.4995006 3.3801260 129.2212303 0.7949808 121.6649096 26.3858489 32.3723210 31.3033092 32.6776984 28.0836837 29.8021030 75.2406298 21.4554640 79.2706279 0.4514169 20.1826753 28.9429120 0.2862360 0.7177417 0.6134110 0.6451669 0.7485266 0.7594711 0.7549691 0.5429816 0.7849374 0.5930303 0.4991456 0.5983010 0.6991564 0.5585333 0.3775491 0.4510540 0.5034338 0.5277428 1.0000000 1.0000000 1.0000000 0.1000000 0.1000000 0 175.0000000 1.0000000 5.0000000 5.0000000 10.0000000 20.0000000 5.0000000 3.0000000 0 36.0000000 0 0 5.0000000 0 0 0 0 0 0 0 0 0 0.2500000 0 0 0.2500000 0 0 0.2500000 0 0 890.0000000 8.0000000 2.0000000 100.0000000 999.9000000 3.0000000 1125.00 255.0000000 190.0000000 250.0000000 194.0000000 250.0000000 180.0000000 255.0000000 140.0000000 608.0000000 2.0000000 100.0000000 120.0000000 1.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 2.0000000 4.0000000 4.0000000 4.0000000 status N Obs Variable N Mean Std Dev Minimum Maximum Legendary 39 pokedex_number generation type_number height_m weight_kg abilities_number total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience egg_type_number percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 39 39 39 39 38 39 39 39 39 39 39 39 39 33 33 33 39 0 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 607.5897436 5.1282051 1.7179487 6.8948718 381.3868421 1.1794872 679.5641026 111.4358974 125.7435897 108.2051282 122.5384615 110.6153846 101.0256410 19.6666667 0 296.0909091 1.0000000 . 120.0000000 0.8333333 0.8461538 0.8269231 0.8397436 0.7756410 1.4871795 0.8269231 0.7820513 1.0897436 0.9871795 0.8717949 0.9294872 1.1089744 1.3974359 1.2948718 1.3076923 0.9871795 1.3461538 233.4817380 2.2026239 0.4558808 15.6674814 278.4542528 0.4514185 127.1772064 37.7824764 34.7931617 32.4379403 39.1514241 32.9182523 24.7795408 45.6526743 0 60.2605611 0 . 0 0.3311331 0.4574507 0.3938418 0.5691845 0.4652209 1.0032000 0.5594695 0.4558808 0.6269369 0.2921603 0.4363115 0.5063697 0.6996119 0.6803587 0.6561245 0.7310355 0.4514185 0.5753014 150.0000000 1.0000000 1.0000000 0.1000000 0.1000000 0 200.0000000 43.0000000 29.0000000 31.0000000 29.0000000 31.0000000 37.0000000 3.0000000 0 40.0000000 1.0000000 . 120.0000000 0 0 0 0 0.2500000 0.5000000 0 0 0 0.5000000 0 0.2500000 0.2500000 0.5000000 0 0.5000000 0.5000000 0.5000000 890.0000000 8.0000000 2.0000000 100.0000000 999.9000000 2.0000000 1125.00 255.0000000 190.0000000 250.0000000 194.0000000 250.0000000 148.0000000 255.0000000 0 351.0000000 1.0000000 . 120.0000000 1.0000000 2.0000000 2.0000000 2.0000000 2.0000000 4.0000000 2.0000000 2.0000000 2.0000000 2.0000000 2.0000000 2.0000000 4.0000000 4.0000000 2.0000000 4.0000000 2.0000000 2.0000000 Mythical 29 pokedex_number generation type_number height_m weight_kg abilities_number total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience egg_type_number percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 29 29 29 29 29 29 29 29 29 29 29 29 29 27 27 27 29 0 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 29 573.6896552 4.7241379 1.5517241 1.2551724 86.8241379 1.0000000 594.4827586 82.8275862 108.1724138 93.2758621 113.7931034 95.1724138 101.2413793 10.2222222 53.7037037 272.3333333 1.0689655 . 104.1379310 0.8103448 1.1982759 0.9482759 1.1034483 1.0344828 0.9741379 0.9655172 0.8965517 1.2413793 1.1206897 0.8189655 1.2758621 0.9655172 1.2931034 0.8275862 1.2931034 1.0086207 1.0862069 180.2022878 1.6234124 0.5061202 1.2477369 173.2970626 0 69.4666522 23.2502979 28.8754332 29.3937512 30.1524452 30.2738324 33.7180062 15.6606644 45.6490078 18.7780559 0.2578807 . 34.0448487 0.3109258 0.7831350 0.4500958 0.4701692 0.5619010 0.7081945 0.5658596 0.5408213 0.5609413 0.5453566 0.5625855 0.9850327 0.4211174 0.8185052 0.3347670 0.7850983 0.8672688 0.5187376 151.0000000 1.0000000 1.0000000 0.2000000 1.1000000 1.0000000 300.0000000 46.0000000 65.0000000 20.0000000 55.0000000 20.0000000 34.0000000 3.0000000 0 216.0000000 1.0000000 . 10.0000000 0 0.2500000 0 0 0.2500000 0.2500000 0 0 0 0.5000000 0 0.2500000 0.5000000 0 0 0.5000000 0.2500000 0.5000000 809.0000000 7.0000000 2.0000000 6.5000000 800.0000000 1.0000000 720.0000000 135.0000000 180.0000000 160.0000000 180.0000000 160.0000000 180.0000000 45.0000000 100.0000000 324.0000000 2.0000000 . 120.0000000 1.0000000 4.0000000 2.0000000 2.0000000 2.0000000 4.0000000 2.0000000 2.0000000 2.0000000 2.0000000 2.0000000 4.0000000 2.0000000 4.0000000 1.0000000 4.0000000 4.0000000 2.0000000 Normal 915 pokedex_number generation type_number height_m weight_kg abilities_number total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience egg_type_number percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 915 915 915 915 915 915 915 915 915 915 915 915 915 819 819 819 915 780 914 915 915 915 915 915 915 915 915 915 915 915 915 915 915 915 915 915 915 419.2950820 3.9256831 1.5136612 1.1003279 51.7150820 2.4163934 415.2098361 66.5049180 76.1836066 71.3431694 67.7224044 68.5180328 64.9377049 103.2954823 67.8815629 138.4041514 1.3027322 54.7596154 20.9682713 0.8715847 1.1368852 1.0642077 1.0442623 1.0237705 1.1852459 1.0961749 0.9625683 1.0836066 1.1795082 0.9890710 0.9904372 1.2540984 0.9786885 0.9666667 1.0464481 0.9784153 1.0715847 258.7784896 2.2421627 0.5000867 1.0339266 86.0836478 0.7264570 104.5766798 23.8902654 30.1980544 29.6655361 28.8544505 25.3129310 28.0347832 73.3318186 12.4159527 69.0416340 0.4667772 19.5212658 6.2765825 0.2850893 0.7234866 0.6276282 0.6566295 0.7715152 0.7437106 0.7642960 0.5377240 0.7764160 0.5887608 0.4983734 0.5840034 0.7018046 0.5352298 0.3480696 0.4136629 0.4913210 0.5166126 1.0000000 1.0000000 1.0000000 0.1000000 0.1000000 1.0000000 175.0000000 1.0000000 5.0000000 5.0000000 10.0000000 20.0000000 5.0000000 3.0000000 0 36.0000000 0 0 5.0000000 0 0 0 0 0 0 0 0 0 0.2500000 0 0 0.2500000 0 0 0.2500000 0 0 887.0000000 8.0000000 2.0000000 14.5000000 942.9000000 3.0000000 700.0000000 255.0000000 185.0000000 230.0000000 175.0000000 230.0000000 160.0000000 255.0000000 140.0000000 608.0000000 2.0000000 100.0000000 40.0000000 1.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 2.0000000 2.0000000 2.0000000 4.0000000 4.0000000 Sub Legendary 45 pokedex_number generation type_number height_m weight_kg abilities_number total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience egg_type_number percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 12 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 577.3111111 4.8444444 1.6222222 2.0955556 162.3822222 1.3777778 581.4222222 87.2444444 102.5333333 96.8222222 104.9777778 97.4222222 92.4222222 12.6000000 49.3333333 258.8444444 1.0000000 70.8333333 94.8888889 0.8722222 1.0888889 1.1000000 0.9555556 0.7833333 1.3166667 1.0166667 0.9277778 1.0000000 1.0833333 0.9777778 0.9055556 1.2388889 1.1444444 0.9777778 1.1000000 0.9944444 1.1277778 215.8664173 1.9994949 0.4903101 1.5385878 226.9741698 0.4903101 39.8648221 25.1632448 27.2335021 34.2943733 28.8361270 31.4870023 25.6342180 17.3118457 43.8437308 29.0915136 0 45.0168319 34.3195030 0.2535107 0.7094243 0.5287206 0.5416958 0.4695259 0.8125437 0.7876894 0.6921274 1.1381804 0.8393721 0.5107432 0.6060511 0.7574545 0.5289593 0.5107432 0.4954337 0.4956885 0.6584724 144.0000000 1.0000000 1.0000000 0.3000000 0.1000000 1.0000000 420.0000000 53.0000000 50.0000000 37.0000000 50.0000000 31.0000000 13.0000000 3.0000000 0 107.0000000 1.0000000 0 10.0000000 0 0 0.5000000 0 0.2500000 0.2500000 0 0 0 0.2500000 0 0.2500000 0.2500000 0 0 0.5000000 0.2500000 0.2500000 806.0000000 7.0000000 2.0000000 9.2000000 999.9000000 2.0000000 700.0000000 223.0000000 181.0000000 211.0000000 173.0000000 200.0000000 151.0000000 45.0000000 140.0000000 315.0000000 1.0000000 100.0000000 120.0000000 1.0000000 4.0000000 2.0000000 2.0000000 2.0000000 4.0000000 4.0000000 4.0000000 4.0000000 4.0000000 2.0000000 2.0000000 4.0000000 2.0000000 2.0000000 2.0000000 2.0000000 4.0000000 For even higher levels of detail, Proc Univariate will provide variability, tests for location, quantiles, skewness, and will identify the extreme observations for you. You can also get histograms for variables, even specifying distributions you’d like to be fit to the data (if that’s something you want). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 ODS GRAPHICS ON; 9 PROC UNIVARIATE DATA = classdat.poke; 10 VAR attack defense sp_attack sp_defense speed; 11 HISTOGRAM attack defense sp_attack sp_defense speed; 12 RUN; NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 0.77 seconds cpu time 0.38 seconds 13 ODS GRAPHICS OFF; ERROR: Errors printed on pages 23,25. Variable: attack Moments N 1028 Sum Weights 1028 Mean 80.1196498 Sum Observations 82363 Std Deviation 32.372321 Variance 1047.96717 Skewness 0.4954672 Kurtosis 0.04128599 Uncorrected SS 7675157 Corrected SS 1076262.28 Coeff Variation 40.4049707 Std Error Mean 1.00966495 Basic Statistical Measures Location Variability Mean 80.1196 Std Deviation 32.37232 Median 76.0000 Variance 1048 Mode 100.0000 Range 185.00000 Interquartile Range 45.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 79.35271 Pr &gt; |t| &lt;.0001 Sign M 514 Pr &gt;= |M| &lt;.0001 Signed Rank S 264453 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 190 99% 165 95% 137 90% 125 75% Q3 100 50% Median 76 25% Q1 55 10% 40 5% 30 1% 20 0% Min 5 Extreme Observations Lowest Highest Value Obs Value Obs 5 521 180 459 5 146 180 462 10 289 181 926 10 257 185 259 10 165 190 190 Variable: defense Moments N 1028 Sum Weights 1028 Mean 74.4756809 Sum Observations 76561 Std Deviation 31.3033092 Variance 979.897168 Skewness 1.18859927 Kurtosis 3.02873186 Uncorrected SS 6708287 Corrected SS 1006354.39 Coeff Variation 42.0315851 Std Error Mean 0.97632339 Basic Statistical Measures Location Variability Mean 74.47568 Std Deviation 31.30331 Median 70.00000 Variance 979.89717 Mode 70.00000 Range 245.00000 Interquartile Range 40.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 76.28177 Pr &gt; |t| &lt;.0001 Sign M 514 Pr &gt;= |M| &lt;.0001 Signed Rank S 264453 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 250 99% 180 95% 130 90% 115 75% Q3 90 50% Median 70 25% Q1 50 10% 40 5% 35 1% 20 0% Min 5 Extreme Observations Lowest Highest Value Obs Value Obs 5 521 211 936 5 146 230 251 10 289 230 257 15 285 230 363 15 215 250 1028 Variable: sp_attack Moments N 1028 Sum Weights 1028 Mean 72.7324903 Sum Observations 74769 Std Deviation 32.6776984 Variance 1067.83197 Skewness 0.73105741 Kurtosis 0.23641395 Uncorrected SS 6534799 Corrected SS 1096663.43 Coeff Variation 44.928612 Std Error Mean 1.01918941 Basic Statistical Measures Location Variability Mean 72.73249 Std Deviation 32.67770 Median 65.00000 Variance 1068 Mode 40.00000 Range 184.00000 Interquartile Range 45.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 71.36307 Pr &gt; |t| &lt;.0001 Sign M 514 Pr &gt;= |M| &lt;.0001 Signed Rank S 264453 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 194 99% 165 95% 135 90% 120 75% Q3 95 50% Median 65 25% Q1 50 10% 35 5% 30 1% 20 0% Min 10 Extreme Observations Lowest Highest Value Obs Value Obs 10 519 175 85 10 411 180 455 10 257 180 459 10 38 180 462 15 649 194 191 Variable: sp_defense Moments N 1028 Sum Weights 1028 Mean 72.1322957 Sum Observations 74152 Std Deviation 28.0836837 Variance 788.693289 Skewness 0.95486787 Kurtosis 2.48388443 Uncorrected SS 6158742 Corrected SS 809988.008 Coeff Variation 38.9335781 Std Error Mean 0.87590603 Basic Statistical Measures Location Variability Mean 72.13230 Std Deviation 28.08368 Median 70.00000 Variance 788.69329 Mode 50.00000 Range 230.00000 Interquartile Range 40.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 82.35164 Pr &gt; |t| &lt;.0001 Sign M 514 Pr &gt;= |M| &lt;.0001 Signed Rank S 264453 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 250 99% 150 95% 120 90% 107 75% Q3 90 50% Median 70 25% Q1 50 10% 40 5% 33 1% 25 0% Min 20 Extreme Observations Lowest Highest Value Obs Value Obs 20 1006 160 455 20 462 160 463 20 377 200 448 20 215 230 257 20 165 250 1028 Variable: speed Moments N 1028 Sum Weights 1028 Mean 68.5340467 Sum Observations 70453 Std Deviation 29.802103 Variance 888.165344 Skewness 0.38189525 Kurtosis -0.2879875 Uncorrected SS 5740575 Corrected SS 912145.808 Coeff Variation 43.4851062 Std Error Mean 0.92950205 Basic Statistical Measures Location Variability Mean 68.53405 Std Deviation 29.80210 Median 65.00000 Variance 888.16534 Mode 50.00000 Range 175.00000 Interquartile Range 45.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 73.732 Pr &gt; |t| &lt;.0001 Sign M 514 Pr &gt;= |M| &lt;.0001 Signed Rank S 264453 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 180 99% 145 95% 120 90% 109 75% Q3 90 50% Median 65 25% Q1 45 10% 30 5% 25 1% 15 0% Min 5 Extreme Observations Lowest Highest Value Obs Value Obs 5 898 150 461 5 528 150 462 5 257 151 923 10 960 160 345 10 696 180 464 Proc Corr allows you to examine the relationship between two quantitative variables. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 ODS GRAPHICS ON; 9 PROC CORR DATA = classdat.poke PLOTS( 9 ! MAXPOINTS=200000)=MATRIX(HISTOGRAM); 10 VAR attack defense sp_attack sp_defense speed ; 11 RUN; NOTE: PROCEDURE CORR used (Total process time): real time 1.07 seconds cpu time 0.14 seconds 12 ODS GRAPHICS OFF; ERROR: Errors printed on pages 23,25. 5 Variables: attack defense sp_attack sp_defense speed Simple Statistics Variable N Mean Std Dev Sum Minimum Maximum attack 1028 80.11965 32.37232 82363 5.00000 190.00000 defense 1028 74.47568 31.30331 76561 5.00000 250.00000 sp_attack 1028 72.73249 32.67770 74769 10.00000 194.00000 sp_defense 1028 72.13230 28.08368 74152 20.00000 250.00000 speed 1028 68.53405 29.80210 70453 5.00000 180.00000 Pearson Correlation Coefficients, N = 1028 Prob &gt; |r| under H0: Rho=0 attack defense sp_attack sp_defense speed attack 1.00000 0.45077 &lt;.0001 0.37621 &lt;.0001 0.26426 &lt;.0001 0.38104 &lt;.0001 defense 0.45077 &lt;.0001 1.00000 0.22606 &lt;.0001 0.54251 &lt;.0001 0.00934 0.7649 sp_attack 0.37621 &lt;.0001 0.22606 &lt;.0001 1.00000 0.51154 &lt;.0001 0.44297 &lt;.0001 sp_defense 0.26426 &lt;.0001 0.54251 &lt;.0001 0.51154 &lt;.0001 1.00000 0.23366 &lt;.0001 speed 0.38104 &lt;.0001 0.00934 0.7649 0.44297 &lt;.0001 0.23366 &lt;.0001 1.00000 The plot here is called a scatterplot matrix. It contains histograms on the diagonal, and pairwise scatterplots on off-diagonals. It can be useful for spotting strong correlations among multiple variables which may affect the way you build a model. Try it out One of the datasets we read in above records incidents of police violence around the country. Explore the variables present in this dataset (see code in the spreadsheets section to read it in). Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis. It is useful to memorize the SAS PROC options you use most frequently, but it’s also a good idea to reference the SAS documentation - it provides a list of all viable options for each procedure, and generally has decent examples to show how those options are used. Solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 ODS GRAPHICS ON; 9 PROC CONTENTS DATA = classdat.police; /* see what&#39;s in the 9 ! dataset */ 10 RUN; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 11 12 PROC FREQ DATA = classdat.police ORDER=FREQ; /* Examine Freq of 12 ! common vars */ 13 TABLES Victim_s_gender Victim_s_race State Cause_of_death 14 Unarmed Geography__via_Trulia_methodolog / MAXLEVELS = 14 ! 10; 15 RUN; NOTE: MAXLEVELS=10 is greater than or equal to the total number of levels, 4. The table of Victim_s_gender displays all levels. NOTE: MAXLEVELS=10 is greater than or equal to the total number of levels, 8. The table of Victim_s_race displays all levels. NOTE: MAXLEVELS=10 is greater than or equal to the total number of levels, 4. The table of Unarmed displays all levels. NOTE: MAXLEVELS=10 is greater than or equal to the total number of levels, 4. The table of Geography__via_Trulia_methodolog displays all levels. NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: PROCEDURE FREQ used (Total process time): real time 0.03 seconds cpu time 0.04 seconds 16 17 PROC FREQ DATA = classdat.police ORDER=FREQ; /* Combinations of 17 ! vars */ 18 TABLES Unarmed * Criminal_Charges_ / NOCUM NOPERCENT NOCOL NOROW 18 ! MAXLEVELS=10; 19 RUN; NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: PROCEDURE FREQ used (Total process time): real time 0.16 seconds cpu time 0.15 seconds 20 21 PROC MEANS DATA = classdat.police; /* Numeric variable 21 ! exploration */ 22 VAR num_age; /* Only numeric variable in this set */ 23 RUN; NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: PROCEDURE MEANS used (Total process time): real time 0.02 seconds cpu time 0.02 seconds 24 25 PROC UNIVARIATE DATA = classdat.police; /* Investigating 25 ! age/date info */ 26 HISTOGRAM num_age date; 27 RUN; NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 0.39 seconds cpu time 0.21 seconds 28 ODS GRAPHICS OFF; ERROR: Errors printed on pages 23,25. Data Set Name CLASSDAT.POLICE Observations 7663 Member Type DATA Variables 25 Engine V9 Indexes 0 Created 05/09/2021 10:42:50 Observation Length 896 Last Modified 05/09/2021 10:42:50 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding utf-8 Unicode (UTF-8) Engine/Host Dependent Information Data Set Page Size 73728 Number of Data Set Pages 94 First Data Page 1 Max Obs per Page 82 Obs in First Data Page 75 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/2020-stat850/sas/police.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 40408714 Access Permission rw-rw-r– Owner Name susan File Size 7MB File Size (bytes) 7004160 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat Label 10 Agency_responsible_for_death Char 177 $177. $177. Agency responsible for death 17 Alleged_Threat_Level__Source__Wa Char 12 $12. $12. Alleged Threat Level (Source: WaPo) 11 Cause_of_death Char 39 $39. $39. Cause of death 6 City Char 29 $29. $29. City 9 County Char 80 $80. $80. County 13 Criminal_Charges_ Char 77 $77. $77. Criminal Charges? 22 Geography__via_Trulia_methodolog Char 8 $8. $8. Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) 23 ID Num 8 BEST. ID 21 Off_Duty_Killing_ Char 8 $8. $8. Off-Duty Killing? 12 Official_disposition_of_death__j Char 176 $176. $176. Official disposition of death (justified or other) 7 State Char 2 $2. $2. State 5 Street_Address_of_Incident Char 73 $73. $73. Street Address of Incident 14 Symptoms_of_mental_illness_ Char 19 $19. $19. Symptoms of mental illness? 15 Unarmed Char 15 $15. $15. Unarmed 16 VAR20 Char 32 $32. $32. Alleged Weapon (Source: WaPo) 18 VAR22 Char 11 $11. $11. Fleeing (Source: WaPo) 19 VAR23 Char 18 $18. $18. Body Camera (Source: WaPo) 2 Victim_s_age Char 7 $7. $7. Victim's age 3 Victim_s_gender Char 11 $11. $11. Victim's gender 1 Victim_s_name Char 49 $49. $49. Victim's name 4 Victim_s_race Char 16 $16. $16. Victim's race 20 WaPo_ID__If_included_in_WaPo_dat Num 8 BEST. WaPo ID (If included in WaPo database) 8 Zipcode Char 5 $5. $5. Zipcode 24 date Num 8 MMDDYY10. 25 num_age Num 8 Victim's gender Victim_s_gender Frequency Percent Cumulative Frequency Cumulative Percent Male 7253 94.75 7253 94.75 Female 391 5.11 7644 99.86 Transgender 7 0.09 7651 99.95 Unknown 4 0.05 7655 100.00 Frequency Missing = 8 Victim's race Victim_s_race Frequency Percent Cumulative Frequency Cumulative Percent White 3378 44.08 3378 44.08 Black 1944 25.37 5322 69.45 Hispanic 1335 17.42 6657 86.87 Unknown race 670 8.74 7327 95.62 Asian 118 1.54 7445 97.16 Native American 112 1.46 7557 98.62 Unknown Race 64 0.84 7621 99.45 Pacific Islander 42 0.55 7663 100.00 State State Frequency Percent Cumulative Frequency Cumulative Percent CA 1186 15.48 1186 15.48 TX 719 9.38 1905 24.86 FL 540 7.05 2445 31.91 AZ 343 4.48 2788 36.38 GA 265 3.46 3053 39.84 CO 227 2.96 3280 42.80 WA 218 2.84 3498 45.65 OH 215 2.81 3713 48.45 OK 214 2.79 3927 51.25 NC 204 2.66 4131 53.91 The first 10 levels are displayed. Cause of death Cause_of_death Frequency Percent Cumulative Frequency Cumulative Percent Gunshot 7059 92.12 7059 92.12 Taser 246 3.21 7305 95.33 Gunshot, Taser 223 2.91 7528 98.24 Vehicle 33 0.43 7561 98.67 Beaten 30 0.39 7591 99.06 Asphyxiated 14 0.18 7605 99.24 Physical Restraint 11 0.14 7616 99.39 Physical restraint 9 0.12 7625 99.50 Gunshot, Police Dog 5 0.07 7630 99.57 Other 5 0.07 7635 99.63 The first 10 levels are displayed. Unarmed Unarmed Frequency Percent Cumulative Frequency Cumulative Percent Allegedly Armed 5428 70.83 5428 70.83 Unarmed 1073 14.00 6501 84.84 Unclear 649 8.47 7150 93.31 Vehicle 513 6.69 7663 100.00 Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) Geography__via_Trulia_methodolog Frequency Percent Cumulative Frequency Cumulative Percent Suburban 3805 49.65 3805 49.65 Urban 2088 27.25 5893 76.90 Rural 1703 22.22 7596 99.13 #N/A 67 0.87 7663 100.00 Frequency Table of Unarmed by Criminal_Charges_ Unarmed(Unarmed) Criminal_Charges_(Criminal Charges?) No known charges Charged witha crime No Charged, Acquitted Charged, Mistrial Charged, Convicted Charged, ChargesTossed Charged, Convicted,Sentenced to30 years in prison Charged, Convicted,Sentenced to5 years probation. Charged, Convicted,Sentenced tolife in prison Charged withmanslaughter Charged, ChargesDropped Charged, Convicted,Sentenced to1 year in jail,3 years suspended Charged, Convicted,Sentenced to1 year in prison Charged, Convicted,Sentenced to16 years in prison Charged, Convicted,Sentenced to18 months Charged, Convicted,Sentenced to2.5 years inprison Charged, Convicted,Sentenced to20 years in prison Charged, Convicted,Sentenced to3 months in jail Charged, Convicted,Sentenced to3 years probation Charged, Convicted,Sentenced to4 years Charged, Convicted,Sentenced to40 years in prison Charged, Convicted,Sentenced to5 years in prison Charged, Convicted,Sentenced to50 years Charged, Convicted,Sentenced to6 years Charged, Convicted,Sentenced toLife in Prison Charged, Convicted,Sentenced tolife in prisonwithout parole,plus 16 years Charged, Mistrial,Plead Guiltyto Civil RightsCharges NO Total Allegedly Armed 5385 6 26 6 3 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 5428 Unarmed 998 35 4 5 2 4 2 2 2 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1073 Unclear 640 4 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 649 Vehicle 501 4 6 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 513 Total 7524 49 37 17 5 4 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 7663 Analysis Variable : num_age N Mean Std Dev Minimum Maximum 7457 36.7964329 13.2086517 1.0000000 107.0000000 Variable: WaPo_ID__If_included_in_WaPo_dat (WaPo ID (If included in WaPo database)) Moments N 4878 Sum Weights 4878 Mean 2723.53465 Sum Observations 13285402 Std Deviation 1534.3303 Variance 2354169.46 Skewness 0.00194849 Kurtosis -1.1959645 Uncorrected SS 4.76645E10 Corrected SS 1.14813E10 Coeff Variation 56.3359934 Std Error Mean 21.9683765 Basic Statistical Measures Location Variability Mean 2723.535 Std Deviation 1534 Median 2722.000 Variance 2354169 Mode 3232.000 Range 5436 Interquartile Range 2649 Note: The mode displayed is the smallest of 2 modes with a count of 2. Tests for Location: Mu0=0 Test Statistic p Value Student's t t 123.9752 Pr &gt; |t| &lt;.0001 Sign M 2439 Pr &gt;= |M| &lt;.0001 Signed Rank S 5949941 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 5439 99% 5320 95% 5112 90% 4847 75% Q3 4051 50% Median 2722 25% Q1 1402 10% 614 5% 336 1% 90 0% Min 3 Extreme Observations Lowest Highest Value Obs Value Obs 3 5505 5422 681 4 5506 5423 680 5 5504 5437 682 8 5501 5438 677 9 5503 5439 675 Missing Values MissingValue Count Percent Of All Obs Missing Obs . 2785 36.34 100.00 Variable: ID (ID) Moments N 7663 Sum Weights 7663 Mean 3832.89012 Sum Observations 29371437 Std Deviation 2213.3393 Variance 4898870.85 Skewness 0.0007161 Kurtosis -1.199928 Uncorrected SS 1.50113E11 Corrected SS 3.75351E10 Coeff Variation 57.7459626 Std Error Mean 25.284163 Basic Statistical Measures Location Variability Mean 3832.890 Std Deviation 2213 Median 3832.000 Variance 4898871 Mode . Range 7666 Interquartile Range 3834 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 151.5925 Pr &gt; |t| &lt;.0001 Sign M 3831.5 Pr &gt;= |M| &lt;.0001 Signed Rank S 14682308 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 7667 99% 7591 95% 7283 90% 6900 75% Q3 5750 50% Median 3832 25% Q1 1916 10% 767 5% 384 1% 77 0% Min 1 Extreme Observations Lowest Highest Value Obs Value Obs 1 7659 7663 6 2 7660 7664 1 3 7658 7665 2 4 7661 7666 7 5 7662 7667 5 Variable: date Moments N 7663 Sum Weights 7663 Mean 20641.9815 Sum Observations 158179504 Std Deviation 739.237367 Variance 546471.884 Skewness -0.0178679 Kurtosis -1.1972873 Uncorrected SS 3.26933E12 Corrected SS 4187067577 Coeff Variation 3.58123259 Std Error Mean 8.4447053 Basic Statistical Measures Location Variability Mean 20641.98 Std Deviation 739.23737 Median 20642.00 Variance 546472 Mode 19525.00 Range 2555 Interquartile Range 1275 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 2444.37 Pr &gt; |t| &lt;.0001 Sign M 3831.5 Pr &gt;= |M| &lt;.0001 Signed Rank S 14682308 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 21914 99% 21893 95% 21790 90% 21659 75% Q3 21278 50% Median 20642 25% Q1 20003 10% 19601 5% 19488 1% 19379 0% Min 19359 Extreme Observations Lowest Highest Value Obs Value Obs 19359 7663 21914 3 19359 7662 21914 4 19359 7661 21914 5 19359 7660 21914 6 19359 7659 21914 7 Variable: num_age Moments N 7457 Sum Weights 7457 Mean 36.7964329 Sum Observations 274391 Std Deviation 13.2086517 Variance 174.46848 Skewness 0.7439124 Kurtosis 0.3391187 Uncorrected SS 11397447 Corrected SS 1300836.99 Coeff Variation 35.8965548 Std Error Mean 0.15295949 Basic Statistical Measures Location Variability Mean 36.79643 Std Deviation 13.20865 Median 34.00000 Variance 174.46848 Mode 25.00000 Range 106.00000 Interquartile Range 18.00000 Tests for Location: Mu0=0 Test Statistic p Value Student's t t 240.5633 Pr &gt; |t| &lt;.0001 Sign M 3728.5 Pr &gt;= |M| &lt;.0001 Signed Rank S 13903577 Pr &gt;= |S| &lt;.0001 Quantiles (Definition 5) Level Quantile 100% Max 107 99% 73 95% 61 90% 55 75% Q3 45 50% Median 34 25% Q1 27 10% 22 5% 19 1% 16 0% Min 1 Extreme Observations Lowest Highest Value Obs Value Obs 1 7371 89 7525 1 4751 91 3109 1 1896 93 6214 5 7584 95 5969 5 6516 107 6865 Missing Values MissingValue Count Percent Of All Obs Missing Obs . 206 2.69 100.00 Oddities to note: - Gender - Unknown should be recoded as missing (’ ’) - Victim_s_race - Unknown race and Unknown Race should be recoded as missing - State - might need to check to make sure all states are valid (but top 10 are, at least) - Cause of death - sometimes, there are multiple causes. Also, varying capitalizations… - Geography - #N/A should be recoded as missing - Criminal_Charges_ - What does No/NO mean? (would need to look up in the codebook) - Age - the maximum age recorded is 107, which bears some investigation… other extreme observations between 89 and 95 are also fairly interesting and could be investigated further. There are also several infants/young children included, which is horribly sad, but believable. - Date - PROC UNIVARIATE doesn’t display date results with a meaningful format, even though format is specified. - Conclusions (ok, probably obvious before this analysis): - It’s much more likely for charges to be filed if the suspect was unarmed (but still very rare) - Data is relatively evenly distributed between 2013 and 2019. - It’s fairly rare for police to kill female or transgender individuals - around 5% of all victims - California, Texas, and Florida, while populous, seem to have a disproportionate number of killings, especially compared to e.g. NY, which is also a high population state. To really make the state numbers meaningful, though, we’d need to know population counts. There’s also an issue of accurate comparisons - some states may not report police killings with the same standards as other states. 5.6.2 R In SAS, EDA is fairly straightforward - you use specific procedures for each data type, and the plots which may be most useful come along with those procedures. It’s something like ordering off of a menu of pre-defined meals, and then slightly customizing your order. In R, you put your whole order together from the a la carte menu. That is, R will give you all of the same summary information (and possibly more), but you have to assemble a series of commands to get each portion. This can be more efficient (since you don’t have to wade through pages of output to get the piece you want) but may take a bit more coding as well. In this section, I will mostly be using the plot commands that come with base R and require no extra packages. The R for Data Science book shows plot commands which use the ggplot2 library. We will learn this library later in this class - it produces beautiful plots - and if you want to use it at this point, you may. It requires a bit more thought as to how to specify the plot, though, which may not be desireable. The first, and most basic EDA command in R is summary(). For numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all “other” category. library(readr) url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; poke &lt;- read_csv(url) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## name = col_character(), ## german_name = col_character(), ## japanese_name = col_character(), ## status = col_character(), ## species = col_character(), ## type_1 = col_character(), ## type_2 = col_character(), ## ability_1 = col_character(), ## ability_2 = col_character(), ## ability_hidden = col_character(), ## growth_rate = col_character(), ## egg_type_1 = col_character(), ## egg_type_2 = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. # Make types into factors to demonstrate the difference poke$type_1 &lt;- factor(poke$type_1) poke$type_2 &lt;- factor(poke$type_2) summary(poke) ## X1 pokedex_number name german_name ## Min. : 0.0 Min. : 1.0 Length:1028 Length:1028 ## 1st Qu.: 256.8 1st Qu.:213.8 Class :character Class :character ## Median : 513.5 Median :433.5 Mode :character Mode :character ## Mean : 513.5 Mean :437.7 ## 3rd Qu.: 770.2 3rd Qu.:663.2 ## Max. :1027.0 Max. :890.0 ## ## japanese_name generation status species ## Length:1028 Min. :1.000 Length:1028 Length:1028 ## Class :character 1st Qu.:2.000 Class :character Class :character ## Mode :character Median :4.000 Mode :character Mode :character ## Mean :4.034 ## 3rd Qu.:6.000 ## Max. :8.000 ## ## type_number type_1 type_2 height_m weight_kg ## Min. :1.000 Water :134 Flying :109 Min. : 0.100 Min. : 0.10 ## 1st Qu.:1.000 Normal :115 Fairy : 41 1st Qu.: 0.600 1st Qu.: 8.80 ## Median :2.000 Grass : 91 Ground : 39 Median : 1.000 Median : 28.50 ## Mean :1.527 Bug : 81 Poison : 38 Mean : 1.368 Mean : 69.75 ## 3rd Qu.:2.000 Psychic: 76 Psychic: 38 3rd Qu.: 1.500 3rd Qu.: 69.10 ## Max. :2.000 Fire : 65 (Other):277 Max. :100.000 Max. :999.90 ## (Other):466 NA&#39;s :486 NA&#39;s :1 ## abilities_number ability_1 ability_2 ability_hidden ## Min. :0.000 Length:1028 Length:1028 Length:1028 ## 1st Qu.:2.000 Class :character Class :character Class :character ## Median :2.000 Mode :character Mode :character Mode :character ## Mean :2.284 ## 3rd Qu.:3.000 ## Max. :3.000 ## ## total_points hp attack defense ## Min. : 175.0 Min. : 1.00 Min. : 5.00 Min. : 5.00 ## 1st Qu.: 330.0 1st Qu.: 50.00 1st Qu.: 55.00 1st Qu.: 50.00 ## Median : 455.0 Median : 66.50 Median : 76.00 Median : 70.00 ## Mean : 437.6 Mean : 69.58 Mean : 80.12 Mean : 74.48 ## 3rd Qu.: 510.0 3rd Qu.: 80.00 3rd Qu.:100.00 3rd Qu.: 90.00 ## Max. :1125.0 Max. :255.00 Max. :190.00 Max. :250.00 ## ## sp_attack sp_defense speed catch_rate ## Min. : 10.00 Min. : 20.00 Min. : 5.00 Min. : 3.00 ## 1st Qu.: 50.00 1st Qu.: 50.00 1st Qu.: 45.00 1st Qu.: 45.00 ## Median : 65.00 Median : 70.00 Median : 65.00 Median : 60.00 ## Mean : 72.73 Mean : 72.13 Mean : 68.53 Mean : 93.17 ## 3rd Qu.: 95.00 3rd Qu.: 90.00 3rd Qu.: 90.00 3rd Qu.:127.00 ## Max. :194.00 Max. :250.00 Max. :180.00 Max. :255.00 ## NA&#39;s :104 ## base_friendship base_experience growth_rate egg_type_number ## Min. : 0.00 Min. : 36.0 Length:1028 Min. :0.000 ## 1st Qu.: 70.00 1st Qu.: 67.0 Class :character 1st Qu.:1.000 ## Median : 70.00 Median :159.0 Mode :character Median :1.000 ## Mean : 64.14 Mean :153.8 Mean :1.271 ## 3rd Qu.: 70.00 3rd Qu.:201.5 3rd Qu.:2.000 ## Max. :140.00 Max. :608.0 Max. :2.000 ## NA&#39;s :104 NA&#39;s :104 ## egg_type_1 egg_type_2 percentage_male egg_cycles ## Length:1028 Length:1028 Min. : 0 Min. : 5.00 ## Class :character Class :character 1st Qu.: 50 1st Qu.: 20.00 ## Mode :character Mode :character Median : 50 Median : 20.00 ## Mean : 55 Mean : 30.32 ## 3rd Qu.: 50 3rd Qu.: 25.00 ## Max. :100 Max. :120.00 ## NA&#39;s :236 NA&#39;s :1 ## against_normal against_fire against_water against_electric ## Min. :0.0000 Min. :0.000 Min. :0.000 Min. :0.000 ## 1st Qu.:1.0000 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.500 ## Median :1.0000 Median :1.000 Median :1.000 Median :1.000 ## Mean :0.8684 Mean :1.125 Mean :1.054 Mean :1.034 ## 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:1.000 3rd Qu.:1.000 ## Max. :1.0000 Max. :4.000 Max. :4.000 Max. :4.000 ## ## against_grass against_ice against_fight against_poison ## Min. :0.000 Min. :0.000 Min. :0.000 Min. :0.0000 ## 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.5000 ## Median :1.000 Median :1.000 Median :1.000 Median :1.0000 ## Mean :1.004 Mean :1.196 Mean :1.079 Mean :0.9523 ## 3rd Qu.:1.000 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:1.0000 ## Max. :4.000 Max. :4.000 Max. :4.000 Max. :4.0000 ## ## against_ground against_flying against_psychic against_bug ## Min. :0.000 Min. :0.250 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.500 1st Qu.:1.000 1st Qu.:1.0000 1st Qu.:0.5000 ## Median :1.000 Median :1.000 Median :1.0000 Median :1.0000 ## Mean :1.085 Mean :1.166 Mean :0.9793 Mean :0.9925 ## 3rd Qu.:1.625 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :4.000 Max. :4.000 Max. :4.0000 Max. :4.0000 ## ## against_rock against_ghost against_dragon against_dark ## Min. :0.25 Min. :0.000 Min. :0.0000 Min. :0.250 ## 1st Qu.:1.00 1st Qu.:1.000 1st Qu.:1.0000 1st Qu.:1.000 ## Median :1.00 Median :1.000 Median :1.0000 Median :1.000 ## Mean :1.24 Mean :1.011 Mean :0.9757 Mean :1.066 ## 3rd Qu.:2.00 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:1.000 ## Max. :4.00 Max. :4.000 Max. :2.0000 Max. :4.000 ## ## against_steel against_fairy ## Min. :0.0000 Min. :0.000 ## 1st Qu.:0.5000 1st Qu.:1.000 ## Median :1.0000 Median :1.000 ## Mean :0.9803 Mean :1.085 ## 3rd Qu.:1.0000 3rd Qu.:1.000 ## Max. :4.0000 Max. :4.000 ## One common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis. There is one pokemon who appears to not have a weight specified. Let’s investigate further: poke[is.na(poke$weight_kg),] # Show any rows where weight.kg is NA ## # A tibble: 1 x 51 ## X1 pokedex_number name german_name japanese_name generation status species ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1027 890 Eter… &lt;NA&gt; &lt;NA&gt; 8 Legen… Gigant… ## # … with 43 more variables: type_number &lt;dbl&gt;, type_1 &lt;fct&gt;, type_2 &lt;fct&gt;, ## # height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ## # ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, ## # attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, ## # speed &lt;dbl&gt;, catch_rate &lt;dbl&gt;, base_friendship &lt;dbl&gt;, ## # base_experience &lt;dbl&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, ## # egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;dbl&gt;, ## # egg_cycles &lt;dbl&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, ## # against_water &lt;dbl&gt;, against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, ## # against_ice &lt;dbl&gt;, against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, ## # against_ground &lt;dbl&gt;, against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, ## # against_bug &lt;dbl&gt;, against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, ## # against_dragon &lt;dbl&gt;, against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, ## # against_fairy &lt;dbl&gt; This is the last row of our data frame, and this pokemon appears to have many missing values. We are often also interested in the distribution of values. We can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). table(poke$generation) ## ## 1 2 3 4 5 6 7 8 ## 192 107 165 121 171 85 99 88 plot(table(poke$generation)) # bar plot table(poke$type_1, poke$type_2) ## ## Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass ## Bug 0 0 0 4 2 4 2 14 1 6 ## Dark 0 0 4 0 3 2 3 5 2 0 ## Dragon 0 0 0 1 1 2 1 6 3 0 ## Electric 0 2 2 0 2 0 1 6 1 1 ## Fairy 0 0 0 0 0 0 0 2 0 0 ## Fighting 0 1 0 0 0 0 0 1 1 0 ## Fire 2 1 2 0 0 7 0 7 2 0 ## Flying 0 0 2 0 0 0 0 0 0 0 ## Ghost 0 1 2 0 1 0 3 3 0 11 ## Grass 0 3 5 0 5 3 0 7 1 0 ## Ground 0 3 2 1 0 0 1 4 4 0 ## Ice 2 0 0 0 1 0 1 2 1 0 ## Normal 0 0 1 0 5 4 0 27 0 2 ## Poison 1 5 4 0 1 2 2 3 0 0 ## Psychic 0 1 1 0 9 3 1 7 3 1 ## Rock 2 2 2 3 3 1 2 6 0 2 ## Steel 0 0 2 0 4 1 0 2 4 0 ## Water 2 7 3 2 4 3 0 7 2 3 ## ## Ground Ice Normal Poison Psychic Rock Steel Water ## Bug 2 0 0 12 2 3 7 3 ## Dark 0 2 5 0 2 0 2 0 ## Dragon 7 3 0 0 4 0 0 0 ## Electric 0 2 2 3 1 0 4 1 ## Fairy 0 0 0 0 0 0 1 0 ## Fighting 0 1 0 0 3 0 3 0 ## Fire 3 0 2 0 2 1 1 1 ## Flying 0 0 0 0 0 0 1 1 ## Ghost 2 0 0 4 0 0 0 0 ## Grass 1 3 0 15 2 0 3 0 ## Ground 0 0 0 0 2 3 4 0 ## Ice 3 0 0 0 2 0 2 3 ## Normal 1 0 0 0 3 0 0 1 ## Poison 2 0 0 0 0 0 0 3 ## Psychic 0 2 2 0 0 0 2 0 ## Rock 6 2 0 1 2 0 4 6 ## Steel 2 0 0 0 7 3 0 0 ## Water 10 4 0 3 6 5 1 0 plot(table(poke$type_1, poke$type_2)) # mosaic plot - hard to read b/c too many categories There are better options for examining this data, but they are easier to get in ggplot2. library(ggplot2) # define the x and y axis variables first ggplot(data = poke, aes(x = type_1, y = type_2)) + # define what will be plotted (points) # and what aesthetics will be used (size, color) # and how those aesthetics will be mapped to values # (proportional to the count in a 2d bin) geom_point(aes(size = ..count.., color = ..count..), stat = &quot;bin2d&quot;) We can also generate histograms or bar charts19 By default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desireable result. If we do not specify breaks, R will pick them for us. hist(poke$generation) # This isn&#39;t really optimal... we only have whole numbers. hist(poke$generation, breaks = 0:8) # Much better. For continuous variables, we can use histograms, or we can examine kernel density plots. Remember that %&gt;% is the “pipe” and takes the left side of the pipe to pass as an argument to the right side. This makes code easier to read because it becomes a step-wise “recipe.” library(magrittr) # This provides the pipe command, %&gt;% hist(poke$weight_kg) poke$weight_kg %&gt;% log10() %&gt;% # Take the log - will transformation be useful w/ modeling? hist() # create a histogram poke$weight_kg %&gt;% density(na.rm = T) %&gt;% # First, we compute the kernel density # (na.rm = T says to ignore NA values) plot() # Then, we plot the result poke$weight_kg %&gt;% log10() %&gt;% # Transform the variable density(na.rm = T) %&gt;% # Compute the density ignoring missing values plot(main = &quot;Density of Log10 pokemon weight in Kg&quot;) # Plot the result, # changing the title of the plot to a meaningful value We may also want to look at correlations between variables. In R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms. To examine the relationship between a categorical variable and a continuous variable, we might look at boxplots: boxplot(log10(height_m) ~ status, data = poke) boxplot(total_points ~ species, data = poke) In the second boxplot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn’t usually about creating pretty plots (or we’d be using ggplot right now) but rather about identifying things which may come up in the analysis later. To look at the relationship between numeric variables, we could compute a numeric correlation, but a plot is more useful. plot(defense ~ attack, data = poke, type = &quot;p&quot;) cor(poke$defense, poke$attack) ## [1] 0.4507656 Sometimes, we discover that a variable which appears to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset. plot(x = poke$base_experience, y = poke$base_friendship, type = &quot;p&quot;) A scatterplot matrix can also be a useful way to visualize relationships between several variables. pairs(poke[,19:23]) # hp - sp_defense columns There’s more information on how to customize these plots here. Try it out Explore the variables present in the police violence data (see code in the spreadsheets section to read it in). Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis. How does your analysis in R differ from the way that you approached the data in SAS? Solution if (!&quot;readxl&quot; %in% installed.packages()) install.packages(&quot;readxl&quot;) library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1, guess_max = 7000) police_violence$`Victim&#39;s age` &lt;- as.numeric(police_violence$`Victim&#39;s age`) ## Warning: NAs introduced by coercion summary(police_violence) ## Victim&#39;s name Victim&#39;s age Victim&#39;s gender Victim&#39;s race ## Length:7663 Min. : 1.0 Length:7663 Length:7663 ## Class :character 1st Qu.: 27.0 Class :character Class :character ## Mode :character Median : 34.0 Mode :character Mode :character ## Mean : 36.8 ## 3rd Qu.: 45.0 ## Max. :107.0 ## NA&#39;s :206 ## URL of image of victim Date of Incident (month/day/year) ## Length:7663 Min. :2013-01-01 00:00:00 ## Class :character 1st Qu.:2014-10-07 00:00:00 ## Mode :character Median :2016-07-07 00:00:00 ## Mean :2016-07-06 23:33:18 ## 3rd Qu.:2018-04-04 00:00:00 ## Max. :2019-12-31 00:00:00 ## ## Street Address of Incident City State ## Length:7663 Length:7663 Length:7663 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## Zipcode County Agency responsible for death ## Length:7663 Length:7663 Length:7663 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## Cause of death ## Length:7663 ## Class :character ## Mode :character ## ## ## ## ## A brief description of the circumstances surrounding the death ## Length:7663 ## Class :character ## Mode :character ## ## ## ## ## Official disposition of death (justified or other) Criminal Charges? ## Length:7663 Length:7663 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## ## Link to news article or photo of official document Symptoms of mental illness? ## Length:7663 Length:7663 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## ## Unarmed Alleged Weapon (Source: WaPo) ## Length:7663 Length:7663 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## ## Alleged Threat Level (Source: WaPo) Fleeing (Source: WaPo) ## Length:7663 Length:7663 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## ## Body Camera (Source: WaPo) WaPo ID (If included in WaPo database) ## Length:7663 Min. : 3 ## Class :character 1st Qu.:1402 ## Mode :character Median :2722 ## Mean :2724 ## 3rd Qu.:4051 ## Max. :5439 ## NA&#39;s :2785 ## Off-Duty Killing? ## Length:7663 ## Class :character ## Mode :character ## ## ## ## ## Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) ## Length:7663 ## Class :character ## Mode :character ## ## ## ## ## ID ## Min. : 1 ## 1st Qu.:1916 ## Median :3832 ## Mean :3833 ## 3rd Qu.:5750 ## Max. :7667 ## Let’s examine the numeric and date variables first: hist(police_violence$`Victim&#39;s age`) # hist(police_violence$`Date of Incident (month/day/year)`) # This didn&#39;t work - it wants me to specify breaks # Instead, lets see if ggplot handles it better - from R4DS library(ggplot2) ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) + geom_density() Let’s look at the victims’ gender and race: table(police_violence$`Victim&#39;s race`, useNA = &#39;ifany&#39;) ## ## Asian Black Hispanic Native American ## 118 1944 1335 112 ## Pacific Islander Unknown race Unknown Race White ## 42 670 64 3378 table(police_violence$`Victim&#39;s gender`) ## ## Female Male Transgender Unknown ## 391 7253 7 4 table(police_violence$`Victim&#39;s race`, police_violence$`Victim&#39;s gender`) ## ## Female Male Transgender Unknown ## Asian 6 111 0 1 ## Black 69 1871 2 1 ## Hispanic 49 1283 1 0 ## Native American 7 104 0 0 ## Pacific Islander 2 40 0 0 ## Unknown race 33 632 1 2 ## Unknown Race 2 62 0 0 ## White 223 3150 3 0 plot(table(police_violence$`Victim&#39;s race`, police_violence$`Victim&#39;s gender`), main = &quot;Police Killing by Race, Gender&quot;) We can also look at the age range for each race: police_violence %&gt;% # get groups with at least 100 observations that aren&#39;t unknown subset(`Victim&#39;s race` %in% c(&quot;Asian&quot;, &quot;Black&quot;, &quot;Native American&quot;, &quot;Hispanic&quot;, &quot;White&quot;)) %&gt;% boxplot(`Victim&#39;s age` ~ `Victim&#39;s race`, data = .) And examine the age range for each gender as well: police_violence %&gt;% boxplot(`Victim&#39;s age` ~ `Victim&#39;s gender`, data = .) The thing I’m honestly most surprised at with this plot is that there are so many elderly individuals (of both genders) shot. That’s not a realization I’d normally construct this plot for, but the visual emphasis on the outliers in a boxplot makes it much easier to focus on that aspect of the data. My analysis in R was a bit more free-form than in SAS - in SAS, I proceeded fairly directly through each procedure, while in R, I could investigate things that caught my eye along the way more easily. I didn’t focus as much on what we’d need to clean up in R (because the same problems exist that we identified when using SAS). 5.6.2.1 skimr package I discovered this package while looking over the material in this chapter a second time (so this is new as of 2020/09/07). if (!&quot;skimr&quot; %in% installed.packages()) install.packages(&quot;skimr&quot;) library(skimr) skim(police_violence) Table 5.1: Data summary Name police_violence Number of rows 7663 Number of columns 27 _______________________ Column type frequency: character 23 numeric 3 POSIXct 1 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Victim’s name 0 1.00 7 49 0 7411 0 Victim’s gender 8 1.00 4 11 0 4 0 Victim’s race 0 1.00 5 16 0 8 0 URL of image of victim 3462 0.55 27 10527 0 4192 0 Street Address of Incident 83 0.99 3 73 0 7487 0 City 6 1.00 3 29 0 2884 0 State 0 1.00 2 2 0 51 0 Zipcode 39 0.99 4 5 0 4996 0 County 15 1.00 3 80 0 1110 0 Agency responsible for death 16 1.00 7 177 0 2853 0 Cause of death 0 1.00 4 39 0 30 0 A brief description of the circumstances surrounding the death 20 1.00 30 1631 0 7581 0 Official disposition of death (justified or other) 256 0.97 7 176 0 97 0 Criminal Charges? 0 1.00 2 77 0 29 0 Link to news article or photo of official document 12 1.00 21 312 0 7560 0 Symptoms of mental illness? 11 1.00 2 19 0 6 0 Unarmed 0 1.00 7 15 0 4 0 Alleged Weapon (Source: WaPo) 0 1.00 2 32 0 169 0 Alleged Threat Level (Source: WaPo) 2382 0.69 5 12 0 3 0 Fleeing (Source: WaPo) 2616 0.66 1 11 0 8 0 Body Camera (Source: WaPo) 2869 0.63 2 18 0 5 0 Off-Duty Killing? 7437 0.03 8 8 0 1 0 Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) 67 0.99 5 8 0 3 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Victim’s age 206 0.97 36.80 13.21 1 27.00 34 45.00 107 ▂▇▃▁▁ WaPo ID (If included in WaPo database) 2785 0.64 2723.53 1534.33 3 1402.25 2722 4050.75 5439 ▇▇▇▇▇ ID 0 1.00 3832.89 2213.34 1 1916.50 3832 5749.50 7667 ▇▇▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique Date of Incident (month/day/year) 0 1 2013-01-01 2019-12-31 2016-07-07 2404 You may find the summary tables given by the skimr package to be more appealing - it separates the variables out by type, provides histograms of numeric variables, and is compatible with rmarkdown/knitr. If you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I’m cheating a bit by mentioning it now, but it’s just so useful!) library(dplyr) police_violence %&gt;% # get variables which are important select(matches(&quot;age$|race|gender|Cause|Symptoms|Unarmed&quot;)) %&gt;% group_by(Unarmed) %&gt;% skim() Table 5.2: Data summary Name Piped data Number of rows 7663 Number of columns 6 _______________________ Column type frequency: character 4 numeric 1 ________________________ Group variables Unarmed Variable type: character skim_variable Unarmed n_missing complete_rate min max empty n_unique whitespace Victim’s gender Allegedly Armed 3 1.00 4 11 0 4 0 Victim’s gender Unarmed 0 1.00 4 11 0 4 0 Victim’s gender Unclear 2 1.00 4 7 0 3 0 Victim’s gender Vehicle 3 0.99 4 11 0 3 0 Victim’s race Allegedly Armed 0 1.00 5 16 0 8 0 Victim’s race Unarmed 0 1.00 5 16 0 8 0 Victim’s race Unclear 0 1.00 5 16 0 8 0 Victim’s race Vehicle 0 1.00 5 16 0 8 0 Cause of death Allegedly Armed 0 1.00 4 39 0 21 0 Cause of death Unarmed 0 1.00 5 39 0 17 0 Cause of death Unclear 0 1.00 5 25 0 8 0 Cause of death Vehicle 0 1.00 5 14 0 4 0 Symptoms of mental illness? Allegedly Armed 10 1.00 2 19 0 6 0 Symptoms of mental illness? Unarmed 0 1.00 2 19 0 4 0 Symptoms of mental illness? Unclear 1 1.00 2 19 0 4 0 Symptoms of mental illness? Vehicle 0 1.00 2 19 0 4 0 Variable type: numeric skim_variable Unarmed n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Victim’s age Allegedly Armed 139 0.97 37.82 13.64 14 27 35 47 107 ▇▇▃▁▁ Victim’s age Unarmed 16 0.99 34.36 12.18 1 25 33 41 89 ▁▇▅▁▁ Victim’s age Unclear 34 0.95 35.36 11.60 15 26 34 43 76 ▇▇▆▂▁ Victim’s age Vehicle 17 0.97 32.82 10.75 15 25 31 38 77 ▆▇▃▁▁ This summary allows us to see very quickly that there is a difference in the age distribution of unarmed individuals who died during an encounter with police - unarmed individuals are likely to be significantly older on average. If you are using skimr in knitr/rmarkdown, your data frame will automatically render as a custom-printed table if the last line in the code chunk is a skim_df object. There are many ways to customize the summary statistics detailed in the package that I’m not going to go into here, but you are free to investigate if you like the way these summaries look. I mention this package now because it is appropriate for EDA, but it may not be intuitive or easy to use in the way you might want to use it until after we cover the dplyr package in the manipulating data module and the tidyr package in the transforming data module. 5.6.2.2 janitor package The janitor package has functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing. janitor and clean_names() by Allison Horst 5.6.3 Comparison You must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of “give me every possible statistic that could be calculated from this model, whether or not it makes sense.” The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. – Douglas Bates References and Links Reading JSON in SAS – You know SAS documentation is getting weird when they advertise a method as “the sexiest way to import JSON data into SAS.” Reading Rdata files in SAS Common problems with SAS data files U.S. Department of Transportation, Federal Highway Administration, 2009 National Household Travel Survey. URL: http://nhts.ornl.gov. Data acquired from data.world. RSQLite vignette Slides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.) The vroom package works like read_csv but allows you to read in and write to many files at incredible speeds. though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times↩︎ I tried, and it crashed SAS on my machine.↩︎ A currently maintained version of the library is here and should work for UNIX platforms. It may be possible to install the library on Windows using the UNIX subsystem, per this thread↩︎ On one of my machines, I also had to make sure the file libodbc.so existed - it was named libodbc.so.1 on my laptop, so a symbolic link fixed the issue.↩︎ A histogram is a chart which breaks up a continuous variable into ranges, where the height of the bar is proportional to the number of items in the range. A bar chart is similar, but shows the number of occurrences of a discrete variable.↩︎ "],["manipulating-data.html", "Module 6 Manipulating Data Manipulating Data: Module Objectives 6.1 Tidy Data 6.2 Filter: Pick cases (rows) based on their values 6.3 Select: Pick columns 6.4 Mutate: Add and transform variables 6.5 Summarize 6.6 Group By + (?) = Power! 6.7 Other dplyr functions: across, relocate Try it out References", " Module 6 Manipulating Data In this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis20 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on. In R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy21 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to the tidyverse - specifically, readr. In SAS, there is no tidyverse, but there is a relatively consistent structure for how to accomplish each task. Most data cleaning in SAS is accomplished in data steps. In the interests of not confusing terms too much between languages, I’m going to use the tidyverse “verbs” to describe operations in both SAS and R. dplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in SAS (without the underlying framework). Each verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone. This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy. There is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter. Manipulating Data: Module Objectives Filter, subset, and clean data to prepare a dataset for analysis Describe and document operations performed on a data set transparently, and implement the operations using reproducible steps. Create summaries of data appropriate for additional analysis or display 6.1 Tidy Data There are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes: Each variable has its own column Each observation has its own row Each value has its own cell These attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your dirty clothes are folded, but they aren’t clean just because they’re folded). We’ll get more into how to work with different “messy” data configurations in the next module, but it’s worth keeping rules 1 and 3 in mind while working through this module. 6.2 Filter: Pick cases (rows) based on their values Filter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original. dplyr filter() by Allison Horst Let’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies. Data set up This data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union data(starwars) starwars ## # A tibble: 87 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke S… 172 77 blond fair blue 19 male mascu… ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… ## 4 Darth … 202 136 none white yellow 41.9 male mascu… ## 5 Leia O… 150 49 brown light brown 19 fema… femin… ## 6 Owen L… 178 120 brown, grey light blue 52 male mascu… ## 7 Beru W… 165 75 brown light blue 47 fema… femin… ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… ## 9 Biggs … 183 84 black light brown 24 male mascu… ## 10 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; In the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by SAS. You can access the csv data here. Note that I exported the data using ‘.’ as the NA/missing character so that it will be easy to read into SAS. library(readr) ## write_csv(starwars[,1:11], &quot;data/starwars.csv&quot;, na = &#39;.&#39;) Let’s set that data up first: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 filename swdat &quot;data/starwars.csv&quot;; 8 9 PROC IMPORT DATAFILE = swdat OUT = classdat.starwars 10 DBMS = CSV 11 REPLACE; 12 GETNAMES = YES; 13 RUN; 14 /************************************************************** 14 ! ******** 15 * PRODUCT: SAS 16 * VERSION: 9.4 17 * CREATOR: External File Interface 18 * DATE: 09MAY21 19 * DESC: Generated SAS Datastep Code 20 * TEMPLATE SOURCE: (None Specified.) 21 *************************************************************** 21 ! ********/ 22 data CLASSDAT.STARWARS ; 23 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 23 ! */ 24 infile SWDAT delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 25 informat name $21. ; 26 informat height best32. ; 27 informat mass best32. ; 28 informat hair_color $15. ; 29 informat skin_color $18. ; 30 informat eye_color $9. ; 31 informat birth_year best32. ; 32 informat sex $14. ; 33 informat gender $9. ; 34 informat homeworld $10. ; 35 informat species $14. ; 36 format name $21. ; 37 format height best12. ; 38 format mass best12. ; 39 format hair_color $15. ; 40 format skin_color $18. ; 41 format eye_color $9. ; 42 format birth_year best12. ; 43 format sex $14. ; 44 format gender $9. ; 45 format homeworld $10. ; 46 format species $14. ; 47 input 48 name $ 49 height 50 mass 51 hair_color $ 52 skin_color $ 53 eye_color $ 54 birth_year 55 sex $ 56 gender $ 57 homeworld $ 58 species $ 59 ; 60 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 60 ! detection macro variable */ 61 run; NOTE: The infile SWDAT is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/sta rwars.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=17Jun2020:11:59:52, File Size (bytes)=6080 NOTE: 87 records were read from the infile SWDAT. The minimum record length was 45. The maximum record length was 102. NOTE: The data set CLASSDAT.STARWARS has 87 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 87 rows created in CLASSDAT.STARWARS from SWDAT. NOTE: CLASSDAT.STARWARS data set was successfully created. NOTE: The data set CLASSDAT.STARWARS has 87 observations and 11 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 0.13 seconds cpu time 0.12 seconds 62 63 PROC PRINT DATA=classdat.starwars (obs=10); 64 RUN; NOTE: There were 10 observations read from the data set CLASSDAT.STARWARS. NOTE: PROCEDURE PRINT used (Total process time): real time 0.03 seconds cpu time 0.04 seconds Obs name height mass hair_color skin_color eye_color birth_year sex gender homeworld species 1 Luke Skywalker 172 77 blond fair blue 19 male masculine Tatooine Human 2 C-3PO 167 75 gold yellow 112 none masculine Tatooine Droid 3 R2-D2 96 32 white, blue red 33 none masculine Naboo Droid 4 Darth Vader 202 136 none white yellow 41.9 male masculine Tatooine Human 5 Leia Organa 150 49 brown light brown 19 female feminine Alderaan Human 6 Owen Lars 178 120 brown, grey light blue 52 male masculine Tatooine Human 7 Beru Whitesun lars 165 75 brown light blue 47 female feminine Tatooine Human 8 R5-D4 97 32 white, red red . none masculine Tatooine Droid 9 Biggs Darklighter 183 84 black light brown 24 male masculine Tatooine Human 10 Obi-Wan Kenobi 182 77 auburn, white fair blue-gray 57 male masculine Stewjon Human If you want to directly load the SAS datafile, you can find it here Once the data is set up, using filter is actually very simple. Demonstration of filter() in R # Get only the people filter(starwars, species == &quot;Human&quot;) ## # A tibble: 35 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke S… 172 77 blond fair blue 19 male mascu… ## 2 Darth … 202 136 none white yellow 41.9 male mascu… ## 3 Leia O… 150 49 brown light brown 19 fema… femin… ## 4 Owen L… 178 120 brown, grey light blue 52 male mascu… ## 5 Beru W… 165 75 brown light blue 47 fema… femin… ## 6 Biggs … 183 84 black light brown 24 male mascu… ## 7 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… ## 8 Anakin… 188 84 blond fair blue 41.9 male mascu… ## 9 Wilhuf… 180 NA auburn, gr… fair blue 64 male mascu… ## 10 Han So… 180 80 brown fair brown 29 male mascu… ## # … with 25 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, ## # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; # Get only the people who come from Tatooine filter(starwars, species == &quot;Human&quot;, homeworld == &quot;Tatooine&quot;) ## # A tibble: 8 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Sk… 172 77 blond fair blue 19 male mascu… ## 2 Darth V… 202 136 none white yellow 41.9 male mascu… ## 3 Owen La… 178 120 brown, grey light blue 52 male mascu… ## 4 Beru Wh… 165 75 brown light blue 47 fema… femin… ## 5 Biggs D… 183 84 black light brown 24 male mascu… ## 6 Anakin … 188 84 blond fair blue 41.9 male mascu… ## 7 Shmi Sk… 163 NA black fair brown 72 fema… femin… ## 8 Cliegg … 183 NA brown fair blue 82 male mascu… ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; In SAS, as in SQL, the filter() operation is accomplished using a where clause. Multiple clauses can be connected using and, and compound statements can be grouped with parentheses. Demonstration of where in SAS Rather than output the whole data table (which would take up a lot of space), I’ve linked the log file from each chunk below the chunk. If you are running this code in SAS, you should NOT copy the proc printto line. libname classdat &quot;sas/&quot;; /* SAS limits dataset names to 8 characters, which is super annoying. */ /* Sorry the names aren&#39;t descriptive... */ DATA tmp1; /* this is the out dataset */ /* By not having a library attached, SAS places this in WORK */ /* It&#39;s a temporary dataset */ set classdat.starwars; where (species = &#39;Human&#39;); run; See the log file here libname classdat &quot;sas/&quot;; DATA tmp2; set classdat.starwars; where (species = &#39;Human&#39;) and (homeworld = &#39;Tatooine&#39;); run; See the log file here At this point, you’ve seen the traditional SAS Data step options, but there is another SAS PROC that may be more useful (and more similar to dplyr). dplyr was developed to provide SQL-like syntax while enabling the use of more advanced computations than are supported in SQL. While SAS doesn’t have anything quite the same as dplyr, it does have PROC SQL. SAS PROC SQL In SQL, as in the SAS DATA step, filter() operations are performed using the keyword WHERE. To limit the output I’m going to cheat a bit and use SELECT statements before I officially teach them to you - this is mostly so you don’t get a table with all 49 variables in it. Similarly, I’m limiting the dataset to the first 5 observations that meet the condition so that we don’t have to see all the water type pokemon. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 SELECT pokedex_number, name, type_1, type_number FROM 9 ! classdat.poke (obs=5) 10 WHERE type_1 = &quot;Water&quot;; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds pokedex_number name type_1 type_number 7 Squirtle Water 1 8 Wartortle Water 1 9 Blastoise Water 1 9 Mega Blastoise Water 1 54 Psyduck Water 1 If we want to store the output of our query to a new table, we can do that by starting our query with CREATE TABLE &lt;table name&gt; AS - this creates a table with our results. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 NOTE: The PROCEDURE SQL printed page 2. NOTE: PROCEDURE SQL used (Total process time): real time 0.06 seconds cpu time 0.06 seconds 8 PROC SQL; 9 CREATE TABLE aquapoke AS 10 SELECT pokedex_number, name, type_1, type_2, type_number FROM 10 ! classdat.poke 11 WHERE (type_1 = &quot;Water&quot; OR type_2 = &quot;Water&quot;); NOTE: Table WORK.AQUAPOKE created, with 153 rows and 5 columns. 12 NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 PROC PRINT DATA=aquapoke (obs=10); 14 RUN; NOTE: There were 10 observations read from the data set WORK.AQUAPOKE. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.01 seconds Obs pokedex_number name type_1 type_2 type_number 1 7 Squirtle Water 1 2 8 Wartortle Water 1 3 9 Blastoise Water 1 4 9 Mega Blastoise Water 1 5 54 Psyduck Water 1 6 55 Golduck Water 1 7 60 Poliwag Water 1 8 61 Poliwhirl Water 1 9 62 Poliwrath Water Fighting 2 10 72 Tentacool Water Poison 2 6.2.1 Common Filter() Tasks In dplyr, there are a few helper functions which may be useful when constructing filter statements. row_number() - this is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table. poke &lt;- read_csv(&quot;data/pokemon_ascii.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## name = col_character(), ## german_name = col_character(), ## status = col_character(), ## species = col_character(), ## type_1 = col_character(), ## type_2 = col_character(), ## ability_1 = col_character(), ## ability_2 = col_character(), ## ability_hidden = col_character(), ## catch_rate = col_character(), ## base_friendship = col_character(), ## base_experience = col_character(), ## growth_rate = col_character(), ## egg_type_1 = col_character(), ## egg_type_2 = col_character(), ## percentage_male = col_character(), ## egg_cycles = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. ## Warning: 1 parsing failure. ## row col expected actual file ## 1028 weight_kg a double . &#39;data/pokemon_ascii.csv&#39; filter(poke, (row_number() %% 2 == 0)) ## # A tibble: 514 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 Ivys… Bisaknosp 1 Normal Seed P… 2 Grass ## 2 3 Mega… Bisaflor 1 Normal Seed P… 2 Grass ## 3 5 Char… Glutexo 1 Normal Flame … 1 Fire ## 4 6 Mega… Glurak 1 Normal Flame … 2 Fire ## 5 7 Squi… Schiggy 1 Normal Tiny T… 1 Water ## 6 9 Blas… Turtok 1 Normal Shellf… 1 Water ## 7 10 Cate… Raupy 1 Normal Worm P… 1 Bug ## 8 12 Butt… Smettbo 1 Normal Butter… 2 Bug ## 9 14 Kaku… Kokuna 1 Normal Cocoon… 2 Bug ## 10 15 Mega… Bibor 1 Normal Poison… 2 Bug ## # … with 504 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, ## # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; # There are several pokemon who have multiple entries in the table, # so the pokedex_number doesn&#39;t line up with the row number. arrange() - sort rows in the table by one or more variables arrange(poke, desc(total_points)) ## # A tibble: 1,028 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 890 Eter… . 8 Legen… Gigant… 2 Poison ## 2 150 Mega… Mewtu 1 Legen… Geneti… 2 Psych… ## 3 150 Mega… Mewtu 1 Legen… Geneti… 1 Psych… ## 4 384 Mega… Rayquaza 3 Legen… Sky Hi… 2 Dragon ## 5 382 Prim… Kyogre 3 Legen… Sea Ba… 1 Water ## 6 383 Prim… Groudon 3 Legen… Contin… 2 Ground ## 7 800 Ultr… Necrozma 7 Legen… Prism … 2 Psych… ## 8 493 Arce… Arceus 4 Mythi… Alpha … 1 Normal ## 9 888 Zaci… . 8 Legen… Warrio… 2 Fairy ## 10 889 Zama… . 8 Legen… Warrio… 2 Fight… ## # … with 1,018 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, ## # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; slice_max() - this will keep the top values of a specified variable. It’s like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a lot more code. slice_max(poke, order_by = total_points, n = 5) %&gt;% arrange(desc(total_points)) # Sort decreasing ## # A tibble: 6 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 890 Etern… . 8 Legen… Gigant… 2 Poison ## 2 150 Mega … Mewtu 1 Legen… Geneti… 2 Psych… ## 3 150 Mega … Mewtu 1 Legen… Geneti… 1 Psych… ## 4 384 Mega … Rayquaza 3 Legen… Sky Hi… 2 Dragon ## 5 382 Prima… Kyogre 3 Legen… Sea Ba… 1 Water ## 6 383 Prima… Groudon 3 Legen… Contin… 2 Ground ## # … with 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;, ## # abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; By default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows. slice_max(poke, order_by = total_points, n = 5, with_ties = F) %&gt;% arrange(desc(total_points)) # Sort decreasing ## # A tibble: 5 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 890 Etern… . 8 Legen… Gigant… 2 Poison ## 2 150 Mega … Mewtu 1 Legen… Geneti… 2 Psych… ## 3 150 Mega … Mewtu 1 Legen… Geneti… 1 Psych… ## 4 384 Mega … Rayquaza 3 Legen… Sky Hi… 2 Dragon ## 5 382 Prima… Kyogre 3 Legen… Sea Ba… 1 Water ## # … with 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;, ## # abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; In SAS, these same tasks can sometimes require a bit more code. Keeping only certain rows in SAS In SAS, to use a variable, you have to define it in one data step, then make another data step in order to use that variable. But, like dplyr, SAS has a row number counter that we can use for this purpose. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA tmp; 9 SET classdat.poke; 10 rownum=_n_; /* SAS shorthand for row number */ 11 RUN; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: The data set WORK.TMP has 1028 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 12 13 DATA evenrow; 14 SET WORK.tmp; 15 WHERE MOD(rownum, 2) = 0; 16 DROP rownum; /* ditch temp variable */ 17 RUN; NOTE: There were 514 observations read from the data set WORK.TMP. WHERE MOD(rownum, 2)=0; NOTE: The data set WORK.EVENROW has 514 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Top N values We’re going to want to use PROC SORT to get the data arranged before we take the top N values. According to this, we can’t use _n_ in a where statement, and the proposed solution isn’t reliable. So we’ll do it the long way. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SORT DATA = classdat.poke 9 OUT = pokesort; 10 BY descending total_points; 11 RUN; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: The data set WORK.POKESORT has 1028 observations and 49 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 DATA poken; 14 SET WORK.pokesort; 15 rownum = _n_; 16 RUN; NOTE: There were 1028 observations read from the data set WORK.POKESORT. NOTE: The data set WORK.POKEN has 1028 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 DATA poken; 19 SET WORK.poken; 20 WHERE rownum &lt;= 5; 21 DROP rownum; 22 RUN; NOTE: There were 5 observations read from the data set WORK.POKEN. WHERE rownum&lt;=5; NOTE: The data set WORK.POKEN has 5 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 23 24 PROC PRINT DATA = poken; 25 VAR pokedex_number name status species type_1 total_points; 26 RUN; NOTE: There were 5 observations read from the data set WORK.POKEN. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds Obs pokedex_number name status species type_1 total_points 1 890 Eternatus Eternamax Legendary Gigantic Pokemon Poison 1125 2 150 Mega Mewtwo X Legendary Genetic Pokemon Psychic 780 3 150 Mega Mewtwo Y Legendary Genetic Pokemon Psychic 780 4 384 Mega Rayquaza Legendary Sky High Pokemon Dragon 780 5 382 Primal Kyogre Legendary Sea Basin Pokemon Water 770 In both cases, the SAS statements required to perform the task require a WHERE clause, but also a few other statements to get things working. The equivalent base R code would be about the same (though tricky in different spots). The thing that makes the tidyverse philosophy so addictive is that it makes these common, everyday tasks both easy and concise (that is, few lines of code are required). PROC SQL filter statements SQL doesn’t have an intrinsic notion of ordered rows, so in order to select even rows, we need to create a temporary dataset with _n_ copied into a variable (just like last time). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA poke; 9 SET classdat.poke; 10 rownum=_n_; 11 RUN; NOTE: There were 1028 observations read from the data set CLASSDAT.POKE. NOTE: The data set WORK.POKE has 1028 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 12 13 PROC SQL; 14 SELECT * FROM poke(obs=5) 15 WHERE mod(rownum, 2) = 0; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.02 seconds pokedex_number name german_name generation status species type_number type_1 type_2 height_m weight_kg abilities_number ability_1 ability_2 ability_hidden total_points hp attack defense sp_attack sp_defense speed catch_rate base_friendship base_experience growth_rate egg_type_number egg_type_1 egg_type_2 percentage_male egg_cycles against_normal against_fire against_water against_electric against_grass against_ice against_fight against_poison against_ground against_flying against_psychic against_bug against_rock against_ghost against_dragon against_dark against_steel against_fairy rownum 2 Ivysaur Bisaknosp 1 Normal Seed Pokemon 2 Grass Poison 1 13 2 Overgrow Chlorophyll 405 60 62 63 80 80 60 45 70 142 Medium Slow 2 Grass Monster 87.5 20 1 2 0.5 0.5 0.25 2 0.5 1 1 2 2 1 1 1 1 1 1 0.5 2 3 Mega Venusaur Bisaflor 1 Normal Seed Pokemon 2 Grass Poison 2.4 155.5 1 Thick Fat 625 80 100 123 122 120 80 45 70 281 Medium Slow 2 Grass Monster 87.5 20 1 1 0.5 0.5 0.25 1 0.5 1 1 2 2 1 1 1 1 1 1 0.5 4 5 Charmeleon Glutexo 1 Normal Flame Pokemon 1 Fire 1.1 19 2 Blaze Solar Power 405 58 64 58 80 65 80 45 70 142 Medium Slow 2 Dragon Monster 87.5 20 1 0.5 2 1 0.5 0.5 1 1 2 1 1 0.5 2 1 1 1 0.5 0.5 6 6 Mega Charizard X Glurak 1 Normal Flame Pokemon 2 Fire Dragon 1.7 110.5 1 Tough Claws 634 78 130 111 130 85 100 45 70 285 Medium Slow 2 Dragon Monster 87.5 20 1 0.25 1 0.5 0.25 1 1 1 2 1 1 0.5 2 1 2 1 0.5 1 8 7 Squirtle Schiggy 1 Normal Tiny Turtle Pokemon 1 Water 0.5 9 2 Torrent Rain Dish 314 44 48 65 50 64 43 45 70 63 Medium Slow 2 Monster Water 1 87.5 20 1 0.5 0.5 2 2 0.5 1 1 1 1 1 1 1 1 1 1 0.5 1 10 SELECT * says to select all variables. We’ll talk about SELECT in the next section, but with SQL it’s not reqlly possible to avoid using SELECT. If we want the 5 pokemon with the highest total points, we can use ORDER BY to sort the table, and then specify that we only want 5 rows. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 NOTE: The PROCEDURE SQL printed pages 2-3. NOTE: PROCEDURE SQL used (Total process time): real time 0.07 seconds cpu time 0.07 seconds 8 PROC SQL; 9 SELECT pokedex_number, name, status, species, type_1, 9 ! total_points 10 FROM classdat.poke(obs=5) 11 ORDER BY total_points DESC; NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds pokedex_number name status species type_1 total_points 3 Mega Venusaur Normal Seed Pokemon Grass 625 3 Venusaur Normal Seed Pokemon Grass 525 2 Ivysaur Normal Seed Pokemon Grass 405 1 Bulbasaur Normal Seed Pokemon Grass 318 4 Charmander Normal Lizard Pokemon Fire 309 As a reminder, if we want to store this new data into a new dataset, we have to start our statement with CREATE TABLE AS, and then follow the statement with our query. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 NOTE: The PROCEDURE SQL printed page 4. NOTE: PROCEDURE SQL used (Total process time): real time 0.07 seconds cpu time 0.06 seconds 8 PROC SQL; 9 CREATE TABLE poketmp AS 10 SELECT pokedex_number, name, status, species, type_1, 10 ! total_points 11 FROM classdat.poke(obs=5) 12 ORDER BY total_points DESC; NOTE: Table WORK.POKETMP created, with 5 rows and 6 columns. 13 NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 14 PROC PRINT DATA=poketmp; 15 RUN; NOTE: There were 5 observations read from the data set WORK.POKETMP. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds Obs pokedex_number name status species type_1 total_points 1 3 Mega Venusaur Normal Seed Pokemon Grass 625 2 3 Venusaur Normal Seed Pokemon Grass 525 3 2 Ivysaur Normal Seed Pokemon Grass 405 4 1 Bulbasaur Normal Seed Pokemon Grass 318 5 4 Charmander Normal Lizard Pokemon Fire 309 Try it out Using the pokemon data, can you create a new data set or data frame (SAS and R, respectively) that has only water type pokemon? Can you write a filter statement that looks for any pokemon which has water type for either type1 or type2? R poke &lt;- read_csv(&quot;data/pokemon_ascii.csv&quot;) ## Warning: 1 parsing failure. ## row col expected actual file ## 1028 weight_kg a double . &#39;data/pokemon_ascii.csv&#39; filter(poke, type_1 == &quot;Water&quot;) ## # A tibble: 134 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7 Squi… Schiggy 1 Normal Tiny T… 1 Water ## 2 8 Wart… Schillok 1 Normal Turtle… 1 Water ## 3 9 Blas… Turtok 1 Normal Shellf… 1 Water ## 4 9 Mega… Turtok 1 Normal Shellf… 1 Water ## 5 54 Psyd… Enton 1 Normal Duck P… 1 Water ## 6 55 Gold… Entoron 1 Normal Duck P… 1 Water ## 7 60 Poli… Quapsel 1 Normal Tadpol… 1 Water ## 8 61 Poli… Quaputzi 1 Normal Tadpol… 1 Water ## 9 62 Poli… Quappo 1 Normal Tadpol… 2 Water ## 10 72 Tent… Tentacha 1 Normal Jellyf… 2 Water ## # … with 124 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, ## # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; filter(poke, type_1 == &quot;Water&quot; | type_2 == &quot;Water&quot;) ## # A tibble: 153 x 49 ## pokedex_number name german_name generation status species type_number type_1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 7 Squi… Schiggy 1 Normal Tiny T… 1 Water ## 2 8 Wart… Schillok 1 Normal Turtle… 1 Water ## 3 9 Blas… Turtok 1 Normal Shellf… 1 Water ## 4 9 Mega… Turtok 1 Normal Shellf… 1 Water ## 5 54 Psyd… Enton 1 Normal Duck P… 1 Water ## 6 55 Gold… Entoron 1 Normal Duck P… 1 Water ## 7 60 Poli… Quapsel 1 Normal Tadpol… 1 Water ## 8 61 Poli… Quaputzi 1 Normal Tadpol… 1 Water ## 9 62 Poli… Quappo 1 Normal Tadpol… 2 Water ## 10 72 Tent… Tentacha 1 Normal Jellyf… 2 Water ## # … with 143 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, ## # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ## # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, ## # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, ## # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, ## # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, ## # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, ## # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, ## # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, ## # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, ## # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, ## # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, ## # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; # The conditions have to be separated by |, which means &quot;or&quot; SAS DATA Step libname classdat &quot;sas/&quot;; DATA water1; SET classdat.poke; WHERE type_1 = &quot;Water&quot;; RUN; DATA water2; SET classdat.poke; WHERE (type_1 = &quot;Water&quot; OR type_2 = &quot;Water&quot;); RUN; In the interests of only showing the parts of the log that are useful, I’ve just pasted them into this chunk. Not reproducible, but faster to read. NOTE: There were 134 observations read from the data set CLASSDAT.POKE. WHERE type_1=&#39;Water&#39;; NOTE: The data set WORK.WATER1 has 134 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: There were 153 observations read from the data set CLASSDAT.POKE. WHERE (type_1=&#39;Water&#39;) or (type_2=&#39;Water&#39;); NOTE: The data set WORK.WATER2 has 153 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 6.3 Select: Pick columns Sometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)], or VAR statements, but that can get tedious). In dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple. select(.data, …) So as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions.” So what can go in there? An exhaustive list of ways to select variables in dplyr First, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).22 Most dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions. var3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5 !(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses (&lt;set of vars 1&gt;) &amp; (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2. c() combines sets of variables. dplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them). everything() matches all variables last_col() matches the last variable. last_col(offset = n) selects the n-th to last variable. starts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well. contains(\"xyz\") will match any columns with names containing the literal string “xyz.” Note, contains does not work with regular expressions (you don’t need to know what that means right now). matches(regex) takes a regular expression as an argument and returns all columns matching that expression. num_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range. There are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables. all_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error. any_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set. There’s one final selector - where() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative (but I’ve never actually needed to use it). Let’s try these selector functions out and see what we can accomplish! if (!&quot;nycflights13&quot; %in% installed.packages()) install.packages(&quot;nycflights13&quot;) library(nycflights13) data(flights) str(flights) ## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame) ## $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... ## $ month : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... ## $ day : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... ## $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ... ## $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ... ## $ dep_delay : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... ## $ arr_time : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ... ## $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ... ## $ arr_delay : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ... ## $ carrier : chr [1:336776] &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... ## $ flight : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ... ## $ tailnum : chr [1:336776] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... ## $ origin : chr [1:336776] &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... ## $ dest : chr [1:336776] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... ## $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ... ## $ distance : num [1:336776] 1400 1416 1089 1576 762 ... ## $ hour : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ... ## $ minute : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ... ## $ time_hour : POSIXct[1:336776], format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... We’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands. You might want to try out your EDA skills from the previous module to see what you can find out about the dataset, before seeing how select() works. We could get a data frame of departure information for each flight: select(flights, flight, year:day, tailnum, origin, matches(&quot;dep&quot;)) ## # A tibble: 336,776 x 9 ## flight year month day tailnum origin dep_time sched_dep_time dep_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1545 2013 1 1 N14228 EWR 517 515 2 ## 2 1714 2013 1 1 N24211 LGA 533 529 4 ## 3 1141 2013 1 1 N619AA JFK 542 540 2 ## 4 725 2013 1 1 N804JB JFK 544 545 -1 ## 5 461 2013 1 1 N668DN LGA 554 600 -6 ## 6 1696 2013 1 1 N39463 EWR 554 558 -4 ## 7 507 2013 1 1 N516JB EWR 555 600 -5 ## 8 5708 2013 1 1 N829AS LGA 557 600 -3 ## 9 79 2013 1 1 N593JB JFK 557 600 -3 ## 10 301 2013 1 1 N3ALAA LGA 558 600 -2 ## # … with 336,766 more rows Perhaps we want the plane and flight ID information to be the first columns: flights %&gt;% select(carrier:dest, everything()) ## # A tibble: 336,776 x 19 ## carrier flight tailnum origin dest year month day dep_time sched_dep_time ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 UA 1545 N14228 EWR IAH 2013 1 1 517 515 ## 2 UA 1714 N24211 LGA IAH 2013 1 1 533 529 ## 3 AA 1141 N619AA JFK MIA 2013 1 1 542 540 ## 4 B6 725 N804JB JFK BQN 2013 1 1 544 545 ## 5 DL 461 N668DN LGA ATL 2013 1 1 554 600 ## 6 UA 1696 N39463 EWR ORD 2013 1 1 554 558 ## 7 B6 507 N516JB EWR FLL 2013 1 1 555 600 ## 8 EV 5708 N829AS LGA IAD 2013 1 1 557 600 ## 9 B6 79 N593JB JFK MCO 2013 1 1 557 600 ## 10 AA 301 N3ALAA LGA ORD 2013 1 1 558 600 ## # … with 336,766 more rows, and 9 more variables: dep_delay &lt;dbl&gt;, ## # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Note that everything() won’t duplicate columns you’ve already added. Exploring the difference between bare name selection and all_of()/any_of() flights %&gt;% select(carrier, flight, tailnum, matches(&quot;time&quot;)) ## # A tibble: 336,776 x 9 ## carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 UA 1545 N14228 517 515 830 819 ## 2 UA 1714 N24211 533 529 850 830 ## 3 AA 1141 N619AA 542 540 923 850 ## 4 B6 725 N804JB 544 545 1004 1022 ## 5 DL 461 N668DN 554 600 812 837 ## 6 UA 1696 N39463 554 558 740 728 ## 7 B6 507 N516JB 555 600 913 854 ## 8 EV 5708 N829AS 557 600 709 723 ## 9 B6 79 N593JB 557 600 838 846 ## 10 AA 301 N3ALAA 558 600 753 745 ## # … with 336,766 more rows, and 2 more variables: air_time &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; varlist &lt;- c(&quot;carrier&quot;, &quot;flight&quot;, &quot;tailnum&quot;, &quot;dep_time&quot;, &quot;sched_dep_time&quot;, &quot;arr_time&quot;, &quot;sched_arr_time&quot;, &quot;air_time&quot;) flights %&gt;% select(all_of(varlist)) ## # A tibble: 336,776 x 8 ## carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 UA 1545 N14228 517 515 830 819 ## 2 UA 1714 N24211 533 529 850 830 ## 3 AA 1141 N619AA 542 540 923 850 ## 4 B6 725 N804JB 544 545 1004 1022 ## 5 DL 461 N668DN 554 600 812 837 ## 6 UA 1696 N39463 554 558 740 728 ## 7 B6 507 N516JB 555 600 913 854 ## 8 EV 5708 N829AS 557 600 709 723 ## 9 B6 79 N593JB 557 600 838 846 ## 10 AA 301 N3ALAA 558 600 753 745 ## # … with 336,766 more rows, and 1 more variable: air_time &lt;dbl&gt; varlist &lt;- c(varlist, &quot;whoops&quot;) flights %&gt;% select(all_of(varlist)) # this errors out b/c whoops doesn&#39;t exist ## Error: Can&#39;t subset columns that don&#39;t exist. ## x Column `whoops` doesn&#39;t exist. flights %&gt;% select(any_of(varlist)) # this runs just fine ## # A tibble: 336,776 x 8 ## carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 UA 1545 N14228 517 515 830 819 ## 2 UA 1714 N24211 533 529 850 830 ## 3 AA 1141 N619AA 542 540 923 850 ## 4 B6 725 N804JB 544 545 1004 1022 ## 5 DL 461 N668DN 554 600 812 837 ## 6 UA 1696 N39463 554 558 740 728 ## 7 B6 507 N516JB 555 600 913 854 ## 8 EV 5708 N829AS 557 600 709 723 ## 9 B6 79 N593JB 557 600 838 846 ## 10 AA 301 N3ALAA 558 600 753 745 ## # … with 336,766 more rows, and 1 more variable: air_time &lt;dbl&gt; So for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select). Unfortunately, SAS doesn’t make column selection quite as easy. It’s still not hard, but it can be tedious. In SAS, there are two primary methods to select variables: KEEP selects variables, DROP removes variables. # Export flights data for SAS flights %&gt;% sample_frac(size = .25) %&gt;% # Keep file from being too big write_csv(&quot;data/flights.csv&quot;, na = &quot;.&quot;) 6 /* Read in data */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 8 filename fileloc 8 ! &#39;~/Projects/Class/unl-stat850/2020-stat850/data/flights.csv&#39;; 9 PROC IMPORT datafile = fileloc out=classdat.flights 10 DBMS = csv; 10 ! /* comma delimited file */ 11 GETNAMES = YES; 12 RUN; NOTE: Import cancelled. Output dataset CLASSDAT.FLIGHTS already exists. Specify REPLACE option to overwrite it. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds In SAS, a partial variable name either preceded or followed by : serves as a wildcard. Ranges of variables can be specified with two dashes, e.g. var3 -- var5. SAS KEEP statement Unfortunately, the wildcard doesn’t work on both ends, so to get the equivalent of matches(\"dep\"), we have to use two different options in our KEEP statement (plus the extra variables that don’t have dep in them). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA tmpfly; 9 KEEP flight year--day tailnum origin dep: sched_dep:; 10 SET classdat.flights; NOTE: Data file CLASSDAT.FLIGHTS.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 11 RUN; NOTE: There were 84194 observations read from the data set CLASSDAT.FLIGHTS. NOTE: The data set WORK.TMPFLY has 84194 observations and 9 variables. NOTE: DATA statement used (Total process time): real time 0.18 seconds cpu time 0.18 seconds 12 13 PROC PRINT DATA = tmpfly (obs=10); 14 RUN; NOTE: There were 10 observations read from the data set WORK.TMPFLY. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.02 seconds Obs year month day dep_time sched_dep_time dep_delay flight tailnum origin 1 2013 5 9 1855 1730 85 419 N203FR LGA 2 2013 9 11 1349 1355 -6 686 N667AW EWR 3 2013 5 23 2109 1745 204 785 N3DSAA LGA 4 2013 3 30 1251 1300 -9 315 N3752 JFK 5 2013 10 11 1555 1529 26 1246 N78524 EWR 6 2013 11 20 659 705 -6 1107 N3FUAA LGA 7 2013 1 4 837 830 7 313 N4WWAA LGA 8 2013 2 12 1900 1855 5 4649 N513MQ LGA 9 2013 5 20 1137 1140 -3 1191 N355JB JFK 10 2013 9 22 2136 1930 126 3497 N904XJ JFK Note also that SAS doesn’t reorder the columns for us like select() does. If we’d prefer to carve out columns (rather than assembling a new dataset with the columns we want to keep), we can use a DROP statement, which works exactly the same way. Let’s see what columns we removed implicitly last time by dropping everything we’d previously kept: SAS DROP statement 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA tmpfly; 9 DROP flight year--day tailnum origin dep: sched_dep:; 10 SET classdat.flights; NOTE: Data file CLASSDAT.FLIGHTS.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 11 RUN; NOTE: There were 84194 observations read from the data set CLASSDAT.FLIGHTS. NOTE: The data set WORK.TMPFLY has 84194 observations and 10 variables. NOTE: DATA statement used (Total process time): real time 0.16 seconds cpu time 0.17 seconds 12 13 PROC PRINT DATA = tmpfly (obs=10); 14 RUN; NOTE: There were 10 observations read from the data set WORK.TMPFLY. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.02 seconds Obs arr_time sched_arr_time arr_delay carrier dest air_time distance hour minute time_hour 1 2116 1956 80 F9 DEN 224 1620 17 30 20130509T210000+0000 2 1544 1602 -18 US PHX 272 2133 13 55 20130911T170000+0000 3 2350 2050 180 AA DFW 204 1389 17 45 20130523T210000+0000 4 1630 1649 -19 DL SJU 200 1598 13 0 20130330T170000+0000 5 1851 1847 4 UA SFO 332 2565 15 29 20131011T190000+0000 6 958 955 3 AA DFW 201 1389 7 5 20131120T120000+0000 7 1044 1015 29 AA ORD 115 733 8 30 20130104T130000+0000 8 2042 2100 -18 MQ MSP 143 1020 18 55 20130212T230000+0000 9 1239 1247 -8 B6 ACK 45 199 11 40 20130520T150000+0000 10 2334 2125 129 9E RDU 68 427 19 30 20130922T230000+0000 As with the filter statements, we can also use PROC SQL instead of a SAS DATA step. There are even ways to (sort-of) use elements of both. SAS PROC SQL SELECT statement 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE tmpfly 10 AS 11 SELECT flight, year, month, day, tailnum, origin 12 FROM classdat.flights; NOTE: Data file CLASSDAT.FLIGHTS.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: Table WORK.TMPFLY created, with 84194 rows and 6 columns. 13 NOTE: PROCEDURE SQL used (Total process time): real time 0.09 seconds cpu time 0.10 seconds 14 PROC PRINT DATA = tmpfly(obs=10); 15 RUN; NOTE: There were 10 observations read from the data set WORK.TMPFLY. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds Obs flight year month day tailnum origin 1 419 2013 5 9 N203FR LGA 2 686 2013 9 11 N667AW EWR 3 785 2013 5 23 N3DSAA LGA 4 315 2013 3 30 N3752 JFK 5 1246 2013 10 11 N78524 EWR 6 1107 2013 11 20 N3FUAA LGA 7 313 2013 1 4 N4WWAA LGA 8 4649 2013 2 12 N513MQ LGA 9 1191 2013 5 20 N355JB JFK 10 3497 2013 9 22 N904XJ JFK Note that PROC SQL doesn’t have a RUN statement - it is executed immediately. But, using the PROC SQL syntax, we still have to list out all of the variables, and that’s a drag. Luckily, PROC SQL will also let us use some of the DATA step options, if we’re careful about it: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE tmpfly 10 AS 11 SELECT * 12 FROM classdat.flights(drop=year--day flight tailnum origin dep: 12 ! sched_dep:); NOTE: Data file CLASSDAT.FLIGHTS.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: Table WORK.TMPFLY created, with 84194 rows and 10 columns. 13 NOTE: PROCEDURE SQL used (Total process time): real time 0.15 seconds cpu time 0.14 seconds 14 PROC PRINT DATA = tmpfly(obs=10); 15 RUN; NOTE: There were 10 observations read from the data set WORK.TMPFLY. NOTE: PROCEDURE PRINT used (Total process time): real time 0.02 seconds cpu time 0.03 seconds Obs arr_time sched_arr_time arr_delay carrier dest air_time distance hour minute time_hour 1 2116 1956 80 F9 DEN 224 1620 17 30 20130509T210000+0000 2 1544 1602 -18 US PHX 272 2133 13 55 20130911T170000+0000 3 2350 2050 180 AA DFW 204 1389 17 45 20130523T210000+0000 4 1630 1649 -19 DL SJU 200 1598 13 0 20130330T170000+0000 5 1851 1847 4 UA SFO 332 2565 15 29 20131011T190000+0000 6 958 955 3 AA DFW 201 1389 7 5 20131120T120000+0000 7 1044 1015 29 AA ORD 115 733 8 30 20130104T130000+0000 8 2042 2100 -18 MQ MSP 143 1020 18 55 20130212T230000+0000 9 1239 1247 -8 B6 ACK 45 199 11 40 20130520T150000+0000 10 2334 2125 129 9E RDU 68 427 19 30 20130922T230000+0000 Note the difference - we’re selecting everything (in SQL) but dropping columns when we tell SQL where to look for the data. For the most part, that is what you need to functionally replicate select() syntax. It may be a bit more work because there aren’t the same convenience functions, but it’ll do and you don’t have to remember as many keywords, so that’s a plus. 6.4 Mutate: Add and transform variables Up to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate. However, it’s probably best to start this section out with a very short demonstration of how this process worked in R before the tidyverse came around. Pre-tidyverse base R “mutating” a data frame Lets use the police violence data to demonstrate. Remember the issues you identified with the data during EDA in Module 4 (in SAS)(in R)? The gsub function is basically R’s version of “find and replace.” library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) # There are two categories for &quot;unknown race&quot; table(police_violence$`Victim&#39;s race`, useNA = &#39;ifany&#39;) ## ## Asian Black Hispanic Native American ## 118 1944 1335 112 ## Pacific Islander Unknown race Unknown Race White ## 42 670 64 3378 # This line substitutes &quot;race&quot; for &quot;Race&quot; so that there&#39;s consistent capitalization police_violence$race &lt;- gsub(&quot;Race&quot;, &quot;race&quot;, police_violence$`Victim&#39;s race`) # Fixed! table(police_violence$race) ## ## Asian Black Hispanic Native American ## 118 1944 1335 112 ## Pacific Islander Unknown race White ## 42 734 3378 You could do a simple operation like that in a single line, but you had to use the name of the data multiple times, and it very quickly becomes a complicated operation. The process in SAS is very similar. It’s recommended that you use one data step to read in your data, and then a separate data step to clean the data, so that you are separating the two operations. SAS DATA STEP - create a new variable We can create our variable a couple of different ways in SAS: - Use the TRANWRD function for find and replace. - Use an if statement and define the replacement ourselves Both are demonstrated below: 2 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 3 4 DATA pvtmp; 5 SET classdat.police; 6 race = tranwrd(victim_s_race, &quot;Race&quot;, &quot;race&quot;); 7 race2 = victim_s_race; /* initialize it to current value */ 8 IF victim_s_race=&#39;Unknown Race&#39; THEN race2 = &#39;Unknown race&#39;; 9 RUN; NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: The data set WORK.PVTMP has 7663 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.02 seconds 10 11 PROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */ 12 TABLES victim_s_race * race victim_s_race * race2 / 13 NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10; 14 RUN; NOTE: There were 7663 observations read from the data set WORK.PVTMP. NOTE: The PROCEDURE FREQ printed page 10. NOTE: PROCEDURE FREQ used (Total process time): real time 0.01 seconds cpu time 0.02 seconds ## The SAS System 1 ## Sunday, May 9, 2021 10:47:00 AM ## ## pokedex_ ## Obs number name status ## ## 1 890 Eternatus Eternamax Legendary ## 2 150 Mega Mewtwo X Legendary ## 3 150 Mega Mewtwo Y Legendary ## 4 384 Mega Rayquaza Legendary ## 5 382 Primal Kyogre Legendary ## ## ## Obs species type_1 total_points ## ## 1 Gigantic Pokemon Poison 1125 ## 2 Genetic Pokemon Psychic 780 ## 3 Genetic Pokemon Psychic 780 ## 4 Sky High Pokemon Dragon 780 ## 5 Sea Basin Pokemon Water 770 ## The SAS System 2 ## Sunday, May 9, 2021 10:47:00 AM ## ## pokedex_ ## number name german_name generation ## status species type_number type_1 type_2 ## abilities_ ## height_m weight_kg number ability_1 ## ability_2 ability_hidden total_points hp ## attack defense sp_attack sp_defense speed ## base_ base_ egg_type_ ## catch_rate friendship experience growth_rate number ## percentage_ against_ ## egg_type_1 egg_type_2 male egg_cycles normal ## against_ against_ against_ ## against_fire water electric grass against_ice ## against_ against_ against_ against_ against_ ## fight poison ground flying psychic ## against_ against_ ## against_bug against_rock ghost dragon against_dark ## against_ against_ ## steel fairy rownum ## --------------------------------------------------------------------------- ## 2 Ivysaur Bisaknosp 1 ## Normal Seed Pokemon 2 Grass Poison ## 1 13 2 Overgrow ## Chlorophyll 405 60 ## 62 63 80 80 60 ## 45 70 142 Medium Slow 2 ## Grass Monster 87.5 20 1 ## 2 0.5 0.5 0.25 2 ## 0.5 1 1 2 2 ## 1 1 1 1 1 ## 1 0.5 2 ## ## 3 Mega Venusaur Bisaflor 1 ## Normal Seed Pokemon 2 Grass Poison ## 2.4 155.5 1 Thick Fat ## 625 80 ## 100 123 122 120 80 ## 45 70 281 Medium Slow 2 ## Grass Monster 87.5 20 1 ## 1 0.5 0.5 0.25 1 ## 0.5 1 1 2 2 ## 1 1 1 1 1 ## 1 0.5 4 ## ## 5 Charmeleon Glutexo 1 ## Normal Flame Pokemon 1 Fire ## 1.1 19 2 Blaze ## Solar Power 405 58 ## 64 58 80 65 80 ## 45 70 142 Medium Slow 2 ## Dragon Monster 87.5 20 1 ## 0.5 2 1 0.5 0.5 ## 1 1 2 1 1 ## 0.5 2 1 1 1 ## 0.5 0.5 6 ## The SAS System 3 ## Sunday, May 9, 2021 10:47:00 AM ## ## pokedex_ ## number name german_name generation ## status species type_number type_1 type_2 ## abilities_ ## height_m weight_kg number ability_1 ## ability_2 ability_hidden total_points hp ## attack defense sp_attack sp_defense speed ## base_ base_ egg_type_ ## catch_rate friendship experience growth_rate number ## percentage_ against_ ## egg_type_1 egg_type_2 male egg_cycles normal ## against_ against_ against_ ## against_fire water electric grass against_ice ## against_ against_ against_ against_ against_ ## fight poison ground flying psychic ## against_ against_ ## against_bug against_rock ghost dragon against_dark ## against_ against_ ## steel fairy rownum ## --------------------------------------------------------------------------- ## 6 Mega Charizard X Glurak 1 ## Normal Flame Pokemon 2 Fire Dragon ## 1.7 110.5 1 Tough Claws ## 634 78 ## 130 111 130 85 100 ## 45 70 285 Medium Slow 2 ## Dragon Monster 87.5 20 1 ## 0.25 1 0.5 0.25 1 ## 1 1 2 1 1 ## 0.5 2 1 2 1 ## 0.5 1 8 ## ## 7 Squirtle Schiggy 1 ## Normal Tiny Turtle Pokemon 1 Water ## 0.5 9 2 Torrent ## Rain Dish 314 44 ## 48 65 50 64 43 ## 45 70 63 Medium Slow 2 ## Monster Water 1 87.5 20 1 ## 0.5 0.5 2 2 0.5 ## 1 1 1 1 1 ## 1 1 1 1 1 ## 0.5 1 10 ## The SAS System 4 ## Sunday, May 9, 2021 10:47:00 AM ## ## pokedex_ ## number name status ## species type_1 total_points ## -------------------------------------------------------------- ## 3 Mega Venusaur Normal ## Seed Pokemon Grass 625 ## ## 3 Venusaur Normal ## Seed Pokemon Grass 525 ## ## 2 Ivysaur Normal ## Seed Pokemon Grass 405 ## ## 1 Bulbasaur Normal ## Seed Pokemon Grass 318 ## ## 4 Charmander Normal ## Lizard Pokemon Fire 309 ## The SAS System 5 ## Sunday, May 9, 2021 10:47:00 AM ## ## pokedex_ ## Obs number name status ## ## 1 3 Mega Venusaur Normal ## 2 3 Venusaur Normal ## 3 2 Ivysaur Normal ## 4 1 Bulbasaur Normal ## 5 4 Charmander Normal ## ## ## Obs species type_1 total_points ## ## 1 Seed Pokemon Grass 625 ## 2 Seed Pokemon Grass 525 ## 3 Seed Pokemon Grass 405 ## 4 Seed Pokemon Grass 318 ## 5 Lizard Pokemon Fire 309 ## The SAS System 6 ## Sunday, May 9, 2021 10:47:00 AM ## ## sched_dep_ ## Obs year month day dep_time time ## ## 1 2013 5 9 1855 1730 ## 2 2013 9 11 1349 1355 ## 3 2013 5 23 2109 1745 ## 4 2013 3 30 1251 1300 ## 5 2013 10 11 1555 1529 ## 6 2013 11 20 659 705 ## 7 2013 1 4 837 830 ## 8 2013 2 12 1900 1855 ## 9 2013 5 20 1137 1140 ## 10 2013 9 22 2136 1930 ## ## ## Obs dep_delay flight tailnum origin ## ## 1 85 419 N203FR LGA ## 2 -6 686 N667AW EWR ## 3 204 785 N3DSAA LGA ## 4 -9 315 N3752 JFK ## 5 26 1246 N78524 EWR ## 6 -6 1107 N3FUAA LGA ## 7 7 313 N4WWAA LGA ## 8 5 4649 N513MQ LGA ## 9 -3 1191 N355JB JFK ## 10 126 3497 N904XJ JFK ## The SAS System 7 ## Sunday, May 9, 2021 10:47:00 AM ## ## sched_arr_ ## Obs arr_time time arr_delay carrier ## ## 1 2116 1956 80 F9 ## 2 1544 1602 -18 US ## 3 2350 2050 180 AA ## 4 1630 1649 -19 DL ## 5 1851 1847 4 UA ## 6 958 955 3 AA ## 7 1044 1015 29 AA ## 8 2042 2100 -18 MQ ## 9 1239 1247 -8 B6 ## 10 2334 2125 129 9E ## ## ## Obs dest air_time distance hour minute ## ## 1 DEN 224 1620 17 30 ## 2 PHX 272 2133 13 55 ## 3 DFW 204 1389 17 45 ## 4 SJU 200 1598 13 0 ## 5 SFO 332 2565 15 29 ## 6 DFW 201 1389 7 5 ## 7 ORD 115 733 8 30 ## 8 MSP 143 1020 18 55 ## 9 ACK 45 199 11 40 ## 10 RDU 68 427 19 30 ## ## ## Obs time_hour ## ## 1 20130509T210000+0000 ## 2 20130911T170000+0000 ## 3 20130523T210000+0000 ## 4 20130330T170000+0000 ## 5 20131011T190000+0000 ## 6 20131120T120000+0000 ## 7 20130104T130000+0000 ## 8 20130212T230000+0000 ## 9 20130520T150000+0000 ## 10 20130922T230000+0000 ## The SAS System 8 ## Sunday, May 9, 2021 10:47:00 AM ## ## Obs flight year month day tailnum origin ## ## 1 419 2013 5 9 N203FR LGA ## 2 686 2013 9 11 N667AW EWR ## 3 785 2013 5 23 N3DSAA LGA ## 4 315 2013 3 30 N3752 JFK ## 5 1246 2013 10 11 N78524 EWR ## 6 1107 2013 11 20 N3FUAA LGA ## 7 313 2013 1 4 N4WWAA LGA ## 8 4649 2013 2 12 N513MQ LGA ## 9 1191 2013 5 20 N355JB JFK ## 10 3497 2013 9 22 N904XJ JFK ## The SAS System 9 ## Sunday, May 9, 2021 10:47:00 AM ## ## sched_arr_ ## Obs arr_time time arr_delay carrier ## ## 1 2116 1956 80 F9 ## 2 1544 1602 -18 US ## 3 2350 2050 180 AA ## 4 1630 1649 -19 DL ## 5 1851 1847 4 UA ## 6 958 955 3 AA ## 7 1044 1015 29 AA ## 8 2042 2100 -18 MQ ## 9 1239 1247 -8 B6 ## 10 2334 2125 129 9E ## ## ## Obs dest air_time distance hour minute ## ## 1 DEN 224 1620 17 30 ## 2 PHX 272 2133 13 55 ## 3 DFW 204 1389 17 45 ## 4 SJU 200 1598 13 0 ## 5 SFO 332 2565 15 29 ## 6 DFW 201 1389 7 5 ## 7 ORD 115 733 8 30 ## 8 MSP 143 1020 18 55 ## 9 ACK 45 199 11 40 ## 10 RDU 68 427 19 30 ## ## ## Obs time_hour ## ## 1 20130509T210000+0000 ## 2 20130911T170000+0000 ## 3 20130523T210000+0000 ## 4 20130330T170000+0000 ## 5 20131011T190000+0000 ## 6 20131120T120000+0000 ## 7 20130104T130000+0000 ## 8 20130212T230000+0000 ## 9 20130520T150000+0000 ## 10 20130922T230000+0000 ## ## ## ## The FREQ Procedure ## ## Table of Victim_s_race by race ## ## Victim_s_race(Victim&#39;s race) race ## ## Frequency |White |Black |Hispanic|Unknown | Total ## | | | |race | ## -----------------+--------+--------+--------+--------+ ## White | 3378 | 0 | 0 | 0 | 3378 ## -----------------+--------+--------+--------+--------+ ## Black | 0 | 1944 | 0 | 0 | 1944 ## -----------------+--------+--------+--------+--------+ ## Hispanic | 0 | 0 | 1335 | 0 | 1335 ## -----------------+--------+--------+--------+--------+ ## Unknown race | 0 | 0 | 0 | 670 | 670 ## -----------------+--------+--------+--------+--------+ ## Asian | 0 | 0 | 0 | 0 | 118 ## -----------------+--------+--------+--------+--------+ ## Native American | 0 | 0 | 0 | 0 | 112 ## -----------------+--------+--------+--------+--------+ ## Unknown Race | 0 | 0 | 0 | 64 | 64 ## -----------------+--------+--------+--------+--------+ ## Pacific Islander | 0 | 0 | 0 | 0 | 42 ## -----------------+--------+--------+--------+--------+ ## Total 3378 1944 1335 734 7663 ## (Continued) ## ## Table of Victim_s_race by race ## ## Victim_s_race(Victim&#39;s race) race ## ## Frequency |Asian |Native A|Pacific | Total ## | |merican |Islander| ## -----------------+--------+--------+--------+ ## White | 0 | 0 | 0 | 3378 ## -----------------+--------+--------+--------+ ## Black | 0 | 0 | 0 | 1944 ## -----------------+--------+--------+--------+ ## Hispanic | 0 | 0 | 0 | 1335 ## -----------------+--------+--------+--------+ ## Unknown race | 0 | 0 | 0 | 670 ## -----------------+--------+--------+--------+ ## Asian | 118 | 0 | 0 | 118 ## -----------------+--------+--------+--------+ ## Native American | 0 | 112 | 0 | 112 ## -----------------+--------+--------+--------+ ## Unknown Race | 0 | 0 | 0 | 64 ## -----------------+--------+--------+--------+ ## Pacific Islander | 0 | 0 | 42 | 42 ## -----------------+--------+--------+--------+ ## Total 118 112 42 7663 ## ## ## Table of Victim_s_race by race2 ## ## Victim_s_race(Victim&#39;s race) race2 ## ## Frequency |White |Black |Hispanic|Unknown | Total ## | | | |race | ## -----------------+--------+--------+--------+--------+ ## White | 3378 | 0 | 0 | 0 | 3378 ## -----------------+--------+--------+--------+--------+ ## Black | 0 | 1944 | 0 | 0 | 1944 ## -----------------+--------+--------+--------+--------+ ## Hispanic | 0 | 0 | 1335 | 0 | 1335 ## -----------------+--------+--------+--------+--------+ ## Unknown race | 0 | 0 | 0 | 670 | 670 ## -----------------+--------+--------+--------+--------+ ## Asian | 0 | 0 | 0 | 0 | 118 ## -----------------+--------+--------+--------+--------+ ## Native American | 0 | 0 | 0 | 0 | 112 ## -----------------+--------+--------+--------+--------+ ## Unknown Race | 0 | 0 | 0 | 64 | 64 ## -----------------+--------+--------+--------+--------+ ## Pacific Islander | 0 | 0 | 0 | 0 | 42 ## -----------------+--------+--------+--------+--------+ ## Total 3378 1944 1335 734 7663 ## (Continued) ## ## Table of Victim_s_race by race2 ## ## Victim_s_race(Victim&#39;s race) race2 ## ## Frequency |Asian |Native A|Pacific | Total ## | |merican |Islander| ## -----------------+--------+--------+--------+ ## White | 0 | 0 | 0 | 3378 ## -----------------+--------+--------+--------+ ## Black | 0 | 0 | 0 | 1944 ## -----------------+--------+--------+--------+ ## Hispanic | 0 | 0 | 0 | 1335 ## -----------------+--------+--------+--------+ ## Unknown race | 0 | 0 | 0 | 670 ## -----------------+--------+--------+--------+ ## Asian | 118 | 0 | 0 | 118 ## -----------------+--------+--------+--------+ ## Native American | 0 | 112 | 0 | 112 ## -----------------+--------+--------+--------+ ## Unknown Race | 0 | 0 | 0 | 64 ## -----------------+--------+--------+--------+ ## Pacific Islander | 0 | 0 | 42 | 42 ## -----------------+--------+--------+--------+ ## Total 118 112 42 7663 In both cases we can see that the recode worked the way we wanted and we’ve now gotten rid of the extra “unknown” category\". We can also use PROC SQL to create new variables using relatively complex logic if necessary. SAS PROC SQL - create a new variable In SQL, you define new variables using AS. In SELECT statements, this definition has the computation on the left and the variable on the right23. CASE WHEN is the if-else statement in SQL. When (victim_s_race = ‘Unknown Race’), our variable value will be “Unknown race,” otherwise it will be what ever value is in victim_s_race. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE WORK.pvtmp AS 10 SELECT * , 11 CASE WHEN victim_s_race=&#39;Unknown Race&#39; THEN &#39;Unknown race&#39; ELSE 11 ! victim_s_race END AS race 12 FROM classdat.police; NOTE: Table WORK.PVTMP created, with 7663 rows and 26 columns. 13 14 NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.02 seconds 15 PROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */ 16 TABLES victim_s_race * race / 17 NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10; 18 RUN; NOTE: There were 7663 observations read from the data set WORK.PVTMP. NOTE: PROCEDURE FREQ used (Total process time): real time 0.02 seconds cpu time 0.02 seconds Frequency Table of Victim_s_race by race Victim_s_race(Victim'srace) race White Black Hispanic Unknown race Asian Native American Pacific Islander Total White 3378 0 0 0 0 0 0 3378 Black 0 1944 0 0 0 0 0 1944 Hispanic 0 0 1335 0 0 0 0 1335 Unknown race 0 0 0 670 0 0 0 670 Asian 0 0 0 0 118 0 0 118 Native American 0 0 0 0 0 112 0 112 Unknown Race 0 0 0 64 0 0 0 64 Pacific Islander 0 0 0 0 0 0 42 42 Total 3378 1944 1335 734 118 112 42 7663 The choice of which method to use (DATA step or PROC SQL) involves weighing these competing factors: computational time code readability programmer time Personally, I find PROC SQL easier to work with, but I think the code is ugly. There are similar sql-syntax packages in R, but I don’t feel the need to use them, because (for me) dplyr code is much easier to read (and thus, easier to maintain). dplyr code is not as efficient as SQL (or other packages, like data.table) on big datasets, but there are variants such as dbplyr to handle some of those cases, and I find that they don’t come up very often in my work or research. If I were working at Google or Amazon, my opinion might be very different The fundamentals of mutate are very similar to the approaches above; the power of the dplyr approach is only really evident when you are doing multiple operations in the same step. Once you’re working at that level, the dplyr approach produces much more readable code. Mutate (by Allison Horst) mutate() a new variable # The data was read in above... library(dplyr) police_violence %&gt;% mutate(race = gsub(&quot;Race&quot;, &quot;race&quot;, `Victim&#39;s race`)) %&gt;% select(`Victim&#39;s race`, race) %&gt;% table() ## race ## Victim&#39;s race Asian Black Hispanic Native American Pacific Islander ## Asian 118 0 0 0 0 ## Black 0 1944 0 0 0 ## Hispanic 0 0 1335 0 0 ## Native American 0 0 0 112 0 ## Pacific Islander 0 0 0 0 42 ## Unknown race 0 0 0 0 0 ## Unknown Race 0 0 0 0 0 ## White 0 0 0 0 0 ## race ## Victim&#39;s race Unknown race White ## Asian 0 0 ## Black 0 0 ## Hispanic 0 0 ## Native American 0 0 ## Pacific Islander 0 0 ## Unknown race 670 0 ## Unknown Race 64 0 ## White 0 3378 The last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab like PROC FREQ gave us in SAS. The learning curve here isn’t actually knowing how to use mutate (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. I’m not going to be able to teach you how to handle every task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.). It’s actually something of a common meme… In this class, my goal is to expose you to solutions to common problems; unfortunately, there are too many common problems for us to work through line-by-line. Part of the goal of this class is for you to learn how to read through a package description and evaluate whether the package will do what you want; we’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own. Figure 6.1: Apologies for the noninclusive language, but the sentiment is real. Here is a quick list of packages in R which will solve some of the more common problems. Between that and the R cheatsheet, you should be set. In SAS, there are fewer options, so it’s less bewildering to google solutions (but I’ll link you to relevant pieces for the common SAS stuff too). Dates and times: lubridate package in R (esp. ymd_hms() and variants, decimal_date(), and other convenience functions). SAS Dates and Times. String manipulation: stringr package in R (str_replace(), str_remove(), str_detect(), str_split()) Regular Expression Cheatsheet (R) Common String operations in SAS Regular Expressions in SAS We’ll talk more about strings in the next module… 6.5 Summarize The next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and in SAS, PROC MEANS is essentially the same thing. Here (in a trivial example), I compute the overall average age of a victim of police violence, and then also compute the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes. police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) police_violence %&gt;% mutate(age = as.numeric(`Victim&#39;s age`), name_length = nchar(`Victim&#39;s name`)) %&gt;% summarize(age = mean(age, na.rm = T), name_length = mean(name_length)) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion ## # A tibble: 1 x 2 ## age name_length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 36.8 16.6 In SAS, we can do something similar: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA pv; 9 SET classdat.police; 10 age = INPUT(victim_s_age, 3.); 11 name_len = LENGTH(victim_s_name); 12 RUN; NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=13600 Vanowen St City=Van Nuys State=CA Zipcode=91405 County=Los Angeles Agency_responsible_for_death=Los Angeles Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=4340 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6567 date=12/31/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1099 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=5345 Memorial Drive City=Stone Mountain State=GA Zipcode=30083 County=DeKalb Agency_responsible_for_death=Pine Lake Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_=Off-Duty Geography__via_Trulia_methodolog=Suburban ID=6566 date=12/31/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1101 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Nathan Shepard Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=3373 Alice Hall Rd City=Golden State=MS Zipcode=38847 County=Itawamba Agency_responsible_for_death=Itawamba County Sheriff&#39;s Office, Mississippi Bureau of Investigation, Mississippi Highway Patrol Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6559 date=12/29/2018 num_age=. age=. name_len=14 _ERROR_=1 _N_=1108 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Chaney St and Collier Ave City=Lake Elsinore State=CA Zipcode=92530 County=Riverside Agency_responsible_for_death=Riverside County Sheriff&#39;s Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=4319 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6538 date=12/20/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1128 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=N 36th St &amp; E Monte Vista Rd City=Phoenix State=AZ Zipcode=85008 County=Maricopa Agency_responsible_for_death=Phoenix Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Unclear VAR20=undetermined Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=4313 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6537 date=12/19/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1131 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=N Benson Ave &amp; W 11th St City=Upland State=CA Zipcode=91786 County=San Bernardino Agency_responsible_for_death=Upland Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unarmed VAR20=toy Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6515 date=12/13/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1155 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=White Street_Address_of_Incident= City=Spanaway State=WA Zipcode= County= Agency_responsible_for_death= Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_= Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Foot VAR23=No WaPo_ID__If_included_in_WaPo_dat=4019 Off_Duty_Killing_= Geography__via_Trulia_methodolog=#N/A ID=6235 date=09/10/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1431 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=10900 Neiderhouse Rd City=Perrysburg State=OH Zipcode=43551 County=Wood Agency_responsible_for_death=Perrysburg Township Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22=Foot VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6203 date=08/27/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1465 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=8801 Airline Dr City=Houston State=TX Zipcode=77037 County=Harris Agency_responsible_for_death=Harris County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6198 date=08/27/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1466 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=202 E First Street City=Santa Ana State=CA Zipcode=92701 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6182 date=08/23/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1485 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=Kings Canyon Rd and Chestnut Ave City=Fresno State=CA Zipcode=93702 County=Fresno Agency_responsible_for_death=Fresno Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun and knife Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=3967 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6162 date=08/14/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1505 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=21000 Neely Dr City=Flint State=TX Zipcode=75762 County=Smith Agency_responsible_for_death=Smith County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6154 date=08/12/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1512 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=SW 24th Ave &amp; SW 21st St City=Okeechobee State=FL Zipcode=34974 County=Okeechobee Agency_responsible_for_death=Okeechobee County Sheriff&#39;s Office, Okeechobee Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6135 date=08/06/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1532 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=2400 W 65th Ave City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Aurora Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Vehicle VAR20=vehicle Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=3900 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6107 date=07/28/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1561 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=11900 Canal St City=Willis State=TX Zipcode=77318 County=Montgomery Agency_responsible_for_death=Montgomery County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3852 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6044 date=07/11/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1621 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Magnolia Street and Bolsa Avenue City=Westminster State=CA Zipcode=92683 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3829 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6037 date=07/08/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1629 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=W 70th Ave and Broadway City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Adams County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unknown weapon Alleged_Threat_Level__Source__Wa=attack VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3835 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6036 date=07/07/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1632 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=1020 West Civic Center Dr City=Santa Ana State=CA Zipcode=92703 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=metal object Alleged_Threat_Level__Source__Wa=undetermined VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3814 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6018 date=07/01/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1647 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Jason Erik Washington Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Black Street_Address_of_Incident=1939 SW 6th Ave City=Portland State=OR Zipcode=97201 County=Multnomah Agency_responsible_for_death=Portland State University Department of Public Safety Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=undetermined VAR22= VAR23=No WaPo_ID__If_included_in_WaPo_dat=3815 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6010 date=06/29/2018 num_age=. age=. name_len=21 _ERROR_=1 _N_=1654 NOTE: Invalid argument to function INPUT at line 10 column 9. WARNING: Limit set by ERRORS= option reached. Further errors of this type will not be printed. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=4860 Rolando Ct City=San Diego State=CA Zipcode=92115 County=San Diego Agency_responsible_for_death=San Diego Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3800 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=5994 date=06/23/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1668 NOTE: Mathematical operations could not be performed at the following places. The results of the operations have been set to missing values. Each place is given by: (Number of times) at (Line):(Column). 139 at 10:9 NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: The data set WORK.PV has 7663 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 13 14 PROC MEANS DATA=pv; 15 VAR age name_len; 16 RUN; NOTE: There were 7663 observations read from the data set WORK.PV. NOTE: PROCEDURE MEANS used (Total process time): real time 0.01 seconds cpu time 0.02 seconds Variable N Mean Std Dev Minimum Maximum age name_len 7457 7663 36.7964329 16.6259950 13.2086517 4.5027632 1.0000000 7.0000000 107.0000000 49.0000000 By default, with SAS, we get a bit more than we bargained for; we can turn the extra output off with options. Another option is to use PROC SQL in SAS, which will have a logical flow similar to dplyr. 6 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 8 9 DATA pv; 10 SET classdat.police; 11 age = INPUT(victim_s_age, 3.); 12 name_len = LENGTH(victim_s_name); 13 RUN; NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=13600 Vanowen St City=Van Nuys State=CA Zipcode=91405 County=Los Angeles Agency_responsible_for_death=Los Angeles Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=4340 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6567 date=12/31/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1099 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=5345 Memorial Drive City=Stone Mountain State=GA Zipcode=30083 County=DeKalb Agency_responsible_for_death=Pine Lake Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_=Off-Duty Geography__via_Trulia_methodolog=Suburban ID=6566 date=12/31/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1101 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Nathan Shepard Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=3373 Alice Hall Rd City=Golden State=MS Zipcode=38847 County=Itawamba Agency_responsible_for_death=Itawamba County Sheriff&#39;s Office, Mississippi Bureau of Investigation, Mississippi Highway Patrol Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6559 date=12/29/2018 num_age=. age=. name_len=14 _ERROR_=1 _N_=1108 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Chaney St and Collier Ave City=Lake Elsinore State=CA Zipcode=92530 County=Riverside Agency_responsible_for_death=Riverside County Sheriff&#39;s Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=4319 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6538 date=12/20/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1128 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=N 36th St &amp; E Monte Vista Rd City=Phoenix State=AZ Zipcode=85008 County=Maricopa Agency_responsible_for_death=Phoenix Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Unclear VAR20=undetermined Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=4313 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6537 date=12/19/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1131 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=N Benson Ave &amp; W 11th St City=Upland State=CA Zipcode=91786 County=San Bernardino Agency_responsible_for_death=Upland Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unarmed VAR20=toy Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6515 date=12/13/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1155 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=White Street_Address_of_Incident= City=Spanaway State=WA Zipcode= County= Agency_responsible_for_death= Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_= Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Foot VAR23=No WaPo_ID__If_included_in_WaPo_dat=4019 Off_Duty_Killing_= Geography__via_Trulia_methodolog=#N/A ID=6235 date=09/10/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1431 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=10900 Neiderhouse Rd City=Perrysburg State=OH Zipcode=43551 County=Wood Agency_responsible_for_death=Perrysburg Township Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22=Foot VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6203 date=08/27/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1465 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=8801 Airline Dr City=Houston State=TX Zipcode=77037 County=Harris Agency_responsible_for_death=Harris County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6198 date=08/27/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1466 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=202 E First Street City=Santa Ana State=CA Zipcode=92701 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6182 date=08/23/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1485 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=Kings Canyon Rd and Chestnut Ave City=Fresno State=CA Zipcode=93702 County=Fresno Agency_responsible_for_death=Fresno Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun and knife Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=3967 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6162 date=08/14/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1505 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=21000 Neely Dr City=Flint State=TX Zipcode=75762 County=Smith Agency_responsible_for_death=Smith County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6154 date=08/12/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1512 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=SW 24th Ave &amp; SW 21st St City=Okeechobee State=FL Zipcode=34974 County=Okeechobee Agency_responsible_for_death=Okeechobee County Sheriff&#39;s Office, Okeechobee Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6135 date=08/06/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1532 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=2400 W 65th Ave City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Aurora Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Vehicle VAR20=vehicle Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=3900 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6107 date=07/28/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1561 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=11900 Canal St City=Willis State=TX Zipcode=77318 County=Montgomery Agency_responsible_for_death=Montgomery County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3852 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6044 date=07/11/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1621 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Magnolia Street and Bolsa Avenue City=Westminster State=CA Zipcode=92683 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3829 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6037 date=07/08/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1629 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=W 70th Ave and Broadway City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Adams County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unknown weapon Alleged_Threat_Level__Source__Wa=attack VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3835 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6036 date=07/07/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1632 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=1020 West Civic Center Dr City=Santa Ana State=CA Zipcode=92703 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=metal object Alleged_Threat_Level__Source__Wa=undetermined VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3814 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6018 date=07/01/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1647 NOTE: Invalid argument to function INPUT at line 11 column 9. Victim_s_name=Jason Erik Washington Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Black Street_Address_of_Incident=1939 SW 6th Ave City=Portland State=OR Zipcode=97201 County=Multnomah Agency_responsible_for_death=Portland State University Department of Public Safety Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=undetermined VAR22= VAR23=No WaPo_ID__If_included_in_WaPo_dat=3815 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6010 date=06/29/2018 num_age=. age=. name_len=21 _ERROR_=1 _N_=1654 NOTE: Invalid argument to function INPUT at line 11 column 9. WARNING: Limit set by ERRORS= option reached. Further errors of this type will not be printed. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=4860 Rolando Ct City=San Diego State=CA Zipcode=92115 County=San Diego Agency_responsible_for_death=San Diego Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3800 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=5994 date=06/23/2018 num_age=. age=. name_len=23 _ERROR_=1 _N_=1668 NOTE: Mathematical operations could not be performed at the following places. The results of the operations have been set to missing values. Each place is given by: (Number of times) at (Line):(Column). 139 at 11:9 NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: The data set WORK.PV has 7663 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 14 15 PROC SQL; 16 SELECT AVG(age) as age, AVG(name_len) as name_len 17 FROM pv; NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.01 seconds age name_len 36.79643 16.626 The real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail. 6.6 Group By + (?) = Power! Frequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to split my dataset into smaller datasets - one for each day compute summary values for each smaller dataset put my summarized data back together into a single dataset group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well. Replicating frequency tables using dplyr Let’s start with a trivial example: Suppose we want to count up every occurrence of a variable in a dataset. We can already do this with e.g. table(), but work with me for a moment. pv &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) %&gt;% mutate(race = gsub(&quot;Race&quot;, &quot;race&quot;, `Victim&#39;s race`), age = as.numeric(`Victim&#39;s age`)) %&gt;% select(name = `Victim&#39;s name`, age, gender = `Victim&#39;s gender`, race) ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion # You can rename variables with a select statement # I&#39;m doing this b/c I don&#39;t like to use backticks if I can help it. # Lazy coding = best coding. grouped_pv &lt;- pv %&gt;% group_by(race) grouped_pv ## # A tibble: 7,663 x 4 ## # Groups: race [7] ## name age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Eric M. Tellez 28 Male White ## 2 Name withheld by police NA Male Unknown race ## 3 Terry Hudson 57 Male Black ## 4 Malik Williams 23 Male Black ## 5 Frederick Perkins 37 Male Black ## 6 Michael Vincent Davis 49 Male White ## 7 Brian Elkins 47 Male Unknown race ## 8 Debra D. Arbuckle 51 Female White ## 9 Name withheld by police NA Male Unknown race ## 10 Cody McCaulou 27 Male White ## # … with 7,653 more rows So we can see that the object has been somehow grouped by the categorical variable race, in that the grouping is attached to the stored object (strictly speaking, group_by adds an attribute to the table). What matters for our purposes, though, is that each sub-table is treated as a separate entity for calculation purposes. pv_race_sum &lt;- grouped_pv %&gt;% summarize(n = n()) # This counts the number of rows in each group pv_race_sum ## # A tibble: 7 x 2 ## race n ## &lt;chr&gt; &lt;int&gt; ## 1 Asian 118 ## 2 Black 1944 ## 3 Hispanic 1335 ## 4 Native American 112 ## 5 Pacific Islander 42 ## 6 Unknown race 734 ## 7 White 3378 When we run summarize, we get back a data frame that is not grouped, with one line for each of the previously existing groups. summarize removes one “layer” of grouping with each run. One layer of grouping? What does that mean? tmp &lt;- pv %&gt;% group_by(gender, race) tmp ## # A tibble: 7,663 x 4 ## # Groups: gender, race [26] ## name age gender race ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Eric M. Tellez 28 Male White ## 2 Name withheld by police NA Male Unknown race ## 3 Terry Hudson 57 Male Black ## 4 Malik Williams 23 Male Black ## 5 Frederick Perkins 37 Male Black ## 6 Michael Vincent Davis 49 Male White ## 7 Brian Elkins 47 Male Unknown race ## 8 Debra D. Arbuckle 51 Female White ## 9 Name withheld by police NA Male Unknown race ## 10 Cody McCaulou 27 Male White ## # … with 7,653 more rows tmp %&gt;% summarize(min_age = min(age, na.rm = T), max_age = max(age, na.rm = T)) ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf ## `summarise()` has grouped output by &#39;gender&#39;. You can override using the `.groups` argument. ## # A tibble: 26 x 4 ## # Groups: gender [5] ## gender race min_age max_age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female Asian 27 49 ## 2 Female Black 20 93 ## 3 Female Hispanic 16 57 ## 4 Female Native American 23 52 ## 5 Female Pacific Islander 26 50 ## 6 Female Unknown race 23 74 ## 7 Female White 5 89 ## 8 Male Asian 16 76 ## 9 Male Black 1 107 ## 10 Male Hispanic 1 80 ## # … with 16 more rows Apart from some warnings about how it’s hard to take the minimum or maximum of a bunch of missing data (which is fair), we can see a message: summarise() regrouping output by ‘gender’ (override with ‘.groups’ argument) What this message is saying is that it is essentially dropping one layer of grouping (race) and grouping only by gender – but it’s also nice enough to tell you that you can override the default option if you want to do so by using the .groups argument. ?summarize24 gives you several options for how to handle the grouping of the result. We grouped pv by gender and race, then ran summarize, which created one row for each combination of gender and race and “glued” them together. The resulting data frame is still grouped by gender, but because there’s only one row for each race (for each level/group of gender), there’s no reason to have that level of grouping anymore. So it’s dropped by default. Replicating PROC FREQ using PROC SQL 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 NOTE: The PROCEDURE SQL printed page 14. NOTE: PROCEDURE SQL used (Total process time): real time 0.06 seconds cpu time 0.05 seconds 8 DATA pv; 9 SET classdat.police; 10 age = INPUT(victim_s_age, 3.); 11 IF victim_s_gender=&#39; &#39; THEN victim_s_gender=&#39;Unknown&#39;; 12 race = victim_s_race; /* initialize it to current value */ 13 IF victim_s_race=&#39;Unknown Race&#39; THEN race = &#39;Unknown race&#39;; 14 RUN; NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=13600 Vanowen St City=Van Nuys State=CA Zipcode=91405 County=Los Angeles Agency_responsible_for_death=Los Angeles Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=4340 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6567 date=12/31/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1099 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=5345 Memorial Drive City=Stone Mountain State=GA Zipcode=30083 County=DeKalb Agency_responsible_for_death=Pine Lake Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_=Off-Duty Geography__via_Trulia_methodolog=Suburban ID=6566 date=12/31/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1101 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Nathan Shepard Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=3373 Alice Hall Rd City=Golden State=MS Zipcode=38847 County=Itawamba Agency_responsible_for_death=Itawamba County Sheriff&#39;s Office, Mississippi Bureau of Investigation, Mississippi Highway Patrol Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6559 date=12/29/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1108 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Chaney St and Collier Ave City=Lake Elsinore State=CA Zipcode=92530 County=Riverside Agency_responsible_for_death=Riverside County Sheriff&#39;s Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=4319 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6538 date=12/20/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1128 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=N 36th St &amp; E Monte Vista Rd City=Phoenix State=AZ Zipcode=85008 County=Maricopa Agency_responsible_for_death=Phoenix Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Unclear VAR20=undetermined Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=4313 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6537 date=12/19/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1131 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=N Benson Ave &amp; W 11th St City=Upland State=CA Zipcode=91786 County=San Bernardino Agency_responsible_for_death=Upland Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unarmed VAR20=toy Alleged_Threat_Level__Source__Wa=attack VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6515 date=12/13/2018 num_age=. age=. race=Hispanic _ERROR_=1 _N_=1155 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=White Street_Address_of_Incident= City=Spanaway State=WA Zipcode= County= Agency_responsible_for_death= Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_= Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Foot VAR23=No WaPo_ID__If_included_in_WaPo_dat=4019 Off_Duty_Killing_= Geography__via_Trulia_methodolog=#N/A ID=6235 date=09/10/2018 num_age=. age=. race=White _ERROR_=1 _N_=1431 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=10900 Neiderhouse Rd City=Perrysburg State=OH Zipcode=43551 County=Wood Agency_responsible_for_death=Perrysburg Township Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22=Foot VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6203 date=08/27/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1465 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=8801 Airline Dr City=Houston State=TX Zipcode=77037 County=Harris Agency_responsible_for_death=Harris County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6198 date=08/27/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1466 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=202 E First Street City=Santa Ana State=CA Zipcode=92701 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unclear Alleged_Threat_Level__Source__Wa= VAR22= VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6182 date=08/23/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1485 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Hispanic Street_Address_of_Incident=Kings Canyon Rd and Chestnut Ave City=Fresno State=CA Zipcode=93702 County=Fresno Agency_responsible_for_death=Fresno Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun and knife Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=Yes WaPo_ID__If_included_in_WaPo_dat=3967 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6162 date=08/14/2018 num_age=. age=. race=Hispanic _ERROR_=1 _N_=1505 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=21000 Neely Dr City=Flint State=TX Zipcode=75762 County=Smith Agency_responsible_for_death=Smith County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6154 date=08/12/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1512 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=SW 24th Ave &amp; SW 21st St City=Okeechobee State=FL Zipcode=34974 County=Okeechobee Agency_responsible_for_death=Okeechobee County Sheriff&#39;s Office, Okeechobee Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Unknown Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23= WaPo_ID__If_included_in_WaPo_dat=. Off_Duty_Killing_= Geography__via_Trulia_methodolog=Rural ID=6135 date=08/06/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1532 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=2400 W 65th Ave City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Aurora Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Vehicle VAR20=vehicle Alleged_Threat_Level__Source__Wa=attack VAR22=Car VAR23=No WaPo_ID__If_included_in_WaPo_dat=3900 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6107 date=07/28/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1561 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Female Victim_s_race=Unknown race Street_Address_of_Incident=11900 Canal St City=Willis State=TX Zipcode=77318 County=Montgomery Agency_responsible_for_death=Montgomery County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=Yes Unarmed=Allegedly Armed VAR20=knife Alleged_Threat_Level__Source__Wa=other VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3852 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6044 date=07/11/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1621 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=Magnolia Street and Bolsa Avenue City=Westminster State=CA Zipcode=92683 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=other VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3829 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6037 date=07/08/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1629 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=W 70th Ave and Broadway City=Denver State=CO Zipcode=80221 County=Adams Agency_responsible_for_death=Adams County Sheriff&#39;s Office Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=unknown weapon Alleged_Threat_Level__Source__Wa=attack VAR22=Other VAR23=No WaPo_ID__If_included_in_WaPo_dat=3835 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Suburban ID=6036 date=07/07/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1632 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=1020 West Civic Center Dr City=Santa Ana State=CA Zipcode=92703 County=Orange Agency_responsible_for_death=Santa Ana Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Unclear VAR20=metal object Alleged_Threat_Level__Source__Wa=undetermined VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3814 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6018 date=07/01/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1647 NOTE: Invalid argument to function INPUT at line 10 column 9. Victim_s_name=Jason Erik Washington Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Black Street_Address_of_Incident=1939 SW 6th Ave City=Portland State=OR Zipcode=97201 County=Multnomah Agency_responsible_for_death=Portland State University Department of Public Safety Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=undetermined VAR22= VAR23=No WaPo_ID__If_included_in_WaPo_dat=3815 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=6010 date=06/29/2018 num_age=. age=. race=Black _ERROR_=1 _N_=1654 NOTE: Invalid argument to function INPUT at line 10 column 9. WARNING: Limit set by ERRORS= option reached. Further errors of this type will not be printed. Victim_s_name=Name withheld by police Victim_s_age=Unknown Victim_s_gender=Male Victim_s_race=Unknown race Street_Address_of_Incident=4860 Rolando Ct City=San Diego State=CA Zipcode=92115 County=San Diego Agency_responsible_for_death=San Diego Police Department Cause_of_death=Gunshot Official_disposition_of_death__j=Pending investigation Criminal_Charges_=No known charges Symptoms_of_mental_illness_=No Unarmed=Allegedly Armed VAR20=gun Alleged_Threat_Level__Source__Wa=attack VAR22=Not fleeing VAR23=No WaPo_ID__If_included_in_WaPo_dat=3800 Off_Duty_Killing_= Geography__via_Trulia_methodolog=Urban ID=5994 date=06/23/2018 num_age=. age=. race=Unknown race _ERROR_=1 _N_=1668 NOTE: Mathematical operations could not be performed at the following places. The results of the operations have been set to missing values. Each place is given by: (Number of times) at (Line):(Column). 139 at 10:9 NOTE: There were 7663 observations read from the data set CLASSDAT.POLICE. NOTE: The data set WORK.PV has 7663 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 15 16 PROC SQL; 17 SELECT victim_s_gender AS gender, race, count(*) AS n 18 FROM pv 19 GROUP BY race; NOTE: The query requires remerging summary statistics back with the original data. 20 NOTE: PROCEDURE SQL used (Total process time): real time 1.37 seconds cpu time 1.37 seconds 21 PROC SQL; 22 SELECT victim_s_gender AS gender, race, count(*) AS n, 22 ! min(age) AS min_age, max(age) AS max_age 23 FROM pv 24 GROUP BY gender, race; NOTE: PROCEDURE SQL used (Total process time): real time 0.03 seconds cpu time 0.03 seconds Victim's gender race n Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Unknown Asian 118 Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Female Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Asian 118 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Transgender Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Unknown Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Unknown Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Transgender Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Female Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Black 1944 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Unknown Hispanic 1335 Unknown Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Transgender Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Female Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Male Hispanic 1335 Unknown Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Female Native American 112 Female Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Female Native American 112 Female Native American 112 Male Native American 112 Male Native American 112 Female Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Female Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Male Native American 112 Female Native American 112 Male Native American 112 Male Native American 112 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Female Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Female Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Pacific Islander 42 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Unknown Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Unknown Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Unknown Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Transgender Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Unknown Unknown race 734 Female Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male Unknown race 734 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Unknown White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Transgender White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Unknown White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Transgender White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Transgender White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Female White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Male White 3378 Victim's gender race n min_age max_age Female Asian 6 27 49 Female Black 69 20 93 Female Hispanic 49 16 57 Female Native American 7 23 52 Female Pacific Islander 2 26 50 Female Unknown race 35 23 74 Female White 223 5 89 Male Asian 111 16 76 Male Black 1871 1 107 Male Hispanic 1283 1 80 Male Native American 104 14 61 Male Pacific Islander 40 15 60 Male Unknown race 694 15 89 Male White 3150 1 95 Transgender Black 2 27 30 Transgender Hispanic 1 28 28 Transgender Unknown race 1 32 32 Transgender White 3 21 24 Unknown Asian 1 25 25 Unknown Black 2 29 29 Unknown Hispanic 2 . . Unknown Native American 1 . . Unknown Unknown race 4 . . Unknown White 2 . . Let’s try a non-trivial example, using the storms dataset that is part of the dplyr package: Reading in the data (R and SAS) library(dplyr) library(lubridate) # for the make_datetime() function ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union data(storms) storms ## # A tibble: 10,010 x 13 ## name year month day hour lat long status category wind pressure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropical d… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropical d… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropical d… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropical d… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropical d… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropical d… -1 25 1012 ## 7 Amy 1975 6 28 12 33.3 -78 tropical d… -1 25 1011 ## 8 Amy 1975 6 28 18 34 -77 tropical d… -1 30 1006 ## 9 Amy 1975 6 29 0 34.4 -75.8 tropical s… 0 35 1004 ## 10 Amy 1975 6 29 6 34 -74.8 tropical s… 0 40 1002 ## # … with 10,000 more rows, and 2 more variables: ts_diameter &lt;dbl&gt;, ## # hu_diameter &lt;dbl&gt; storms &lt;- storms %&gt;% # Construct a time variable that behaves like a number but is formatted as a date mutate(time = make_datetime(year, month, day, hour)) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 filename fileloc &#39;data/storms.csv&#39;; NOTE: The PROCEDURE SQL printed page 157. NOTE: PROCEDURE SQL used (Total process time): real time 0.07 seconds cpu time 0.07 seconds 8 PROC IMPORT datafile = fileloc out=classdat.storms REPLACE 9 DBMS = csv; 9 ! /* comma delimited file */ 10 GUESSINGROWS=500; 11 GETNAMES = YES; 12 RUN; 13 /************************************************************** 13 ! ******** 14 * PRODUCT: SAS 15 * VERSION: 9.4 16 * CREATOR: External File Interface 17 * DATE: 09MAY21 18 * DESC: Generated SAS Datastep Code 19 * TEMPLATE SOURCE: (None Specified.) 20 *************************************************************** 20 ! ********/ 21 data CLASSDAT.STORMS ; 22 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 22 ! */ 23 infile FILELOC delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 24 informat name $9. ; 25 informat year best32. ; 26 informat month best32. ; 27 informat day best32. ; 28 informat hour best32. ; 29 informat lat best32. ; 30 informat long best32. ; 31 informat status $19. ; 32 informat category best32. ; 33 informat wind best32. ; 34 informat pressure best32. ; 35 informat ts_diameter best32. ; 36 informat hu_diameter best32. ; 37 format name $9. ; 38 format year best12. ; 39 format month best12. ; 40 format day best12. ; 41 format hour best12. ; 42 format lat best12. ; 43 format long best12. ; 44 format status $19. ; 45 format category best12. ; 46 format wind best12. ; 47 format pressure best12. ; 48 format ts_diameter best12. ; 49 format hu_diameter best12. ; 50 input 51 name $ 52 year 53 month 54 day 55 hour 56 lat 57 long 58 status $ 59 category 60 wind 61 pressure 62 ts_diameter 63 hu_diameter 64 ; 65 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 65 ! detection macro variable */ 66 run; NOTE: The infile FILELOC is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/sto rms.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=21Jun2020:16:26:50, File Size (bytes)=630591 NOTE: 10010 records were read from the infile FILELOC. The minimum record length was 46. The maximum record length was 90. NOTE: The data set CLASSDAT.STORMS has 10010 observations and 13 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.04 seconds 10010 rows created in CLASSDAT.STORMS from FILELOC. NOTE: CLASSDAT.STORMS data set was successfully created. NOTE: The data set CLASSDAT.STORMS has 10010 observations and 13 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 0.31 seconds cpu time 0.30 seconds 67 68 DATA classdat.storms; 69 SET classdat.storms; 70 date = MDY(month, day, year); 71 time = DHMS(date, hour, 0, 0); 72 FORMAT time DATETIME.; 73 RUN; NOTE: There were 10010 observations read from the data set CLASSDAT.STORMS. NOTE: The data set CLASSDAT.STORMS has 10010 observations and 15 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds We have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes). One thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way: The points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause). If there’s a tie, we might want to know when the maximum wind speed occurred. If that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred. group_by + filter + summary in R max_power_storm &lt;- storms %&gt;% # Storm names can be reused, so we need to have year to be sure it&#39;s the same instance group_by(name, year) %&gt;% filter(pressure == min(pressure, na.rm = T)) %&gt;% filter(wind == max(wind, na.rm = T)) %&gt;% summarize(pressure = mean(pressure), wind = mean(wind), category = unique(category), status = unique(status), time = mean(time)) %&gt;% arrange(time) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;name&#39;. You can override using the `.groups` argument. max_power_storm ## # A tibble: 426 x 7 ## name year pressure wind category status time ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 Amy 1975 981 60 0 tropical storm 1975-07-02 12:00:00 ## 2 Caroline 1975 963 100 3 hurricane 1975-08-31 06:00:00 ## 3 Doris 1975 965 95 2 hurricane 1975-09-02 21:00:00 ## 4 Belle 1976 957 105 3 hurricane 1976-08-09 00:00:00 ## 5 Gloria 1976 970 80 1 hurricane 1976-09-30 00:00:00 ## 6 Anita 1977 926 150 5 hurricane 1977-09-02 06:00:00 ## 7 Clara 1977 993 65 1 hurricane 1977-09-08 12:00:00 ## 8 Evelyn 1977 994 65 1 hurricane 1977-10-15 00:00:00 ## 9 Amelia 1978 1005 45 0 tropical storm 1978-07-31 00:00:00 ## 10 Bess 1978 1005 40 0 tropical storm 1978-08-07 12:00:00 ## # … with 416 more rows If we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm. library(ggplot2) ggplot(max_power_storm, aes(x = pressure)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We could also look to see whether there has been any change over time in pressure. ggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point() It seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic (see this coverage map from 1995). Proc SQL in SAS In SAS, this is going to require some work. Specifically, while dplyr commands are stated in recipe order (do this, then this), SQL statements… aren’t. WHERE comes after SELECT xxx FROM yyy, and GROUP BY comes after that again. There are a couple of ways to handle that: sub-queries, and creating temporary tables. I think the temporary tables approach will be easier to demonstrate, read, and understand, so lets go with that. Another challenge will be the fact that SAS PROC SQL doesn’t handle missing data quite as easily as dplyr does (na.rm is a very nice function, all things considered). We can think through the steps we need to take: 1. Create a table where wind and pressure observations aren’t missing. We’ll call that tmp1. 2. Filter tmp1, keeping only rows with minimum pressure and maximum wind for each storm/year combination (HAVING is like WHERE, but after the GROUP BY clause has been applied). We’ll call that tmp2. We can also select the variables we care about in this step. 3. Summarize tmp2, keeping columns name, year, pressure, wind, category, status, and time, where time is the mean of all maximum-power observations. The other variables should have only one value each. We can accomplish this task using the combination of SELECT and DISTINCT. DISTINCT says “keep only rows with new combinations of these values.” (Note also that we can format values inline in proc SQL Select statements. That forces SAS to treat time as a date-time variable, which will force it to format correctly in e.g. plots.) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 9 PROC SQL; 10 CREATE TABLE tmp1 AS 11 SELECT * 12 FROM classdat.storms 13 WHERE (NOT missing(pressure)) AND (NOT missing(wind)); NOTE: Table WORK.TMP1 created, with 10010 rows and 15 columns. 14 15 CREATE TABLE tmp2 AS 16 SELECT name, year, pressure, wind, category, status, time, 17 min(pressure) AS minpressure, max(wind) AS maxwind 18 FROM tmp1 19 GROUP BY year, name 20 HAVING pressure = minpressure AND wind = maxwind; NOTE: The query requires remerging summary statistics back with the original data. NOTE: Table WORK.TMP2 created, with 620 rows and 9 columns. 21 22 CREATE TABLE maxpwr AS 23 SELECT DISTINCT name, year, pressure, wind, category, status, 24 mean(time) AS time format=DATETIME. 25 FROM tmp2 26 GROUP BY year, name; NOTE: The query requires remerging summary statistics back with the original data. NOTE: Table WORK.MAXPWR created, with 381 rows and 7 columns. 27 28 QUIT; NOTE: PROCEDURE SQL used (Total process time): real time 0.05 seconds cpu time 0.06 seconds 29 30 PROC PRINT DATA=maxpwr (obs=5); 31 RUN; NOTE: There were 5 observations read from the data set WORK.MAXPWR. NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 32 33 ODS GRAPHICS ON; 34 ODS TRACE ON; /* this allows us to select only the plot and not 34 ! tables */ 35 ODS SELECT HISTOGRAM; 36 PROC UNIVARIATE DATA=maxpwr; 37 VAR pressure; 38 HISTOGRAM; 39 RUN; Output Added: ------------- Name: Histogram Label: Panel 1 Template: base.univariate.Graphics.Histogram Path: Univariate.pressure.Histogram.Histogram ------------- NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 3.91 seconds cpu time 0.15 seconds 40 ODS TRACE OFF; 41 42 PROC SGPLOT DATA=maxpwr; 43 scatter X = time Y = pressure; 44 RUN; NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.47 seconds cpu time 0.05 seconds NOTE: The column format DATETIME16 is replaced by an auto-generated format on the axis. NOTE: There were 381 observations read from the data set WORK.MAXPWR. 45 46 ODS GRAPHICS OFF; 47 QUIT; Obs name year pressure wind category status time 1 AL011993 1993 999 30 -1 tropical depression 01JUN93:21:00:00 2 AL012000 2000 1008 25 -1 tropical depression 07JUN00:18:00:00 3 AL021992 1992 1007 30 -1 tropical depression 26JUN92:03:00:00 4 AL021994 1994 1015 30 -1 tropical depression 20JUL94:13:00:00 5 AL021999 1999 1004 30 -1 tropical depression 03JUL99:02:00:00 Another interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time? storm_strength_duration &lt;- storms %&gt;% group_by(name, year) %&gt;% summarize(duration = difftime(max(time), min(time), units = &quot;days&quot;), max_strength = max(category)) %&gt;% ungroup() %&gt;% arrange(desc(max_strength)) ## `summarise()` has grouped output by &#39;name&#39;. You can override using the `.groups` argument. storm_strength_duration %&gt;% ggplot(aes(x = max_strength, y = duration)) + geom_boxplot() ## Don&#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous. You don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with. In SAS, we have to know that datetimes are stored in seconds. So if we subtract two date time values, and we want our answer in days, then we need to divide by the number of seconds in a day: 24*60*60 = 86400. R has helper functions to do this for us, but it’s not that much harder to just do the computuation ourselves. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 PROC SQL; 8 CREATE TABLE stormlencat AS 9 SELECT name, year, (max(time) - min(time))/86400 AS duration, 9 ! max(category) AS max_strength 10 FROM classdat.storms 11 GROUP BY year, name 12 ORDER BY max_strength; NOTE: Table WORK.STORMLENCAT created, with 426 rows and 4 columns. 13 NOTE: PROCEDURE SQL used (Total process time): real time 0.03 seconds cpu time 0.04 seconds 14 PROC BOXPLOT DATA=stormlencat; 15 PLOT duration * max_strength; 16 RUN; NOTE: Processing beginning for PLOT statement number 1. NOTE: There were 426 observations read from the data set WORK.STORMLENCAT. NOTE: PROCEDURE BOXPLOT used (Total process time): real time 0.64 seconds cpu time 0.25 seconds 17 QUIT; We could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate) Diameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables). Note the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorial) strength and convert that into something numeric that can be plotted. storm_evolution &lt;- storms %&gt;% filter(!is.na(hu_diameter)) %&gt;% group_by(name, year) %&gt;% mutate(time_since_start = difftime(time, min(time), units = &quot;days&quot;)) %&gt;% ungroup() ggplot(storm_evolution, aes(x = time_since_start, y = hu_diameter, group = name)) + geom_line(alpha = .2) + facet_wrap(~year, scales = &quot;free_y&quot;) ## Don&#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous. For this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms. We can do something similar in SAS; this time, I decided to get rid of any storm which never had hurricane-force winds - that will get rid of a lot of lines that never leave the x-axis. libname classdat &quot;sas/&quot;; PROC SQL; CREATE TABLE stormevo AS SELECT name, year, (time - min(time))/86400 AS time_since_start, category, status, hu_diameter, ts_diameter, max(hu_diameter) AS max_hu_diameter FROM classdat.storms WHERE NOT MISSING(hu_diameter) GROUP BY year, name HAVING max_hu_diameter &gt; 0 ORDER BY year, name, time_since_start; PROC SGPANEL DATA=stormevo; PANELBY year / COLUMNS = 4 ROWS = 3; SERIES X = time_since_start Y = hu_diameter / GROUP = name; RUN; QUIT; PROC SGPANEL in SAS does essentially the same thing as facet_wrap() in R - it allows you to select one or more variables to create sub-plots for. We do have to manually specify how many rows and columns (or SAS will give us 3 separate plots with 4 panels each). The essential components of the graph specification are the same - instead of specifying the use of a line, we specify “series” (which means plot a line). We specify the same x, y, and group variables, though the syntax differs a bit. It seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately. storm_evolution %&gt;% filter(year == 2008) %&gt;% arrange(desc(time_since_start)) ## # A tibble: 327 x 15 ## name year month day hour lat long status category wind pressure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 Ike 2008 9 14 6 35.5 -93.7 tropical s… 0 35 985 ## 2 Ike 2008 9 14 0 33.5 -94.9 tropical s… 0 35 980 ## 3 Ike 2008 9 13 18 31.7 -95.3 tropical s… 0 50 974 ## 4 Ike 2008 9 13 12 30.3 -95.2 hurricane 2 85 959 ## 5 Ike 2008 9 13 7 29.3 -94.7 hurricane 2 95 950 ## 6 Ike 2008 9 13 6 29.1 -94.6 hurricane 2 95 951 ## 7 Ike 2008 9 13 0 28.3 -94 hurricane 2 95 952 ## 8 Fay 2008 8 27 0 35 -85.8 tropical d… -1 15 1005 ## 9 Ike 2008 9 12 18 27.5 -93.2 hurricane 2 95 954 ## 10 Fay 2008 8 26 18 34.6 -86.5 tropical d… -1 20 1004 ## # … with 317 more rows, and 4 more variables: ts_diameter &lt;dbl&gt;, ## # hu_diameter &lt;dbl&gt;, time &lt;dttm&gt;, time_since_start &lt;drtn&gt; storm_evolution %&gt;% filter(name == &quot;Ike&quot;) %&gt;% ggplot(aes(x = time, y = hu_diameter, color = category)) + geom_point() The SAS code for this is fairly similar (though I’ll admit to not having the finesse with SAS to get a truly nice looking plot). At this point, we’re going for quick-and-dirty graphics that show us what we want to know - we can figure out how to customize things later. libname classdat &quot;sas/&quot;; PROC SQL; CREATE TABLE ike AS SELECT * FROM classdat.storms WHERE name = &quot;Ike&quot; ORDER BY time; PROC SGPLOT DATA=ike; SCATTER X = time Y = hu_diameter / COLORRESPONSE=category /* color by another variable */ MARKERATTRS=(symbol=CircleFilled) /* use circles for points */ DATALABEL=category; /* label the circles with the value */ RUN; QUIT; While I’m tempted to plot out the diameter and location on a map, it’s a bit excessive for this particular problem. Luckily, Wikipedia has us covered: It looks like Ike went long-ways across Cuba, which weakened it. When hurricanes weaken, often their wind fields expand (as they no longer have the angular momentum to maintain a tight structure). Ike crossed into the Gulf of Mexico, restrengthened, and then hit Houston just about dead-on. (I was living just northwest of Houston when it hit (in College Station), and I can verify that it was not a fun time). 6.7 Other dplyr functions: across, relocate The dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time. dplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst) Suppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead. library(lubridate) # for the make_datetime() function data(storms) storms &lt;- storms %&gt;% # Construct a time variable that behaves like a number but is formatted as a date mutate(time = make_datetime(year, month, day, hour)) # Use across to get average of all numeric variables avg_hurricane_intensity &lt;- storms %&gt;% filter(status == &quot;hurricane&quot;) %&gt;% group_by(name) %&gt;% summarize(across(where(is.numeric), mean, na.rm = T), .groups = &quot;drop&quot;) avg_hurricane_intensity %&gt;% select(name, year, month, wind, pressure, ts_diameter, hu_diameter) %&gt;% arrange(desc(wind)) %&gt;% # get top 10 filter(row_number() &lt;= 10) %&gt;% knitr::kable() # Make into a pretty table name year month wind pressure ts_diameter hu_diameter Andrew 1992.000 8.000000 118.2609 946.6522 NaN NaN Mitch 1998.000 10.000000 115.9091 945.3182 NaN NaN Rita 2005.000 9.000000 114.7368 931.6316 305.2952 111.6934 Isabel 2003.000 9.000000 112.1875 946.5417 NaN NaN Gilbert 1988.000 9.000000 110.8929 945.4286 NaN NaN Wilma 2005.000 10.000000 110.3030 939.4242 402.5812 136.1756 Hugo 1989.000 9.000000 106.5789 950.9211 NaN NaN David 1979.000 8.457143 105.1429 956.1429 NaN NaN Luis 1995.000 8.893617 104.1489 951.8298 NaN NaN Ivan 1996.652 9.269663 103.5393 953.9438 315.9642 115.2031 Another handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here if only because it also comes with this handy cartoon illustration. relocate lets you rearrange columns (by Allison Horst) avg_hurricane_intensity %&gt;% relocate(c(wind, pressure), .after = month) ## # A tibble: 121 x 11 ## name year month wind pressure day hour lat long ts_diameter ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AL121991 1991 11 65 980. 1.5 9 38.8 -66.1 NaN ## 2 Alberto 1999. 7.92 78.3 978. 13.4 8.82 30.8 -49.0 NaN ## 3 Alex 2006. 7.38 83.1 966. 10 8.24 33 -77.1 304. ## 4 Alicia 1983 8 84.4 974. 17.5 7.62 28.4 -94.5 NaN ## 5 Allison 1995 6 65 988. 4.33 10 26.2 -86.2 NaN ## 6 Andrew 1992 8 118. 947. 24.0 9.09 26.4 -80.4 NaN ## 7 Anita 1977 8.62 93.1 968. 12.8 9.69 25.3 -94.2 NaN ## 8 Arthur 2014 7 77.3 978. 3.73 7.55 34.6 -76.1 209. ## 9 Barry 1983 8 68.3 988. 28 15.7 25.4 -97.1 NaN ## 10 Belle 1976 8 91 966. 8.5 9 31.2 -74.8 NaN ## # … with 111 more rows, and 1 more variable: hu_diameter &lt;dbl&gt; # move numeric variables to the front avg_hurricane_intensity %&gt;% relocate(where(is.numeric)) ## # A tibble: 121 x 11 ## year month day hour lat long wind pressure ts_diameter hu_diameter ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1991 11 1.5 9 38.8 -66.1 65 980. NaN NaN ## 2 1999. 7.92 13.4 8.82 30.8 -49.0 78.3 978. NaN NaN ## 3 2006. 7.38 10 8.24 33 -77.1 83.1 966. 304. 63.6 ## 4 1983 8 17.5 7.62 28.4 -94.5 84.4 974. NaN NaN ## 5 1995 6 4.33 10 26.2 -86.2 65 988. NaN NaN ## 6 1992 8 24.0 9.09 26.4 -80.4 118. 947. NaN NaN ## 7 1977 8.62 12.8 9.69 25.3 -94.2 93.1 968. NaN NaN ## 8 2014 7 3.73 7.55 34.6 -76.1 77.3 978. 209. 58.1 ## 9 1983 8 28 15.7 25.4 -97.1 68.3 988. NaN NaN ## 10 1976 8 8.5 9 31.2 -74.8 91 966. NaN NaN ## # … with 111 more rows, and 1 more variable: name &lt;chr&gt; Try it out Data Setup if (!&quot;gapminder&quot; %in% installed.packages()) install.packages(&quot;gapminder&quot;) library(gapminder) gapminder_unfiltered ## # A tibble: 3,313 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 3,303 more rows readr::write_csv(gapminder_unfiltered, &quot;data/gapminder.csv&quot;, na = &#39;.&#39;) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 filename fileloc &#39;data/gapminder.csv&#39;; 9 PROC IMPORT datafile = fileloc out=classdat.gapminder REPLACE 10 DBMS = csv; 10 ! /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 /************************************************************** 14 ! ******** 15 * PRODUCT: SAS 16 * VERSION: 9.4 17 * CREATOR: External File Interface 18 * DATE: 10MAY21 19 * DESC: Generated SAS Datastep Code 20 * TEMPLATE SOURCE: (None Specified.) 21 *************************************************************** 21 ! ********/ 22 data CLASSDAT.GAPMINDER ; 23 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 23 ! */ 24 infile FILELOC delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 25 informat country $22. ; 26 informat continent $8. ; 27 informat year best32. ; 28 informat lifeExp best32. ; 29 informat pop best32. ; 30 informat gdpPercap best32. ; 31 format country $22. ; 32 format continent $8. ; 33 format year best12. ; 34 format lifeExp best12. ; 35 format pop best12. ; 36 format gdpPercap best12. ; 37 input 38 country $ 39 continent $ 40 year 41 lifeExp 42 pop 43 gdpPercap 44 ; 45 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 45 ! detection macro variable */ 46 run; NOTE: The infile FILELOC is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/gap minder.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=10May2021:09:22:54, File Size (bytes)=157902 NOTE: 3313 records were read from the infile FILELOC. The minimum record length was 35. The maximum record length was 63. NOTE: The data set CLASSDAT.GAPMINDER has 3313 observations and 6 variables. NOTE: DATA statement used (Total process time): real time 0.04 seconds cpu time 0.01 seconds 3313 rows created in CLASSDAT.GAPMINDER from FILELOC. NOTE: CLASSDAT.GAPMINDER data set was successfully created. NOTE: The data set CLASSDAT.GAPMINDER has 3313 observations and 6 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 0.56 seconds cpu time 0.27 seconds You can read about the gapminder project here. The gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. I’ve exported the gapminder_unfiltered table to CSV for import into SAS as well - try to do these tasks in both languages. Task 1: How bad is it? Using your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use WHERE or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information. R gapminder_unfiltered %&gt;% group_by(country) %&gt;% summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), missingpop = sum(is.na(pop)), missingGDP = sum(is.na(gdpPercap))) %&gt;% filter(n != length(seq(1952, 2007, by = 5))) ## # A tibble: 83 x 5 ## country n missinglifeExp missingpop missingGDP ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Armenia 4 0 0 0 ## 2 Aruba 8 0 0 0 ## 3 Australia 56 0 0 0 ## 4 Austria 57 0 0 0 ## 5 Azerbaijan 4 0 0 0 ## 6 Bahamas 10 0 0 0 ## 7 Barbados 10 0 0 0 ## 8 Belarus 18 0 0 0 ## 9 Belgium 57 0 0 0 ## 10 Belize 11 0 0 0 ## # … with 73 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE gapsummary AS 10 SELECT DISTINCT country, COUNT(*) AS n, 11 SUM(MISSING(lifeExp)) AS missinglifeExp, 12 SUM(MISSING(pop)) AS missingpop, 13 SUM(MISSING(gdpPercap)) AS missingGDP 14 FROM classdat.gapminder 15 GROUP BY country; NOTE: Table WORK.GAPSUMMARY created, with 187 rows and 5 columns. 16 17 /* Print the problem countries only */ NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 18 PROC PRINT DATA = gapsummary; 19 WHERE n ^= 12; 20 RUN; NOTE: There were 83 observations read from the data set WORK.GAPSUMMARY. WHERE n not = 12; NOTE: PROCEDURE PRINT used (Total process time): real time 0.04 seconds cpu time 0.04 seconds Obs country n missinglifeExp missingpop missingGDP 6 Armenia 4 0 0 0 7 Aruba 8 0 0 0 8 Australia 56 0 0 0 9 Austria 57 0 0 0 10 Azerbaijan 4 0 0 0 11 Bahamas 10 0 0 0 14 Barbados 10 0 0 0 15 Belarus 18 0 0 0 16 Belgium 57 0 0 0 17 Belize 11 0 0 0 19 Bhutan 8 0 0 0 24 Brunei 8 0 0 0 25 Bulgaria 57 0 0 0 30 Canada 57 0 0 0 31 Cape Verde 11 0 0 0 35 China 36 0 0 0 40 Costa Rica 13 0 0 0 43 Cuba 13 0 0 0 44 Cyprus 8 0 0 0 45 Czech Republic 58 0 0 0 46 Denmark 58 0 0 0 54 Estonia 18 0 0 0 56 Fiji 10 0 0 0 57 Finland 58 0 0 0 58 France 57 0 0 0 59 French Guiana 1 0 0 0 60 French Polynesia 9 0 0 0 63 Georgia 9 0 0 0 64 Germany 26 0 0 0 66 Greece 13 0 0 0 67 Grenada 8 0 0 0 68 Guadeloupe 1 0 0 0 72 Guyana 10 0 0 0 76 Hungary 57 0 0 0 77 Iceland 58 0 0 0 82 Ireland 13 0 0 0 84 Italy 56 0 0 0 86 Japan 58 0 0 0 88 Kazakhstan 4 0 0 0 93 Latvia 42 0 0 0 97 Libya 13 0 0 0 98 Lithuania 18 0 0 0 99 Luxembourg 49 0 0 0 100 Macao, China 8 0 0 0 104 Maldives 8 0 0 0 106 Malta 10 0 0 0 107 Martinique 1 0 0 0 110 Mexico 13 0 0 0 111 Micronesia, Fed. Sts. 8 0 0 0 112 Moldova 5 0 0 0 120 Netherlands 58 0 0 0 121 Netherlands Antilles 8 0 0 0 122 New Caledonia 9 0 0 0 123 New Zealand 55 0 0 0 127 Norway 58 0 0 0 131 Papua New Guinea 10 0 0 0 135 Poland 52 0 0 0 136 Portugal 58 0 0 0 137 Puerto Rico 13 0 0 0 138 Qatar 8 0 0 0 141 Russia 20 0 0 0 143 Samoa 7 0 0 0 150 Slovak Republic 58 0 0 0 151 Slovenia 32 0 0 0 152 Solomon Islands 9 0 0 0 155 Spain 58 0 0 0 156 Sri Lanka 13 0 0 0 158 Suriname 8 0 0 0 160 Sweden 58 0 0 0 161 Switzerland 58 0 0 0 163 Taiwan 58 0 0 0 164 Tajikistan 4 0 0 0 166 Thailand 13 0 0 0 167 Timor-Leste 4 0 0 0 169 Tonga 7 0 0 0 173 Turkmenistan 4 0 0 0 174 Uganda 13 0 0 0 175 Ukraine 20 0 0 0 176 United Arab Emirates 8 0 0 0 177 United Kingdom 13 0 0 0 178 United States 57 0 0 0 180 Uzbekistan 4 0 0 0 181 Vanuatu 7 0 0 0 In order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. Task 2: Exclude any data which isn’t at 5-year increments, starting in 1952 (so 1952, 1957, 1962, …, 2007). R gapminder_unfiltered %&gt;% filter(year %in% seq(1952, 2007, by = 5)) ## # A tibble: 2,013 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 2,003 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE gap5 AS 10 SELECT * 11 FROM classdat.gapminder 12 WHERE MOD(year, 5) = 2; NOTE: Table WORK.GAP5 created, with 2013 rows and 6 columns. 13 14 /* Aus had too much data, so use it to see if the command worked 14 ! */ NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 PROC PRINT DATA = gap5; 16 WHERE country = &quot;Australia&quot;; 17 RUN; NOTE: There were 12 observations read from the data set WORK.GAP5. WHERE country=&#39;Australia&#39;; NOTE: PROCEDURE PRINT used (Total process time): real time 0.01 seconds cpu time 0.02 seconds Obs country continent year lifeExp pop gdpPercap 73 Australia Oceania 1952 69.12 8691212 10039.59564 74 Australia Oceania 1957 70.33 9712569 10949.64959 75 Australia Oceania 1962 70.93 10794968 12217.22686 76 Australia Oceania 1967 71.1 11872264 14526.12465 77 Australia Oceania 1972 71.93 13177000 16788.62948 78 Australia Oceania 1977 73.49 14074100 18334.19751 79 Australia Oceania 1982 74.74 15184200 19477.00928 80 Australia Oceania 1987 76.32 16257249 21888.88903 81 Australia Oceania 1992 77.56 17481977 23424.76683 82 Australia Oceania 1997 78.83 18565243 26997.93657 83 Australia Oceania 2002 80.37 19546792 30687.75473 84 Australia Oceania 2007 81.235 20434176 34435.36744 Task 3: Exclude any countries that don’t have a full set of observations from 1952 - 2007 in 5-year increments. R gapminder_unfiltered %&gt;% filter(year %in% seq(1952, 2007, by = 5)) %&gt;% group_by(country) %&gt;% mutate(nobs = n()) %&gt;% # Use mutate instead of summarize so that all rows stay filter(nobs == 12) %&gt;% select(-nobs) ## # A tibble: 1,704 x 6 ## # Groups: country [142] ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SQL; 9 CREATE TABLE gap5 AS 10 SELECT * 11 FROM classdat.gapminder 12 WHERE MOD(year, 5) = 2; NOTE: Table WORK.GAP5 created, with 2013 rows and 6 columns. 13 14 CREATE TABLE gap_clean AS 15 SELECT *, COUNT(*) as n 16 FROM gap5 17 GROUP BY country 18 HAVING n = 12; NOTE: The query requires remerging summary statistics back with the original data. NOTE: Table WORK.GAP_CLEAN created, with 1704 rows and 7 columns. 19 20 /* Clean up extra column */ 21 ALTER TABLE gap_clean 22 DROP n; NOTE: Table WORK.GAP_CLEAN has been modified, with 6 columns. 23 NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 24 PROC PRINT DATA = gap_clean; 25 RUN; NOTE: There were 1704 observations read from the data set WORK.GAP_CLEAN. NOTE: PROCEDURE PRINT used (Total process time): real time 1.07 seconds cpu time 1.03 seconds Obs country continent year lifeExp pop gdpPercap 1 Afghanistan Asia 1977 38.438 14880372 786.11336 2 Afghanistan Asia 2002 42.129 25268405 726.7340548 3 Afghanistan Asia 1972 36.088 13079460 739.9811058 4 Afghanistan Asia 1982 39.854 12881816 978.0114388 5 Afghanistan Asia 1987 40.822 13867957 852.3959448 6 Afghanistan Asia 1967 34.02 11537966 836.1971382 7 Afghanistan Asia 1957 30.332 9240934 820.8530296 8 Afghanistan Asia 1952 28.801 8425333 779.4453145 9 Afghanistan Asia 1962 31.997 10267083 853.10071 10 Afghanistan Asia 2007 43.828 31889923 974.5803384 11 Afghanistan Asia 1997 41.763 22227415 635.341351 12 Afghanistan Asia 1992 41.674 16317921 649.3413952 13 Albania Europe 1977 68.93 2509048 3533.00391 14 Albania Europe 1962 64.82 1728137 2312.888958 15 Albania Europe 2002 75.651 3508512 4604.211737 16 Albania Europe 1967 66.22 1984060 2760.196931 17 Albania Europe 1957 59.28 1476505 1942.284244 18 Albania Europe 1987 72 3075321 3738.932735 19 Albania Europe 1997 72.95 3428038 3193.054604 20 Albania Europe 1952 55.23 1282697 1601.056136 21 Albania Europe 2007 76.423 3600523 5937.029526 22 Albania Europe 1972 67.69 2263554 3313.422188 23 Albania Europe 1982 70.42 2780097 3630.880722 24 Albania Europe 1992 71.581 3326498 2497.437901 25 Algeria Africa 1982 61.368 20033753 5745.160213 26 Algeria Africa 1962 48.303 11000948 2550.81688 27 Algeria Africa 1952 43.077 9279525 2449.008185 28 Algeria Africa 1967 51.407 12760499 3246.991771 29 Algeria Africa 1972 54.518 14760787 4182.663766 30 Algeria Africa 1977 58.014 17152804 4910.416756 31 Algeria Africa 1957 45.685 10270856 3013.976023 32 Algeria Africa 1997 69.152 29072015 4797.295051 33 Algeria Africa 2002 70.994 31287142 5288.040382 34 Algeria Africa 2007 72.301 33333216 6223.367465 35 Algeria Africa 1987 65.799 23254956 5681.358539 36 Algeria Africa 1992 67.744 26298373 5023.216647 37 Angola Africa 1957 31.999 4561361 3827.940465 38 Angola Africa 1992 40.647 8735988 2627.845685 39 Angola Africa 2002 41.003 10866106 2773.287312 40 Angola Africa 1987 39.906 7874230 2430.208311 41 Angola Africa 2007 42.731 12420476 4797.231267 42 Angola Africa 1997 40.963 9875024 2277.140884 43 Angola Africa 1982 39.942 7016384 2756.953672 44 Angola Africa 1952 30.015 4232095 3520.610273 45 Angola Africa 1967 35.985 5247469 5522.776375 46 Angola Africa 1972 37.928 5894858 5473.288005 47 Angola Africa 1977 39.483 6162675 3008.647355 48 Angola Africa 1962 34 4826015 4269.276742 49 Argentina Americas 2002 74.34 38331121 8797.640716 50 Argentina Americas 1997 73.275 36203463 10967.28195 51 Argentina Americas 1992 71.868 33958947 9308.41871 52 Argentina Americas 1987 70.774 31620918 9139.671389 53 Argentina Americas 1967 65.634 22934225 8052.953021 54 Argentina Americas 1977 68.481 26983828 10079.02674 55 Argentina Americas 2007 75.32 40301927 12779.37964 56 Argentina Americas 1972 67.065 24779799 9443.038526 57 Argentina Americas 1962 65.142 21283783 7133.166023 58 Argentina Americas 1952 62.485 17876956 5911.315053 59 Argentina Americas 1982 69.942 29341374 8997.897412 60 Argentina Americas 1957 64.399 19610538 6856.856212 61 Australia Oceania 1987 76.32 16257249 21888.88903 62 Australia Oceania 2002 80.37 19546792 30687.75473 63 Australia Oceania 1972 71.93 13177000 16788.62948 64 Australia Oceania 1992 77.56 17481977 23424.76683 65 Australia Oceania 1982 74.74 15184200 19477.00928 66 Australia Oceania 1952 69.12 8691212 10039.59564 67 Australia Oceania 2007 81.235 20434176 34435.36744 68 Australia Oceania 1962 70.93 10794968 12217.22686 69 Australia Oceania 1997 78.83 18565243 26997.93657 70 Australia Oceania 1977 73.49 14074100 18334.19751 71 Australia Oceania 1967 71.1 11872264 14526.12465 72 Australia Oceania 1957 70.33 9712569 10949.64959 73 Austria Europe 2002 78.98 8148312 32417.60769 74 Austria Europe 1962 69.54 7129864 10750.72111 75 Austria Europe 2007 79.829 8199783 36126.4927 76 Austria Europe 1957 67.48 6965860 8842.59803 77 Austria Europe 1977 72.17 7568430 19749.4223 78 Austria Europe 1982 73.18 7574613 21597.08362 79 Austria Europe 1992 76.04 7914969 27042.01868 80 Austria Europe 1997 77.51 8069876 29095.92066 81 Austria Europe 1967 70.14 7376998 12834.6024 82 Austria Europe 1952 66.8 6927772 6137.076492 83 Austria Europe 1987 74.94 7578903 23687.82607 84 Austria Europe 1972 70.63 7544201 16661.6256 85 Bahrain Asia 1957 53.832 138655 11635.79945 86 Bahrain Asia 2007 75.635 708573 29796.04834 87 Bahrain Asia 2002 74.795 656397 23403.55927 88 Bahrain Asia 1997 73.925 598561 20292.01679 89 Bahrain Asia 1992 72.601 529491 19035.57917 90 Bahrain Asia 1977 65.593 297410 19340.10196 91 Bahrain Asia 1967 59.923 202182 14804.6727 92 Bahrain Asia 1972 63.3 230800 18268.65839 93 Bahrain Asia 1982 69.052 377967 19211.14731 94 Bahrain Asia 1952 50.939 120447 9867.084765 95 Bahrain Asia 1962 56.923 171863 12753.27514 96 Bahrain Asia 1987 70.75 454612 18524.02406 97 Bangladesh Asia 1962 41.216 56839289 686.3415538 98 Bangladesh Asia 1972 45.252 70759295 630.2336265 99 Bangladesh Asia 1967 43.453 62821884 721.1860862 100 Bangladesh Asia 1952 37.484 46886859 684.2441716 101 Bangladesh Asia 1957 39.348 51365468 661.6374577 102 Bangladesh Asia 1992 56.018 113704579 837.8101643 103 Bangladesh Asia 1977 46.923 80428306 659.8772322 104 Bangladesh Asia 2002 62.013 135656790 1136.39043 105 Bangladesh Asia 1997 59.412 123315288 972.7700352 106 Bangladesh Asia 1987 52.819 103764241 751.9794035 107 Bangladesh Asia 1982 50.009 93074406 676.9818656 108 Bangladesh Asia 2007 64.062 150448339 1391.253792 109 Belgium Europe 2002 78.32 10311970 30485.88375 110 Belgium Europe 1972 71.44 9709100 16672.14356 111 Belgium Europe 1992 76.46 10045622 25575.57069 112 Belgium Europe 1987 75.35 9870200 22525.56308 113 Belgium Europe 2007 79.441 10392226 33692.60508 114 Belgium Europe 1982 73.93 9856303 20979.84589 115 Belgium Europe 1977 72.8 9821800 19117.97448 116 Belgium Europe 1957 69.24 8989111 9714.960623 117 Belgium Europe 1997 77.53 10199787 27561.19663 118 Belgium Europe 1962 70.25 9218400 10991.20676 119 Belgium Europe 1967 70.94 9556500 13149.04119 120 Belgium Europe 1952 68 8730405 8343.105127 121 Benin Africa 1992 53.919 4981671 1191.207681 122 Benin Africa 1997 54.777 6066080 1232.975292 123 Benin Africa 1987 52.337 4243788 1225.85601 124 Benin Africa 2002 54.406 7026113 1372.877931 125 Benin Africa 1982 50.904 3641603 1277.897616 126 Benin Africa 1977 49.19 3168267 1029.161251 127 Benin Africa 2007 56.728 8078314 1441.284873 128 Benin Africa 1967 44.885 2427334 1035.831411 129 Benin Africa 1957 40.358 1925173 959.6010805 130 Benin Africa 1952 38.223 1738315 1062.7522 131 Benin Africa 1972 47.014 2761407 1085.796879 132 Benin Africa 1962 42.618 2151895 949.4990641 133 Bolivia Americas 2002 63.883 8445134 3413.26269 134 Bolivia Americas 1997 62.05 7693188 3326.143191 135 Bolivia Americas 1987 57.251 6156369 2753.69149 136 Bolivia Americas 1982 53.859 5642224 3156.510452 137 Bolivia Americas 1992 59.957 6893451 2961.699694 138 Bolivia Americas 1972 46.714 4565872 2980.331339 139 Bolivia Americas 1967 45.032 4040665 2586.886053 140 Bolivia Americas 1962 43.428 3593918 2180.972546 141 Bolivia Americas 1957 41.89 3211738 2127.686326 142 Bolivia Americas 1952 40.414 2883315 2677.326347 143 Bolivia Americas 1977 50.023 5079716 3548.097832 144 Bolivia Americas 2007 65.554 9119152 3822.137084 145 Bosnia and Herzegovina Europe 2002 74.09 4165416 6018.975239 146 Bosnia and Herzegovina Europe 1987 71.14 4338977 4314.114757 147 Bosnia and Herzegovina Europe 1997 73.244 3607000 4766.355904 148 Bosnia and Herzegovina Europe 1992 72.178 4256013 2546.781445 149 Bosnia and Herzegovina Europe 1982 70.69 4172693 4126.613157 150 Bosnia and Herzegovina Europe 1977 69.86 4086000 3528.481305 151 Bosnia and Herzegovina Europe 1972 67.45 3819000 2860.16975 152 Bosnia and Herzegovina Europe 2007 74.852 4552198 7446.298803 153 Bosnia and Herzegovina Europe 1967 64.79 3585000 2172.352423 154 Bosnia and Herzegovina Europe 1957 58.45 3076000 1353.989176 155 Bosnia and Herzegovina Europe 1952 53.82 2791000 973.5331948 156 Bosnia and Herzegovina Europe 1962 61.93 3349000 1709.683679 157 Botswana Africa 1997 52.556 1536536 8647.142313 158 Botswana Africa 2002 46.634 1630347 11003.60508 159 Botswana Africa 1987 63.622 1151184 6205.88385 160 Botswana Africa 1982 61.484 970347 4551.14215 161 Botswana Africa 1992 62.745 1342614 7954.111645 162 Botswana Africa 2007 50.728 1639131 12569.85177 163 Botswana Africa 1972 56.024 619351 2263.611114 164 Botswana Africa 1957 49.618 474639 918.2325349 165 Botswana Africa 1977 59.319 781472 3214.857818 166 Botswana Africa 1952 47.622 442308 851.2411407 167 Botswana Africa 1967 53.298 553541 1214.709294 168 Botswana Africa 1962 51.52 512764 983.6539764 169 Brazil Americas 1997 69.388 168546719 7957.980824 170 Brazil Americas 1987 65.205 142938076 7807.095818 171 Brazil Americas 1982 63.336 128962939 7030.835878 172 Brazil Americas 2002 71.006 179914212 8131.212843 173 Brazil Americas 1992 67.057 155975974 6950.283021 174 Brazil Americas 2007 72.39 190010647 9065.800825 175 Brazil Americas 1972 59.504 100840058 4985.711467 176 Brazil Americas 1967 57.632 88049823 3429.864357 177 Brazil Americas 1957 53.285 65551171 2487.365989 178 Brazil Americas 1952 50.917 56602560 2108.944355 179 Brazil Americas 1977 61.489 114313951 6660.118654 180 Brazil Americas 1962 55.665 76039390 3336.585802 181 Bulgaria Europe 1987 71.34 8971958 8239.854824 182 Bulgaria Europe 1992 71.19 8658506 6302.623438 183 Bulgaria Europe 1982 71.08 8892098 8224.191647 184 Bulgaria Europe 1977 70.81 8797022 7612.240438 185 Bulgaria Europe 1967 70.42 8310226 5577.0028 186 Bulgaria Europe 1962 69.51 8012946 4254.337839 187 Bulgaria Europe 1972 70.9 8576200 6597.494398 188 Bulgaria Europe 1957 66.61 7651254 3008.670727 189 Bulgaria Europe 1952 59.6 7274900 2444.286648 190 Bulgaria Europe 2007 73.005 7322858 10680.79282 191 Bulgaria Europe 2002 72.14 7661799 7696.777725 192 Bulgaria Europe 1997 70.32 8066057 5970.38876 193 Burkina Faso Africa 2007 52.295 14326203 1217.032994 194 Burkina Faso Africa 1997 50.324 10352843 946.2949618 195 Burkina Faso Africa 1982 48.122 6634596 807.1985855 196 Burkina Faso Africa 1992 50.26 8878303 931.7527731 197 Burkina Faso Africa 1987 49.557 7586551 912.0631417 198 Burkina Faso Africa 1977 46.137 5889574 743.3870368 199 Burkina Faso Africa 2002 50.65 12251209 1037.645221 200 Burkina Faso Africa 1972 43.591 5433886 854.7359763 201 Burkina Faso Africa 1962 37.814 4919632 722.5120206 202 Burkina Faso Africa 1967 40.697 5127935 794.8265597 203 Burkina Faso Africa 1957 34.906 4713416 617.1834648 204 Burkina Faso Africa 1952 31.975 4469979 543.2552413 205 Burundi Africa 1997 45.326 6121610 463.1151478 206 Burundi Africa 2002 47.36 7021078 446.4035126 207 Burundi Africa 2007 49.58 8390505 430.0706916 208 Burundi Africa 1992 44.736 5809236 631.6998778 209 Burundi Africa 1987 48.211 5126023 621.8188189 210 Burundi Africa 1977 45.91 3834415 556.1032651 211 Burundi Africa 1972 44.057 3529983 464.0995039 212 Burundi Africa 1962 42.045 2961915 355.2032273 213 Burundi Africa 1967 43.548 3330989 412.9775136 214 Burundi Africa 1982 47.471 4580410 559.603231 215 Burundi Africa 1957 40.533 2667518 379.5646281 216 Burundi Africa 1952 39.031 2445618 339.2964587 217 Cambodia Asia 2007 59.723 14131858 1713.778686 218 Cambodia Asia 2002 56.752 12926707 896.2260153 219 Cambodia Asia 1997 56.534 11782962 734.28517 220 Cambodia Asia 1992 55.803 10150094 682.3031755 221 Cambodia Asia 1987 53.914 8371791 683.8955732 222 Cambodia Asia 1982 50.957 7272485 624.4754784 223 Cambodia Asia 1977 31.22 6978607 524.9721832 224 Cambodia Asia 1972 40.317 7450606 421.6240257 225 Cambodia Asia 1967 45.415 6960067 523.4323142 226 Cambodia Asia 1957 41.366 5322536 434.0383364 227 Cambodia Asia 1952 39.417 4693836 368.4692856 228 Cambodia Asia 1962 43.415 6083619 496.9136476 229 Cameroon Africa 2007 50.43 17696293 2042.09524 230 Cameroon Africa 2002 49.856 15929988 1934.011449 231 Cameroon Africa 1997 52.199 14195809 1694.337469 232 Cameroon Africa 1992 54.314 12467171 1793.163278 233 Cameroon Africa 1987 54.985 10780667 2602.664206 234 Cameroon Africa 1982 52.961 9250831 2367.983282 235 Cameroon Africa 1977 49.355 7959865 1783.432873 236 Cameroon Africa 1972 47.049 7021028 1684.146528 237 Cameroon Africa 1967 44.799 6335506 1508.453148 238 Cameroon Africa 1957 40.428 5359923 1313.048099 239 Cameroon Africa 1952 38.523 5009067 1172.667655 240 Cameroon Africa 1962 42.643 5793633 1399.607441 241 Canada Americas 2007 80.653 33390141 36319.23501 242 Canada Americas 2002 79.77 31902268 33328.96507 243 Canada Americas 1997 78.61 30305843 28954.92589 244 Canada Americas 1992 77.95 28523502 26342.88426 245 Canada Americas 1987 76.86 26549700 26626.51503 246 Canada Americas 1977 74.21 23796400 22090.88306 247 Canada Americas 1972 72.88 22284500 18970.57086 248 Canada Americas 1967 72.13 20819767 16076.58803 249 Canada Americas 1982 75.76 25201900 22898.79214 250 Canada Americas 1957 69.96 17010154 12489.95006 251 Canada Americas 1952 68.75 14785584 11367.16112 252 Canada Americas 1962 71.3 18985849 13462.48555 253 Central African Republ Africa 1992 49.396 3265124 747.9055252 254 Central African Republ Africa 1997 46.066 3696513 740.5063317 255 Central African Republ Africa 2002 43.308 4048013 738.6906068 256 Central African Republ Africa 2007 44.741 4369038 706.016537 257 Central African Republ Africa 1982 48.295 2476971 956.7529907 258 Central African Republ Africa 1977 46.775 2167533 1109.374338 259 Central African Republ Africa 1967 41.478 1733638 1136.056615 260 Central African Republ Africa 1972 43.457 1927260 1070.013275 261 Central African Republ Africa 1987 50.485 2840009 844.8763504 262 Central African Republ Africa 1962 39.475 1523478 1193.068753 263 Central African Republ Africa 1952 35.463 1291695 1071.310713 264 Central African Republ Africa 1957 37.464 1392284 1190.844328 265 Chad Africa 2007 50.651 10238807 1704.063724 266 Chad Africa 2002 50.525 8835739 1156.18186 267 Chad Africa 1997 51.573 7562011 1004.961353 268 Chad Africa 1992 51.724 6429417 1058.0643 269 Chad Africa 1982 49.517 4875118 797.9081006 270 Chad Africa 1967 43.601 3495967 1196.810565 271 Chad Africa 1972 45.569 3899068 1104.103987 272 Chad Africa 1987 51.051 5498955 952.386129 273 Chad Africa 1962 41.716 3150417 1389.817618 274 Chad Africa 1952 38.092 2682462 1178.665927 275 Chad Africa 1957 39.881 2894855 1308.495577 276 Chad Africa 1977 47.383 4388260 1133.98495 277 Chile Americas 2007 78.553 16284741 13171.63885 278 Chile Americas 2002 77.86 15497046 10778.78385 279 Chile Americas 1997 75.816 14599929 10118.05318 280 Chile Americas 1992 74.126 13572994 7596.125964 281 Chile Americas 1987 72.492 12463354 5547.063754 282 Chile Americas 1977 67.052 10599793 4756.763836 283 Chile Americas 1972 63.441 9717524 5494.024437 284 Chile Americas 1967 60.523 8858908 5106.654313 285 Chile Americas 1962 57.924 7961258 4519.094331 286 Chile Americas 1957 56.074 7048426 4315.622723 287 Chile Americas 1952 54.745 6377619 3939.978789 288 Chile Americas 1982 70.565 11487112 5095.665738 289 China Asia 2007 72.961 1318683096 4959.114854 290 China Asia 2002 72.028 1280400000 3119.280896 291 China Asia 1992 68.69 1164970000 1655.784158 292 China Asia 1987 67.274 1084035000 1378.904018 293 China Asia 1997 70.426 1230075000 2289.234136 294 China Asia 1982 65.525 1000281000 962.4213805 295 China Asia 1977 63.96736 943455000 741.2374699 296 China Asia 1972 63.11888 862030000 676.9000921 297 China Asia 1967 58.38112 754550000 612.7056934 298 China Asia 1962 44.50136 665770000 487.6740183 299 China Asia 1957 50.54896 637408000 575.9870009 300 China Asia 1952 44 556263527 400.448611 301 Colombia Americas 2007 72.889 44227550 7006.580419 302 Colombia Americas 2002 71.682 41008227 5755.259962 303 Colombia Americas 1997 70.313 37657830 6117.361746 304 Colombia Americas 1992 68.421 34202721 5444.648617 305 Colombia Americas 1987 67.768 30964245 4903.2191 306 Colombia Americas 1982 66.653 27764644 4397.575659 307 Colombia Americas 1977 63.837 25094412 3815.80787 308 Colombia Americas 1972 61.623 22542890 3264.660041 309 Colombia Americas 1962 57.863 17009885 2492.351109 310 Colombia Americas 1957 55.118 14485993 2323.805581 311 Colombia Americas 1967 59.963 19764027 2678.729839 312 Colombia Americas 1952 50.643 12350771 2144.115096 313 Comoros Africa 2007 65.152 710960 986.1478792 314 Comoros Africa 2002 62.974 614382 1075.811558 315 Comoros Africa 1997 60.66 527982 1173.618235 316 Comoros Africa 1992 57.939 454429 1246.90737 317 Comoros Africa 1987 54.926 395114 1315.980812 318 Comoros Africa 1982 52.933 348643 1267.100083 319 Comoros Africa 1977 50.939 304739 1172.603047 320 Comoros Africa 1972 48.944 250027 1937.577675 321 Comoros Africa 1967 46.472 217378 1876.029643 322 Comoros Africa 1962 44.467 191689 1406.648278 323 Comoros Africa 1957 42.46 170928 1211.148548 324 Comoros Africa 1952 40.715 153936 1102.990936 325 Congo, Dem. Rep. Africa 2007 46.462 64606759 277.5518587 326 Congo, Dem. Rep. Africa 2002 44.966 55379852 241.1658765 327 Congo, Dem. Rep. Africa 1997 42.587 47798986 312.188423 328 Congo, Dem. Rep. Africa 1992 45.548 41672143 457.7191807 329 Congo, Dem. Rep. Africa 1987 47.412 35481645 672.774812 330 Congo, Dem. Rep. Africa 1982 47.784 30646495 673.7478181 331 Congo, Dem. Rep. Africa 1977 47.804 26480870 795.757282 332 Congo, Dem. Rep. Africa 1972 45.989 23007669 904.8960685 333 Congo, Dem. Rep. Africa 1967 44.056 19941073 861.5932424 334 Congo, Dem. Rep. Africa 1962 42.122 17486434 896.3146335 335 Congo, Dem. Rep. Africa 1957 40.652 15577932 905.8602303 336 Congo, Dem. Rep. Africa 1952 39.143 14100005 780.5423257 337 Congo, Rep. Africa 2007 55.322 3800610 3632.557798 338 Congo, Rep. Africa 2002 52.97 3328795 3484.06197 339 Congo, Rep. Africa 1997 52.962 2800947 3484.164376 340 Congo, Rep. Africa 1992 56.433 2409073 4016.239529 341 Congo, Rep. Africa 1987 57.47 2064095 4201.194937 342 Congo, Rep. Africa 1982 56.695 1774735 4879.507522 343 Congo, Rep. Africa 1977 55.625 1536769 3259.178978 344 Congo, Rep. Africa 1972 54.907 1340458 3213.152683 345 Congo, Rep. Africa 1967 52.04 1179760 2677.939642 346 Congo, Rep. Africa 1962 48.435 1047924 2464.783157 347 Congo, Rep. Africa 1957 45.053 940458 2315.056572 348 Congo, Rep. Africa 1952 42.111 854885 2125.621418 349 Costa Rica Americas 2007 78.782 4133884 9645.06142 350 Costa Rica Americas 2002 78.123 3834934 7723.447195 351 Costa Rica Americas 1997 77.26 3518107 6677.045314 352 Costa Rica Americas 1987 74.752 2799811 5629.915318 353 Costa Rica Americas 1982 73.45 2424367 5262.734751 354 Costa Rica Americas 1977 70.75 2108457 5926.876967 355 Costa Rica Americas 1972 67.849 1834796 5118.146939 356 Costa Rica Americas 1967 65.424 1588717 4161.727834 357 Costa Rica Americas 1962 62.842 1345187 3460.937025 358 Costa Rica Americas 1957 60.026 1112300 2990.010802 359 Costa Rica Americas 1952 57.206 926317 2627.009471 360 Costa Rica Americas 1992 75.713 3173216 6160.416317 361 Cote d'Ivoire Africa 2007 48.328 18013409 1544.750112 362 Cote d'Ivoire Africa 2002 46.832 16252726 1648.800823 363 Cote d'Ivoire Africa 1997 47.991 14625967 1786.265407 364 Cote d'Ivoire Africa 1992 52.044 12772596 1648.073791 365 Cote d'Ivoire Africa 1987 54.655 10761098 2156.956069 366 Cote d'Ivoire Africa 1982 53.983 9025951 2602.710169 367 Cote d'Ivoire Africa 1977 52.374 7459574 2517.736547 368 Cote d'Ivoire Africa 1972 49.801 6071696 2378.201111 369 Cote d'Ivoire Africa 1967 47.35 4744870 2052.050473 370 Cote d'Ivoire Africa 1962 44.93 3832408 1728.869428 371 Cote d'Ivoire Africa 1957 42.469 3300000 1500.895925 372 Cote d'Ivoire Africa 1952 40.477 2977019 1388.594732 373 Croatia Europe 2007 75.748 4493312 14619.22272 374 Croatia Europe 2002 74.876 4481020 11628.38895 375 Croatia Europe 1997 73.68 4444595 9875.604515 376 Croatia Europe 1992 72.527 4494013 8447.794873 377 Croatia Europe 1987 71.52 4484310 13822.58394 378 Croatia Europe 1982 70.46 4413368 13221.82184 379 Croatia Europe 1977 70.64 4318673 11305.38517 380 Croatia Europe 1972 69.61 4225310 9164.090127 381 Croatia Europe 1967 68.5 4174366 6960.297861 382 Croatia Europe 1962 67.13 4076557 5477.890018 383 Croatia Europe 1957 64.77 3991242 4338.231617 384 Croatia Europe 1952 61.21 3882229 3119.23652 385 Cuba Americas 2007 78.273 11416987 8948.102923 386 Cuba Americas 2002 77.158 11226999 6340.646683 387 Cuba Americas 1997 76.151 10983007 5431.990415 388 Cuba Americas 1992 74.414 10723260 5592.843963 389 Cuba Americas 1987 74.174 10239839 7532.924763 390 Cuba Americas 1982 73.717 9789224 7316.918107 391 Cuba Americas 1977 72.649 9537988 6380.494966 392 Cuba Americas 1972 70.723 8831348 5305.445256 393 Cuba Americas 1967 68.29 8139332 5690.268015 394 Cuba Americas 1962 65.246 7254373 5180.75591 395 Cuba Americas 1957 62.325 6640752 6092.174359 396 Cuba Americas 1952 59.421 6007797 5586.53878 397 Czech Republic Europe 2007 76.486 10228744 22833.30851 398 Czech Republic Europe 2002 75.51 10256295 17596.21022 399 Czech Republic Europe 1997 74.01 10300707 16048.51424 400 Czech Republic Europe 1992 72.4 10315702 14297.02122 401 Czech Republic Europe 1987 71.58 10311597 16310.4434 402 Czech Republic Europe 1982 70.96 10303704 15377.22855 403 Czech Republic Europe 1977 70.71 10161915 14800.16062 404 Czech Republic Europe 1972 70.29 9862158 13108.4536 405 Czech Republic Europe 1967 70.38 9835109 11399.44489 406 Czech Republic Europe 1962 69.9 9620282 10136.86713 407 Czech Republic Europe 1957 69.03 9513758 8256.343918 408 Czech Republic Europe 1952 66.87 9125183 6876.14025 409 Denmark Europe 2007 78.332 5468120 35278.41874 410 Denmark Europe 2002 77.18 5374693 32166.50006 411 Denmark Europe 1997 76.11 5283663 29804.34567 412 Denmark Europe 1992 75.33 5171393 26406.73985 413 Denmark Europe 1987 74.8 5127024 25116.17581 414 Denmark Europe 1982 74.63 5117810 21688.04048 415 Denmark Europe 1977 74.69 5088419 20422.9015 416 Denmark Europe 1972 73.47 4991596 18866.20721 417 Denmark Europe 1967 72.96 4838800 15937.21123 418 Denmark Europe 1962 72.35 4646899 13583.31351 419 Denmark Europe 1957 71.81 4487831 11099.65935 420 Denmark Europe 1952 70.78 4334000 9692.385245 421 Djibouti Africa 1977 46.519 228694 3081.761022 422 Djibouti Africa 1972 44.366 178848 3694.212352 423 Djibouti Africa 1967 42.074 127617 3020.050513 424 Djibouti Africa 1962 39.693 89898 3020.989263 425 Djibouti Africa 1957 37.328 71851 2864.969076 426 Djibouti Africa 1952 34.812 63149 2669.529475 427 Djibouti Africa 2007 54.791 496374 2082.481567 428 Djibouti Africa 2002 53.373 447416 1908.260867 429 Djibouti Africa 1997 53.157 417908 1895.016984 430 Djibouti Africa 1992 51.604 384156 2377.156192 431 Djibouti Africa 1987 50.04 311025 2880.102568 432 Djibouti Africa 1982 48.812 305991 2879.468067 433 Dominican Republic Americas 2007 72.235 9319622 6025.374752 434 Dominican Republic Americas 2002 70.847 8650322 4563.808154 435 Dominican Republic Americas 1997 69.957 7992357 3614.101285 436 Dominican Republic Americas 1992 68.457 7351181 3044.214214 437 Dominican Republic Americas 1987 66.046 6655297 2899.842175 438 Dominican Republic Americas 1982 63.727 5968349 2861.092386 439 Dominican Republic Americas 1977 61.788 5302800 2681.9889 440 Dominican Republic Americas 1972 59.631 4671329 2189.874499 441 Dominican Republic Americas 1967 56.751 4049146 1653.723003 442 Dominican Republic Americas 1962 53.459 3453434 1662.137359 443 Dominican Republic Americas 1952 45.928 2491346 1397.717137 444 Dominican Republic Americas 1957 49.828 2923186 1544.402995 445 Ecuador Americas 2007 74.994 13755680 6873.262326 446 Ecuador Americas 2002 74.173 12921234 5773.044512 447 Ecuador Americas 1997 72.312 11911819 7429.455877 448 Ecuador Americas 1992 69.613 10748394 7103.702595 449 Ecuador Americas 1987 67.231 9545158 6481.776993 450 Ecuador Americas 1982 64.342 8365850 7213.791267 451 Ecuador Americas 1977 61.31 7278866 6679.62326 452 Ecuador Americas 1972 58.796 6298651 5280.99471 453 Ecuador Americas 1967 56.678 5432424 4579.074215 454 Ecuador Americas 1962 54.64 4681707 4086.114078 455 Ecuador Americas 1952 48.357 3548753 3522.110717 456 Ecuador Americas 1957 51.356 4058385 3780.546651 457 Egypt Africa 2007 71.338 80264543 5581.180998 458 Egypt Africa 2002 69.806 73312559 4754.604414 459 Egypt Africa 1997 67.217 66134291 4173.181797 460 Egypt Africa 1992 63.674 59402198 3794.755195 461 Egypt Africa 1987 59.797 52799062 3885.46071 462 Egypt Africa 1982 56.006 45681811 3503.729636 463 Egypt Africa 1977 53.319 38783863 2785.493582 464 Egypt Africa 1972 51.137 34807417 2024.008147 465 Egypt Africa 1967 49.293 31681188 1814.880728 466 Egypt Africa 1962 46.992 28173309 1693.335853 467 Egypt Africa 1957 44.444 25009741 1458.915272 468 Egypt Africa 1952 41.893 22223309 1418.822445 469 El Salvador Americas 2007 71.878 6939688 5728.353514 470 El Salvador Americas 2002 70.734 6353681 5351.568666 471 El Salvador Americas 1997 69.535 5783439 5154.825496 472 El Salvador Americas 1992 66.798 5274649 4444.2317 473 El Salvador Americas 1987 63.154 4842194 4140.442097 474 El Salvador Americas 1982 56.604 4474873 4098.344175 475 El Salvador Americas 1977 56.696 4282586 5138.922374 476 El Salvador Americas 1972 58.207 3790903 4520.246008 477 El Salvador Americas 1967 55.855 3232927 4358.595393 478 El Salvador Americas 1962 52.307 2747687 3776.803627 479 El Salvador Americas 1957 48.57 2355805 3421.523218 480 El Salvador Americas 1952 45.262 2042865 3048.3029 481 Equatorial Guinea Africa 2007 51.579 551201 12154.08975 482 Equatorial Guinea Africa 2002 49.348 495627 7703.4959 483 Equatorial Guinea Africa 1997 48.245 439971 2814.480755 484 Equatorial Guinea Africa 1992 47.545 387838 1132.055034 485 Equatorial Guinea Africa 1987 45.664 341244 966.8968149 486 Equatorial Guinea Africa 1982 43.662 285483 927.8253427 487 Equatorial Guinea Africa 1977 42.024 192675 958.5668124 488 Equatorial Guinea Africa 1967 38.987 259864 915.5960025 489 Equatorial Guinea Africa 1972 40.516 277603 672.4122571 490 Equatorial Guinea Africa 1962 37.485 249220 582.8419714 491 Equatorial Guinea Africa 1957 35.983 232922 426.0964081 492 Equatorial Guinea Africa 1952 34.482 216964 375.6431231 493 Eritrea Africa 2007 58.04 4906585 641.3695236 494 Eritrea Africa 2002 55.24 4414865 765.3500015 495 Eritrea Africa 1997 53.378 4058319 913.47079 496 Eritrea Africa 1992 49.991 3668440 582.8585102 497 Eritrea Africa 1987 46.453 2915959 521.1341333 498 Eritrea Africa 1982 43.89 2637297 524.8758493 499 Eritrea Africa 1977 44.535 2512642 505.7538077 500 Eritrea Africa 1972 44.142 2260187 514.3242082 501 Eritrea Africa 1967 42.189 1820319 468.7949699 502 Eritrea Africa 1962 40.158 1666618 380.9958433 503 Eritrea Africa 1957 38.047 1542611 344.1618859 504 Eritrea Africa 1952 35.928 1438760 328.9405571 505 Ethiopia Africa 2007 52.947 76511887 690.8055759 506 Ethiopia Africa 2002 50.725 67946797 530.0535319 507 Ethiopia Africa 1997 49.402 59861301 515.8894013 508 Ethiopia Africa 1992 48.091 52088559 421.3534653 509 Ethiopia Africa 1987 46.684 42999530 573.7413142 510 Ethiopia Africa 1982 44.916 38111756 577.8607471 511 Ethiopia Africa 1977 44.51 34617799 556.8083834 512 Ethiopia Africa 1972 43.515 30770372 566.2439442 513 Ethiopia Africa 1967 42.115 27860297 516.1186438 514 Ethiopia Africa 1962 40.059 25145372 419.4564161 515 Ethiopia Africa 1957 36.667 22815614 378.9041632 516 Ethiopia Africa 1952 34.078 20860941 362.1462796 517 Finland Europe 2007 79.313 5238460 33207.0844 518 Finland Europe 2002 78.37 5193039 28204.59057 519 Finland Europe 1997 77.13 5134406 23723.9502 520 Finland Europe 1992 75.7 5041039 20647.16499 521 Finland Europe 1987 74.83 4931729 21141.01223 522 Finland Europe 1982 74.55 4826933 18533.15761 523 Finland Europe 1977 72.52 4738902 15605.42283 524 Finland Europe 1972 70.87 4639657 14358.8759 525 Finland Europe 1967 69.83 4605744 10921.63626 526 Finland Europe 1962 68.75 4491443 9371.842561 527 Finland Europe 1957 67.49 4324000 7545.415386 528 Finland Europe 1952 66.55 4090500 6424.519071 529 France Europe 2002 79.59 59925035 28926.03234 530 France Europe 1997 78.64 58623428 25889.78487 531 France Europe 1992 77.46 57374179 24703.79615 532 France Europe 2007 80.657 61083916 30470.0167 533 France Europe 1987 76.34 55630100 22066.44214 534 France Europe 1982 74.89 54433565 20293.89746 535 France Europe 1977 73.83 53165019 18292.63514 536 France Europe 1972 72.38 51732000 16107.19171 537 France Europe 1967 71.55 49569000 12999.91766 538 France Europe 1962 70.51 47124000 10560.48553 539 France Europe 1957 68.93 44310863 8662.834898 540 France Europe 1952 67.41 42459667 7029.809327 541 Gabon Africa 2007 56.735 1454867 13206.48452 542 Gabon Africa 2002 56.761 1299304 12521.71392 543 Gabon Africa 1997 60.461 1126189 14722.84188 544 Gabon Africa 1992 61.366 985739 13522.15752 545 Gabon Africa 1987 60.19 880397 11864.40844 546 Gabon Africa 1982 56.564 753874 15113.36194 547 Gabon Africa 1977 52.79 706367 21745.57328 548 Gabon Africa 1972 48.69 537977 11401.94841 549 Gabon Africa 1967 44.598 489004 8358.761987 550 Gabon Africa 1962 40.489 455661 6631.459222 551 Gabon Africa 1957 38.999 434904 4976.198099 552 Gabon Africa 1952 37.003 420702 4293.476475 553 Gambia Africa 2007 59.448 1688359 752.7497265 554 Gambia Africa 2002 58.041 1457766 660.5855997 555 Gambia Africa 1997 55.861 1235767 653.7301704 556 Gambia Africa 1992 52.644 1025384 665.6244126 557 Gambia Africa 1987 49.265 848406 611.6588611 558 Gambia Africa 1982 45.58 715523 835.8096108 559 Gambia Africa 1977 41.842 608274 884.7552507 560 Gambia Africa 1972 38.308 517101 756.0868363 561 Gambia Africa 1967 35.857 439593 734.7829124 562 Gambia Africa 1962 33.896 374020 599.650276 563 Gambia Africa 1957 32.065 323150 520.9267111 564 Gambia Africa 1952 30 284320 485.2306591 565 Germany Europe 2007 79.406 82400996 32170.37442 566 Germany Europe 2002 78.67 82350671 30035.80198 567 Germany Europe 1997 77.34 82011073 27788.88416 568 Germany Europe 1992 76.07 80597764 26505.30317 569 Germany Europe 1987 74.847 77718298 24639.18566 570 Germany Europe 1982 73.8 78335266 22031.53274 571 Germany Europe 1977 72.5 78160773 20512.92123 572 Germany Europe 1972 71 78717088 18016.18027 573 Germany Europe 1962 70.3 73739117 12902.46291 574 Germany Europe 1967 70.8 76368453 14745.62561 575 Germany Europe 1957 69.1 71019069 10187.82665 576 Germany Europe 1952 67.5 69145952 7144.114393 577 Ghana Africa 2007 60.022 22873338 1327.60891 578 Ghana Africa 2002 58.453 20550751 1111.984578 579 Ghana Africa 1997 58.556 18418288 1005.245812 580 Ghana Africa 1992 57.501 16278738 925.060154 581 Ghana Africa 1987 55.729 14168101 847.0061135 582 Ghana Africa 1982 53.744 11400338 876.032569 583 Ghana Africa 1977 51.756 10538093 993.2239571 584 Ghana Africa 1972 49.875 9354120 1178.223708 585 Ghana Africa 1962 46.452 7355248 1190.041118 586 Ghana Africa 1967 48.072 8490213 1125.69716 587 Ghana Africa 1957 44.779 6391288 1043.561537 588 Ghana Africa 1952 43.149 5581001 911.2989371 589 Greece Europe 2007 79.483 10706290 27538.41188 590 Greece Europe 2002 78.256 10603863 22514.2548 591 Greece Europe 1997 77.869 10502372 18747.69814 592 Greece Europe 1992 77.03 10325429 17541.49634 593 Greece Europe 1987 76.67 9974490 16120.52839 594 Greece Europe 1982 75.24 9786480 15268.42089 595 Greece Europe 1977 73.68 9308479 14195.52428 596 Greece Europe 1972 72.34 8888628 12724.82957 597 Greece Europe 1967 71 8716441 8513.097016 598 Greece Europe 1962 69.51 8448233 6017.190733 599 Greece Europe 1957 67.86 8096218 4916.299889 600 Greece Europe 1952 65.86 7733250 3530.690067 601 Guatemala Americas 2007 70.259 12572928 5186.050003 602 Guatemala Americas 2002 68.978 11178650 4858.347495 603 Guatemala Americas 1997 66.322 9803875 4684.313807 604 Guatemala Americas 1992 63.373 8486949 4439.45084 605 Guatemala Americas 1987 60.782 7326406 4246.485974 606 Guatemala Americas 1982 58.137 6395630 4820.49479 607 Guatemala Americas 1972 53.738 5149581 4031.408271 608 Guatemala Americas 1967 50.016 4690773 3242.531147 609 Guatemala Americas 1962 46.954 4208858 2750.364446 610 Guatemala Americas 1977 56.029 5703430 4879.992748 611 Guatemala Americas 1957 44.142 3640876 2617.155967 612 Guatemala Americas 1952 42.023 3146381 2428.237769 613 Guinea Africa 2002 53.676 8807818 945.5835837 614 Guinea Africa 1997 51.455 8048834 869.4497668 615 Guinea Africa 1992 48.576 6990574 794.3484384 616 Guinea Africa 2007 56.007 9947814 942.6542111 617 Guinea Africa 1987 45.552 5650262 805.5724718 618 Guinea Africa 1982 42.891 4710497 857.2503577 619 Guinea Africa 1977 40.762 4227026 874.6858643 620 Guinea Africa 1972 38.842 3811387 741.6662307 621 Guinea Africa 1967 37.197 3451418 708.7595409 622 Guinea Africa 1962 35.753 3140003 686.3736739 623 Guinea Africa 1957 34.558 2876726 576.2670245 624 Guinea Africa 1952 33.609 2664249 510.1964923 625 Guinea-Bissau Africa 2002 45.504 1332459 575.7047176 626 Guinea-Bissau Africa 1997 44.873 1193708 796.6644681 627 Guinea-Bissau Africa 1992 43.266 1050938 745.5398706 628 Guinea-Bissau Africa 2007 46.388 1472041 579.231743 629 Guinea-Bissau Africa 1987 41.245 927524 736.4153921 630 Guinea-Bissau Africa 1982 39.327 825987 838.1239671 631 Guinea-Bissau Africa 1977 37.465 745228 764.7259628 632 Guinea-Bissau Africa 1972 36.486 625361 820.2245876 633 Guinea-Bissau Africa 1967 35.492 601287 715.5806402 634 Guinea-Bissau Africa 1962 34.488 627820 522.0343725 635 Guinea-Bissau Africa 1957 33.489 601095 431.7904566 636 Guinea-Bissau Africa 1952 32.5 580653 299.850319 637 Haiti Americas 2007 60.916 8502814 1201.637154 638 Haiti Americas 2002 58.137 7607651 1270.364932 639 Haiti Americas 1997 56.671 6913545 1341.726931 640 Haiti Americas 1992 55.089 6326682 1456.309517 641 Haiti Americas 1987 53.636 5756203 1823.015995 642 Haiti Americas 1982 51.461 5198399 2011.159549 643 Haiti Americas 1977 49.923 4908554 1874.298931 644 Haiti Americas 1972 48.042 4698301 1654.456946 645 Haiti Americas 1967 46.243 4318137 1452.057666 646 Haiti Americas 1962 43.59 3880130 1796.589032 647 Haiti Americas 1957 40.696 3507701 1726.887882 648 Haiti Americas 1952 37.579 3201488 1840.366939 649 Honduras Americas 2007 70.198 7483763 3548.330846 650 Honduras Americas 2002 68.565 6677328 3099.72866 651 Honduras Americas 1997 67.659 5867957 3160.454906 652 Honduras Americas 1992 66.399 5077347 3081.694603 653 Honduras Americas 1987 64.492 4372203 3023.096699 654 Honduras Americas 1982 60.909 3669448 3121.760794 655 Honduras Americas 1977 57.402 3055235 3203.208066 656 Honduras Americas 1972 53.884 2965146 2529.842345 657 Honduras Americas 1967 50.924 2500689 2538.269358 658 Honduras Americas 1962 48.041 2090162 2291.156835 659 Honduras Americas 1957 44.665 1770390 2220.487682 660 Honduras Americas 1952 41.912 1517453 2194.926204 661 Hong Kong, China Asia 2007 82.208 6980412 39724.97867 662 Hong Kong, China Asia 2002 81.495 6762476 30209.01516 663 Hong Kong, China Asia 1997 80 6495918 28377.63219 664 Hong Kong, China Asia 1992 77.601 5829696 24757.60301 665 Hong Kong, China Asia 1987 76.2 5584510 20038.47269 666 Hong Kong, China Asia 1982 75.45 5264500 14560.53051 667 Hong Kong, China Asia 1977 73.6 4583700 11186.14125 668 Hong Kong, China Asia 1972 72 4115700 8315.928145 669 Hong Kong, China Asia 1967 70 3722800 6197.962814 670 Hong Kong, China Asia 1962 67.65 3305200 4692.648272 671 Hong Kong, China Asia 1957 64.75 2736300 3629.076457 672 Hong Kong, China Asia 1952 60.96 2125900 3054.421209 673 Hungary Europe 2007 73.338 9956108 18008.94444 674 Hungary Europe 2002 72.59 10083313 14843.93556 675 Hungary Europe 1997 71.04 10244684 11712.7768 676 Hungary Europe 1992 69.17 10348684 10535.62855 677 Hungary Europe 1987 69.58 10612740 12986.47998 678 Hungary Europe 1982 69.39 10705535 12545.99066 679 Hungary Europe 1977 69.95 10637171 11674.83737 680 Hungary Europe 1972 69.76 10394091 10168.65611 681 Hungary Europe 1967 69.5 10223422 9326.64467 682 Hungary Europe 1962 67.96 10063000 7550.359877 683 Hungary Europe 1957 66.41 9839000 6040.180011 684 Hungary Europe 1952 64.03 9504000 5263.673816 685 Iceland Europe 2007 81.757 301931 36180.78919 686 Iceland Europe 2002 80.5 288030 31163.20196 687 Iceland Europe 1997 78.95 271192 28061.09966 688 Iceland Europe 1992 78.77 259012 25144.39201 689 Iceland Europe 1987 77.23 244676 26923.20628 690 Iceland Europe 1982 76.99 233997 23269.6075 691 Iceland Europe 1977 76.11 221823 19654.96247 692 Iceland Europe 1972 74.46 209275 15798.06362 693 Iceland Europe 1967 73.73 198676 13319.89568 694 Iceland Europe 1962 73.68 182053 10350.15906 695 Iceland Europe 1957 73.47 165110 9244.001412 696 Iceland Europe 1952 72.49 147962 7267.688428 697 India Asia 2007 64.698 1110396331 2452.210407 698 India Asia 2002 62.879 1034172547 1746.769454 699 India Asia 1997 61.765 959000000 1458.817442 700 India Asia 1992 60.223 872000000 1164.406809 701 India Asia 1987 58.553 788000000 976.5126756 702 India Asia 1982 56.596 708000000 855.7235377 703 India Asia 1977 54.208 634000000 813.337323 704 India Asia 1972 50.651 567000000 724.032527 705 India Asia 1967 47.193 506000000 700.7706107 706 India Asia 1962 43.605 454000000 658.3471509 707 India Asia 1957 40.249 409000000 590.061996 708 India Asia 1952 37.373 372000000 546.5657493 709 Indonesia Asia 2007 70.65 223547000 3540.651564 710 Indonesia Asia 2002 68.588 211060000 2873.91287 711 Indonesia Asia 1997 66.041 199278000 3119.335603 712 Indonesia Asia 1992 62.681 184816000 2383.140898 713 Indonesia Asia 1987 60.137 169276000 1748.356961 714 Indonesia Asia 1982 56.159 153343000 1516.872988 715 Indonesia Asia 1977 52.702 136725000 1382.702056 716 Indonesia Asia 1972 49.203 121282000 1111.107907 717 Indonesia Asia 1967 45.964 109343000 762.4317721 718 Indonesia Asia 1962 42.518 99028000 849.2897701 719 Indonesia Asia 1957 39.918 90124000 858.9002707 720 Indonesia Asia 1952 37.468 82052000 749.6816546 721 Iran Asia 2007 70.964 69453570 11605.71449 722 Iran Asia 2002 69.451 66907826 9240.761975 723 Iran Asia 1997 68.042 63327987 8263.590301 724 Iran Asia 1992 65.742 60397973 7235.653188 725 Iran Asia 1987 63.04 51889696 6642.881371 726 Iran Asia 1982 59.62 43072751 7608.334602 727 Iran Asia 1977 57.702 35480679 11888.59508 728 Iran Asia 1972 55.234 30614000 9613.818607 729 Iran Asia 1967 52.469 26538000 5906.731805 730 Iran Asia 1962 49.325 22874000 4187.329802 731 Iran Asia 1957 47.181 19792000 3290.257643 732 Iran Asia 1952 44.869 17272000 3035.326002 733 Iraq Asia 2007 59.545 27499638 4471.061906 734 Iraq Asia 2002 57.046 24001816 4390.717312 735 Iraq Asia 1997 58.811 20775703 3076.239795 736 Iraq Asia 1992 59.461 17861905 3745.640687 737 Iraq Asia 1987 65.044 16543189 11643.57268 738 Iraq Asia 1982 62.038 14173318 14517.90711 739 Iraq Asia 1977 60.413 11882916 14688.23507 740 Iraq Asia 1972 56.95 10061506 9576.037596 741 Iraq Asia 1967 54.459 8519282 8931.459811 742 Iraq Asia 1962 51.457 7240260 8341.737815 743 Iraq Asia 1957 48.437 6248643 6229.333562 744 Iraq Asia 1952 45.32 5441766 4129.766056 745 Ireland Europe 2007 78.885 4109086 40675.99635 746 Ireland Europe 2002 77.783 3879155 34077.04939 747 Ireland Europe 1997 76.122 3667233 24521.94713 748 Ireland Europe 1992 75.467 3557761 17558.81555 749 Ireland Europe 1987 74.36 3539900 13872.86652 750 Ireland Europe 1982 73.1 3480000 12618.32141 751 Ireland Europe 1977 72.03 3271900 11150.98113 752 Ireland Europe 1972 71.28 3024400 9530.772896 753 Ireland Europe 1967 71.08 2900100 7655.568963 754 Ireland Europe 1962 70.29 2830000 6631.597314 755 Ireland Europe 1957 68.9 2878220 5599.077872 756 Ireland Europe 1952 66.91 2952156 5210.280328 757 Israel Asia 2007 80.745 6426679 25523.2771 758 Israel Asia 2002 79.696 6029529 21905.59514 759 Israel Asia 1997 78.269 5531387 20896.60924 760 Israel Asia 1992 76.93 4936550 18051.52254 761 Israel Asia 1987 75.6 4203148 17122.47986 762 Israel Asia 1982 74.45 3858421 15367.0292 763 Israel Asia 1977 73.06 3495918 13306.61921 764 Israel Asia 1972 71.63 3095893 12786.93223 765 Israel Asia 1967 70.75 2693585 8393.741404 766 Israel Asia 1962 69.39 2310904 7105.630706 767 Israel Asia 1957 67.84 1944401 5385.278451 768 Israel Asia 1952 65.39 1620914 4086.522128 769 Italy Europe 2007 80.546 58147733 28569.7197 770 Italy Europe 2002 80.24 57926999 27968.09817 771 Italy Europe 1997 78.82 57479469 24675.02446 772 Italy Europe 1992 77.44 56840847 22013.64486 773 Italy Europe 1987 76.42 56729703 19207.23482 774 Italy Europe 1982 74.98 56535636 16537.4835 775 Italy Europe 1977 73.48 56059245 14255.98475 776 Italy Europe 1972 72.19 54365564 12269.27378 777 Italy Europe 1967 71.06 52667100 10022.40131 778 Italy Europe 1962 69.24 50843200 8243.58234 779 Italy Europe 1957 67.81 49182000 6248.656232 780 Italy Europe 1952 65.94 47666000 4931.404155 781 Jamaica Americas 2007 72.567 2780132 7320.880262 782 Jamaica Americas 2002 72.047 2664659 6994.774861 783 Jamaica Americas 1997 72.262 2531311 7121.924704 784 Jamaica Americas 1992 71.766 2378618 7404.923685 785 Jamaica Americas 1987 71.77 2326606 6351.237495 786 Jamaica Americas 1982 71.21 2298309 6068.05135 787 Jamaica Americas 1977 70.11 2156814 6650.195573 788 Jamaica Americas 1972 69 1997616 7433.889293 789 Jamaica Americas 1967 67.51 1861096 6124.703451 790 Jamaica Americas 1962 65.61 1665128 5246.107524 791 Jamaica Americas 1957 62.61 1535090 4756.525781 792 Jamaica Americas 1952 58.53 1426095 2898.530881 793 Japan Asia 2007 82.603 127467972 31656.06806 794 Japan Asia 2002 82 127065841 28604.5919 795 Japan Asia 1997 80.69 125956499 28816.58499 796 Japan Asia 1992 79.36 124329269 26824.89511 797 Japan Asia 1987 78.67 122091325 22375.94189 798 Japan Asia 1982 77.11 118454974 19384.10571 799 Japan Asia 1977 75.38 113872473 16610.37701 800 Japan Asia 1972 73.42 107188273 14778.78636 801 Japan Asia 1967 71.43 100825279 9847.788607 802 Japan Asia 1962 68.73 95831757 6576.649461 803 Japan Asia 1957 65.5 91563009 4317.694365 804 Japan Asia 1952 63.03 86459025 3216.956347 805 Jordan Asia 2007 72.535 6053193 4519.461171 806 Jordan Asia 2002 71.263 5307470 3844.917194 807 Jordan Asia 1997 69.772 4526235 3645.379572 808 Jordan Asia 1992 68.015 3867409 3431.593647 809 Jordan Asia 1987 65.869 2820042 4448.679912 810 Jordan Asia 1982 63.739 2347031 4161.415959 811 Jordan Asia 1977 61.134 1937652 2852.351568 812 Jordan Asia 1972 56.528 1613551 2110.856309 813 Jordan Asia 1967 51.629 1255058 2741.796252 814 Jordan Asia 1962 48.126 933559 2348.009158 815 Jordan Asia 1957 45.669 746559 1886.080591 816 Jordan Asia 1952 43.158 607914 1546.907807 817 Kenya Africa 2007 54.11 35610177 1463.249282 818 Kenya Africa 2002 50.992 31386842 1287.514732 819 Kenya Africa 1997 54.407 28263827 1360.485021 820 Kenya Africa 1992 59.285 25020539 1341.921721 821 Kenya Africa 1987 59.339 21198082 1361.936856 822 Kenya Africa 1982 58.766 17661452 1348.225791 823 Kenya Africa 1977 56.155 14500404 1267.613204 824 Kenya Africa 1972 53.559 12044785 1222.359968 825 Kenya Africa 1967 50.654 10191512 1056.736457 826 Kenya Africa 1962 47.949 8678557 896.9663732 827 Kenya Africa 1957 44.686 7454779 944.4383152 828 Kenya Africa 1952 42.27 6464046 853.540919 829 Korea, Dem. Rep. Asia 2007 67.297 23301725 1593.06548 830 Korea, Dem. Rep. Asia 2002 66.662 22215365 1646.758151 831 Korea, Dem. Rep. Asia 1997 67.727 21585105 1690.756814 832 Korea, Dem. Rep. Asia 1992 69.978 20711375 3726.063507 833 Korea, Dem. Rep. Asia 1987 70.647 19067554 4106.492315 834 Korea, Dem. Rep. Asia 1982 69.1 17647518 4106.525293 835 Korea, Dem. Rep. Asia 1977 67.159 16325320 4106.301249 836 Korea, Dem. Rep. Asia 1972 63.983 14781241 3701.621503 837 Korea, Dem. Rep. Asia 1967 59.942 12617009 2143.540609 838 Korea, Dem. Rep. Asia 1962 56.656 10917494 1621.693598 839 Korea, Dem. Rep. Asia 1957 54.081 9411381 1571.134655 840 Korea, Dem. Rep. Asia 1952 50.056 8865488 1088.277758 841 Korea, Rep. Asia 2007 78.623 49044790 23348.13973 842 Korea, Rep. Asia 2002 77.045 47969150 19233.98818 843 Korea, Rep. Asia 1997 74.647 46173816 15993.52796 844 Korea, Rep. Asia 1992 72.244 43805450 12104.27872 845 Korea, Rep. Asia 1987 69.81 41622000 8533.088805 846 Korea, Rep. Asia 1982 67.123 39326000 5622.942464 847 Korea, Rep. Asia 1977 64.766 36436000 4657.22102 848 Korea, Rep. Asia 1972 62.612 33505000 3030.87665 849 Korea, Rep. Asia 1967 57.716 30131000 2029.228142 850 Korea, Rep. Asia 1962 55.292 26420307 1536.344387 851 Korea, Rep. Asia 1957 52.681 22611552 1487.593537 852 Korea, Rep. Asia 1952 47.453 20947571 1030.592226 853 Kuwait Asia 2007 77.588 2505559 47306.98978 854 Kuwait Asia 2002 76.904 2111561 35110.10566 855 Kuwait Asia 1997 76.156 1765345 40300.61996 856 Kuwait Asia 1992 75.19 1418095 34932.91959 857 Kuwait Asia 1987 74.174 1891487 28118.42998 858 Kuwait Asia 1982 71.309 1497494 31354.03573 859 Kuwait Asia 1977 69.343 1140357 59265.47714 860 Kuwait Asia 1972 67.712 841934 109347.867 861 Kuwait Asia 1967 64.624 575003 80894.88326 862 Kuwait Asia 1962 60.47 358266 95458.11176 863 Kuwait Asia 1957 58.033 212846 113523.1329 864 Kuwait Asia 1952 55.565 160000 108382.3529 865 Lebanon Asia 1992 69.292 3219994 6890.806854 866 Lebanon Asia 1987 67.926 3089353 5377.091329 867 Lebanon Asia 1982 66.983 3086876 7640.519521 868 Lebanon Asia 1977 66.099 3115787 8659.696836 869 Lebanon Asia 1972 65.421 2680018 7486.384341 870 Lebanon Asia 1967 63.87 2186894 6006.983042 871 Lebanon Asia 1962 62.094 1886848 5714.560611 872 Lebanon Asia 1957 59.489 1647412 6089.786934 873 Lebanon Asia 1952 55.928 1439529 4834.804067 874 Lebanon Asia 2007 71.993 3921278 10461.05868 875 Lebanon Asia 2002 71.028 3677780 9313.93883 876 Lebanon Asia 1997 70.265 3430388 8754.96385 877 Lesotho Africa 2007 42.592 2012649 1569.331442 878 Lesotho Africa 2002 44.593 2046772 1275.184575 879 Lesotho Africa 1997 55.558 1982823 1186.147994 880 Lesotho Africa 1992 59.685 1803195 977.4862725 881 Lesotho Africa 1987 57.18 1599200 773.9932141 882 Lesotho Africa 1982 55.078 1411807 797.2631074 883 Lesotho Africa 1977 52.208 1251524 745.3695408 884 Lesotho Africa 1972 49.767 1116779 496.5815922 885 Lesotho Africa 1967 48.492 996380 498.6390265 886 Lesotho Africa 1962 47.747 893143 411.8006266 887 Lesotho Africa 1957 45.047 813338 335.9971151 888 Lesotho Africa 1952 42.138 748747 298.8462121 889 Liberia Africa 2007 45.678 3193942 414.5073415 890 Liberia Africa 2002 43.753 2814651 531.4823679 891 Liberia Africa 1997 42.221 2200725 609.1739508 892 Liberia Africa 1992 40.802 1912974 636.6229191 893 Liberia Africa 1987 46.027 2269414 506.1138573 894 Liberia Africa 1982 44.852 1956875 572.1995694 895 Liberia Africa 1977 43.764 1703617 640.3224383 896 Liberia Africa 1972 42.614 1482628 803.0054535 897 Liberia Africa 1967 41.536 1279406 713.6036483 898 Liberia Africa 1962 40.502 1112796 634.1951625 899 Liberia Africa 1957 39.486 975950 620.9699901 900 Liberia Africa 1952 38.48 863308 575.5729961 901 Libya Africa 2007 73.952 6036914 12057.49928 902 Libya Africa 2002 72.737 5368585 9534.677467 903 Libya Africa 1997 71.555 4759670 9467.446056 904 Libya Africa 1992 68.755 4364501 9640.138501 905 Libya Africa 1987 66.234 3799845 11770.5898 906 Libya Africa 1982 62.155 3344074 17364.27538 907 Libya Africa 1977 57.442 2721783 21951.21176 908 Libya Africa 1972 52.773 2183877 21011.49721 909 Libya Africa 1967 50.227 1759224 18772.75169 910 Libya Africa 1962 47.808 1441863 6757.030816 911 Libya Africa 1957 45.289 1201578 3448.284395 912 Libya Africa 1952 42.723 1019729 2387.54806 913 Madagascar Africa 2007 59.443 19167654 1044.770126 914 Madagascar Africa 2002 57.286 16473477 894.6370822 915 Madagascar Africa 1997 54.978 14165114 986.2958956 916 Madagascar Africa 1992 52.214 12210395 1040.67619 917 Madagascar Africa 1987 49.35 10568642 1155.441948 918 Madagascar Africa 1982 48.969 9171477 1302.878658 919 Madagascar Africa 1977 46.881 8007166 1544.228586 920 Madagascar Africa 1972 44.851 7082430 1748.562982 921 Madagascar Africa 1967 42.881 6334556 1634.047282 922 Madagascar Africa 1962 40.848 5703324 1643.38711 923 Madagascar Africa 1957 38.865 5181679 1589.20275 924 Madagascar Africa 1952 36.681 4762912 1443.011715 925 Malawi Africa 2007 48.303 13327079 759.3499101 926 Malawi Africa 2002 45.009 11824495 665.4231186 927 Malawi Africa 1997 47.495 10419991 692.2758103 928 Malawi Africa 1992 49.42 10014249 563.2000145 929 Malawi Africa 1987 47.457 7824747 635.5173634 930 Malawi Africa 1982 45.642 6502825 632.8039209 931 Malawi Africa 1977 43.767 5637246 663.2236766 932 Malawi Africa 1972 41.766 4730997 584.6219709 933 Malawi Africa 1967 39.487 4147252 495.5147806 934 Malawi Africa 1962 38.41 3628608 427.9010856 935 Malawi Africa 1957 37.207 3221238 416.3698064 936 Malawi Africa 1952 36.256 2917802 369.1650802 937 Malaysia Asia 2007 74.241 24821286 12451.6558 938 Malaysia Asia 2002 73.044 22662365 10206.97794 939 Malaysia Asia 1997 71.938 20476091 10132.90964 940 Malaysia Asia 1992 70.693 18319502 7277.912802 941 Malaysia Asia 1987 69.5 16331785 5249.802653 942 Malaysia Asia 1982 68 14441916 4920.355951 943 Malaysia Asia 1977 65.256 12845381 3827.921571 944 Malaysia Asia 1972 63.01 11441462 2849.09478 945 Malaysia Asia 1967 59.371 10154878 2277.742396 946 Malaysia Asia 1962 55.737 8906385 2036.884944 947 Malaysia Asia 1957 52.102 7739235 1810.066992 948 Malaysia Asia 1952 48.463 6748378 1831.132894 949 Mali Africa 2007 54.467 12031795 1042.581557 950 Mali Africa 2002 51.818 10580176 951.4097518 951 Mali Africa 1997 49.903 9384984 790.2579846 952 Mali Africa 1992 48.388 8416215 739.014375 953 Mali Africa 1987 46.364 7634008 684.1715576 954 Mali Africa 1982 43.916 6998256 618.0140641 955 Mali Africa 1977 41.714 6491649 686.3952693 956 Mali Africa 1972 39.977 5828158 581.3688761 957 Mali Africa 1967 38.487 5212416 545.0098873 958 Mali Africa 1962 36.936 4690372 496.1743428 959 Mali Africa 1957 35.307 4241884 490.3821867 960 Mali Africa 1952 33.685 3838168 452.3369807 961 Mauritania Africa 2007 64.164 3270065 1803.151496 962 Mauritania Africa 2002 62.247 2828858 1579.019543 963 Mauritania Africa 1997 60.43 2444741 1483.136136 964 Mauritania Africa 1992 58.333 2119465 1361.369784 965 Mauritania Africa 1987 56.145 1841240 1421.603576 966 Mauritania Africa 1982 53.599 1622136 1481.150189 967 Mauritania Africa 1977 50.852 1456688 1497.492223 968 Mauritania Africa 1972 48.437 1332786 1586.851781 969 Mauritania Africa 1967 46.289 1230542 1421.145193 970 Mauritania Africa 1962 44.248 1146757 1055.896036 971 Mauritania Africa 1957 42.338 1076852 846.1202613 972 Mauritania Africa 1952 40.543 1022556 743.1159097 973 Mauritius Africa 2007 72.801 1250882 10956.99112 974 Mauritius Africa 2002 71.954 1200206 9021.815894 975 Mauritius Africa 1997 70.736 1149818 7425.705295 976 Mauritius Africa 1992 69.745 1096202 6058.253846 977 Mauritius Africa 1987 68.74 1042663 4783.586903 978 Mauritius Africa 1982 66.711 992040 3688.037739 979 Mauritius Africa 1977 64.93 913025 3710.982963 980 Mauritius Africa 1972 62.944 851334 2575.484158 981 Mauritius Africa 1967 61.557 789309 2475.387562 982 Mauritius Africa 1962 60.246 701016 2529.067487 983 Mauritius Africa 1957 58.089 609816 2034.037981 984 Mauritius Africa 1952 50.986 516556 1967.955707 985 Mexico Americas 2007 76.195 108700891 11977.57496 986 Mexico Americas 2002 74.902 102479927 10742.44053 987 Mexico Americas 1997 73.67 95895146 9767.29753 988 Mexico Americas 1992 71.455 88111030 9472.384295 989 Mexico Americas 1987 69.498 80122492 8688.156003 990 Mexico Americas 1982 67.405 71640904 9611.147541 991 Mexico Americas 1977 65.032 63759976 7674.929108 992 Mexico Americas 1972 62.361 55984294 6809.40669 993 Mexico Americas 1967 60.11 47995559 5754.733883 994 Mexico Americas 1962 58.299 41121485 4581.609385 995 Mexico Americas 1957 55.19 35015548 4131.546641 996 Mexico Americas 1952 50.789 30144317 3478.125529 997 Mongolia Asia 2007 66.803 2874127 3095.772271 998 Mongolia Asia 2002 65.033 2674234 2140.739323 999 Mongolia Asia 1997 63.625 2494803 1902.2521 1000 Mongolia Asia 1992 61.271 2312802 1785.402016 1001 Mongolia Asia 1987 60.222 2015133 2338.008304 1002 Mongolia Asia 1982 57.489 1756032 2000.603139 1003 Mongolia Asia 1977 55.491 1528000 1647.511665 1004 Mongolia Asia 1972 53.754 1320500 1421.741975 1005 Mongolia Asia 1967 51.253 1149500 1226.04113 1006 Mongolia Asia 1962 48.251 1010280 1056.353958 1007 Mongolia Asia 1957 45.248 882134 912.6626085 1008 Mongolia Asia 1952 42.244 800663 786.5668575 1009 Montenegro Europe 2007 74.543 684736 9253.896111 1010 Montenegro Europe 2002 73.981 720230 6557.194282 1011 Montenegro Europe 1997 75.445 692651 6465.613349 1012 Montenegro Europe 1992 75.435 621621 7003.339037 1013 Montenegro Europe 1987 74.865 569473 11732.51017 1014 Montenegro Europe 1982 74.101 562548 11222.58762 1015 Montenegro Europe 1977 73.066 560073 9595.929905 1016 Montenegro Europe 1972 70.636 527678 7778.414017 1017 Montenegro Europe 1967 67.178 501035 5907.850937 1018 Montenegro Europe 1962 63.728 474528 4649.593785 1019 Montenegro Europe 1957 61.448 442829 3682.259903 1020 Montenegro Europe 1952 59.164 413834 2647.585601 1021 Morocco Africa 2007 71.164 33757175 3820.17523 1022 Morocco Africa 2002 69.615 31167783 3258.495584 1023 Morocco Africa 1997 67.66 28529501 2982.101858 1024 Morocco Africa 1992 65.393 25798239 2948.047252 1025 Morocco Africa 1987 62.677 22987397 2755.046991 1026 Morocco Africa 1982 59.65 20198730 2702.620356 1027 Morocco Africa 1977 55.73 18396941 2370.619976 1028 Morocco Africa 1972 52.862 16660670 1930.194975 1029 Morocco Africa 1967 50.335 14770296 1711.04477 1030 Morocco Africa 1962 47.924 13056604 1566.353493 1031 Morocco Africa 1957 45.423 11406350 1642.002314 1032 Morocco Africa 1952 42.873 9939217 1688.20357 1033 Mozambique Africa 2007 42.082 19951656 823.6856205 1034 Mozambique Africa 2002 44.026 18473780 633.6179466 1035 Mozambique Africa 1997 46.344 16603334 472.3460771 1036 Mozambique Africa 1992 44.284 13160731 410.8968239 1037 Mozambique Africa 1987 42.861 12891952 389.8761846 1038 Mozambique Africa 1982 42.795 12587223 462.2114149 1039 Mozambique Africa 1977 42.495 11127868 502.3197334 1040 Mozambique Africa 1972 40.328 9809596 724.9178037 1041 Mozambique Africa 1967 38.113 8680909 566.6691539 1042 Mozambique Africa 1962 36.161 7788944 556.6863539 1043 Mozambique Africa 1957 33.779 7038035 495.5868333 1044 Mozambique Africa 1952 31.286 6446316 468.5260381 1045 Myanmar Asia 2007 62.069 47761980 944 1046 Myanmar Asia 2002 59.908 45598081 611 1047 Myanmar Asia 1997 60.328 43247867 415 1048 Myanmar Asia 1992 59.32 40546538 347 1049 Myanmar Asia 1987 58.339 38028578 385 1050 Myanmar Asia 1982 58.056 34680442 424 1051 Myanmar Asia 1977 56.059 31528087 371 1052 Myanmar Asia 1972 53.07 28466390 357 1053 Myanmar Asia 1967 49.379 25870271 349 1054 Myanmar Asia 1962 45.108 23634436 388 1055 Myanmar Asia 1957 41.905 21731844 350 1056 Myanmar Asia 1952 36.319 20092996 331 1057 Namibia Africa 2007 52.906 2055080 4811.060429 1058 Namibia Africa 2002 51.479 1972153 4072.324751 1059 Namibia Africa 1997 58.909 1774766 3899.52426 1060 Namibia Africa 1992 61.999 1554253 3804.537999 1061 Namibia Africa 1987 60.835 1278184 3693.731337 1062 Namibia Africa 1982 58.968 1099010 4191.100511 1063 Namibia Africa 1977 56.437 977026 3876.485958 1064 Namibia Africa 1972 53.867 821782 3746.080948 1065 Namibia Africa 1967 51.159 706640 3793.694753 1066 Namibia Africa 1962 48.386 621392 3173.215595 1067 Namibia Africa 1957 45.226 548080 2621.448058 1068 Namibia Africa 1952 41.725 485831 2423.780443 1069 Nepal Asia 2007 63.785 28901790 1091.359778 1070 Nepal Asia 2002 61.34 25873917 1057.206311 1071 Nepal Asia 1997 59.426 23001113 1010.892138 1072 Nepal Asia 1992 55.727 20326209 897.7403604 1073 Nepal Asia 1987 52.537 17917180 775.6324501 1074 Nepal Asia 1982 49.594 15796314 718.3730947 1075 Nepal Asia 1977 46.748 13933198 694.1124398 1076 Nepal Asia 1972 43.971 12412593 674.7881296 1077 Nepal Asia 1967 41.472 11261690 676.4422254 1078 Nepal Asia 1962 39.393 10332057 652.3968593 1079 Nepal Asia 1957 37.686 9682338 597.9363558 1080 Nepal Asia 1952 36.157 9182536 545.8657229 1081 Netherlands Europe 2007 79.762 16570613 36797.93332 1082 Netherlands Europe 2002 78.53 16122830 33724.75778 1083 Netherlands Europe 1997 78.03 15604464 30246.13063 1084 Netherlands Europe 1992 77.42 15174244 26790.94961 1085 Netherlands Europe 1987 76.83 14665278 23651.32361 1086 Netherlands Europe 1982 76.05 14310401 21399.46046 1087 Netherlands Europe 1977 75.24 13852989 21209.0592 1088 Netherlands Europe 1972 73.75 13329874 18794.74567 1089 Netherlands Europe 1967 73.82 12596822 15363.25136 1090 Netherlands Europe 1962 73.23 11805689 12790.84956 1091 Netherlands Europe 1957 72.99 11026383 11276.19344 1092 Netherlands Europe 1952 72.13 10381988 8941.571858 1093 New Zealand Oceania 2007 80.204 4115771 25185.00911 1094 New Zealand Oceania 2002 79.11 3908037 23189.80135 1095 New Zealand Oceania 1997 77.55 3676187 21050.41377 1096 New Zealand Oceania 1992 76.33 3437674 18363.32494 1097 New Zealand Oceania 1987 74.32 3317166 19007.19129 1098 New Zealand Oceania 1982 73.84 3210650 17632.4104 1099 New Zealand Oceania 1977 72.22 3164900 16233.7177 1100 New Zealand Oceania 1972 71.89 2929100 16046.03728 1101 New Zealand Oceania 1967 71.52 2728150 14463.91893 1102 New Zealand Oceania 1962 71.24 2488550 13175.678 1103 New Zealand Oceania 1957 70.26 2229407 12247.39532 1104 New Zealand Oceania 1952 69.39 1994794 10556.57566 1105 Nicaragua Americas 2007 72.899 5675356 2749.320965 1106 Nicaragua Americas 2002 70.836 5146848 2474.548819 1107 Nicaragua Americas 1997 68.426 4609572 2253.023004 1108 Nicaragua Americas 1992 65.843 4017939 2170.151724 1109 Nicaragua Americas 1987 62.008 3344353 2955.984375 1110 Nicaragua Americas 1982 59.298 2979423 3470.338156 1111 Nicaragua Americas 1977 57.47 2554598 5486.371089 1112 Nicaragua Americas 1972 55.151 2182908 4688.593267 1113 Nicaragua Americas 1967 51.884 1865490 4643.393534 1114 Nicaragua Americas 1962 48.632 1590597 3634.364406 1115 Nicaragua Americas 1957 45.432 1358828 3457.415947 1116 Nicaragua Americas 1952 42.314 1165790 3112.363948 1117 Niger Africa 2007 56.867 12894865 619.6768924 1118 Niger Africa 2002 54.496 11140655 601.0745012 1119 Niger Africa 1997 51.313 9666252 580.3052092 1120 Niger Africa 1992 47.391 8392818 581.182725 1121 Niger Africa 1987 44.555 7332638 668.3000228 1122 Niger Africa 1982 42.598 6437188 909.7221354 1123 Niger Africa 1977 41.291 5682086 808.8970728 1124 Niger Africa 1972 40.546 5060262 954.2092363 1125 Niger Africa 1967 40.118 4534062 1054.384891 1126 Niger Africa 1962 39.487 4076008 997.7661127 1127 Niger Africa 1957 38.598 3692184 835.5234025 1128 Niger Africa 1952 37.444 3379468 761.879376 1129 Nigeria Africa 2007 46.859 135031164 2013.977305 1130 Nigeria Africa 2002 46.608 119901274 1615.286395 1131 Nigeria Africa 1997 47.464 106207839 1624.941275 1132 Nigeria Africa 1992 47.472 93364244 1619.848217 1133 Nigeria Africa 1987 46.886 81551520 1385.029563 1134 Nigeria Africa 1982 45.826 73039376 1576.97375 1135 Nigeria Africa 1977 44.514 62209173 1981.951806 1136 Nigeria Africa 1972 42.821 53740085 1698.388838 1137 Nigeria Africa 1967 41.04 47287752 1014.514104 1138 Nigeria Africa 1962 39.36 41871351 1150.927478 1139 Nigeria Africa 1957 37.802 37173340 1100.592563 1140 Nigeria Africa 1952 36.324 33119096 1077.281856 1141 Norway Europe 2007 80.196 4627926 49357.19017 1142 Norway Europe 2002 79.05 4535591 44683.97525 1143 Norway Europe 1997 78.32 4405672 41283.16433 1144 Norway Europe 1992 77.32 4286357 33965.66115 1145 Norway Europe 1987 75.89 4186147 31540.9748 1146 Norway Europe 1982 75.97 4114787 26298.63531 1147 Norway Europe 1977 75.37 4043205 23311.34939 1148 Norway Europe 1972 74.34 3933004 18965.05551 1149 Norway Europe 1967 74.08 3786019 16361.87647 1150 Norway Europe 1962 73.47 3638919 13450.40151 1151 Norway Europe 1957 73.44 3491938 11653.97304 1152 Norway Europe 1952 72.67 3327728 10095.42172 1153 Oman Asia 2007 75.64 3204897 22316.19287 1154 Oman Asia 2002 74.193 2713462 19774.83687 1155 Oman Asia 1997 72.499 2283635 19702.05581 1156 Oman Asia 1992 71.197 1915208 18616.70691 1157 Oman Asia 1987 67.734 1593882 18115.22313 1158 Oman Asia 1982 62.728 1301048 12954.79101 1159 Oman Asia 1977 57.367 1004533 11848.34392 1160 Oman Asia 1972 52.143 829050 10618.03855 1161 Oman Asia 1967 46.988 714775 4720.942687 1162 Oman Asia 1962 43.165 628164 2924.638113 1163 Oman Asia 1957 40.08 561977 2242.746551 1164 Oman Asia 1952 37.578 507833 1828.230307 1165 Pakistan Asia 2007 65.483 169270617 2605.94758 1166 Pakistan Asia 2002 63.61 153403524 2092.712441 1167 Pakistan Asia 1997 61.818 135564834 2049.350521 1168 Pakistan Asia 1992 60.838 120065004 1971.829464 1169 Pakistan Asia 1987 58.245 105186881 1704.686583 1170 Pakistan Asia 1982 56.158 91462088 1443.429832 1171 Pakistan Asia 1977 54.043 78152686 1175.921193 1172 Pakistan Asia 1972 51.929 69325921 1049.938981 1173 Pakistan Asia 1967 49.8 60641899 942.4082588 1174 Pakistan Asia 1962 47.67 53100671 803.3427418 1175 Pakistan Asia 1957 45.557 46679944 747.0835292 1176 Pakistan Asia 1952 43.436 41346560 684.5971438 1177 Panama Americas 2007 75.537 3242173 9809.185636 1178 Panama Americas 2002 74.712 2990875 7356.031934 1179 Panama Americas 1997 73.738 2734531 7113.692252 1180 Panama Americas 1992 72.462 2484997 6618.74305 1181 Panama Americas 1987 71.523 2253639 7034.779161 1182 Panama Americas 1982 70.472 2036305 7009.601598 1183 Panama Americas 1977 68.681 1839782 5351.912144 1184 Panama Americas 1972 66.216 1616384 5364.249663 1185 Panama Americas 1967 64.071 1405486 4421.009084 1186 Panama Americas 1962 61.817 1215725 3536.540301 1187 Panama Americas 1957 59.201 1063506 2961.800905 1188 Panama Americas 1952 55.191 940080 2480.380334 1189 Paraguay Americas 2002 70.755 5884491 3783.674243 1190 Paraguay Americas 2007 71.752 6667147 4172.838464 1191 Paraguay Americas 1997 69.4 5154123 4247.400261 1192 Paraguay Americas 1992 68.225 4483945 4196.411078 1193 Paraguay Americas 1987 67.378 3886512 3998.875695 1194 Paraguay Americas 1982 66.874 3366439 4258.503604 1195 Paraguay Americas 1977 66.353 2984494 3248.373311 1196 Paraguay Americas 1972 65.815 2614104 2523.337977 1197 Paraguay Americas 1967 64.951 2287985 2299.376311 1198 Paraguay Americas 1962 64.361 2009813 2148.027146 1199 Paraguay Americas 1957 63.196 1770902 2046.154706 1200 Paraguay Americas 1952 62.649 1555876 1952.308701 1201 Peru Americas 2007 71.421 28674757 7408.905561 1202 Peru Americas 2002 69.906 26769436 5909.020073 1203 Peru Americas 1997 68.386 24748122 5838.347657 1204 Peru Americas 1992 66.458 22430449 4446.380924 1205 Peru Americas 1987 64.134 20195924 6360.943444 1206 Peru Americas 1982 61.406 18125129 6434.501797 1207 Peru Americas 1977 58.447 15990099 6281.290855 1208 Peru Americas 1972 55.448 13954700 5937.827283 1209 Peru Americas 1967 51.445 12132200 5788.09333 1210 Peru Americas 1962 49.096 10516500 4957.037982 1211 Peru Americas 1957 46.263 9146100 4245.256698 1212 Peru Americas 1952 43.902 8025700 3758.523437 1213 Philippines Asia 2002 70.303 82995088 2650.921068 1214 Philippines Asia 2007 71.688 91077287 3190.481016 1215 Philippines Asia 1997 68.564 75012988 2536.534925 1216 Philippines Asia 1992 66.458 67185766 2279.324017 1217 Philippines Asia 1987 64.151 60017788 2189.634995 1218 Philippines Asia 1982 62.082 53456774 2603.273765 1219 Philippines Asia 1977 60.06 46850962 2373.204287 1220 Philippines Asia 1972 58.065 40850141 1989.37407 1221 Philippines Asia 1967 56.393 35356600 1814.12743 1222 Philippines Asia 1962 54.757 30325264 1649.552153 1223 Philippines Asia 1957 51.334 26072194 1547.944844 1224 Philippines Asia 1952 47.752 22438691 1272.880995 1225 Poland Europe 2002 74.67 38625976 12002.23908 1226 Poland Europe 1997 72.75 38654957 10159.58368 1227 Poland Europe 1992 70.99 38370697 7738.881247 1228 Poland Europe 2007 75.563 38518241 15389.92468 1229 Poland Europe 1987 70.98 37740710 9082.351172 1230 Poland Europe 1982 71.32 36227381 8451.531004 1231 Poland Europe 1977 70.67 34621254 9508.141454 1232 Poland Europe 1972 70.85 33039545 8006.506993 1233 Poland Europe 1967 69.61 31785378 6557.152776 1234 Poland Europe 1962 67.64 30329617 5338.752143 1235 Poland Europe 1957 65.77 28235346 4734.253019 1236 Poland Europe 1952 61.31 25730551 4029.329699 1237 Portugal Europe 2002 77.29 10433867 19970.90787 1238 Portugal Europe 2007 78.098 10642836 20509.64777 1239 Portugal Europe 1997 75.97 10156415 17641.03156 1240 Portugal Europe 1992 74.86 9927680 16207.26663 1241 Portugal Europe 1987 74.06 9915289 13039.30876 1242 Portugal Europe 1982 72.77 9859650 11753.84291 1243 Portugal Europe 1977 70.41 9662600 10172.48572 1244 Portugal Europe 1972 69.26 8970450 9022.247417 1245 Portugal Europe 1967 66.6 9103000 6361.517993 1246 Portugal Europe 1962 64.39 9019800 4727.954889 1247 Portugal Europe 1957 61.51 8817650 3774.571743 1248 Portugal Europe 1952 59.82 8526050 3068.319867 1249 Puerto Rico Americas 1997 74.917 3759430 16999.4333 1250 Puerto Rico Americas 1992 73.911 3585176 14641.58711 1251 Puerto Rico Americas 1987 74.63 3444468 12281.34191 1252 Puerto Rico Americas 1982 73.75 3279001 10330.98915 1253 Puerto Rico Americas 1977 73.44 3080828 9770.524921 1254 Puerto Rico Americas 1972 72.16 2847132 9123.041742 1255 Puerto Rico Americas 1967 71.1 2648961 6929.277714 1256 Puerto Rico Americas 1962 69.62 2448046 5108.34463 1257 Puerto Rico Americas 1957 68.54 2260000 3907.156189 1258 Puerto Rico Americas 1952 64.28 2227000 3081.959785 1259 Puerto Rico Americas 2007 78.746 3942491 19328.70901 1260 Puerto Rico Americas 2002 77.778 3859606 18855.60618 1261 Reunion Africa 1972 64.274 461633 5047.658563 1262 Reunion Africa 1982 69.885 517810 5267.219353 1263 Reunion Africa 2007 76.442 798094 7670.122558 1264 Reunion Africa 1977 67.064 492095 4319.804067 1265 Reunion Africa 1997 74.772 684810 6071.941411 1266 Reunion Africa 1992 73.615 622191 6101.255823 1267 Reunion Africa 1957 55.09 308700 2769.451844 1268 Reunion Africa 1962 57.666 358900 3173.72334 1269 Reunion Africa 1987 71.913 562035 5303.377488 1270 Reunion Africa 2002 75.744 743981 6316.1652 1271 Reunion Africa 1952 52.724 257700 2718.885295 1272 Reunion Africa 1967 60.542 414024 4021.175739 1273 Romania Europe 1982 69.66 22356726 9605.314053 1274 Romania Europe 1972 69.21 20662648 8011.414402 1275 Romania Europe 1962 66.8 18680721 4734.997586 1276 Romania Europe 1987 69.53 22686371 9696.273295 1277 Romania Europe 1952 61.05 16630000 3144.613186 1278 Romania Europe 1957 64.1 17829327 3943.370225 1279 Romania Europe 1992 69.36 22797027 6598.409903 1280 Romania Europe 2002 71.322 22404337 7885.360081 1281 Romania Europe 1997 69.72 22562458 7346.547557 1282 Romania Europe 1967 66.8 19284814 6470.866545 1283 Romania Europe 1977 69.46 21658597 9356.39724 1284 Romania Europe 2007 72.476 22276056 10808.47561 1285 Rwanda Africa 1957 41.5 2822082 540.2893983 1286 Rwanda Africa 1997 36.087 7212583 589.9445051 1287 Rwanda Africa 2007 46.242 8860588 863.0884639 1288 Rwanda Africa 1992 23.599 7290203 737.0685949 1289 Rwanda Africa 2002 43.413 7852401 785.6537648 1290 Rwanda Africa 1972 44.6 3992121 590.5806638 1291 Rwanda Africa 1977 45 4657072 670.0806011 1292 Rwanda Africa 1967 44.1 3451079 510.9637142 1293 Rwanda Africa 1962 43 3051242 597.4730727 1294 Rwanda Africa 1987 44.02 6349365 847.991217 1295 Rwanda Africa 1982 46.218 5507565 881.5706467 1296 Rwanda Africa 1952 40 2534927 493.3238752 1297 Sao Tome and Principe Africa 1962 51.893 65345 1071.551119 1298 Sao Tome and Principe Africa 1967 54.425 70787 1384.840593 1299 Sao Tome and Principe Africa 1952 46.471 60011 879.5835855 1300 Sao Tome and Principe Africa 1957 48.945 61325 860.7369026 1301 Sao Tome and Principe Africa 1972 56.48 76595 1532.985254 1302 Sao Tome and Principe Africa 1977 58.55 86796 1737.561657 1303 Sao Tome and Principe Africa 2007 65.528 199579 1598.435089 1304 Sao Tome and Principe Africa 2002 64.337 170372 1353.09239 1305 Sao Tome and Principe Africa 1997 63.306 145608 1339.076036 1306 Sao Tome and Principe Africa 1987 61.728 110812 1516.525457 1307 Sao Tome and Principe Africa 1992 62.742 125911 1428.777814 1308 Sao Tome and Principe Africa 1982 60.351 98593 1890.218117 1309 Saudi Arabia Asia 2002 71.626 24501530 19014.54118 1310 Saudi Arabia Asia 2007 72.777 27601038 21654.83194 1311 Saudi Arabia Asia 1997 70.533 21229759 20586.69019 1312 Saudi Arabia Asia 1992 68.768 16945857 24841.61777 1313 Saudi Arabia Asia 1987 66.295 14619745 21198.26136 1314 Saudi Arabia Asia 1977 58.69 8128505 34167.7626 1315 Saudi Arabia Asia 1982 63.012 11254672 33693.17525 1316 Saudi Arabia Asia 1972 53.886 6472756 24837.42865 1317 Saudi Arabia Asia 1967 49.901 5618198 16903.04886 1318 Saudi Arabia Asia 1957 42.868 4419650 8157.591248 1319 Saudi Arabia Asia 1952 39.875 4005677 6459.554823 1320 Saudi Arabia Asia 1962 45.914 4943029 11626.41975 1321 Senegal Africa 2007 63.062 12267493 1712.472136 1322 Senegal Africa 1987 55.769 7171347 1441.72072 1323 Senegal Africa 2002 61.6 10870037 1519.635262 1324 Senegal Africa 1982 52.379 6147783 1518.479984 1325 Senegal Africa 1997 60.187 9535314 1392.368347 1326 Senegal Africa 1992 58.196 8307920 1367.899369 1327 Senegal Africa 1972 45.815 4588696 1597.712056 1328 Senegal Africa 1962 41.454 3430243 1654.988723 1329 Senegal Africa 1952 37.278 2755589 1450.356983 1330 Senegal Africa 1957 39.329 3054547 1567.653006 1331 Senegal Africa 1967 43.563 3965841 1612.404632 1332 Senegal Africa 1977 48.879 5260855 1561.769116 1333 Serbia Europe 1957 61.685 7271135 4981.090891 1334 Serbia Europe 1992 71.659 9826397 9325.068238 1335 Serbia Europe 2007 74.002 10150265 9786.534714 1336 Serbia Europe 1997 72.232 10336594 7914.320304 1337 Serbia Europe 1987 71.218 9230783 15870.87851 1338 Serbia Europe 1982 70.162 9032824 15181.0927 1339 Serbia Europe 1972 68.7 8313288 10522.06749 1340 Serbia Europe 1967 66.914 7971222 7991.707066 1341 Serbia Europe 1962 64.531 7616060 6289.629157 1342 Serbia Europe 1977 70.3 8686367 12980.66956 1343 Serbia Europe 1952 57.996 6860147 3581.459448 1344 Serbia Europe 2002 73.213 10111559 7236.075251 1345 Sierra Leone Africa 2007 42.568 6144562 862.5407561 1346 Sierra Leone Africa 2002 41.012 5359092 699.489713 1347 Sierra Leone Africa 1992 38.333 4260884 1068.696278 1348 Sierra Leone Africa 1987 40.006 3868905 1294.447788 1349 Sierra Leone Africa 1997 39.897 4578212 574.6481576 1350 Sierra Leone Africa 1982 38.445 3464522 1465.010784 1351 Sierra Leone Africa 1967 34.113 2662190 1206.043465 1352 Sierra Leone Africa 1972 35.4 2879013 1353.759762 1353 Sierra Leone Africa 1962 32.767 2467895 1116.639877 1354 Sierra Leone Africa 1977 36.788 3140897 1348.285159 1355 Sierra Leone Africa 1952 30.331 2143249 879.7877358 1356 Sierra Leone Africa 1957 31.57 2295678 1004.484437 1357 Singapore Asia 1992 75.788 3235865 24769.8912 1358 Singapore Asia 1982 71.76 2651869 15169.16112 1359 Singapore Asia 1977 70.795 2325300 11210.08948 1360 Singapore Asia 1972 69.521 2152400 8597.756202 1361 Singapore Asia 1967 67.946 1977600 4977.41854 1362 Singapore Asia 1962 65.798 1750200 3674.735572 1363 Singapore Asia 1987 73.56 2794552 18861.53081 1364 Singapore Asia 1952 60.396 1127000 2315.138227 1365 Singapore Asia 1997 77.158 3802309 33519.4766 1366 Singapore Asia 1957 63.179 1445929 2843.104409 1367 Singapore Asia 2007 79.972 4553009 47143.17964 1368 Singapore Asia 2002 78.77 4197776 36023.1054 1369 Slovak Republic Europe 2007 74.663 5447502 18678.31435 1370 Slovak Republic Europe 2002 73.8 5410052 13638.77837 1371 Slovak Republic Europe 1987 71.08 5199318 12037.26758 1372 Slovak Republic Europe 1992 71.38 5302888 9498.467723 1373 Slovak Republic Europe 1982 70.8 5048043 11348.54585 1374 Slovak Republic Europe 1977 70.45 4827803 10922.66404 1375 Slovak Republic Europe 1972 70.35 4593433 9674.167626 1376 Slovak Republic Europe 1962 70.33 4237384 7481.107598 1377 Slovak Republic Europe 1967 70.98 4442238 8412.902397 1378 Slovak Republic Europe 1952 64.36 3558137 5074.659104 1379 Slovak Republic Europe 1997 72.71 5383010 12126.23065 1380 Slovak Republic Europe 1957 67.45 3844277 6093.26298 1381 Slovenia Europe 2007 77.926 2009245 25768.25759 1382 Slovenia Europe 1997 75.13 2011612 17161.10735 1383 Slovenia Europe 2002 76.66 2011497 20660.01936 1384 Slovenia Europe 1992 73.64 1999210 14214.71681 1385 Slovenia Europe 1987 72.25 1945870 18678.53492 1386 Slovenia Europe 1982 71.063 1861252 17866.72175 1387 Slovenia Europe 1977 70.97 1746919 15277.03017 1388 Slovenia Europe 1972 69.82 1694510 12383.4862 1389 Slovenia Europe 1967 69.18 1646912 9405.489397 1390 Slovenia Europe 1962 69.15 1582962 7402.303395 1391 Slovenia Europe 1957 67.85 1533070 5862.276629 1392 Slovenia Europe 1952 65.57 1489518 4215.041741 1393 Somalia Africa 2007 48.159 9118773 926.1410683 1394 Somalia Africa 2002 45.936 7753310 882.0818218 1395 Somalia Africa 1997 43.795 6633514 930.5964284 1396 Somalia Africa 1992 39.658 6099799 926.9602964 1397 Somalia Africa 1987 44.501 6921858 1093.244963 1398 Somalia Africa 1982 42.955 5828892 1176.807031 1399 Somalia Africa 1977 41.974 4353666 1450.992513 1400 Somalia Africa 1962 36.981 3080153 1369.488336 1401 Somalia Africa 1967 38.977 3428839 1284.73318 1402 Somalia Africa 1957 34.977 2780415 1258.147413 1403 Somalia Africa 1952 32.978 2526994 1135.749842 1404 Somalia Africa 1972 40.973 3840161 1254.576127 1405 South Africa Africa 2007 49.339 43997828 9269.657808 1406 South Africa Africa 2002 53.365 44433622 7710.946444 1407 South Africa Africa 1997 60.236 42835005 7479.188244 1408 South Africa Africa 1992 61.888 39964159 7225.069258 1409 South Africa Africa 1982 58.161 31140029 8568.266228 1410 South Africa Africa 1987 60.834 35933379 7825.823398 1411 South Africa Africa 1977 55.527 27129932 8028.651439 1412 South Africa Africa 1972 53.696 23935810 7765.962636 1413 South Africa Africa 1967 51.927 20997321 7114.477971 1414 South Africa Africa 1962 49.951 18356657 5768.729717 1415 South Africa Africa 1957 47.985 16151549 5487.104219 1416 South Africa Africa 1952 45.009 14264935 4725.295531 1417 Spain Europe 2007 80.941 40448191 28821.0637 1418 Spain Europe 2002 79.78 40152517 24835.47166 1419 Spain Europe 1997 78.77 39855442 20445.29896 1420 Spain Europe 1992 77.57 39549438 18603.06452 1421 Spain Europe 1987 76.9 38880702 15764.98313 1422 Spain Europe 1982 76.3 37983310 13926.16997 1423 Spain Europe 1977 74.39 36439000 13236.92117 1424 Spain Europe 1972 73.06 34513161 10638.75131 1425 Spain Europe 1967 71.44 32850275 7993.512294 1426 Spain Europe 1962 69.69 31158061 5693.843879 1427 Spain Europe 1957 66.66 29841614 4564.80241 1428 Spain Europe 1952 64.94 28549870 3834.034742 1429 Sri Lanka Asia 2007 72.396 20378239 3970.095407 1430 Sri Lanka Asia 2002 70.815 19576783 3015.378833 1431 Sri Lanka Asia 1997 70.457 18698655 2664.477257 1432 Sri Lanka Asia 1992 70.379 17587060 2153.739222 1433 Sri Lanka Asia 1987 69.011 16495304 1876.766827 1434 Sri Lanka Asia 1982 68.757 15410151 1648.079789 1435 Sri Lanka Asia 1977 65.949 14116836 1348.775651 1436 Sri Lanka Asia 1972 65.042 13016733 1213.39553 1437 Sri Lanka Asia 1967 64.266 11737396 1135.514326 1438 Sri Lanka Asia 1962 62.192 10421936 1074.47196 1439 Sri Lanka Asia 1957 61.456 9128546 1072.546602 1440 Sri Lanka Asia 1952 57.593 7982342 1083.53203 1441 Sudan Africa 2007 58.556 42292929 2602.394995 1442 Sudan Africa 2002 56.369 37090298 1993.398314 1443 Sudan Africa 1997 55.373 32160729 1632.210764 1444 Sudan Africa 1992 53.556 28227588 1492.197043 1445 Sudan Africa 1987 51.744 24725960 1507.819159 1446 Sudan Africa 1982 50.338 20367053 1895.544073 1447 Sudan Africa 1977 47.8 17104986 2202.988423 1448 Sudan Africa 1972 45.083 14597019 1659.652775 1449 Sudan Africa 1962 40.87 11183227 1959.593767 1450 Sudan Africa 1967 42.858 12716129 1687.997641 1451 Sudan Africa 1957 39.624 9753392 1770.337074 1452 Sudan Africa 1952 38.635 8504667 1615.991129 1453 Swaziland Africa 2007 39.613 1133066 4513.480643 1454 Swaziland Africa 2002 43.869 1130269 4128.116943 1455 Swaziland Africa 1997 54.289 1054486 3876.76846 1456 Swaziland Africa 1992 58.474 962344 3553.0224 1457 Swaziland Africa 1987 57.678 779348 3984.839812 1458 Swaziland Africa 1977 52.537 551425 3781.410618 1459 Swaziland Africa 1972 49.552 480105 3364.836625 1460 Swaziland Africa 1967 46.633 420690 2613.101665 1461 Swaziland Africa 1962 44.992 370006 1856.182125 1462 Swaziland Africa 1982 55.561 649901 3895.384018 1463 Swaziland Africa 1957 43.424 326741 1244.708364 1464 Swaziland Africa 1952 41.407 290243 1148.376626 1465 Sweden Europe 2007 80.884 9031088 33859.74835 1466 Sweden Europe 2002 80.04 8954175 29341.63093 1467 Sweden Europe 1997 79.39 8897619 25266.59499 1468 Sweden Europe 1992 78.16 8718867 23880.01683 1469 Sweden Europe 1987 77.19 8421403 23586.92927 1470 Sweden Europe 1977 75.44 8251648 18855.72521 1471 Sweden Europe 1972 74.72 8122293 17832.02464 1472 Sweden Europe 1967 74.16 7867931 15258.29697 1473 Sweden Europe 1962 73.37 7561588 12329.44192 1474 Sweden Europe 1957 72.49 7363802 9911.878226 1475 Sweden Europe 1952 71.86 7124673 8527.844662 1476 Sweden Europe 1982 76.42 8325260 20667.38125 1477 Switzerland Europe 1952 69.62 4815000 14734.23275 1478 Switzerland Europe 2007 81.701 7554661 37506.41907 1479 Switzerland Europe 2002 80.62 7361757 34480.95771 1480 Switzerland Europe 1997 79.37 7193761 32135.32301 1481 Switzerland Europe 1992 78.03 6995447 31871.5303 1482 Switzerland Europe 1987 77.41 6649942 30281.70459 1483 Switzerland Europe 1982 76.21 6468126 28397.71512 1484 Switzerland Europe 1977 75.39 6316424 26982.29052 1485 Switzerland Europe 1972 73.78 6401400 27195.11304 1486 Switzerland Europe 1967 72.77 6063000 22966.14432 1487 Switzerland Europe 1962 71.32 5666000 20431.0927 1488 Switzerland Europe 1957 70.56 5126000 17909.48973 1489 Syria Asia 2007 74.143 19314747 4184.548089 1490 Syria Asia 2002 73.053 17155814 4090.925331 1491 Syria Asia 1997 71.527 15081016 4014.238972 1492 Syria Asia 1992 69.249 13219062 3340.542768 1493 Syria Asia 1987 66.974 11242847 3116.774285 1494 Syria Asia 1982 64.59 9410494 3761.837715 1495 Syria Asia 1977 61.195 7932503 3195.484582 1496 Syria Asia 1972 57.296 6701172 2571.423014 1497 Syria Asia 1967 53.655 5680812 1881.923632 1498 Syria Asia 1962 50.305 4834621 2193.037133 1499 Syria Asia 1957 48.284 4149908 2117.234893 1500 Syria Asia 1952 45.883 3661549 1643.485354 1501 Taiwan Asia 2007 78.4 23174294 28718.27684 1502 Taiwan Asia 2002 76.99 22454239 23235.42329 1503 Taiwan Asia 1997 75.25 21628605 20206.82098 1504 Taiwan Asia 1992 74.26 20686918 15215.6579 1505 Taiwan Asia 1987 73.4 19757799 11054.56175 1506 Taiwan Asia 1982 72.16 18501390 7426.354774 1507 Taiwan Asia 1977 70.59 16785196 5596.519826 1508 Taiwan Asia 1972 69.39 15226039 4062.523897 1509 Taiwan Asia 1967 67.5 13648692 2643.858681 1510 Taiwan Asia 1962 65.2 11918938 1822.879028 1511 Taiwan Asia 1957 62.4 10164215 1507.86129 1512 Taiwan Asia 1952 58.5 8550362 1206.947913 1513 Tanzania Africa 2007 52.517 38139640 1107.482182 1514 Tanzania Africa 2002 49.651 34593779 899.0742111 1515 Tanzania Africa 1997 48.466 30686889 789.1862231 1516 Tanzania Africa 1992 50.44 26605473 825.682454 1517 Tanzania Africa 1987 51.535 23040630 831.8220794 1518 Tanzania Africa 1982 50.608 19844382 874.2426069 1519 Tanzania Africa 1977 49.919 17129565 962.4922932 1520 Tanzania Africa 1972 47.62 14706593 915.9850592 1521 Tanzania Africa 1967 45.757 12607312 848.2186575 1522 Tanzania Africa 1962 44.246 10863958 722.0038073 1523 Tanzania Africa 1957 42.974 9452826 698.5356073 1524 Tanzania Africa 1952 41.215 8322925 716.6500721 1525 Thailand Asia 2007 70.616 65068149 7458.396327 1526 Thailand Asia 2002 68.564 62806748 5913.187529 1527 Thailand Asia 1997 67.521 60216677 5852.625497 1528 Thailand Asia 1992 67.298 56667095 4616.896545 1529 Thailand Asia 1987 66.084 52910342 2982.653773 1530 Thailand Asia 1982 64.597 48827160 2393.219781 1531 Thailand Asia 1977 62.494 44148285 1961.224635 1532 Thailand Asia 1972 60.405 39276153 1524.358936 1533 Thailand Asia 1967 58.285 34024249 1295.46066 1534 Thailand Asia 1962 56.061 29263397 1002.199172 1535 Thailand Asia 1957 53.63 25041917 793.5774148 1536 Thailand Asia 1952 50.848 21289402 757.7974177 1537 Togo Africa 2007 58.42 5701579 882.9699438 1538 Togo Africa 2002 57.561 4977378 886.2205765 1539 Togo Africa 1997 58.39 4320890 982.2869243 1540 Togo Africa 1992 58.061 3747553 1034.298904 1541 Togo Africa 1987 56.941 3154264 1202.201361 1542 Togo Africa 1982 55.471 2644765 1344.577953 1543 Togo Africa 1977 52.887 2308582 1532.776998 1544 Togo Africa 1972 49.759 2056351 1649.660188 1545 Togo Africa 1967 46.769 1735550 1477.59676 1546 Togo Africa 1962 43.922 1528098 1067.53481 1547 Togo Africa 1957 41.208 1357445 925.9083202 1548 Togo Africa 1952 38.596 1219113 859.8086567 1549 Trinidad and Tobago Americas 2007 69.819 1056608 18008.50924 1550 Trinidad and Tobago Americas 2002 68.976 1101832 11460.60023 1551 Trinidad and Tobago Americas 1997 69.465 1138101 8792.573126 1552 Trinidad and Tobago Americas 1992 69.862 1183669 7370.990932 1553 Trinidad and Tobago Americas 1987 69.582 1191336 7388.597823 1554 Trinidad and Tobago Americas 1982 68.832 1116479 9119.528607 1555 Trinidad and Tobago Americas 1977 68.3 1039009 7899.554209 1556 Trinidad and Tobago Americas 1972 65.9 975199 6619.551419 1557 Trinidad and Tobago Americas 1967 65.4 960155 5621.368472 1558 Trinidad and Tobago Americas 1962 64.9 887498 4997.523971 1559 Trinidad and Tobago Americas 1957 61.8 764900 4100.3934 1560 Trinidad and Tobago Americas 1952 59.1 662850 3023.271928 1561 Tunisia Africa 2007 73.923 10276158 7092.923025 1562 Tunisia Africa 2002 73.042 9770575 5722.895655 1563 Tunisia Africa 1997 71.973 9231669 4876.798614 1564 Tunisia Africa 1992 70.001 8523077 4332.720164 1565 Tunisia Africa 1987 66.894 7724976 3810.419296 1566 Tunisia Africa 1982 64.048 6734098 3560.233174 1567 Tunisia Africa 1977 59.837 6005061 3120.876811 1568 Tunisia Africa 1972 55.602 5303507 2753.285994 1569 Tunisia Africa 1967 52.053 4786986 1932.360167 1570 Tunisia Africa 1962 49.579 4286552 1660.30321 1571 Tunisia Africa 1957 47.1 3950849 1395.232468 1572 Tunisia Africa 1952 44.6 3647735 1468.475631 1573 Turkey Europe 2007 71.777 71158647 8458.276384 1574 Turkey Europe 2002 70.845 67308928 6508.085718 1575 Turkey Europe 1997 68.835 63047647 6601.429915 1576 Turkey Europe 1992 66.146 58179144 5678.348271 1577 Turkey Europe 1987 63.108 52881328 5089.043686 1578 Turkey Europe 1982 61.036 47328791 4241.356344 1579 Turkey Europe 1977 59.507 42404033 4269.122326 1580 Turkey Europe 1972 57.005 37492953 3450.69638 1581 Turkey Europe 1967 54.336 33411317 2826.356387 1582 Turkey Europe 1962 52.098 29788695 2322.869908 1583 Turkey Europe 1957 48.079 25670939 2218.754257 1584 Turkey Europe 1952 43.585 22235677 1969.10098 1585 Uganda Africa 2007 51.542 29170398 1056.380121 1586 Uganda Africa 2002 47.813 24739869 927.7210018 1587 Uganda Africa 1997 44.578 21210254 816.559081 1588 Uganda Africa 1992 48.825 18252190 644.1707969 1589 Uganda Africa 1987 51.509 15283050 617.7244065 1590 Uganda Africa 1982 49.849 12939400 682.2662268 1591 Uganda Africa 1977 50.35 11457758 843.7331372 1592 Uganda Africa 1972 51.016 10190285 950.735869 1593 Uganda Africa 1967 48.051 8900294 908.9185217 1594 Uganda Africa 1962 45.344 7688797 767.2717398 1595 Uganda Africa 1957 42.571 6675501 774.3710692 1596 Uganda Africa 1952 39.978 5824797 734.753484 1597 United Kingdom Europe 2007 79.425 60776238 33203.26128 1598 United Kingdom Europe 2002 78.471 59912431 29478.99919 1599 United Kingdom Europe 1997 77.218 58808266 26074.53136 1600 United Kingdom Europe 1992 76.42 57866349 22705.09254 1601 United Kingdom Europe 1987 75.007 56981620 21664.78767 1602 United Kingdom Europe 1982 74.04 56339704 18232.42452 1603 United Kingdom Europe 1977 72.76 56179000 17428.74846 1604 United Kingdom Europe 1972 72.01 56079000 15895.11641 1605 United Kingdom Europe 1967 71.36 54959000 14142.85089 1606 United Kingdom Europe 1962 70.76 53292000 12477.17707 1607 United Kingdom Europe 1957 70.42 51430000 11283.17795 1608 United Kingdom Europe 1952 69.18 50430000 9979.508487 1609 United States Americas 2007 78.242 301139947 42951.65309 1610 United States Americas 2002 77.31 287675526 39097.09955 1611 United States Americas 1997 76.81 272911760 35767.43303 1612 United States Americas 1992 76.09 256894189 32003.93224 1613 United States Americas 1987 75.02 242803533 29884.35041 1614 United States Americas 1982 74.65 232187835 25009.55914 1615 United States Americas 1977 73.38 220239000 24072.63213 1616 United States Americas 1972 71.34 209896000 21806.03594 1617 United States Americas 1967 70.76 198712000 19530.36557 1618 United States Americas 1962 70.21 186538000 16173.14586 1619 United States Americas 1957 69.49 171984000 14847.12712 1620 United States Americas 1952 68.44 157553000 13990.48208 1621 Uruguay Americas 2007 76.384 3447496 10611.46299 1622 Uruguay Americas 2002 75.307 3363085 7727.002004 1623 Uruguay Americas 1997 74.223 3262838 9230.240708 1624 Uruguay Americas 1992 72.752 3149262 8137.004775 1625 Uruguay Americas 1987 71.918 3045153 7452.398969 1626 Uruguay Americas 1982 70.805 2953997 6920.223051 1627 Uruguay Americas 1977 69.481 2873520 6504.339663 1628 Uruguay Americas 1972 68.673 2829526 5703.408898 1629 Uruguay Americas 1967 68.468 2748579 5444.61962 1630 Uruguay Americas 1962 68.253 2598466 5603.357717 1631 Uruguay Americas 1957 67.044 2424959 6150.772969 1632 Uruguay Americas 1952 66.071 2252965 5716.766744 1633 Venezuela Americas 2007 73.747 26084662 11415.80569 1634 Venezuela Americas 1997 72.146 22374398 10165.49518 1635 Venezuela Americas 2002 72.766 24287670 8605.047831 1636 Venezuela Americas 1992 71.15 20265563 10733.92631 1637 Venezuela Americas 1987 70.19 17910182 9883.584648 1638 Venezuela Americas 1982 68.557 15620766 11152.41011 1639 Venezuela Americas 1977 67.456 13503563 13143.95095 1640 Venezuela Americas 1972 65.712 11515649 10505.25966 1641 Venezuela Americas 1967 63.479 9709552 9541.474188 1642 Venezuela Americas 1962 60.77 8143375 8422.974165 1643 Venezuela Americas 1957 57.907 6702668 9802.466526 1644 Venezuela Americas 1952 55.088 5439568 7689.799761 1645 Vietnam Asia 2007 74.249 85262356 2441.576404 1646 Vietnam Asia 1997 70.672 76048996 1385.896769 1647 Vietnam Asia 1992 67.662 69940728 989.0231487 1648 Vietnam Asia 1987 62.82 62826491 820.7994449 1649 Vietnam Asia 2002 73.017 80908147 1764.456677 1650 Vietnam Asia 1982 58.816 56142181 707.2357863 1651 Vietnam Asia 1977 55.764 50533506 713.5371196 1652 Vietnam Asia 1972 50.254 44655014 699.5016441 1653 Vietnam Asia 1967 47.838 39463910 637.1232887 1654 Vietnam Asia 1962 45.363 33796140 772.0491602 1655 Vietnam Asia 1957 42.887 28998543 676.2854478 1656 Vietnam Asia 1952 40.412 26246839 605.0664917 1657 West Bank and Gaza Asia 2007 73.422 4018332 3025.349798 1658 West Bank and Gaza Asia 1997 71.096 2826046 7110.667619 1659 West Bank and Gaza Asia 2002 72.37 3389578 4515.487575 1660 West Bank and Gaza Asia 1992 69.718 2104779 6017.654756 1661 West Bank and Gaza Asia 1987 67.046 1691210 5107.197384 1662 West Bank and Gaza Asia 1982 64.406 1425876 4336.032082 1663 West Bank and Gaza Asia 1977 60.765 1261091 3682.831494 1664 West Bank and Gaza Asia 1972 56.532 1089572 3133.409277 1665 West Bank and Gaza Asia 1967 51.631 1142636 2649.715007 1666 West Bank and Gaza Asia 1962 48.127 1133134 2198.956312 1667 West Bank and Gaza Asia 1957 45.671 1070439 1827.067742 1668 West Bank and Gaza Asia 1952 43.16 1030585 1515.592329 1669 Yemen, Rep. Asia 2007 62.698 22211743 2280.769906 1670 Yemen, Rep. Asia 2002 60.308 18701257 2234.820827 1671 Yemen, Rep. Asia 1997 58.02 15826497 2117.484526 1672 Yemen, Rep. Asia 1992 55.599 13367997 1879.496673 1673 Yemen, Rep. Asia 1987 52.922 11219340 1971.741538 1674 Yemen, Rep. Asia 1982 49.113 9657618 1977.55701 1675 Yemen, Rep. Asia 1977 44.175 8403990 1829.765177 1676 Yemen, Rep. Asia 1972 39.848 7407075 1265.047031 1677 Yemen, Rep. Asia 1967 36.984 6740785 862.4421463 1678 Yemen, Rep. Asia 1962 35.18 6120081 825.6232006 1679 Yemen, Rep. Asia 1957 33.97 5498090 804.8304547 1680 Yemen, Rep. Asia 1952 32.548 4963829 781.7175761 1681 Zambia Africa 1997 40.238 9417789 1071.353818 1682 Zambia Africa 1992 46.1 8381163 1210.884633 1683 Zambia Africa 1987 50.821 7272406 1213.315116 1684 Zambia Africa 2002 39.193 10595811 1071.613938 1685 Zambia Africa 1982 51.821 6100407 1408.678565 1686 Zambia Africa 1977 51.386 5216550 1588.688299 1687 Zambia Africa 1972 50.107 4506497 1773.498265 1688 Zambia Africa 1967 47.768 3900000 1777.077318 1689 Zambia Africa 1962 46.023 3421000 1452.725766 1690 Zambia Africa 1957 44.077 3016000 1311.956766 1691 Zambia Africa 1952 42.038 2672000 1147.388831 1692 Zambia Africa 2007 42.384 11746035 1271.211593 1693 Zimbabwe Africa 2007 43.487 12311143 469.7092981 1694 Zimbabwe Africa 2002 39.989 11926563 672.0386227 1695 Zimbabwe Africa 1997 46.809 11404948 792.4499603 1696 Zimbabwe Africa 1992 60.377 10704340 693.4207856 1697 Zimbabwe Africa 1987 62.351 9216418 706.1573059 1698 Zimbabwe Africa 1982 60.363 7636524 788.8550411 1699 Zimbabwe Africa 1977 57.674 6642107 685.5876821 1700 Zimbabwe Africa 1972 55.635 5861135 799.3621758 1701 Zimbabwe Africa 1967 53.995 4995432 569.7950712 1702 Zimbabwe Africa 1962 52.358 4277736 527.2721818 1703 Zimbabwe Africa 1957 50.469 3646340 518.7642681 1704 Zimbabwe Africa 1952 48.451 3080907 406.8841148 References Introduction to dplyr and Single Table dplyr functions Using WHERE with SAS Procedures PROC SQL documentation R for Data Science: Data Transformations Additional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State) Base R data manipulation SAS data manipulation Videos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well. See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎ The philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎ It accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎ This is equivalent to using right assignment in R with -&gt;, which you shouldn’t do unless you have a really good reason, because it’s hard to read.↩︎ or ?summarise if you like UK English – the developer of this package is from NZ↩︎ "],["transforming-data.html", "Module 7 Transforming Data Transforming Data: Module Objectives 7.1 Identifying the problem: Messy data 7.2 String operations: Creating new variables and separating multi-variable columns 7.3 Pivot operations 7.4 Relational Data and Joining Tables 7.5 Example: Gas Prices Data References", " Module 7 Transforming Data Happy families are all alike; every unhappy family is unhappy in its own way. - Leo Tolstoy Tidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham Transforming Data: Module Objectives Reshape data Transform variables to support analysis and visualization of data Join tables together in order to create a single coherent dataset Most of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data viewing, rather than for statistical analysis - they may be laid out so that there are multiple observations in a single row (e.g., commonly a year’s worth of data, with monthly observations in each column). Unfortunately, this type of data structure is not usually useful to us when we analyze or visualize the data. This section is going to seem like it drags on forever. It covers a lot of material, and a few different concepts. I highly recommend separating it out into 3-4 different “sessions” - Tidy data, Strings, Pivot operations, and Table Joins. For now, you need to know this material well enough to 1) identify what operation needs to happen, and 2) know where to find the sample code for that operation. It will get easier to remember the specific syntax with practice. Load initial packages library(dplyr) # Data wrangling library(tidyr) # Data rearranging library(tibble) # data table 7.1 Identifying the problem: Messy data These datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout. Table 7.1: Table 1 country year cases population Afghanistan 1999 745 19987071 Afghanistan 2000 2666 20595360 Brazil 1999 37737 172006362 Brazil 2000 80488 174504898 China 1999 212258 1272915272 China 2000 213766 1280428583 Here, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column). Table 7.2: Table 2 country year type count Afghanistan 1999 cases 745 Afghanistan 1999 population 19987071 Afghanistan 2000 cases 2666 Afghanistan 2000 population 20595360 Brazil 1999 cases 37737 Brazil 1999 population 172006362 Brazil 2000 cases 80488 Brazil 2000 population 174504898 China 1999 cases 212258 China 1999 population 1272915272 China 2000 cases 213766 China 2000 population 1280428583 Here, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1. Table 7.3: Table 3 country year rate Afghanistan 1999 745/19987071 Afghanistan 2000 2666/20595360 Brazil 1999 37737/172006362 Brazil 2000 80488/174504898 China 1999 212258/1272915272 China 2000 213766/1280428583 This form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being. Table 7.4: Table 4a country 1999 2000 Afghanistan 745 2666 Brazil 37737 80488 China 212258 213766 Table 7.4: Table 4b country 1999 2000 Afghanistan 19987071 20595360 Brazil 172006362 174504898 China 1272915272 1280428583 In this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year. Table 7.5: Table 5 country century year rate Afghanistan 19 99 745/19987071 Afghanistan 20 00 2666/20595360 Brazil 19 99 37737/172006362 Brazil 20 00 80488/174504898 China 19 99 212258/1272915272 China 20 00 213766/1280428583 Table 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would. These variations highlight the principles which can be said to define a tidy dataset: 1. Each variable must have its own column 2. Each observation must have its own row 3. Each value must have its own cell Try it out Go back through the 5 tables and determine whether each table is tidy, and if it is not, which rule or rules it violates. Figure out what you would have to do in order to compute a standardized TB infection rate per 100,000 people. Solution table1 - this is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000. table2 - each variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1. table3 - each value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1. table4a and table 4b - there are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1. table 5 - each variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first). It is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be: Each dataset goes into its own table (or tibble, if you are using R) Each variable gets its own column By the end of this module, you should have the skills to “tidy” each of these tables. 7.2 String operations: Creating new variables and separating multi-variable columns Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated). Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski Alternately, the xkcd version of the above quote The tidyverse package to deal with strings is stringr. The functions in stringr take the form of str_XXX where XXX is a verb. So str_split(), str_replace(), str_remove(), str_to_lower() all should make some sense. For this example, we’ll use a subset of the US Department of Education College Scorecard data. Documentation, Data. I’ve selected a few columns from the institution-level data available on the College Scorecard site. Let’s take a look (Read in the data) college &lt;- read_csv(&quot;data/College_Data_Abbrev.csv&quot;, guess_max = 5000, na = &#39;.&#39;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## UNITID = col_double(), ## INSTNM = col_character(), ## CITY = col_character(), ## STABBR = col_character(), ## ZIP = col_character(), ## ACCREDAGENCY = col_character(), ## INSTURL = col_character(), ## PREDDEG = col_character(), ## MAIN = col_character(), ## NUMBRANCH = col_double(), ## HIGHDEG = col_character(), ## CONTROL = col_character(), ## ST_FIPS = col_double(), ## LOCALE = col_double(), ## LATITUDE = col_double(), ## LONGITUDE = col_double(), ## State = col_character() ## ) str(college) ## spec_tbl_df [6,806 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ UNITID : num [1:6806] 100654 100663 100690 100706 100724 ... ## $ INSTNM : chr [1:6806] &quot;Alabama A &amp; M University&quot; &quot;University of Alabama at Birmingham&quot; &quot;Amridge University&quot; &quot;University of Alabama in Huntsville&quot; ... ## $ CITY : chr [1:6806] &quot;Normal&quot; &quot;Birmingham&quot; &quot;Montgomery&quot; &quot;Huntsville&quot; ... ## $ STABBR : chr [1:6806] &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; ... ## $ ZIP : chr [1:6806] &quot;35762&quot; &quot;35294-0110&quot; &quot;36117-3553&quot; &quot;35899&quot; ... ## $ ACCREDAGENCY: chr [1:6806] &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; ... ## $ INSTURL : chr [1:6806] &quot;www.aamu.edu/&quot; &quot;https://www.uab.edu&quot; &quot;www.amridgeuniversity.edu&quot; &quot;www.uah.edu&quot; ... ## $ PREDDEG : chr [1:6806] &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; ... ## $ MAIN : chr [1:6806] &quot;main campus&quot; &quot;main campus&quot; &quot;main campus&quot; &quot;main campus&quot; ... ## $ NUMBRANCH : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ... ## $ HIGHDEG : chr [1:6806] &quot;Graduate&quot; &quot;Graduate&quot; &quot;Graduate&quot; &quot;Graduate&quot; ... ## $ CONTROL : chr [1:6806] &quot;Public&quot; &quot;Public&quot; &quot;Private Non Profit&quot; &quot;Public&quot; ... ## $ ST_FIPS : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ... ## $ LOCALE : num [1:6806] 12 12 12 12 12 12 32 31 12 13 ... ## $ LATITUDE : num [1:6806] 34.8 33.5 32.4 34.7 32.4 ... ## $ LONGITUDE : num [1:6806] -86.6 -86.8 -86.2 -86.6 -86.3 ... ## $ State : chr [1:6806] &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. UNITID = col_double(), ## .. INSTNM = col_character(), ## .. CITY = col_character(), ## .. STABBR = col_character(), ## .. ZIP = col_character(), ## .. ACCREDAGENCY = col_character(), ## .. INSTURL = col_character(), ## .. PREDDEG = col_character(), ## .. MAIN = col_character(), ## .. NUMBRANCH = col_double(), ## .. HIGHDEG = col_character(), ## .. CONTROL = col_character(), ## .. ST_FIPS = col_double(), ## .. LOCALE = col_double(), ## .. LATITUDE = col_double(), ## .. LONGITUDE = col_double(), ## .. State = col_character() ## .. ) libname classdat &quot;sas/&quot;; filename fileloc &#39;data/College_Data_Abbrev.csv&#39;; PROC IMPORT datafile = fileloc out=classdat.college REPLACE DBMS = csv; /* comma delimited file */ GUESSINGROWS=500; GETNAMES = YES; RUN; PROC PRINT DATA = classdat.college (obs = 5); RUN; Obs UNITID INSTNM CITY STABBR ZIP ACCREDAGENCY INSTURL PREDDEG MAIN NUMBRANCH HIGHDEG CONTROL ST_FIPS LOCALE LATITUDE LONGITUDE State 1 100654 Alabama A &amp; M University Normal AL 35762 Southern Association of Colleges and Schools Commission on Colleges www.aamu.edu/ Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 34.783368 -86.568502 Alabama 2 100663 University of Alabama at Birmingham Birmingham AL 35294-0110 Southern Association of Colleges and Schools Commission on Colleges https://www.uab.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 33.505697 -86.799345 Alabama 3 100690 Amridge University Montgomery AL 36117-3553 Southern Association of Colleges and Schools Commission on Colleges www.amridgeuniversity.edu Predominantly bachelor's-degree granting main campus 1 Graduate Private Non Profit 1 12 32.362609 -86.17401 Alabama 4 100706 University of Alabama in Huntsville Huntsville AL 35899 Southern Association of Colleges and Schools Commission on Colleges www.uah.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 34.724557 -86.640449 Alabama 5 100724 Alabama State University Montgomery AL 36104-0271 Southern Association of Colleges and Schools Commission on Colleges www.alasu.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 32.364317 -86.295677 Alabama 7.2.1 Basic String Operations What proportion of the schools operating in each state have the state’s name in the school name? We’ll use str_detect() to look for the state name in the college name. library(stringr) # string processing # Outside the pipe str_detect(college$INSTNM, pattern = college$State) ## [1] TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [61] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [73] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [109] TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE FALSE TRUE ## [121] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## [133] FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## [157] FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE TRUE ## [169] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [193] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE ## [205] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [217] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [229] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [277] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [337] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [361] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [469] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [493] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [505] FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE ## [517] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [529] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [541] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE ## [553] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [565] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [589] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [601] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [613] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE ## [625] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [637] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [649] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [661] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [673] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [697] FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE ## [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [721] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE ## [733] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE ## [745] FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [781] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE ## [793] FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [829] FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE ## [841] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE ## [853] TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE ## [865] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [925] TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE FALSE ## [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [985] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1009] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [1021] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1045] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1057] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [1069] TRUE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE ## [1081] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1093] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1105] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1117] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1129] TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [1141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [1153] FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1165] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## [1177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [1189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1201] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE TRUE TRUE ## [1213] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE ## [1225] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1237] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [1249] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1261] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [1273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## [1285] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1297] TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [1309] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [1321] FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE ## [1333] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1345] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE ## [1357] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE ## [1369] TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE ## [1381] TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1393] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1405] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE ## [1417] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1429] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [1441] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1453] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1465] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1477] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1489] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1501] FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE ## [1513] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1525] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1537] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1549] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1573] FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [1585] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [1597] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1609] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1621] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [1633] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1645] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE ## [1657] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [1669] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1681] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [1693] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## [1705] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE ## [1717] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1729] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE ## [1741] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1753] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1765] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [1777] TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1789] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE ## [1801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1813] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [1825] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1837] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [1849] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1861] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [1873] FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE ## [1885] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## [1897] FALSE FALSE TRUE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE ## [1909] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [1921] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE ## [1933] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1945] TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE TRUE FALSE TRUE FALSE ## [1957] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [1969] FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [1981] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1993] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2005] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2017] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [2029] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2041] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [2053] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2065] TRUE FALSE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE ## [2077] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2089] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## [2101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [2113] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [2125] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2149] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [2161] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2173] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [2185] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2197] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2209] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2233] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2245] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2257] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [2269] TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [2281] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2293] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2305] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [2317] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2329] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2341] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2353] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2365] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2377] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2389] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2401] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2413] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2425] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2437] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2449] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [2461] TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2473] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2485] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2497] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2509] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2521] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2533] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2545] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2557] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2569] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2581] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2593] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2605] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2617] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [2629] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [2641] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2653] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2665] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE ## [2677] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2689] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2701] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2713] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [2725] FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2737] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2749] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE ## [2761] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [2773] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2785] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2797] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE ## [2809] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [2821] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [2833] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2845] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [2857] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE ## [2869] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE ## [2881] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2893] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2905] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [2917] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2929] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2941] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [2953] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [2965] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [2977] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [2989] TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3001] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3013] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3025] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [3037] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3049] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [3061] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [3073] FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE ## [3085] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3097] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [3121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [3133] TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [3169] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [3181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3193] TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE ## [3205] FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [3217] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [3229] FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## [3241] TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE ## [3253] TRUE TRUE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [3301] TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [3313] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3349] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3373] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [3385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3397] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## [3409] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [3421] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3433] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE TRUE ## [3445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## [3457] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3469] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE ## [3481] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [3493] FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE ## [3505] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE ## [3517] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [3553] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3565] FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE ## [3577] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [3589] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3601] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3613] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [3625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE ## [3649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3673] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE ## [3685] TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE FALSE ## [3697] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [3709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [3721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [3733] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [3745] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3757] FALSE TRUE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE ## [3769] TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [3781] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [3793] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [3805] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [3817] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [3829] TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE TRUE ## [3841] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE ## [3853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3877] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3901] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3925] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3937] FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [3949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [3961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3973] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3985] FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## [3997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [4009] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4033] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [4045] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [4057] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4069] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4081] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4093] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA ## [4105] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE ## [4117] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4141] FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4153] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [4165] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [4201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4213] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4225] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [4237] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [4249] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4273] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE ## [4285] TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [4297] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4309] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## [4321] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [4333] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4345] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4357] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [4369] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [4381] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4393] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [4405] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [4417] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [4429] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE ## [4441] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [4453] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [4465] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [4477] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4489] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [4501] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4513] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [4525] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## [4537] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [4549] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [4561] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4573] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [4585] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4597] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE ## [4609] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4621] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4633] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4645] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [4657] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [4669] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE ## [4681] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4693] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE ## [4705] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [4717] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4729] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [4741] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE ## [4753] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [4765] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE ## [4777] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4789] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4813] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4825] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [4837] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4849] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [4861] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4873] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4885] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4897] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4909] FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4921] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [4933] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE ## [4945] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## [4957] FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE ## [4969] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4981] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [4993] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5005] TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5017] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE ## [5029] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5041] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5053] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5065] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5077] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [5089] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE TRUE ## [5113] FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5125] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5149] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5161] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [5173] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5185] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [5197] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## [5209] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5233] FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [5245] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [5257] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5269] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [5281] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [5293] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5305] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [5317] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5329] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE ## [5341] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [5353] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5365] FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [5377] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5389] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [5401] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [5413] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5425] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [5437] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [5449] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5461] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5473] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [5485] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [5497] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [5509] TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [5521] FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [5533] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5545] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5557] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5569] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [5581] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [5593] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5605] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE ## [5617] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [5629] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5641] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [5653] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5665] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [5677] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5689] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5701] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [5713] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [5725] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5737] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [5749] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5761] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5773] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [5785] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [5797] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [5809] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## [5821] TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5833] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5845] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5857] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5869] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [5881] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5893] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE ## [5905] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE ## [5917] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [5929] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5941] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE ## [5953] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5965] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5977] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5989] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE ## [6001] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6013] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [6025] FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE ## [6037] TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [6049] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE ## [6061] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [6073] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6085] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6097] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6109] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## [6121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [6133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6145] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [6169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6181] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [6193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6229] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6241] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [6253] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6301] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [6313] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [6325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6337] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [6349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## [6361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## [6385] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [6397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6409] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [6421] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [6445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [6457] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6493] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE ## [6505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [6541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6565] FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [6577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [6589] FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [6601] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE ## [6613] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [6625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [6637] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [6649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6661] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE ## [6673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6685] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6697] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [6733] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6745] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6757] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6769] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [6781] TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE ## [6793] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [6805] FALSE FALSE # Using the pipe and mutate: college &lt;- college %&gt;% mutate(uses_st_name = str_detect(INSTNM, State)) library(ggplot2) # graphs and charts # By state - percentage of institution names college %&gt;% group_by(State) %&gt;% summarize(pct_uses_st_name = mean(uses_st_name), n = n()) %&gt;% filter(n &gt; 5) %&gt;% # only states/territories with at least 5 schools # Reorder state factor level by percentage that uses state name mutate(State = reorder(State, -pct_uses_st_name)) %&gt;% ggplot(data = ., aes(x = State, y = pct_uses_st_name)) + geom_col() + coord_flip() + geom_text(aes(y = 1, label = paste(&quot;Total Schools:&quot;, n)), hjust = 1) In SAS, we use find(x, pattern, 't') to find the location of the pattern, which is 0 if the pattern is not found. To get something equivalent to str_detect, we just test whether this quantity is greater than 0. (The R equivalent of find is str_locate()). Note that SAS pads character fields with spaces so that they are all the same length. So if we want to test for “Alabama” we could omit the ‘t’ option in the command, but since we usually don’t want that, we need to tell SAS to trim the fields before searching for the pattern. libname classdat &quot;sas/&quot;; DATA collegetmp; set classdat.college; uses_st_name = find(INSTNM, State, &#39;t&#39;) GT 0; RUN; PROC PRINT DATA = collegetmp (obs = 5); RUN; Obs UNITID INSTNM CITY STABBR ZIP ACCREDAGENCY INSTURL PREDDEG MAIN NUMBRANCH HIGHDEG CONTROL ST_FIPS LOCALE LATITUDE LONGITUDE State uses_st_name 1 100654 Alabama A &amp; M University Normal AL 35762 Southern Association of Colleges and Schools Commission on Colleges www.aamu.edu/ Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 34.783368 -86.568502 Alabama 1 2 100663 University of Alabama at Birmingham Birmingham AL 35294-0110 Southern Association of Colleges and Schools Commission on Colleges https://www.uab.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 33.505697 -86.799345 Alabama 1 3 100690 Amridge University Montgomery AL 36117-3553 Southern Association of Colleges and Schools Commission on Colleges www.amridgeuniversity.edu Predominantly bachelor's-degree granting main campus 1 Graduate Private Non Profit 1 12 32.362609 -86.17401 Alabama 0 4 100706 University of Alabama in Huntsville Huntsville AL 35899 Southern Association of Colleges and Schools Commission on Colleges www.uah.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 34.724557 -86.640449 Alabama 1 5 100724 Alabama State University Montgomery AL 36104-0271 Southern Association of Colleges and Schools Commission on Colleges www.alasu.edu Predominantly bachelor's-degree granting main campus 1 Graduate Public 1 12 32.364317 -86.295677 Alabama 1 What are some common substrings in a set of text? For this, we’ll start with working with the single column INSTNM. head(college$INSTNM) %&gt;% str_split(., &quot; &quot;) # Split on every space ## [[1]] ## [1] &quot;Alabama&quot; &quot;A&quot; &quot;&amp;&quot; &quot;M&quot; &quot;University&quot; ## ## [[2]] ## [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;at&quot; &quot;Birmingham&quot; ## ## [[3]] ## [1] &quot;Amridge&quot; &quot;University&quot; ## ## [[4]] ## [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;in&quot; &quot;Huntsville&quot; ## ## [[5]] ## [1] &quot;Alabama&quot; &quot;State&quot; &quot;University&quot; ## ## [[6]] ## [1] &quot;The&quot; &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; # We may need to fix certain things that should stay together # But doing too much of that gets tedious... str_replace(college$INSTNM, &quot;A &amp; M&quot;, &quot;A&amp;M&quot;) %&gt;% head() %&gt;% str_split(., &quot;[ -]&quot;) # This pattern says &quot;either &#39; &#39; or &#39;-&#39;&quot; ## [[1]] ## [1] &quot;Alabama&quot; &quot;A&amp;M&quot; &quot;University&quot; ## ## [[2]] ## [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;at&quot; &quot;Birmingham&quot; ## ## [[3]] ## [1] &quot;Amridge&quot; &quot;University&quot; ## ## [[4]] ## [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;in&quot; &quot;Huntsville&quot; ## ## [[5]] ## [1] &quot;Alabama&quot; &quot;State&quot; &quot;University&quot; ## ## [[6]] ## [1] &quot;The&quot; &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; # (but the - has to be at the start or the end) So we could take the time to clean up everything, making sure that e.g. San Diego is treated as a single word, but that’s a pain in the rear. Instead, let’s just see what happens if we brute-force it. tmp &lt;- college %&gt;% select(INSTNM, State) %&gt;% mutate(name_words = str_split(INSTNM, &#39;[ -]&#39;)) # This is a list-column tmp ## # A tibble: 6,806 x 3 ## INSTNM State name_words ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 Alabama A &amp; M University Alabama &lt;chr [5]&gt; ## 2 University of Alabama at Birmingham Alabama &lt;chr [5]&gt; ## 3 Amridge University Alabama &lt;chr [2]&gt; ## 4 University of Alabama in Huntsville Alabama &lt;chr [5]&gt; ## 5 Alabama State University Alabama &lt;chr [3]&gt; ## 6 The University of Alabama Alabama &lt;chr [4]&gt; ## 7 Central Alabama Community College Alabama &lt;chr [4]&gt; ## 8 Athens State University Alabama &lt;chr [3]&gt; ## 9 Auburn University at Montgomery Alabama &lt;chr [4]&gt; ## 10 Auburn University Alabama &lt;chr [2]&gt; ## # … with 6,796 more rows unnest(tmp) # Unnest duplicates rows so that the expanded data frame has the ## Warning: `cols` is now required when using unnest(). ## Please use `cols = c(name_words)` ## # A tibble: 28,510 x 3 ## INSTNM State name_words ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alabama A &amp; M University Alabama Alabama ## 2 Alabama A &amp; M University Alabama A ## 3 Alabama A &amp; M University Alabama &amp; ## 4 Alabama A &amp; M University Alabama M ## 5 Alabama A &amp; M University Alabama University ## 6 University of Alabama at Birmingham Alabama University ## 7 University of Alabama at Birmingham Alabama of ## 8 University of Alabama at Birmingham Alabama Alabama ## 9 University of Alabama at Birmingham Alabama at ## 10 University of Alabama at Birmingham Alabama Birmingham ## # … with 28,500 more rows # same structure as the original data List columns are one way to maintain tidy data. They allow you to have several “sub-observations” for each observation and are useful for precisely cases like this, where there are uneven numbers of words in each university’s name. We’re not going to focus on list columns, but if you’re interested, check out purrr and this excellent tutorial. library(purrr) # List columns ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## set_names unnest(tmp) %&gt;% pull(name_words) %&gt;% # this pulls out a single column table() %&gt;% sort(decreasing = T) %&gt;% head(50) ## Warning: `cols` is now required when using unnest(). ## Please use `cols = c(name_words)` ## . ## College of University School Institute ## 2716 1732 1723 821 690 ## Community Beauty Academy State ## 554 443 440 401 389 ## Technical and Center Technology Career ## 366 293 264 230 221 ## Cosmetology Campus &amp; the New ## 216 207 186 160 143 ## The County Medical Hair Nursing ## 143 136 133 132 129 ## San Seminary Health Design Education ## 126 123 118 116 114 ## American Paul at Valley Mitchell ## 109 106 103 102 97 ## Inc City Empire West Barber ## 95 94 92 89 88 ## South California International North Central ## 86 84 84 83 79 ## Florida Strayer Texas Theological Training ## 79 76 76 76 75 In SAS, this is a bit more tricky. Most people I know that use both SAS and R will do the data cleaning in R once things get complicated, and then read the clean data in to SAS. That’s a valid approach, but it’s worth seeing what has to be done in SAS this once. As we get further into this class, I’ll probably be more willing to say “we’re just going to use R for this” for two reasons - 1. I know R better, and 2. R is generally better at handling the weird stuff; SAS is built to quickly handle things that are already formatted in a reasonable way. SAS seems to be highly preferred for e.g. fitting mixed/linear models, but it isn’t the easiest tool to use for data cleaning. But, in this particular case, there is documentation about how to break a sentence into words in SAS. libname classdat &quot;sas/&quot;; DATA collegename; SET classdat.college; numWords = countw(INSTNM, &quot; &quot;); DO i = 1 TO numWords; word = scan(INSTNM, i, &quot; &quot;); OUTPUT; END; KEEP word numWords; ; PROC PRINT DATA=collegename (obs = 30); run; PROC FREQ DATA=collegename ORDER=FREQ; TABLES word / MAXLEVELS=30; RUN; Obs numWords word 1 5 Alabama 2 5 A 3 5 &amp; 4 5 M 5 5 University 6 5 University 7 5 of 8 5 Alabama 9 5 at 10 5 Birmingham 11 2 Amridge 12 2 University 13 5 University 14 5 of 15 5 Alabama 16 5 in 17 5 Huntsville 18 3 Alabama 19 3 State 20 3 University 21 4 The 22 4 University 23 4 of 24 4 Alabama 25 4 Central 26 4 Alabama 27 4 Community 28 4 College 29 3 Athens 30 3 State word Frequency Percent Cumulative Frequency Cumulative Percent College 2324 8.86 2324 8.86 of 1731 6.60 4055 15.47 University 1314 5.01 5369 20.48 School 604 2.30 5973 22.78 Community 553 2.11 6526 24.89 Institute 506 1.93 7032 26.82 Beauty 433 1.65 7465 28.47 State 381 1.45 7846 29.92 Technical 359 1.37 8205 31.29 Academy 311 1.19 8516 32.48 and 293 1.12 8809 33.60 Career 219 0.84 9028 34.43 - 211 0.80 9239 35.24 Campus 206 0.79 9445 36.02 Center 198 0.76 9643 36.78 &amp; 186 0.71 9829 37.49 Technology 175 0.67 10004 38.15 Cosmetology 162 0.62 10166 38.77 the 159 0.61 10325 39.38 The 139 0.53 10464 39.91 Hair 132 0.50 10596 40.41 Medical 132 0.50 10728 40.92 County 130 0.50 10858 41.41 Nursing 121 0.46 10979 41.87 Seminary 117 0.45 11096 42.32 Health 112 0.43 11208 42.75 New 112 0.43 11320 43.17 Education 109 0.42 11429 43.59 American 108 0.41 11537 44.00 Paul 104 0.40 11641 44.40 The first 30 levels are displayed. 7.2.2 Regular Expressions Matching exact strings is easy - it’s just like using find and replace. human_talk &lt;- &quot;blah, blah, blah. Do you want to go for a walk?&quot; dog_hears &lt;- str_extract(human_talk, &quot;walk&quot;) dog_hears ## [1] &quot;walk&quot; But, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler. You may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to both SAS and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to. Short Regular Expression Primer (with R examples) [] enclose sets of characters Ex: [abc] will match any single character a, b, c - specifies a range of characters (A-z matches all upper and lower case letters) to match - exactly, precede with a backslash (outside of []) or put the - last (inside []) . matches any character (except a newline) To match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\\\. will match a literal ., \\\\$ will match a literal $. num_string &lt;- &quot;phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789&quot; ssn &lt;- str_extract(num_string, &quot;[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]&quot;) ssn ## [1] &quot;123-45-6789&quot; Listing out all of those numbers can get repetitive, though. How do we specify repetition? * means repeat between 0 and inf times + means 1 or more times ? means 0 or 1 times – most useful when you’re looking for something optional {a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times” str_extract(&quot;banana&quot;, &quot;[a-z]{1,}&quot;) # match any sequence of lowercase characters ## [1] &quot;banana&quot; str_extract(&quot;banana&quot;, &quot;[ab]{1,}&quot;) # Match any sequence of a and b characters ## [1] &quot;ba&quot; str_extract_all(&quot;banana&quot;, &quot;(..)&quot;) # Match any two characters ## [[1]] ## [1] &quot;ba&quot; &quot;na&quot; &quot;na&quot; str_extract(&quot;banana&quot;, &quot;(..)\\\\1&quot;) # Match a repeated thing ## [1] &quot;anan&quot; num_string &lt;- &quot;phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23&quot; ssn &lt;- str_extract(num_string, &quot;[0-9]{3}-[0-9]{2}-[0-9]{4}&quot;) ssn ## [1] &quot;123-45-6789&quot; phone &lt;- str_extract(num_string, &quot;[0-9]{3}.[0-9]{3}.[0-9]{4}&quot;) phone ## [1] &quot;123-456-7890&quot; nuid &lt;- str_extract(num_string, &quot;[0-9]{8}&quot;) nuid ## [1] &quot;12345678&quot; bank_balance &lt;- str_extract(num_string, &quot;\\\\$[0-9,]+\\\\.[0-9]{2}&quot;) bank_balance ## [1] &quot;$50,000,000.23&quot; There are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end) ^ has multiple meanings: if it’s the first character in a pattern, ^ matches the beginning of a string if it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c.” $ means the end of a string Combined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses address &lt;- &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot; house_num &lt;- str_extract(address, &quot;^[0-9]{1,}&quot;) # Match everything alphanumeric up to the comma street &lt;- str_extract(address, &quot;[A-z0-9 ]{1,}&quot;) street &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number city &lt;- str_extract(address, &quot;,.*,&quot;) %&gt;% str_remove_all(&quot;,&quot;) %&gt;% str_trim() zip &lt;- str_extract(address, &quot;[0-9-]{5,10}$&quot;) # match 5 and 9 digit zip codes () are used to capture information. So ([0-9]{4}) captures any 4-digit number a|b will select a or b. If you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, though, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on. phone_num_variants &lt;- c(&quot;(123) 456-7980&quot;, &quot;123.456.7890&quot;, &quot;+1 123-456-7890&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})&quot; # \\\\( and \\\\) match literal parentheses if they exist # ([0-9]{3})? captures the area code, if it exists # .? matches any character # ([0-9]{3}) captures the exchange code # ([0-9]{4}) captures the 4-digit individual code str_extract(phone_num_variants, phone_regex) ## [1] &quot;(123) 456-7980&quot; &quot;123.456.7890&quot; &quot;123-456-7890&quot; str_replace(phone_num_variants, phone_regex, &quot;\\\\1\\\\2\\\\3&quot;) ## [1] &quot;1234567980&quot; &quot;1234567890&quot; &quot;+1 1234567890&quot; # We didn&#39;t capture the country code, so it remained in the string human_talk &lt;- &quot;blah, blah, blah. Do you want to go for a walk? I think I&#39;m going to treat myself to some ice cream for working so hard. &quot; dog_hears &lt;- str_extract_all(human_talk, &quot;walk|treat&quot;) dog_hears ## [[1]] ## [1] &quot;walk&quot; &quot;treat&quot; In SAS, much the same information is true, though you do not have to double-escape special characters. SAS uses PERL-compatible regular expressions (PCRE for short) (these can also be enabled in base-R string functions). In PCRE regular expressions, ‘/’ are used as delimiters. SAS assigns each sequential regular expression a number (so that you can reference them if necessary). PRXMATCH returns the first position of a string where a match is found (0 otherwise) In this example, however, lets just test to see whether PRXMATCH finds a match or not DATA strings; INFILE DATALINES DSD; /* This allows quoted strings */ INPUT string ~ $150.; /* ~ says deal with quoted strings */ DATALINES; &quot;abcdefghijklmnopqrstuvwxyzABAB&quot; &quot;banana orange strawberry apple&quot; &quot;ana went to montana to eat a banana&quot; &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot; &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot; &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot; RUN; DATA info; set strings; IF PRXMATCH(&quot;/\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})/&quot;, string) GT 0 THEN phone = 1; ELSE phone = 0; IF PRXMATCH(&quot;/(walk|treat)/&quot;, string) GT 0 THEN dog = 1; ELSE dog = 0; IF PRXMATCH(&quot;/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), ([0-9]{5})/&quot;, string) GT 0 THEN addr = 1; ELSE addr = 0; /* Changed to require at least 3 characters in street and city names */ IF PRXMATCH(&quot;/(..)\\1/&quot;, string) GT 0 THEN abab = 1; ELSE abab = 0; ; PROC PRINT DATA=info; RUN; Obs string phone dog addr abab 1 “abcdefghijklmnopqrstuvwxyzABAB” 0 0 0 1 2 “banana orange strawberry apple” 0 0 0 1 3 “ana went to montana to eat a banana” 0 0 0 1 4 “call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.” 1 1 0 1 5 “phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23” 1 0 0 1 6 “1600 Pennsylvania Ave NW, Washington D.C., 20500” 0 0 1 1 Note that the equivalent syntax in R would be: strings &lt;- c(&quot;abcdefghijklmnopqrstuvwxyzABAB&quot;, &quot;banana orange strawberry apple&quot;, &quot;ana went to montana to eat a banana&quot;, &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot;, &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot;, &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})&quot; dog_regex &lt;- &quot;(walk|treat)&quot; addr_regex &lt;- &quot;([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})&quot; abab_regex &lt;- &quot;(..)\\\\1&quot; tibble( text = strings, phone = str_detect(strings, phone_regex), dog = str_detect(strings, dog_regex), addr = str_detect(strings, addr_regex), abab = str_detect(strings, abab_regex)) ## # A tibble: 6 x 5 ## text phone dog addr abab ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 abcdefghijklmnopqrstuvwxyzABAB FALSE FALSE FALSE TRUE ## 2 banana orange strawberry apple FALSE FALSE FALSE TRUE ## 3 ana went to montana to eat a banana FALSE FALSE FALSE TRUE ## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE TRUE FALSE FALSE ## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE FALSE FALSE FALSE ## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500 FALSE FALSE TRUE FALSE When doing various operations with regular expressions, it can be useful to save the regular expression for later use. PRXPARSE saves a regex for use later PRXSUBSTR saves the starting location and length of a string match SUBSTR extracts the string given the starting location and length PRXPARSE, PRXSUBSTR, SUBSTR DATA strings; INFILE DATALINES DSD; /* This allows quoted strings */ INPUT string ~ $150.; /* ~ says deal with quoted strings */ DATALINES; &quot;abcdefghijklmnopqrstuvwxyzABAB&quot; &quot;banana orange strawberry apple&quot; &quot;ana went to montana to eat a banana&quot; &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot; &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot; &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot; RUN; DATA info; SET strings; /* This says use these variables for all rows */ RETAIN REphone REdog REaddr REabab; /* This block defined our variables */ IF _N_ = 1 THEN DO; REphone = PRXPARSE(&quot;/\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})/&quot;); REdog = PRXPARSE(&quot;/(walk|treat)/&quot;); REaddr = PRXPARSE(&quot;/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), ([0-9]{5})/&quot;); REabab = PRXPARSE(&quot;/(..)\\1/&quot;); END; /* This block identifies string start and length for matches */ /* Note that phonestart, phonelength, dogstart, doglength, ... are all defined implicitly in this block */ CALL PRXSUBSTR(REphone, string, phonestart, phonelength); CALL PRXSUBSTR(REdog, string, dogstart, doglength); CALL PRXSUBSTR(REaddr, string, addrstart, addrlength); CALL PRXSUBSTR(REabab, string, ababstart, abablength); /* This block extracts all of the matches */ IF phonestart GT 0 THEN DO; phonenumber = SUBSTR(string, phonestart, phonelength); END; IF dogstart GT 0 THEN DO; dogword = SUBSTR(string, dogstart, doglength); END; IF addrstart GT 0 THEN DO; addr = SUBSTR(string, addrstart, addrlength); END; IF ababstart GT 0 THEN DO; abab = SUBSTR(string, ababstart, abablength); END; /* This block keeps only rows with a phone number, dog keyword, or address */ IF (phonestart GT 0) OR (dogstart GT 0) OR (addrstart GT 0) OR (ababstart GT 0) THEN DO; OUTPUT; END; /* This keeps only the variables we care about */ KEEP string phonenumber dogword addr abab; ; PROC PRINT DATA=info; RUN; Obs string phonenumber dogword addr abab 1 “abcdefghijklmnopqrstuvwxyzABAB” ABAB 2 “banana orange strawberry apple” anan 3 “ana went to montana to eat a banana” anan 4 “call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.” 432-394-2873 walk 5 “phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23” 456-7890 6 “1600 Pennsylvania Ave NW, Washington D.C., 20500” 1600 Pennsylvania Ave NW, Washington D.C., 20500 Note that the equivalent syntax in R would be: strings &lt;- c(&quot;abcdefghijklmnopqrstuvwxyzABAB&quot;, &quot;banana orange strawberry apple&quot;, &quot;ana went to montana to eat a banana&quot;, &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot;, &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot;, &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})&quot; dog_regex &lt;- &quot;(walk|treat)&quot; addr_regex &lt;- &quot;([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})&quot; abab_regex &lt;- &quot;(..)\\\\1&quot; tibble( text = strings, phone = str_extract(strings, phone_regex), dog = str_extract(strings, dog_regex), addr = str_extract(strings, addr_regex), abab = str_extract(strings, abab_regex)) ## # A tibble: 6 x 5 ## text phone dog addr abab ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 abcdefghijklmnopqrstuvwxyzABAB &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ABAB ## 2 banana orange strawberry apple &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; anan ## 3 ana went to montana to eat a bana… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; anan ## 4 call me at 432-394-2873. Do you w… 432-394-… walk &lt;NA&gt; &lt;NA&gt; ## 5 phone: (123) 456-7890, nuid: 1234… (123) 45… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 1600 Pennsylvania Ave NW, Washing… &lt;NA&gt; &lt;NA&gt; 1600 Pennsylvania Av… &lt;NA&gt; Find and Replace with PRXCHANGE The next major task is find and replace, where we get to see another feature of perl-style regular expressions. ‘s/xxx/yyy/’ is the general form of a find-and-replace regular expression. Think “substitue yyy for xxx.” DATA ducks; INFILE DATALINES DSD; INPUT line ~ $50.; DATALINES; &quot;Five little ducks went out one day,&quot; &quot;Over the hills and far away.&quot; &quot;Mother duck said, quack quack quack quack,&quot; &quot;But only four little ducks came back.&quot; RUN; DATA cats; SET ducks; /* define lengths of output strings */ LENGTH new_text $ 50 new_text2 $ 50; IF _N_ = 1 THEN DO; REanimal = PRXPARSE(&quot;s/duck/cat/&quot;); REnoise = PRXPARSE(&quot;s/quack/meow/&quot;); END; RETAIN REanimal REnoise; /* First, replace duck with cat */ CALL PRXCHANGE(REanimal, -1, line, new_text, r_length, trunc, n_of_changes); /* Then, fix noises */ CALL PRXCHANGE(REnoise, -1, new_text, new_text2, r_length2, trunc2, n_of_changes2); /* Warnings if anything was truncated */ IF trunc THEN PUT &quot;Note: new_text was truncated&quot;; IF trunc2 THEN PUT &quot;Note: new_text2 was truncated&quot;; RUN; PROC PRINT DATA=cats; RUN; Obs line new_text new_text2 REanimal REnoise r_length trunc n_of_changes r_length2 trunc2 n_of_changes2 1 “Five little ducks went out one day,” “Five little cats went out one day,” “Five little cats went out one day,” 1 2 49 0 1 50 0 0 2 “Over the hills and far away.” “Over the hills and far away.” “Over the hills and far away.” 1 2 50 0 0 50 0 0 3 “Mother duck said, quack quack quack quack,” “Mother cat said, quack quack quack quack,” “Mother cat said, meow meow meow meow,” 1 2 49 0 1 46 0 4 4 “But only four little ducks came back.” “But only four little cats came back.” “But only four little cats came back.” 1 2 49 0 1 50 0 0 The equivalent R code: line &lt;- c( &quot;Five little ducks went out one day,&quot;, &quot;Over the hills and far away.&quot;, &quot;Mother duck said, quack quack quack quack,&quot;, &quot;But only four little ducks came back.&quot; ) str_replace_all(line, &quot;duck&quot;, &quot;cat&quot;) %&gt;% str_replace_all(&quot;quack&quot;, &quot;meow&quot;) ## [1] &quot;Five little cats went out one day,&quot; ## [2] &quot;Over the hills and far away.&quot; ## [3] &quot;Mother cat said, meow meow meow meow,&quot; ## [4] &quot;But only four little cats came back.&quot; # Or, in one line... str_replace_all(line, c(&quot;duck&quot; = &quot;cat&quot;, &quot;quack&quot; = &quot;meow&quot;)) ## [1] &quot;Five little cats went out one day,&quot; ## [2] &quot;Over the hills and far away.&quot; ## [3] &quot;Mother cat said, meow meow meow meow,&quot; ## [4] &quot;But only four little cats came back.&quot; In PCRE, a backreference in the same expression would be \\1, \\2, etc., but if you are in the replace block of the regex, you would use $1, $2, …. Don’t expect too much out of yourself as far as regular expressions go. I used them for almost a decade before I (mostly) quit googling “regex for …” to find somewhere to start. Another thing to realize - regular expressions are 100% a language you write, but don’t ever expect to read. So leave yourself lots of comments. Try it out - Squirrel Census The Squirrel Census (https://www.thesquirrelcensus.com/) is a multimedia science, design, and storytelling project focusing on the Eastern gray (Sciurus carolinensis) in NYC’s Central Park. They count squirrels and present their findings to the public. This table contains squirrel data for each of the 3,023 sightings, including location coordinates, age, primary and secondary fur color, elevation, activities, communications, and interactions between squirrels and with humans. Task 1: Fix the date! In both SAS and R, read in the data (link) and format the date correctly. You can do this by carefully specifying how the date is read in (?read_csv in R, informat in SAS) R solution library(readr) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union squirrels &lt;- read_csv(&quot;data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv&quot;) %&gt;% mutate(Date = as.character(Date) %&gt;% mdy()) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character(), ## X = col_double(), ## Y = col_double(), ## Date = col_double(), ## `Hectare Squirrel Number` = col_double(), ## Running = col_logical(), ## Chasing = col_logical(), ## Climbing = col_logical(), ## Eating = col_logical(), ## Foraging = col_logical(), ## Kuks = col_logical(), ## Quaas = col_logical(), ## Moans = col_logical(), ## `Tail flags` = col_logical(), ## `Tail twitches` = col_logical(), ## Approaches = col_logical(), ## Indifferent = col_logical(), ## `Runs from` = col_logical(), ## `Zip Codes` = col_double(), ## `Community Districts` = col_double(), ## `Borough Boundaries` = col_double() ## # ... with 2 more columns ## ) ## ℹ Use `spec()` for the full column specifications. SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 filename fileloc 8 ! &#39;data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv&#39;; 9 PROC IMPORT datafile = fileloc out=classdat.squirrel REPLACE 10 DBMS = csv; 10 ! /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; Name Combination of Primary and Highlight Color truncated to Combination_of_Primary_and_Highl. Problems were detected with provided names. See LOG. 14 /************************************************************** 14 ! ******** 15 * PRODUCT: SAS 16 * VERSION: 9.4 17 * CREATOR: External File Interface 18 * DATE: 10MAY21 19 * DESC: Generated SAS Datastep Code 20 * TEMPLATE SOURCE: (None Specified.) 21 *************************************************************** 21 ! ********/ 22 data CLASSDAT.SQUIRREL ; 23 %let _EFIERR_ = 0; /* set the ERROR detection macro variable 23 ! */ 24 infile FILELOC delimiter = &#39;,&#39; MISSOVER DSD firstobs=2 ; 25 informat X best32. ; 26 informat Y best32. ; 27 informat Unique_Squirrel_ID $14. ; 28 informat Hectare $3. ; 29 informat Shift $2. ; 30 informat Date best32. ; 31 informat Hectare_Squirrel_Number best32. ; 32 informat Age $8. ; 33 informat Primary_Fur_Color $8. ; 34 informat Highlight_Fur_Color $24. ; 35 informat Combination_of_Primary_and_Highl $29. ; 36 informat Color_notes $110. ; 37 informat Location $12. ; 38 informat Above_Ground_Sighter_Measurement $5. ; 39 informat Specific_Location $58. ; 40 informat Running $5. ; 41 informat Chasing $5. ; 42 informat Climbing $5. ; 43 informat Eating $5. ; 44 informat Foraging $5. ; 45 informat Other_Activities $134. ; 46 informat Kuks $5. ; 47 informat Quaas $5. ; 48 informat Moans $5. ; 49 informat Tail_flags $5. ; 50 informat Tail_twitches $5. ; 51 informat Approaches $5. ; 52 informat Indifferent $5. ; 53 informat Runs_from $5. ; 54 informat Other_Interactions $70. ; 55 informat Lat_Long $45. ; 56 informat Zip_Codes $1. ; 57 informat Community_Districts best32. ; 58 informat Borough_Boundaries best32. ; 59 informat City_Council_Districts best32. ; 60 informat Police_Precincts best32. ; 61 format X best12. ; 62 format Y best12. ; 63 format Unique_Squirrel_ID $14. ; 64 format Hectare $3. ; 65 format Shift $2. ; 66 format Date best12. ; 67 format Hectare_Squirrel_Number best12. ; 68 format Age $8. ; 69 format Primary_Fur_Color $8. ; 70 format Highlight_Fur_Color $24. ; 71 format Combination_of_Primary_and_Highl $29. ; 72 format Color_notes $110. ; 73 format Location $12. ; 74 format Above_Ground_Sighter_Measurement $5. ; 75 format Specific_Location $58. ; 76 format Running $5. ; 77 format Chasing $5. ; 78 format Climbing $5. ; 79 format Eating $5. ; 80 format Foraging $5. ; 81 format Other_Activities $134. ; 82 format Kuks $5. ; 83 format Quaas $5. ; 84 format Moans $5. ; 85 format Tail_flags $5. ; 86 format Tail_twitches $5. ; 87 format Approaches $5. ; 88 format Indifferent $5. ; 89 format Runs_from $5. ; 90 format Other_Interactions $70. ; 91 format Lat_Long $45. ; 92 format Zip_Codes $1. ; 93 format Community_Districts best12. ; 94 format Borough_Boundaries best12. ; 95 format City_Council_Districts best12. ; 96 format Police_Precincts best12. ; 97 input 98 X 99 Y 100 Unique_Squirrel_ID $ 101 Hectare $ 102 Shift $ 103 Date 104 Hectare_Squirrel_Number 105 Age $ 106 Primary_Fur_Color $ 107 Highlight_Fur_Color $ 108 Combination_of_Primary_and_Highl $ 109 Color_notes $ 110 Location $ 111 Above_Ground_Sighter_Measurement $ 112 Specific_Location $ 113 Running $ 114 Chasing $ 115 Climbing $ 116 Eating $ 117 Foraging $ 118 Other_Activities $ 119 Kuks $ 120 Quaas $ 121 Moans $ 122 Tail_flags $ 123 Tail_twitches $ 124 Approaches $ 125 Indifferent $ 126 Runs_from $ 127 Other_Interactions $ 128 Lat_Long $ 129 Zip_Codes $ 130 Community_Districts 131 Borough_Boundaries 132 City_Council_Districts 133 Police_Precincts 134 ; 135 if _ERROR_ then call symputx(&#39;_EFIERR_&#39;,1); /* set ERROR 135 ! detection macro variable */ 136 run; NOTE: The infile FILELOC is: Filename=/home/susan/Projects/Class/unl-stat850/2020-stat850/data/201 8_Central_Park_Squirrel_Census_-_Squirrel_Data.csv, Owner Name=susan,Group Name=susan, Access Permission=-rw-rw-r--, Last Modified=23Jun2020:09:12:28, File Size (bytes)=784170 NOTE: 3023 records were read from the infile FILELOC. The minimum record length was 210. The maximum record length was 433. NOTE: The data set CLASSDAT.SQUIRREL has 3023 observations and 36 variables. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.03 seconds 3023 rows created in CLASSDAT.SQUIRREL from FILELOC. NOTE: CLASSDAT.SQUIRREL data set was successfully created. NOTE: The data set CLASSDAT.SQUIRREL has 3023 observations and 36 variables. NOTE: PROCEDURE IMPORT used (Total process time): real time 1.10 seconds cpu time 1.10 seconds 137 138 PROC CONTENTS DATA=classdat.squirrel; 139 RUN; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.12 seconds cpu time 0.07 seconds 140 141 DATA classdat.cleanSquirrel; 142 SET classdat.squirrel; 143 month = FLOOR(date/1000000); 144 day = MOD(FLOOR(date/10000), 100); 145 year = MOD(date, 10000); 146 date = MDY(month, day, year); 147 format date MMDDYY10.; 148 RUN; NOTE: Data file CLASSDAT.CLEANSQUIRREL.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. ERROR: Some character data was lost during transcoding in the dataset CLASSDAT.CLEANSQUIRREL. Either the data contains characters that are not representable in the new encoding or truncation occurred during transcoding. NOTE: The DATA step has been abnormally terminated. NOTE: The SAS System stopped processing this step because of errors. NOTE: Due to ERROR(s) above, SAS set option OBS=0, enabling syntax check mode. This prevents execution of subsequent data modification statements. NOTE: There were 1180 observations read from the data set CLASSDAT.SQUIRREL. WARNING: The data set CLASSDAT.CLEANSQUIRREL may be incomplete. When this step was stopped there were 1179 observations and 39 variables. WARNING: Data set CLASSDAT.CLEANSQUIRREL was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.03 seconds cpu time 0.03 seconds ERROR: Errors printed on page 6. Data Set Name CLASSDAT.SQUIRREL Observations 3023 Member Type DATA Variables 36 Engine V9 Indexes 0 Created 05/10/2021 09:49:22 Observation Length 656 Last Modified 05/10/2021 09:49:22 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding utf-8 Unicode (UTF-8) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 31 First Data Page 1 Max Obs per Page 99 Obs in First Data Page 89 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/2020-stat850/sas/squirrel.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 40370885 Access Permission rw-rw-r– Owner Name susan File Size 2MB File Size (bytes) 2097152 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat 14 Above_Ground_Sighter_Measurement Char 5 $5. $5. 8 Age Char 8 $8. $8. 27 Approaches Char 5 $5. $5. 34 Borough_Boundaries Num 8 BEST12. BEST32. 17 Chasing Char 5 $5. $5. 35 City_Council_Districts Num 8 BEST12. BEST32. 18 Climbing Char 5 $5. $5. 12 Color_notes Char 110 $110. $110. 11 Combination_of_Primary_and_Highl Char 29 $29. $29. 33 Community_Districts Num 8 BEST12. BEST32. 6 Date Num 8 BEST12. BEST32. 19 Eating Char 5 $5. $5. 20 Foraging Char 5 $5. $5. 4 Hectare Char 3 $3. $3. 7 Hectare_Squirrel_Number Num 8 BEST12. BEST32. 10 Highlight_Fur_Color Char 24 $24. $24. 28 Indifferent Char 5 $5. $5. 22 Kuks Char 5 $5. $5. 31 Lat_Long Char 45 $45. $45. 13 Location Char 12 $12. $12. 24 Moans Char 5 $5. $5. 21 Other_Activities Char 134 $134. $134. 30 Other_Interactions Char 70 $70. $70. 36 Police_Precincts Num 8 BEST12. BEST32. 9 Primary_Fur_Color Char 8 $8. $8. 23 Quaas Char 5 $5. $5. 16 Running Char 5 $5. $5. 29 Runs_from Char 5 $5. $5. 5 Shift Char 2 $2. $2. 15 Specific_Location Char 58 $58. $58. 25 Tail_flags Char 5 $5. $5. 26 Tail_twitches Char 5 $5. $5. 3 Unique_Squirrel_ID Char 14 $14. $14. 1 X Num 8 BEST12. BEST32. 2 Y Num 8 BEST12. BEST32. 32 Zip_Codes Char 1 $1. $1. Task 2: Clean up the Combination of primary and highlight fur color column A. Get rid of leading and trailing + characters B. Where two highlight colors exist, add the primary color to both of them (so Gray+Cinnamon, White becomes Gray+Cinnamon, Gray+White) You can do this by working with the original values or the combination values; whatever is easiest. R solution squirrels_colorfix &lt;- squirrels %&gt;% # Make it easier to join things back together... mutate(id = 1:n()) %&gt;% # keep the stuff we need for this select(id, primary = `Primary Fur Color`, highlight = `Highlight Fur Color`, combo = `Combination of Primary and Highlight Color`) %&gt;% # Remove all single character strings. # ^ means &quot;front of string&quot;, $ means &quot;end of string&quot;, and . is a wildcard mutate(combo = str_remove(combo, &quot;^.$&quot;)) %&gt;% # Remove trailing + signs mutate(combo = str_remove(combo, &quot;\\\\+$&quot;)) %&gt;% # This allows only two highlight colors mutate(combo = str_replace( combo, &quot;^(Black|Cinnamon|Gray)\\\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$&quot;, &quot;\\\\1+\\\\2, \\\\1+\\\\3&quot;)) %&gt;% # This allows three highlight colors mutate(combo = str_replace( combo, &quot;^(Black|Cinnamon|Gray)\\\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$&quot;, &quot;\\\\1+\\\\2, \\\\1+\\\\3, \\\\1+\\\\4&quot;)) table(squirrels_colorfix$combo) ## ## Black ## 55 74 ## Black+Cinnamon Black+Cinnamon, Black+White ## 15 3 ## Black+Gray Black+Gray, Black+White ## 8 1 ## Black+White Cinnamon ## 2 62 ## Cinnamon+Black Cinnamon+Black, Cinnamon+White ## 10 3 ## Cinnamon+Gray Cinnamon+Gray, Cinnamon+Black ## 162 3 ## Cinnamon+Gray, Cinnamon+White Cinnamon+White ## 58 94 ## Gray Gray+Black ## 895 24 ## Gray+Black, Gray+Cinnamon Gray+Black, Gray+Cinnamon, Gray+White ## 9 32 ## Gray+Black, Gray+White Gray+Cinnamon ## 7 752 ## Gray+Cinnamon, Gray+White Gray+White ## 265 489 SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 DATA squirrelcolor; 9 SET classdat.cleanSquirrel; NOTE: Data file CLASSDAT.CLEANSQUIRREL.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 10 11 LENGTH orig $50 new_text1 $ 50 new_text2 $ 50 new_text3 $ 50; 12 orig = Combination_of_Primary_and_Highl; 13 14 IF _N_ = 1 THEN DO; 15 REthree = 15 ! PRXPARSE(&quot;s/^(Gray|Cinnamon|Black)\\+(Black|Cinnamon|Gray|White), 15 ! (Black|Cinnamon|Gray|White), 15 ! (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3, $1+$4/&quot;); 16 REtwo = 16 ! PRXPARSE(&quot;s/^(Gray|Cinnamon|Black)\\+(Black|Cinnamon|Gray|White), 16 ! (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3/&quot;); 17 REplus = PRXPARSE(&quot;s/[^A-z]$//&quot;); 18 END; 19 RETAIN REplus REtwo REthree; 20 21 CALL PRXCHANGE(REplus, -1, trim(orig), new_text1); 22 CALL PRXCHANGE(REthree, -1, trim(new_text1), new_text2); 23 CALL PRXCHANGE(REtwo, -1, trim(new_text2), new_text3); 24 25 keep orig new_text1 new_text2 new_text3; 26 RUN; NOTE: The data set WORK.SQUIRRELCOLOR has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.02 seconds cpu time 0.01 seconds 27 28 /* print all combinations that occur w/ new value */ 29 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 30 SELECT DISTINCT orig, new_text3 FROM squirrelcolor; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on page 9. 7.2.3 Joining and Splitting Variables There’s another string-related task that is fairly commonly encountered: separating variables into two different columns (as in Table 3 above). Figure 7.1: A visual representation of what separating variables means for data set operations. Separating Variables We can use str_extract() if we want, but it’s actually faster to use separate(), which is part of the tidyr package. There is also extract(), which is another tidyr function that uses regular expressions and capture groups to split variables up. table3 %&gt;% separate(col = rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, remove = F) ## # A tibble: 6 x 5 ## country year rate cases population ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 745 19987071 ## 2 Afghanistan 2000 2666/20595360 2666 20595360 ## 3 Brazil 1999 37737/172006362 37737 172006362 ## 4 Brazil 2000 80488/174504898 80488 174504898 ## 5 China 1999 212258/1272915272 212258 1272915272 ## 6 China 2000 213766/1280428583 213766 1280428583 I’ve left the rate column in the original data frame just to make it easy to compare and verify that yes, it worked. separate() will also take a full on regular expression if you want to capture only parts of a string to put into new columns. The scan() function in SAS can be used similarly, though it doesn’t have quite the simplicity and convenience of separate. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.04 seconds cpu time 0.04 seconds 6 data table3; 7 length country $12 rate $20; 8 input country $ year rate $; 9 datalines; NOTE: The data set WORK.TABLE3 has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 ; 17 data table3split; 18 set table3; 19 length var1-var2 $10.; 20 array var(2) $; NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 21 do i = 1 to dim(var); 22 var[i]=scan(rate, i, &#39;/&#39;, &#39;M&#39;); 23 end; 24 count = var1; 25 population = var2; 26 run; NOTE: The data set WORK.TABLE3SPLIT has 0 observations and 8 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on page 9. And, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value. Figure 7.2: A visual representation of what uniting variables means for data set operations. Joining Variables separate() has a complement, unite(), which is useful for handling situations like in table5: table5 %&gt;% unite(col = &quot;year&quot;, century:year, sep = &#39;&#39;) %&gt;% separate(col = rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Note that separate and unite both work with character variables - it’s not necessarily true that you’ll always be working with character formats when you need to do these operations. For instance, it’s relatively common to need to separate dates into year, month, and day as separate columns (or to join them together). Of course, it’s much easier just to do a similar two-step operation (we have to convert to numeric variables to do math) table5 %&gt;% mutate(year = as.numeric(century)*100 + as.numeric(year)) %&gt;% select(-century) ## # A tibble: 6 x 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 (Handy shortcut functions in dplyr don’t completely remove the need to think). Similarly, it is possible to do this operation in SAS as well (by string concatenation or using the numeric approach), as shown below: 6 /* read in the data */ 7 DATA table5; 8 LENGTH country $12 century year rate $20; 9 INPUT country $ century year rate $; 10 DATALINES; NOTE: The data set WORK.TABLE5 has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 17 ; 18 19 /* Format the data */ 20 DATA table5split; 21 SET table5; 22 LENGTH v1-v2 $10. yyc $3. centc $3. yyyyc $4.; 23 ARRAY v(2) $; 24 DO i = 1 TO dim(v); 25 v[i]=scan(rate, i, &#39;/&#39;, &#39;M&#39;); 26 END; 27 count = v1; 28 population = v2; 29 /* Numeric version */ 30 year = century*100 + year; 31 /* Character version */ 32 yyc = PUT(year, 2.); /* convert to character */ 33 centc = PUT(century, 2.); /* convert to character */ 34 yyyyc = CATT(&#39;&#39;, centc, yearc); /* catt is truncate, then 34 ! concatenate */ 35 RUN; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 30:10 30:24 NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 30:22 NOTE: The data set WORK.TABLE5SPLIT has 0 observations and 13 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 36 37 /* Print the data */ 38 PROC PRINT DATA=table5split; 39 VAR country year yyyyc count population rate centc century yyc 39 ! ; 40 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 9. 7.3 Pivot operations It’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis. The two operations we’ll learn here are wide -&gt; long and long -&gt; wide. This animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and SAS. 7.3.1 Longer In many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable. Tables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively. table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 The solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below). Figure 7.3: A visual representation of what the pivot_longer operation looks like in practice. Once our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data. In R, wide-to-long conversions are performed using pivot_longer() tba &lt;- table4a %&gt;% pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;cases&quot;) tbb &lt;- table4b %&gt;% pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;population&quot;) # To get the tidy data, we join the two together (see Table joins below) left_join(tba, tbb, by = c(&quot;country&quot;, &quot;year&quot;)) %&gt;% # make year numeric b/c it&#39;s dumb not to mutate(year = as.numeric(year)) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 The columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember). We identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways: select only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\") select variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation) Which option is easier depends how many things you’re pivoting (and how the columns are structured). If we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form. # Create ID columns table4a.x &lt;- table4a %&gt;% mutate(id = &quot;cases&quot;) table4b.x &lt;- table4b %&gt;% mutate(id = &quot;population&quot;) # Create one table table4 &lt;- bind_rows(table4a.x, table4b.x) table4_long &lt;- table4 %&gt;% # rearrange columns select(country, id, `1999`, `2000`) %&gt;% # Don&#39;t pivot country or id pivot_longer(-c(country:id), names_to = &quot;year&quot;, values_to = &quot;count&quot;) # Intermediate fully-long form table4_long ## # A tibble: 12 x 4 ## country id year count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan cases 1999 745 ## 2 Afghanistan cases 2000 2666 ## 3 Brazil cases 1999 37737 ## 4 Brazil cases 2000 80488 ## 5 China cases 1999 212258 ## 6 China cases 2000 213766 ## 7 Afghanistan population 1999 19987071 ## 8 Afghanistan population 2000 20595360 ## 9 Brazil population 1999 172006362 ## 10 Brazil population 2000 174504898 ## 11 China population 1999 1272915272 ## 12 China population 2000 1280428583 # make wider, with case and population columns table4_tidy &lt;- table4_long %&gt;% pivot_wider(names_from = id, values_from = count) table4_tidy ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 SAS will let you do a single transpose operation, where tidyr requires two separate pivots – this is because tidyr is trying to make the steps readable, even though it means writing more code. In SAS, we use PROC TRANSPOSE to perform wide-to-long pivot operations Friendly guide to PROC TRANSPOSE 6 DATA table4a; 7 input country $12. _1999 _2000; 8 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 12 ; 13 14 PROC TRANSPOSE DATA=table4a out=table4atmp 15 (rename=(COL1 = cases)) /* rename output variable 15 ! (&#39;values_to&#39;) */ 16 NAME = year /* where column names go (&#39;names_to&#39;) */ 17 ; 18 BY country; /* The combination of BY variables defines a row */ 19 VAR _1999 _2000; /* Specify variables to pivot */ 20 RUN; WARNING: The variable COL1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set WORK.TABLE4A. NOTE: The data set WORK.TABLE4ATMP has 2 observations and 2 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 22 DATA table4b; 23 input country $12. _1999 _2000; 24 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 28 ; 29 30 PROC TRANSPOSE DATA=table4b out=table4btmp 31 (rename=(COL1 = population)) /* rename output variable 31 ! (&#39;values_to&#39;) */ 32 NAME = year /* where the column names go (&#39;names_to&#39;) */ 33 ; 34 BY country; /* The combination of BY variables defines a row */ 35 VAR _1999 _2000; /* Specify variables to pivot */ 36 RUN; WARNING: The variable COL1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set WORK.TABLE4B. NOTE: The data set WORK.TABLE4BTMP has 2 observations and 2 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 DATA table4clean; 39 /* merge the two tables together */ 40 /* (country and year selected automatically as merge vars) */ 41 MERGE table4atmp table4btmp; 42 /* Remove the first character of year, which is _ */ 43 /* Then convert to a numeric variable */ 44 year = input(SUBSTR(year, 2, 5), 4.); 45 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 44:8 NOTE: The data set WORK.TABLE4CLEAN has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 46 47 PROC PRINT DATA=table4clean; 48 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 9. In the above code, we let SAS name our output variable COL1, and then renamed it at the end by modifying the output statement. Another option would be to create an ID variable when inputting each data set, and use the ID statement to indicate that variable. 6 DATA table4a; 7 input country $12. _1999 _2000; 8 id = &quot;cases&quot;; 9 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ; 14 15 PROC TRANSPOSE DATA=table4a out=table4atmp 16 NAME = year /* where column names go (&#39;names_to&#39;) */ 17 ; 18 BY country; /* The combination of BY variables defines a row */ 19 VAR _1999 _2000; /* Specify variables to pivot */ 20 ID id; /* This variable holds the output variable name 20 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 21 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4ATMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.TABLE4ATMP was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.02 seconds 22 23 DATA table4b; 24 input country $12. _1999 _2000; 25 id = &quot;population&quot;; 26 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4B was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 30 ; 31 32 PROC TRANSPOSE DATA=table4b out=table4btmp 33 NAME = year /* where the column names go (&#39;names_to&#39;) */ 34 ; 35 BY country; /* The combination of BY variables defines a row */ 36 VAR _1999 _2000; /* Specify variables to pivot */ 37 ID id; /* This variable holds the output variable name 37 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 38 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4BTMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.TABLE4BTMP was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 39 40 DATA table4clean; 41 /* merge the two tables together */ 42 /* (country and year selected automatically as merge vars) */ 43 MERGE table4atmp table4btmp; 44 /* Remove the first character of year, which is _ */ 45 /* Then convert to a numeric variable */ 46 year = input(SUBSTR(year, 2, 5), 4.); 47 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 46:8 NOTE: The data set WORK.TABLE4CLEAN has 0 observations and 2 variables. WARNING: Data set WORK.TABLE4CLEAN was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 48 49 PROC PRINT DATA=table4clean; 50 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 9,11. This seems a bit odd, as we’re defining constant variables, but it works – and provides some insight into how the reverse operation (long to wide) might work (e.g. nonconstant variables). Imagine instead of transposing each dataset and then merging them, we just stack the two wide-format datasets on top of each other. Then we can do the same transpose operation, but we’ll end up with two columns: cases, and population. 6 DATA table4a; 7 input country $12. _1999 _2000; 8 id = &quot;cases&quot;; 9 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 13 ; 14 15 DATA table4b; 16 input country $12. _1999 _2000; 17 id = &quot;population&quot;; 18 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4B was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 ; 23 24 DATA table4; 25 /* stack the two datasets on top of each other */ 26 SET table4b table4a; 27 /* sort by country */ 28 BY country; 29 RUN; NOTE: The data set WORK.TABLE4 has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 30 31 PROC TRANSPOSE DATA=table4 out=table4tmp NAME = year; 32 BY country; 33 VAR _1999 _2000; 34 ID id; /* This variable holds the output variable names 34 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 35 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4TMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 36 37 DATA table4tmp; 38 SET table4tmp; 39 /* Remove the first character of year, which is _ */ 40 /* Then convert to a numeric variable */ 41 year = input(SUBSTR(year, 2, 5), 4.); 42 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 41:21 NOTE: The data set WORK.TABLE4TMP has 0 observations and 1 variables. WARNING: Data set WORK.TABLE4TMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 43 44 PROC PRINT DATA=table4tmp; 45 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12. It’s not too complicated – and it definitely beats doing that operation by hand, even for short, simple tables. You can imagine how messy the cut/copy/paste job would be in Excel. It takes some getting used to, but once you get a feel for how to do these transpose operations, you’ll be able to handle messy data reproducibly - instead of describing how you did XYZ operations in Excel, you can provide a script that will take the original data as input and spit out clean, analysis-ready data as output. Because wide-to-long transformations end up combining values from several columns into a single column, you can run into issues with type conversions that happen implicitly. If you try to pivot_longer() using a character column mixed in with numeric columns, your “value” column will be converted to a character automatically. Now, let’s look at a “real data” example using HIV case data from the World Health Organization. (download page here). WHO HIV data set up url &lt;- &quot;https://apps.who.int/gho/athena/data/xmart.csv?target=GHO/HIV_0000000026,SDGHIV&amp;profile=crosstable&amp;filter=COUNTRY:*;REGION:*;AGEGROUP:-&amp;x-sideaxis=COUNTRY&amp;x-topaxis=GHO;YEAR;SEX&amp;x-collapse=true&quot; # create colnames in shorter form hiv &lt;- read_csv(url, na = &quot;No data&quot;, ) %&gt;% select(-2) # get rid of only column that has raw totals ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. # work with the names to make them shorter and more readable # otherwise, they&#39;re too long for SAS newnames &lt;- names(hiv) %&gt;% str_remove(&quot;New HIV infections \\\\(per 1000 uninfected population\\\\); &quot;) %&gt;% str_replace_all(&quot;(\\\\d{4}); (Male|Female|Both)( sexes)?&quot;, &quot;Rate_\\\\1_\\\\2&quot;) hiv &lt;- set_names(hiv, newnames) %&gt;% # transliterate - get rid of non-ascii characters, replace w/ closest equiv mutate(Country = iconv(Country, to=&quot;ASCII//TRANSLIT&quot;)) write_csv(hiv, path = &quot;data/who_hiv.csv&quot;, na = &#39;.&#39;) # make it easy for SAS ## Warning: The `path` argument of `write_csv()` is deprecated as of readr 1.4.0. ## Please use the `file` argument instead. Since I’ve cheated a bit to make this easier to read in using SAS… hopefully that will be uneventful. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 filename fileloc &#39;data/who_hiv.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.hiv REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 ERROR: Errors printed on pages 9,11,12,13. Original Data Structure (WHO HIV data) hiv %&gt;% # Only look at 1st 6 cols, because there are too many select(1:6) %&gt;% head() ## # A tibble: 6 x 6 ## Country `Number of new H… `Number of new H… `Number of new … `Number of new … ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghani… 1400 [&amp;lt;500–51… 1300 [&amp;lt;500–48… 1200 [&amp;lt;500–4… 1100 [&amp;lt;500–3… ## 2 Albania &amp;lt;100 [&amp;lt;100… &amp;lt;100 [&amp;lt;100… &amp;lt;200 [&amp;lt;10… &amp;lt;200 [&amp;lt;10… ## 3 Algeria 2000 [&amp;lt;500–36… 1900 [510–3300] 1900 [610–3000] 1800 [780–2800] ## 4 Angola 27 000 [20 000–3… 27 000 [21 000–3… 26 000 [20 000–… 26 000 [20 000–… ## 5 Argenti… 5700 [4100–8100] 6100 [4400–8200] 5900 [4400–8200] 6100 [4700–8300] ## 6 Armenia &amp;lt;200 [&amp;lt;200… &amp;lt;200 [&amp;lt;200… &amp;lt;200 [&amp;lt;20… &amp;lt;200 [&amp;lt;20… ## # … with 1 more variable: Number of new HIV infections; 2014; &lt;chr&gt; Here, the column names (except for the first column) contain information about both group (Male, Female, total) and year. If we want to plot values over time, we’re not going to have much fun. Thinking through the logical steps before writing the code can be helpful - even sketching out what you expect the data to (roughly) look like at each stage. Current data observations: Our column names contain the year and the group (Both, Male, Female) Our values contain estimates with a confidence interval - so est (LB, UB) Some intervals have &amp;lt;, which is HTML code for &lt;. We’ll need to get rid of those. Our final dataset should look like this: Country group year est lb ub narnia both 2020 0.3 0.2 0.4 (give or take column order, capitalization, and/or reality) From this, our steps are: Transpose the data - all columns except Country (our BY variable) Separate the (what was column names) variable into group and year variables convert year to numeric Separate the (what was column values) variable into EST, LB, and UB columns Remove &amp;lt; from the variable so that it’s readable as numeric Remove [, ] and - from the variable so that the values are separated by spaces Read each value into a separate column that’s numeric Rename the columns Clean up any extra variables hanging around. Now that we have a plan, lets execute that plan Wide-to-long transformation in R (WHO HIV data) hiv_tidier &lt;- hiv %&gt;% pivot_longer(-Country, names_to = &quot;key&quot;, values_to = &quot;rate&quot;) hiv_tidier ## # A tibble: 18,530 x 3 ## Country key rate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan Number of new HIV infections; 2018; 1400 [&amp;lt;500–5100] ## 2 Afghanistan Number of new HIV infections; 2017; 1300 [&amp;lt;500–4800] ## 3 Afghanistan Number of new HIV infections; 2016; 1200 [&amp;lt;500–4200] ## 4 Afghanistan Number of new HIV infections; 2015; 1100 [&amp;lt;500–3700] ## 5 Afghanistan Number of new HIV infections; 2014; 1000 [&amp;lt;500–3200] ## 6 Afghanistan Number of new HIV infections; 2013; 920 [&amp;lt;500–2700] ## 7 Afghanistan Number of new HIV infections; 2012; 840 [&amp;lt;500–2300] ## 8 Afghanistan Number of new HIV infections; 2011; 760 [&amp;lt;500–2000] ## 9 Afghanistan Number of new HIV infections; 2010; 700 [&amp;lt;500–1800] ## 10 Afghanistan Number of new HIV infections; 2009; 640 [&amp;lt;500–1600] ## # … with 18,520 more rows From this point, it’s pretty easy to use things we’ve used in the past (regular expressions, separate, extract) hiv_tidy &lt;- hiv_tidier %&gt;% # Split the key into Rate (don&#39;t keep), year, and group (F, M, Both) separate(key, into = c(NA, &quot;year&quot;, &quot;group&quot;), sep = &quot;_&quot;, convert = T) %&gt;% # Fix the HTML sign for less than - we could remove it as well mutate(rate = str_replace_all(rate, &quot;&amp;lt;&quot;, &quot;&lt;&quot;)) %&gt;% # Split the rate into estimate, lower bound, and upper bound extract(rate, into = c(&quot;est&quot;, &quot;lb&quot;, &quot;ub&quot;), regex = &quot;([\\\\d\\\\.]{1,}) .([&lt;\\\\d\\\\.]{1,}) - ([&lt;\\\\d\\\\.]{1,}).&quot;, remove = T) ## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 3230 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 110, ...]. hiv_tidy ## # A tibble: 18,530 x 6 ## Country year group est lb ub ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # … with 18,520 more rows Wide-to-long transformation in SAS (WHO HIV data) I’ve thoroughly commented the code below to hopefully make the logical steps clear. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC TRANSPOSE DATA=classdat.hiv OUT = classdat.hivtidy NOTE: Data file CLASSDAT.HIV.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 9 (rename=(col1=rate)) /* &quot;values_to&quot; in tidyr speak */ 10 NAME = key; /* &quot;names_to&quot; in tidyr speak */ NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 11 /* specify notsorted unless you know your data are sorted */ 12 BY Country NOTSORTED; 13 /* variables to transpose - just list start and end, with -- in 13 ! between */ 14 VAR Rate_2018_Both--Rate_1990_Female; 15 RUN; WARNING: The variable col1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set CLASSDAT.HIV. NOTE: The data set CLASSDAT.HIVTIDY has 87 observations and 2 variables. WARNING: Data set CLASSDAT.HIVTIDY was not replaced because of NOREPLACE option. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.02 seconds cpu time 0.02 seconds 16 17 title &#39;Intermediate result&#39;; 18 PROC PRINT DATA=classdat.hivtidy(obs=5); RUN; NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 20 /* Data step to clean up */ 21 DATA classdat.hivtidy; 22 SET classdat.hivtidy ; NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 23 /* Create group and year variables from key */ 24 group = scan(key,3,&quot;_&quot;); 25 year = input(scan(key, 2, &quot;_&quot;), 4.); 26 27 /* just get rid of the less than sign */ 28 rate = PRXCHANGE(&quot;s/&amp;lt;//&quot;, -1, rate); WARNING: Apparent symbolic reference LT not resolved. 29 rate = PRXCHANGE(&quot;s/[\\[\\]-]//&quot;, -1, rate); 30 31 /* Create 3 columns for the 3 values - est, lb, ub */ 32 length v1-v3 4.2; /* define format */ 33 array v(3) $; /* create a 3 column array to store values in */ 34 do i = 1 to dim(v); 35 v[i]=scan(rate, i, &#39; &#39;); /* get values from each row of rate 35 ! */ 36 end; 37 38 rename v1 = est v2 = lb v3 = ub; /* rename things to be pretty 38 ! */ 39 40 /* get rid of extra vars */ 41 drop _name_ i key rate; 42 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 24:16 25:21 28:36 29:39 35:15 NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 28:10 29:10 35:5 NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. WARNING: Variable v1 cannot be renamed to est because est already exists. WARNING: Variable v2 cannot be renamed to lb because lb already exists. WARNING: Variable v3 cannot be renamed to ub because ub already exists. WARNING: The variable _name_ in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The data set CLASSDAT.HIVTIDY has 0 observations and 9 variables. WARNING: Data set CLASSDAT.HIVTIDY was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 43 44 title &#39;Final result&#39;; 45 PROC PRINT DATA=classdat.hivtidy(obs=5); RUN; NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 46 ERROR: Errors printed on pages 9,11,12,13. Try it out In the next section, we’ll be using the WHO surveillance of disease incidence data (link - 3.1, Excel link) It will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest :) library(readxl) library(purrr) # This uses the map() function as a replacement for for loops. # It&#39;s pretty sweet sheets &lt;- excel_sheets(&quot;data/incidence_series.xls&quot;) sheets &lt;- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name # This command says &quot;for each sheet, read in the excel file with that sheet name&quot; # map_df means paste them all together into a single data frame disease_incidence &lt;- map_df(sheets, ~read_xls(path =&quot;data/incidence_series.xls&quot;, sheet = .)) # Alternately, we could write a loop: disease_incidence2 &lt;- tibble() # Blank data frame for(i in 1:length(sheets)) { disease_incidence2 &lt;- bind_rows( disease_incidence2, read_xls(path = &quot;data/incidence_series.xls&quot;, sheet = sheets[i]) ) } # export for SAS (and R, if you want) write_csv(disease_incidence, path = &quot;data/who_disease_incidence.csv&quot;, na = &quot;.&quot;) Download the exported data here and import it into SAS and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that). Can you make a line plot of cases of measles in Bangladesh over time? R solution who_disease &lt;- read_csv(&quot;data/who_disease_incidence.csv&quot;, na = &quot;.&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## WHO_REGION = col_character(), ## ISO_code = col_character(), ## Cname = col_character(), ## Disease = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. who_disease_long &lt;- who_disease %&gt;% pivot_longer(matches(&quot;\\\\d{4}&quot;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) %&gt;% rename(Country = Cname) %&gt;% mutate(Disease = str_replace(Disease, &quot;CRS&quot;, &quot;Congenital Rubella&quot;), year = as.numeric(year)) filter(who_disease_long, Country == &quot;Bangladesh&quot;, Disease == &quot;measles&quot;) %&gt;% ggplot(aes(x = year, y = cases)) + geom_line() SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 filename fileloc &#39;data/who_disease_incidence.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.who_disease REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 15 /* Sort your data by the variables you want as the ID variables 15 ! */ 16 PROC SORT DATA=classdat.who_disease OUT=who_dis_tmp; NOTE: Data file CLASSDAT.WHO_DISEASE.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 17 BY Disease Cname; 18 RUN; NOTE: The data set WORK.WHO_DIS_TMP has 0 observations and 0 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 19 20 PROC TRANSPOSE DATA = who_dis_tmp OUT = classdat.disease_long; NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 21 BY Disease Cname; /* Same variable order as used to sort */ ERROR: Variable DISEASE not found. ERROR: Variable CNAME not found. 22 VAR _2018--_1980; /* variables to transpose */ ERROR: Variable _2018 not found. 23 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_LONG may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set CLASSDAT.DISEASE_LONG was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 24 25 title &#39;Intermediate result 1&#39;; 26 PROC PRINT DATA=classdat.disease_long(obs=5); RUN; NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 27 28 /* Data step to clean up */ 29 DATA classdat.disease_long; 30 SET classdat.disease_long (rename=col1=cases); NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. ERROR: Variable col1 is not on file CLASSDAT.DISEASE_LONG. ERROR: Invalid DROP, KEEP, or RENAME option on file CLASSDAT.DISEASE_LONG. 31 year = input(scan(_name_,1,&quot;_&quot;), 4.); 32 drop _name_; 33 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 31:19 NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_LONG may be incomplete. When this step was stopped there were 0 observations and 1 variables. WARNING: Data set CLASSDAT.DISEASE_LONG was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 34 35 title &#39;Final result&#39;; 36 PROC PRINT DATA=classdat.disease_long(obs=5); RUN; NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 39 CREATE TABLE bangladesh AS 40 SELECT * FROM classdat.disease_long WHERE (Cname = &quot;Bangladesh&quot;) 40 ! &amp; (Disease = &quot;measles&quot;); NOTE: Statement not executed due to NOEXEC option. 41 42 title &#39;Measles in Bangladesh&#39;; 43 ODS GRAPHICS ON; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 44 PROC SGPLOT DATA=bangladesh; ERROR: File WORK.BANGLADESH.DATA does not exist. 45 SERIES X = year Y = cases; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 46 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 47 ODS GRAPHICS OFF; ERROR: Errors printed on pages 9,11,12,13,15. 7.3.2 Wider While it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again. Table 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows. Figure 7.4: A visual representation of what the pivot_wider operation looks like in practice. In R, long-to-wide conversion operations are performed using pivot_wider() table2 %&gt;% pivot_wider(names_from = type, values_from = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 In SAS, we use PROC TRANSPOSE again 6 DATA table2; 7 input country $12. year type$12. count 12.; 8 datalines; NOTE: The data set WORK.TABLE2 has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 ; 22 23 PROC TRANSPOSE DATA=table2 out=table2tmp; 24 ID type; /* Equivalent to names_from */ 25 BY country year; /* The combination of BY variables defines a 25 ! row */ 26 VAR count; /* Equivalent to values_from */ 27 RUN; NOTE: There were 0 observations read from the data set WORK.TABLE2. NOTE: The data set WORK.TABLE2TMP has 1 observations and 3 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 28 29 PROC PRINT DATA=table2tmp; 30 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16. If you don’t sort your data properly before PROC TRANSPOSE in SAS, you may get a result that has an unexpected shape. SAS works rowwise (compared to R’s column-wise operations) so the row order actually matters in SAS (it generally doesn’t matter much in R). Returning to our WHO HIV example, we might want our data to look like this: Country year measurement Both Male Female Afghanistan 2018 est .02 .03 .01 2018 lb .01 .02 .01 2018 ub .04 .06 .03 As a reminder, the data currently looks like this: hiv_tidy ## # A tibble: 18,530 x 6 ## Country year group est lb ub ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # … with 18,520 more rows Think through the steps you’d need to take to get the data into this form. Sketch out any intermediate state your data will need to go through. WHO HIV Data: long-to-wide steps We need est, lb, and ub in long form first, so we need to use a wide-to-long (pivot_longer) transpose operation. names_to = “measure” values_to = “value” BY variables: Country, year, group pivot variables (VAR in SAS): est, lb, ub We need group to be 3 columns: Both, Male, and Female. So we need to use a long-to-wide (pivot_wider) transpose operation. By variables: Country, year, measure (so we’ll need to sort in SAS) ID variable: group VAR variable: value The intermediate form of the data will look like this: Country year group measure value Afghanistan 2018 Both est 0.02 Afghanistan 2018 Both lb 0.01 Afghanistan 2018 Both ub 0.04 Afghanistan 2018 Male est 0.03 WHO HIV Data: long-to-wide in R hiv_tidy %&gt;% pivot_longer(est:ub, names_to = &quot;measure&quot;, values_to = &quot;value&quot;) %&gt;% # Take the opportunity to transform everything to numeric at once... mutate(value = parse_number(value)) %&gt;% pivot_wider(names_from = group, values_from = value) ## Warning: Values are not uniquely identified; output will contain list-cols. ## * Use `values_fn = list` to suppress this warning. ## * Use `values_fn = length` to identify where the duplicates arise ## * Use `values_fn = {summary_fun}` to summarise duplicates ## # A tibble: 15,810 x 7 ## Country year measure `NA` Both Male Female ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 Afghanistan NA est &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; ## 2 Afghanistan NA lb &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; ## 3 Afghanistan NA ub &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; ## 4 Afghanistan 2019 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 5 Afghanistan 2019 lb &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 6 Afghanistan 2019 ub &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 7 Afghanistan 2018 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 8 Afghanistan 2018 lb &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 9 Afghanistan 2018 ub &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## 10 Afghanistan 2017 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; ## # … with 15,800 more rows WHO HIV Data: long-to-wide in SAS In SAS, we can do the pivot operations in one step, but we have to sort everything first. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SORT DATA = classdat.hivtidy; NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 9 BY Country year group; 10 RUN; NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 11 12 PROC TRANSPOSE DATA = classdat.hivtidy OUT = hivtmp name = 12 ! measure; NOTE: Data file CLASSDAT.HIVTIDY.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 13 BY Country year; 14 VAR est lb ub; 15 ID group; 16 RUN; NOTE: There were 0 observations read from the data set CLASSDAT.HIVTIDY. NOTE: The data set WORK.HIVTMP has 3 observations and 3 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 17 18 PROC PRINT data=hivtmp(obs=5); 19 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16. Try it out Use the long-format data you have from the previous Try It Out section (WHO Disease Incidence). Reshape this data into a “wide” format such that each disease is shown in a separate column. Before you start: - Which variable(s) will uniquely identify a row in your output data? - Which variable(s) will be used to create column names? Can you create a plot of polio cases over time for your 3 favorite countries? R solution who_disease_wide &lt;- who_disease_long %&gt;% pivot_wider(id_cols = c(Country, year), names_from = Disease, values_from = cases) who_disease_wide %&gt;% filter(Country %in% c(&quot;Guatemala&quot;, &quot;Central African Republic (the)&quot;, &quot;Pakistan&quot;)) %&gt;% select(Country, year, polio) %&gt;% ggplot(aes(x = year, y = polio, color = Country)) + geom_line() SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/2020-stat850/sas 7 8 PROC SORT DATA = classdat.disease_long OUT = dis_long; NOTE: Data file CLASSDAT.DISEASE_LONG.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 9 BY Cname year; /* Variables we want to define rows of data */ 10 RUN; NOTE: The data set WORK.DIS_LONG has 0 observations and 0 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 11 12 PROC TRANSPOSE DATA = dis_long OUT = classdat.disease_wide; NOTE: Data file CLASSDAT.DISEASE_WIDE.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 13 ID Disease; /* Variable we want to name columns of data */ ERROR: Variable DISEASE not found. 14 VAR cases; /* Variable we want to be the values in each column 14 ! */ ERROR: Variable CASES not found. 15 BY Cname year; /* These define a single row */ ERROR: Variable CNAME not found. ERROR: Variable YEAR not found. 16 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_WIDE may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set CLASSDAT.DISEASE_WIDE was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 title &#39;Polio incidence&#39;; 19 PROC SGPLOT DATA=classdat.disease_wide 20 /* We can use a where clause in the DATA statement, if we want 20 ! */ 21 (WHERE=(Cname in (&quot;Mexico&quot;, &quot;Guatemala&quot;, &quot;Pakistan&quot;))); NOTE: Data file CLASSDAT.DISEASE_WIDE.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 22 /* Specify the colors to use for lines */ 23 styleattrs datacontrastcolors= (green orange purple); 24 /* Map variables to axes */ 25 SERIES X = year Y = polio / 26 /* Color lines by Country */ 27 GROUP = Cname 28 /* Give the color mapping a name so you can modify its legend 28 ! */ 29 name = &quot;a&quot; 30 /* Make lines thicker so they are visible */ 31 lineattrs=(thickness = 3); 32 /* Change the legend title and what it is showing (line color) 32 ! */ 33 KEYLEGEND &quot;a&quot; / title = &quot;Country&quot; type = linecolor; 34 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,17. Congratulations! You now know how to reshape your data into all sorts of different formats. Use this knowledge wisely. 7.4 Relational Data and Joining Tables We now know how to work extensively on one table at a time, but data doesn’t always come organized in one table at a time. Instead, some data may be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between individuals that have to be documented. Example: Primary School Organization Each individual has certain characteristics: - full_name - gender - birth date - ID number Each student has specific characteristics: - ID number - parent name - parent phone number - medical information - Class ID Teachers may also have additional information: - ID number - Class ID - employment start date - education level - compensation level There are also fields like grades, which occur for each student in each class, but multiple times a year. - ID number - Student ID - Class ID - year - term number - subject - grade - comment And for teachers, there are employment records on a yearly basis - ID number - Employee ID - year - rating - comment But each class also has characteristics that describe the whole class as a unit: - location ID - class ID - meeting time - grade level Each location might also have some logistical information attached: - location ID - room number - building - number of seats - AV equipment We could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment It would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well. But, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps: get the Class ID get any teachers that are assigned that Class ID - specifically, get their ID number get any students that are assigned that Class ID - specifically, get their ID number append the results from teachers and students so that there is a list of all individuals in the class look through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days. Table joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information. There are 3 main types of table joins: Filtering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change) Ex: finding all teachers or students who have class ClassID Set operations, which treat observations as set elements (e.g. union, intersection, etc.) Ex: taking the union of all student and teacher IDs to get a list of individual IDs Mutating joins, which add columns from one table to matching rows in another table Ex: adding birthday to the table of all individuals in a class keys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table. We’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins. Feel free to read through the other types of joins here Animating different types of joins Note: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain. If we start with two tables, x and y, We can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns) But what if we want to keep all of the rows in x? We would do a left_join If there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one. If we wanted to keep all of the rows in y, we would do a right_join: (or, we could do a left join with y and x, but… either way is fine). And finally, if we want to keep all of the rows, we’d do a full_join: You can find other animations corresponding to filtering joins and set operations here 7.4.1 Demonstration dataset setup We’ll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you’ll recall, I had to use a small sample the last time we played with this data…). So before we can work with this data, we have to load the tables into SAS, which means saving them out from R. Instructions We’ll use a function in the dbplyr package to do that.25 if (!&quot;nycflights13&quot; %in% installed.packages()) install.packages(&quot;nycflights13&quot;) if (!&quot;dbplyr&quot; %in% installed.packages()) install.packages(&quot;dbplyr&quot;) library(nycflights13) library(dbplyr) ## ## Attaching package: &#39;dbplyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## ident, sql nycflights13_sqlite(path = &quot;data/&quot;) ## Caching nycflights db at data//nycflights13.sqlite ## &lt;SQLiteConnection&gt; ## Path: /home/susan/Projects/Class/unl-stat850/2020-stat850/data/nycflights13.sqlite ## Extensions: TRUE Then, you’ll have to figure out where on your system database locations (DSNs) are stored. On Unix systems, it’s /etc/odbc.ini (for system-wide access) and ~/.odbc.ini. On windows, you’ll need to use the ODBC Data Source Administrator to set this up. [nycflight] Description = NYC flights database Driver = SQLite3 Database = data/nycflights13.sqlite Try it out Sketch a diagram of which fields in each table match fields in other tables. You can find the solution here (scroll down a bit). 7.4.2 Mutating joins A mutating join combines variables in two tables. There are excellent visual representations of the types of mutating joins here. Every join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side. Joins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept. Left Side Right Side Join Type Rows Cols Rows Cols inner matching all matching all left all all matching all right matching all all all outer all all all all Mutating joins in R t1 &lt;- tibble(x = c(&quot;A&quot;, &quot;B&quot;, &quot;D&quot;), y = c(1, 2, 3)) t2 &lt;- tibble(x = c(&quot;B&quot;, &quot;C&quot;, &quot;D&quot;), z = c(2, 4, 5)) An inner join keeps only rows that exist on both sides, but keeps all columns. inner_join(t1, t2) ## Joining, by = &quot;x&quot; ## # A tibble: 2 x 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 2 2 ## 2 D 3 5 A left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs. left_join(t1, t2) ## Joining, by = &quot;x&quot; ## # A tibble: 3 x 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 NA ## 2 B 2 2 ## 3 D 3 5 left_join(t2, t1) ## Joining, by = &quot;x&quot; ## # A tibble: 3 x 3 ## x z y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 2 2 ## 2 C 4 NA ## 3 D 5 3 There is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there right_join(t1, t2) ## Joining, by = &quot;x&quot; ## # A tibble: 3 x 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 2 2 ## 2 D 3 5 ## 3 C NA 4 right_join(t2, t1) ## Joining, by = &quot;x&quot; ## # A tibble: 3 x 3 ## x z y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 B 2 2 ## 2 D 5 3 ## 3 A NA 1 An outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join. full_join(t1, t2) ## Joining, by = &quot;x&quot; ## # A tibble: 4 x 3 ## x y z ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1 NA ## 2 B 2 2 ## 3 D 3 5 ## 4 C NA 4 In SAS, you can do joins using a data step or PROC SQL. To do a join with data steps, you have to have your data sorted by columns that overlap. PROC SQL has no such requirement. This guide shows the syntax for PROC SQL and DATA step joins side-by-side. PROC SQL Mutating joins 6 data t1; 7 input x $ y; 8 datalines; NOTE: The data set WORK.T1 has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ; 13 14 data t2; 15 input x $ z; 16 datalines; NOTE: The data set WORK.T2 has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 20 ; 21 22 title &#39;Inner join&#39;; 23 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 24 SELECT * FROM t1 as p1 25 INNER JOIN t2 as p2 26 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 27 28 title &#39;Left join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 30 SELECT * FROM t1 as p1 31 LEFT JOIN t2 as p2 32 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 33 34 title &#39;Right join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 35 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 36 SELECT * FROM t1 as p1 37 RIGHT JOIN t2 as p2 38 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 39 40 title &#39;Full Outer join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 41 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 42 SELECT * FROM t1 as p1 43 FULL OUTER JOIN t2 as p2 44 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 45 46 /* Use coalesce to prevent column duplication */ 47 48 title &#39;Inner join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 49 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 50 SELECT COALESCE(p1.x, p2.x) AS x, y, z 51 FROM t1 as p1 INNER JOIN t2 as p2 52 on p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 53 54 title &#39;Left join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 55 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 56 SELECT COALESCE(p1.x, p2.x) AS x, y, z 57 FROM t1 as p1 LEFT JOIN t2 as p2 58 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 59 60 title &#39;Right join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 61 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 62 SELECT COALESCE(p1.x, p2.x) AS x, y, z 63 FROM t1 as p1 RIGHT JOIN t2 as p2 64 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 65 66 title &#39;Full Outer join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 67 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 68 SELECT COALESCE(p1.x, p2.x) AS x, y, z 69 FROM t1 as p1 FULL OUTER JOIN t2 as p2 70 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 71 72 title; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19. Data Step Mutating joins NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 6 data t1; 7 input x $ y; 8 datalines; NOTE: The data set WORK.T1 has 0 observations and 2 variables. WARNING: Data set WORK.T1 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 12 ; 13 14 data t2; 15 input x $ z; 16 datalines; NOTE: The data set WORK.T2 has 0 observations and 2 variables. WARNING: Data set WORK.T2 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ; 21 22 DATA inner; 23 MERGE t1 (IN = p1) t2 (IN = p2); 24 BY x; 25 IF p1 AND p2; 26 RUN; NOTE: The data set WORK.INNER has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 27 28 PROC PRINT DATA=inner;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 DATA left; 31 MERGE t1 (IN = p1) t2 (IN = p2); 32 BY x; 33 IF p1; 34 RUN; NOTE: The data set WORK.LEFT has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 35 36 PROC PRINT DATA=left;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 DATA right; 39 MERGE t1 (IN = p1) t2 (IN = p2); 40 BY x; 41 IF p2; 42 RUN; NOTE: The data set WORK.RIGHT has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 43 44 PROC PRINT DATA=right;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 45 46 DATA outer; 47 MERGE t1 (IN = p1) t2 (IN = p2); 48 BY x; 49 RUN; NOTE: The data set WORK.OUTER has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 50 51 PROC PRINT DATA=outer;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19. As before, these functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let’s determine whether there’s a relationship between the age of a plane and its delays. In R library(nycflights13) plane_age &lt;- planes %&gt;% mutate(age = 2013 - year) %&gt;% # This gets us away from having to deal with 2 different year columns select(tailnum, age, manufacturer) delays_by_plane &lt;- flights %&gt;% select(dep_delay, arr_delay, carrier, flight, tailnum) # We only need to keep delays that have a plane age, so use inner join res &lt;- inner_join(delays_by_plane, plane_age, by = &quot;tailnum&quot;) ggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + geom_boxplot() + ylab(&quot;Departure Delay (min)&quot;) + xlab(&quot;Plane age&quot;) + coord_cartesian(ylim = c(-20, 50)) ## Warning: Removed 5306 rows containing missing values (stat_boxplot). ## Warning: Removed 4068 rows containing non-finite values (stat_boxplot). ggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + geom_boxplot() + ylab(&quot;Arrival Delay (min)&quot;) + xlab(&quot;Plane age&quot;) + coord_cartesian(ylim = c(-30, 60)) ## Warning: Removed 5306 rows containing missing values (stat_boxplot). ## Warning: Removed 5011 rows containing non-finite values (stat_boxplot). It doesn’t look like there’s much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren’t enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old). In SAS 6 libname nycair odbc complete = 6 ! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX; NOTE: Libref NYCAIR was successfully assigned as follows: Engine: ODBC Physical Name: nycflight 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE tmp1 AS 10 SELECT 2013 - year AS age, tailnum FROM nycair.planes 11 WHERE ^missing(year); NOTE: Statement not executed due to NOEXEC option. 12 13 CREATE TABLE tmp2 AS 14 SELECT tailnum, dep_delay, arr_delay, air_time, distance, hour, 14 ! carrier, origin 15 FROM nycair.flights; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19. Now that the prep work is done, we can get on with answering the question. 6 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE agedelay AS 9 SELECT COALESCE(p1.tailnum, p2.tailnum) AS tailnum, 10 age, dep_delay, arr_delay, air_time, distance, hour, 10 ! carrier, origin 11 FROM tmp1 as p1 INNER JOIN tmp2 as p2 12 ON p1.tailnum = p2.tailnum 13 WHERE age &lt; 25 /* only work with areas where there&#39;s enough data 13 ! */ 14 ORDER BY age; NOTE: Statement not executed due to NOEXEC option. 15 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 PROC MEANS DATA = agedelay NOPRINT; ERROR: File WORK.AGEDELAY.DATA does not exist. 17 BY age; ERROR: No data set open to look up variables. 18 VAR dep_delay arr_delay; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 19 OUTPUT OUT = agemeans; 20 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.AGEMEANS may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 22 PROC TRANSPOSE DATA = agemeans(where = (_STAT_=&quot;MEAN&quot;)) 23 OUT = agemeanst(drop= _Label_) ERROR: Variable _STAT_ is not on file WORK.AGEMEANS. 24 name=VAR; 25 BY age _FREQ_; /* Just to keep _FREQ_ around */ ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 26 VAR dep_delay arr_delay; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 27 ID _STAT_; ERROR: No data set open to look up variables. 28 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.AGEMEANST may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22. In theory, we could use some sort of linear model, but lets start with a simple plot of age * average delay and see where that takes us. 6 PROC SGPANEL DATA = agemeanst; 7 PANELBY VAR; ERROR: Variable VAR not found. 8 SCATTER X = age Y = MEAN; ERROR: Variable AGE not found. ERROR: Variable MEAN not found. 9 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPANEL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23. I started wondering about that pattern - to me, it doesn’t look like there’s any particular trend so much as there’s just low - high - low clusters. So I decided to plot frequency as well, and, lo and behold, the frequency count is similarly distributed. So my current working hypothesis is that there are a lot more observations in the middle group of planes that are 5-15 years old. 6 7 PROC SGPLOT DATA = agemeanst; 8 SCATTER X = age Y = _FREQ_; ERROR: Variable AGE not found. ERROR: Variable _FREQ_ not found. 9 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23. It’s also entirely possible that planes between 5-15 years of age have more mechanical issues, but I suspect most delays are due to airport, weather, etc. rather than mechanical malfunctions. 7.5 Example: Gas Prices Data The US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table (or you can look at the screenshot below, as I don’t really trust that the site design will stay the same…) Gas prices at US EIA site The data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable. But looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph. 7.5.1 Option 1: Manual formatting in Excel An excel spreadsheet of the data as downloaded in Sept 2020 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format? What steps are involved? Copy the year-month column, creating one vertical copy for every set of columns Move each block of two columns down to the corresponding vertical copy Delete empty rows Format dates Delete empty columns Here is a video of me doing most of these steps. I skipped out on the data cleaning stage because Excel is miserable for working with dates. 7.5.2 Option 2: R This section shows you how to clean the data up without any sort of database merges, but with 2 pivot operations. The SAS section shows how to clean the data up with database merges and a single pivot operation. You can, of course, use either approach in either language. If you want to try this for yourself first, there is a skeleton file here that you can fill in. Use the intermediate datasets shown to guide you through the steps, then click on the expandable sections to see the code that was used. Reading in the data from the web library(rvest) # scrape data from the web ## Loading required package: xml2 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## pluck ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(xml2) # parse xml data url &lt;- &quot;https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&amp;s=emm_epm0u_pte_nus_dpg&amp;f=w&quot; htmldoc &lt;- read_html(url) gas_prices_raw &lt;- html_table(htmldoc, fill = T, trim = T) [[5]] head(gas_prices_raw) ## Year-Month Week 1 Week 1 Week 2 Week 2 Week 3 Week 3 Week 4 Week 4 ## 1 Year-Month End Date Value End Date Value End Date Value End Date Value ## 2 1994-Nov 11/28 1.175 ## 3 1994-Dec 12/05 1.143 12/12 1.118 12/19 1.099 12/26 1.088 ## 4 ## 5 1995-Jan 01/02 1.104 01/09 1.111 01/16 1.102 01/23 1.110 ## 6 1995-Feb 02/06 1.103 02/13 1.099 02/20 1.093 02/27 1.101 ## Week 5 Week 5 NA NA ## 1 End Date Value NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 01/30 1.109 NA NA ## 6 NA NA Initial data cleaning - fix up column names, get rid of empty rows library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x lubridate::as.difftime() masks base::as.difftime() ## x lubridate::date() masks base::date() ## x tidyr::extract() masks magrittr::extract() ## x dplyr::filter() masks stats::filter() ## x rvest::guess_encoding() masks readr::guess_encoding() ## x dbplyr::ident() masks dplyr::ident() ## x lubridate::intersect() masks base::intersect() ## x dplyr::lag() masks stats::lag() ## x rvest::pluck() masks purrr::pluck() ## x purrr::set_names() masks magrittr::set_names() ## x lubridate::setdiff() masks base::setdiff() ## x dbplyr::sql() masks dplyr::sql() ## x lubridate::union() masks base::union() library(magrittr) # pipe friendly operations # Function to clean up column names # Written as an extra function because it makes the code a lot cleaner fix_gas_names &lt;- function(x) { # Add extra header row information paste(x, c(&quot;&quot;, rep(c(&quot;Date&quot;, &quot;Value&quot;), times = 5))) %&gt;% # trim leading/trailing spaces str_trim() %&gt;% # replace characters in names that aren&#39;t ok for variables in R make.names() } # Clean up the table a bit gas_prices_raw &lt;- gas_prices_raw %&gt;% set_names(fix_gas_names(names(.))) %&gt;% # remove first row that is really an extra header row filter(Year.Month != &quot;Year-Month&quot;) %&gt;% # get rid of empty rows filter(Year.Month != &quot;&quot;) head(gas_prices_raw) ## Year.Month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date ## 1 1994-Nov ## 2 1994-Dec 12/05 1.143 12/12 1.118 12/19 ## 3 1995-Jan 01/02 1.104 01/09 1.111 01/16 ## 4 1995-Feb 02/06 1.103 02/13 1.099 02/20 ## 5 1995-Mar 03/06 1.103 03/13 1.096 03/20 ## 6 1995-Apr 04/03 1.116 04/10 1.134 04/17 ## Week.3.Value Week.4.Date Week.4.Value Week.5.Date Week.5.Value NA. NA.Date ## 1 11/28 1.175 NA NA ## 2 1.099 12/26 1.088 NA NA ## 3 1.102 01/23 1.110 01/30 1.109 NA NA ## 4 1.093 02/27 1.101 NA NA ## 5 1.095 03/27 1.102 NA NA ## 6 1.149 04/24 1.173 NA NA Separate year and month into different columns gas_prices_raw &lt;- gas_prices_raw %&gt;% separate(Year.Month, into = c(&quot;year&quot;, &quot;month&quot;), sep = &quot;-&quot;) ## Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [320]. ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 2 rows [321, ## 322]. head(gas_prices_raw) ## year month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date ## 1 1994 Nov ## 2 1994 Dec 12/05 1.143 12/12 1.118 12/19 ## 3 1995 Jan 01/02 1.104 01/09 1.111 01/16 ## 4 1995 Feb 02/06 1.103 02/13 1.099 02/20 ## 5 1995 Mar 03/06 1.103 03/13 1.096 03/20 ## 6 1995 Apr 04/03 1.116 04/10 1.134 04/17 ## Week.3.Value Week.4.Date Week.4.Value Week.5.Date Week.5.Value NA. NA.Date ## 1 11/28 1.175 NA NA ## 2 1.099 12/26 1.088 NA NA ## 3 1.102 01/23 1.110 01/30 1.109 NA NA ## 4 1.093 02/27 1.101 NA NA ## 5 1.095 03/27 1.102 NA NA ## 6 1.149 04/24 1.173 NA NA Move from wide to long format (part 1: extra-long format) gas_prices_long &lt;- pivot_longer(gas_prices_raw, -c(year, month), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) head(gas_prices_long) ## # A tibble: 6 x 4 ## year month variable value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1994 Nov Week.1.Date &quot;&quot; ## 2 1994 Nov Week.1.Value &quot;&quot; ## 3 1994 Nov Week.2.Date &quot;&quot; ## 4 1994 Nov Week.2.Value &quot;&quot; ## 5 1994 Nov Week.3.Date &quot;&quot; ## 6 1994 Nov Week.3.Value &quot;&quot; Move from wide to long format (part 2: data cleaning and pivot long-to-wide) We need to get our variables into two columns: one for what the value contains, and one indicating which week the value is from. gas_prices_long &lt;- gas_prices_long %&gt;% # First, take &quot;Week.&quot; off of the front mutate(variable = str_remove(variable, &quot;Week\\\\.&quot;)) %&gt;% # Then separate the two values separate(variable, into = c(&quot;week&quot;, &quot;variable&quot;), sep = &quot;\\\\.&quot;) head(gas_prices_long) ## # A tibble: 6 x 5 ## year month week variable value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1994 Nov 1 Date &quot;&quot; ## 2 1994 Nov 1 Value &quot;&quot; ## 3 1994 Nov 2 Date &quot;&quot; ## 4 1994 Nov 2 Value &quot;&quot; ## 5 1994 Nov 3 Date &quot;&quot; ## 6 1994 Nov 3 Value &quot;&quot; Now we’re ready to move back into wide-er form # gas_prices &lt;- gas_prices_long %&gt;% # filter out empty values filter(value != &quot;&quot;) %&gt;% pivot_wider( names_from = variable, values_from = value ) head(gas_prices) ## # A tibble: 6 x 5 ## year month week Date Value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1994 Nov 4 11/28 1.175 ## 2 1994 Dec 1 12/05 1.143 ## 3 1994 Dec 2 12/12 1.118 ## 4 1994 Dec 3 12/19 1.099 ## 5 1994 Dec 4 12/26 1.088 ## 6 1995 Jan 1 01/02 1.104 Clean up dates and format variables properly We can read the date in as MDY format if we just add the year to the end of the month/day column. library(lubridate) # dates and times gas_prices &lt;- gas_prices %&gt;% mutate(Date = paste(Date, year, sep = &quot;/&quot;)) %&gt;% mutate(Date = mdy(Date)) # And now we can get rid of redundant columns gas_prices &lt;- gas_prices %&gt;% select(Date, Value) # Finally, our value variable is a character variable, so lets fix that quick gas_prices &lt;- gas_prices %&gt;% mutate(Value = as.numeric(Value)) head(gas_prices) ## # A tibble: 6 x 2 ## Date Value ## &lt;date&gt; &lt;dbl&gt; ## 1 1994-11-28 1.18 ## 2 1994-12-05 1.14 ## 3 1994-12-12 1.12 ## 4 1994-12-19 1.10 ## 5 1994-12-26 1.09 ## 6 1995-01-02 1.10 # Lets look at our data: ggplot(gas_prices, aes(x = Date, y = Value)) + geom_line() The full code file for this analysis is here. 7.5.3 Option 3: SAS If you want to try this for yourself first, there is a skeleton file here that you can fill in. Use the intermediate datasets shown to guide you through the steps, then click on the expandable sections to see the code that was used. I spent quite a while trying to get collectcode = T to run so that I could put the SAS code in in smaller bites, but couldn’t get it to work. Please forgive the screenshots, but ugh, I’m tired of fooling with SAS at the moment. Read in the data 6 /*********************************************************/ 7 /* Step 1: Read in the data and drop missing rows/cols */ 8 /*********************************************************/ 9 options missing=&#39; &#39;; 10 data gas_raw; 11 infile &#39;data/gas_prices_raw.csv&#39; firstobs=3 delimiter = &#39;,&#39; 11 ! MISSOVER DSD; 12 informat ym $10. ; 13 informat date1 $8. ; informat value1 4.3 ; 14 informat date2 $8. ; informat value2 4.3 ; 15 informat date3 $8. ; informat value3 4.3 ; 16 informat date4 $8. ; informat value4 4.3 ; 17 informat date5 $8. ; informat value5 4.3 ; 18 informat blank $2. ; informat blank $2. ; 19 format ym $10. ; 20 format date1 $8. ; format value1 4.3 ; 21 format date2 $8. ; format value2 4.3 ; 22 format date3 $8. ; format value3 4.3 ; 23 format date4 $8. ; format value4 4.3 ; 24 format date5 $8. ; format value5 4.3 ; 25 format blank $2. ; format blank $2. ; 26 input ym $ date1 $ value1 date2 $ value2 date3 $ value3 date4 $ 26 ! value4 date5 $ value5 blank $ blank $; 27 /* drop extra columns */ 28 drop blank; 29 30 /* delete any rows where ym is not there */ 31 if missing(ym) then delete; 32 run; NOTE: The data set WORK.GAS_RAW has 0 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 33 ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23. Dates are formated as 8-character strings, values as numbers with 3 decimal places. The options missing piece allows us to test for whether information is missing or not. SAS output - gas_raw Split up year and month, format month properly 6 7 proc format; 8 value $ mon &#39;Jan&#39; = 1 &#39;Feb&#39; = 2 &#39;Mar&#39; = 3 &#39;Apr&#39; = 4 &#39;May&#39; = 5 _ 22 76 8 ! &#39;Jun&#39; = 6 &#39;Jul&#39; = 7 &#39;Aug&#39; = 8 &#39;Sep&#39; = 9 &#39;Oct&#39; = 10 &#39;Nov&#39; = 11 8 ! &#39;Dec&#39; = 12; ERROR 22-322: Syntax error, expecting one of the following: a quoted string, a format name. ERROR 76-322: Syntax error, statement will be ignored. 9 run; WARNING: RUN statement ignored due to previous errors. Submit QUIT; to terminate the procedure. NOTE: PROCEDURE FORMAT used (Total process time): real time 0.01 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 10 11 data gas_raw; 12 set gas_raw; 13 length var1-var2 $4.; 14 array var(2) $; NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 15 do i = 1 to dim(var); 16 var[i]=scan(ym,i,&#39;-&#39;); 17 end; 18 rename var1 = year var2 = month; 19 drop i ym; 20 if date1 = &quot;NA&quot; then delete; /* get rid of NA stuff */ 21 run; NOTE: The data set WORK.GAS_RAW has 0 observations and 12 variables. WARNING: Data set WORK.GAS_RAW was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 23 /* format month as a number */ 24 data gas_raw; 25 set gas_raw; 26 format month $ mon.; ____ 48 ERROR 48-59: The format $MON was not found or could not be loaded. 27 run; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.GAS_RAW may be incomplete. When this step was stopped there were 0 observations and 12 variables. WARNING: Data set WORK.GAS_RAW was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23. SAS output - gas_raw Get long data with just the dates 6 7 PROC TRANSPOSE DATA = gas_raw OUT = split_long1 8 (rename=(col1=date)) NAME = week; 9 BY year month NOTSORTED; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. 10 VAR date1 date2 date3 date4 date5; 11 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG1 may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 data split_long1; 14 set split_long1; 15 IF _N_ = 1 THEN DO; 16 REGEXname = PRXPARSE(&quot;s/date//&quot;); 17 END; 18 RETAIN REGEXname; 19 20 CALL PRXCHANGE(REGEXname, -1, week, week); _________ 716 WARNING 716-185: Argument #4 is a numeric variable, while a character variable must be passed to the PRXCHANGE subroutine call in order for the variable to be updated. 21 DROP REGEXname; 22 IF missing(date) THEN delete; 23 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 20:31 20:37 NOTE: The data set WORK.SPLIT_LONG1 has 0 observations and 2 variables. WARNING: Data set WORK.SPLIT_LONG1 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC SORT data = split_long1 out = split_long1; 26 BY year month week; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. ERROR: Variable WEEK not found. 27 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG1 may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.SPLIT_LONG1 was not replaced because this step was stopped. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23,24. SAS output - split_long1 Get long data with just the prices 6 7 PROC TRANSPOSE DATA=gas_split OUT =split_long2 ERROR: File WORK.GAS_SPLIT.DATA does not exist. 8 (rename=(col1=price)) NAME = week; 9 BY year month NOTSORTED; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 10 VAR value1 value2 value3 value4 value5; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 11 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG2 may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 12 13 data split_long2; 14 set split_long2; 15 IF _N_ = 1 THEN DO; 16 REGEXname = PRXPARSE(&quot;s/value//&quot;); 17 END; 18 RETAIN REGEXname; 19 20 CALL PRXCHANGE(REGEXname, -1, week, week); _________ 716 WARNING 716-185: Argument #4 is a numeric variable, while a character variable must be passed to the PRXCHANGE subroutine call in order for the variable to be updated. 21 DROP REGEXname; 22 IF missing(price) THEN delete; 23 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 20:31 20:37 NOTE: The data set WORK.SPLIT_LONG2 has 0 observations and 2 variables. WARNING: Data set WORK.SPLIT_LONG2 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 24 25 PROC SORT data = split_long2 out = split_long2; 26 BY year month week; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. ERROR: Variable WEEK not found. 27 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG2 may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.SPLIT_LONG2 was not replaced because this step was stopped. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23,24,25. SAS output - split_long2 Merge the two long datasets together 6 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE gas_prices AS 9 SELECT COALESCE(p1.year, p2.year) AS year, 10 COALESCE(p1.month, p2.month) AS month, 11 COALESCE(p1.week, p2.week) AS week, 12 date, price 13 FROM split_long1 as p1 14 RIGHT JOIN split_long2 as p2 15 ON p1.year = p2.year AND p1.month = p2.month AND p1.week = 15 ! p2.week; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23,24,25,26. SAS output - gas prices Format dates 6 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.02 seconds cpu time 0.03 seconds 7 DATA split2; 8 SET gas_prices; ERROR: File WORK.GAS_PRICES.DATA does not exist. 9 length var1-var2 3; 10 array var(2); NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 11 do i = 1 to dim(var); 12 var[i]=scan(date,i,&#39;/&#39;); 13 end; 14 RENAME var1 = monthnum var2 = day; 15 DROP i date; /* month is also in numeric form */ 16 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 12:19 NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 12:7 NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT2 may be incomplete. When this step was stopped there were 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 DATA gas_prices; 19 SET split2; 20 date = MDY(monthnum, day, year); 21 FORMAT date yymmdd10.; 22 KEEP date price; 23 RUN; WARNING: The variable price in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The data set WORK.GAS_PRICES has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC SORT DATA=gas_prices OUT = gas_prices; 26 BY date; 27 RUN; NOTE: The data set WORK.GAS_PRICES has 0 observations and 0 variables. WARNING: Data set WORK.GAS_PRICES was not replaced because new file is incomplete. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23,24,25,26. SAS output - gas prices, final version 6 7 PROC SGPLOT DATA = gas_prices; 8 SERIES X = date Y = price; ERROR: Variable PRICE not found. 9 TITLE &#39;Gas Prices&#39;; 10 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 9,11,12,13,15,16,18,19,22,23,24,25,26,27. You can see the full script here. References String manipulation R4DS chapter - strings SAS and Perl regular expressions PCRE tester R regex tester - has a short timeout period and will disconnect you if you’re idle too long. But you can also clone the repo here and run it locally. SAS scan statement Tidy data tutorials/references Tidy Data - Data Skills for Reproducible Science Five Ways to Flip-Flop Your Data PROC TRANSPOSE reference tidyr reference Relational Data &amp; Joins: R4DS chapter - Relational data Merge statement in SAS 5 little known, but highly valuable and widely useful PROC SQL Programming Techniques Other references SAS rename statement SAS graph customization SAS SGPLOT procedure Data Visualization with ggplot R graph gallery Videos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of tidyr/dplyr as well. Way back in Module 2, I briefly mentioned list-columns in tibbles. At the time, you didn’t have enough R knowledge to use that information, but now you do!! You can see a couple of examples here (but they assume that you know things that you’ll only learn in a few modules). Incidentally, dbplyr is basically dplyr for databases, and is worth checking out, even if we aren’t covering it in this class.↩︎ "],["debugging.html", "Module 8 Principles of Debugging Debugging: Module Objectives 8.1 Step 1: Check your spelling. 8.2 Defensive Programming 8.3 General Debugging Strategies 8.4 Debugging Tools in R 8.5 Debugging Tools in SAS 8.6 Minimal Working Examples References", " Module 8 Principles of Debugging Debugging: Module Objectives Break down a complex procedure into simpler steps, mapping each step to a separate function which performs a single task. Simplify a problem to the minimal components necessary to reproduce the error, and use that information to ask for help appropriately. Use built-in debugging tools to trace an error to its source Use online forums and mailing lists to research error messages Note: The skills in this chapter take a lifetime to truly master. The real goal here is that you know how to ask for help appropriately (and in a way that people will respond positively to) and that you know how to do the research to get help yourself. The faces of debugging (by Allison Horst) 8.1 Step 1: Check your spelling. I’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation. 8.2 Defensive Programming One of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively. By that, I mean, code in a way that you will make debugging things easier later. Modularize your code. Each function should do only one task, ideally in the least-complex way possible. Make your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly. Comment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you. Don’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex. Reduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse is notorious for this. It’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constitutent packages are some of the most installed R packages on CRAN.) Add safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a.”) Don’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression. (This is easier if you’re writing modular code to begin with) Collect your often-reused code in packages (R) or scripts (SAS) that you can easily load and make available to “future you” Wikipedia’s article on defensive programming is much more general than the applications to statistical programming, but may be worth scanning. 8.3 General Debugging Strategies Debugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once While defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in Advanced R by H. Wickham26; I’ve copied it here because it’s such a succinct distillation of the process, but I’ve adapted some of the explanations to this class rather than the original c ontext of package development. Realize that you have a bug Google! In R you can automate this with the errorist and searcher packages. In SAS, if the error message isn’t that clear you’ll find a SAS forum page where someone else has made the same mistake - I can almost guarantee it. Make the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help. Use binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.) Generate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. Note which inputs don’t generate the bug – this negative “data” is helpful when asking for help. Figure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements. Fix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked. There are several other general strategies for debugging: Retype (from scratch) your code This works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative. Visualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application. Tracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x,” “returning from function y,” and so on. Rubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, nevermind?” Rubber ducking outsources the problem to a nonjudgemental entity, such as a rubber duck27. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com. Figure 8.1: You may find it helpful to procure a rubber duck expert for each language you work in. I use color-your-own rubber ducks to endow my ducks with certain language expertise. Other people use plain rubber ducks and give them capes. Do not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own t-shirt, in addition to an xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea. 8.4 Debugging Tools in R Now that we’ve discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging in R. 8.4.1 Low tech debugging with print() and other tools Sometimes called “tracing” techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. When the code is executed, you get a window into what the variables look like during execution. Simple example Imagine we start with this: a &lt;- function(x) { b &lt;- function(y) { c &lt;- function(z) { z + y } c(3) } x + b(4) } a(5) ## [1] 12 and the goal is to understand what’s happening in the code. We might add some lines: a &lt;- function(x) { print(paste(&quot;Entering a(). x = &quot;, x)) b &lt;- function(y) { print(paste(&quot;Entering b(). x = &quot;, x, &quot;y = &quot;, y)) c &lt;- function(z) { print(paste(&quot;Entering c(). x = &quot;, x, &quot;y = &quot;, y, &quot;z = &quot;, z)) cres &lt;- z + y print(paste(&quot;Returning&quot;, cres, &quot;from c()&quot;)) cres } bres &lt;- c(3) print(paste(&quot;Returning&quot;, bres, &quot;from b()&quot;)) bres } ares &lt;- x + b(4) print(paste(&quot;Returning&quot;,ares, &quot;from a()&quot;)) ares } a(5) ## [1] &quot;Entering a(). x = 5&quot; ## [1] &quot;Entering b(). x = 5 y = 4&quot; ## [1] &quot;Entering c(). x = 5 y = 4 z = 3&quot; ## [1] &quot;Returning 7 from c()&quot; ## [1] &quot;Returning 7 from b()&quot; ## [1] &quot;Returning 12 from a()&quot; ## [1] 12 For more complex data structures, it can be useful to add str(), head(), or summary() functions. Real world example I was recently writing a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I was assuming that the order of the parents would be “html,” “body,” “div,” “table,” “tbody,” “tr” - descending from outer to inner (if you know anything about HTML/XML structure). library(xml2) # read html search_for_parent &lt;- function(node, tag) { # Get all of the parent nodes parents &lt;- xml2::xml_parents(node) # Get the tags of every parent node tags &lt;- purrr::map_chr(parents, rvest::html_name) print(tags) # Find matching taggs matches &lt;- which(tags == tag) print(matches) # Take the minimum matching tag min_match &lt;- min(matches) if (length(matches) == 1) return(parents[min_match]) else return(NULL) } page &lt;- read_html(&quot;data/realclearpolitics_frag.html&quot;) node &lt;- xml_find_all(page, &quot;//td[@class=&#39;lp-results&#39;]&quot;) # find all poll results in any table search_for_parent(node[1], &quot;table&quot;) # find the table that contains it ## [1] &quot;tr&quot; &quot;tbody&quot; &quot;table&quot; &quot;div&quot; &quot;body&quot; &quot;html&quot; ## [1] 3 ## {xml_nodeset (1)} ## [1] &lt;table cellpadding=&quot;2&quot; cellspacing=&quot;0&quot; class=&quot;sortable&quot;&gt;\\n&lt;thead&gt;&lt;tr clas ... By printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node. Try it out Not all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully. library(ggplot2) library(dplyr) library(magrittr) library(maps) library(ggthemes) worldmap &lt;- map_data(&quot;world&quot;) # Load the data data(storms, package = &quot;dplyr&quot;) The code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code. # Make base map to be used for each iteration basemap &lt;- ggplot() + # Country shapes geom_polygon(aes(x = long, y = lat, group = group), data = worldmap, fill = &quot;white&quot;, color = &quot;black&quot;) + # Zoom in coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + # Don&#39;t need scales b/c maps provide their own geographic context... theme_map() for (i in 1:5) { # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(status == i) # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) print(plot) } Solution First, lets split the setup from the loop. # Make base map to be used for each iteration basemap &lt;- ggplot() + # Country shapes geom_polygon(aes(x = long, y = lat, group = group), data = worldmap, fill = &quot;white&quot;, color = &quot;black&quot;) + # Zoom in coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + # Don&#39;t need scales b/c maps provide their own geographic context... theme_map() print(basemap) # make sure the basemap is fine # Load the data data(storms, package = &quot;dplyr&quot;) str(storms) # make sure the data exists and is formatted as expected ## tibble [10,010 × 13] (S3: tbl_df/tbl/data.frame) ## $ name : chr [1:10010] &quot;Amy&quot; &quot;Amy&quot; &quot;Amy&quot; &quot;Amy&quot; ... ## $ year : num [1:10010] 1975 1975 1975 1975 1975 ... ## $ month : num [1:10010] 6 6 6 6 6 6 6 6 6 6 ... ## $ day : int [1:10010] 27 27 27 27 28 28 28 28 29 29 ... ## $ hour : num [1:10010] 0 6 12 18 0 6 12 18 0 6 ... ## $ lat : num [1:10010] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ... ## $ long : num [1:10010] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ... ## $ status : chr [1:10010] &quot;tropical depression&quot; &quot;tropical depression&quot; &quot;tropical depression&quot; &quot;tropical depression&quot; ... ## $ category : Ord.factor w/ 7 levels &quot;-1&quot;&lt;&quot;0&quot;&lt;&quot;1&quot;&lt;&quot;2&quot;&lt;..: 1 1 1 1 1 1 1 1 2 2 ... ## $ wind : int [1:10010] 25 25 25 25 25 25 25 30 35 40 ... ## $ pressure : int [1:10010] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ... ## $ ts_diameter: num [1:10010] NA NA NA NA NA NA NA NA NA NA ... ## $ hu_diameter: num [1:10010] NA NA NA NA NA NA NA NA NA NA ... Everything looks ok in the setup chunk… for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(status == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) # print(plot) # Don&#39;t print plots - clutters up output at the moment } ## [1] &quot;Category 1 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 2 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 3 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 4 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 5 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; Ok, so from this we can see that something is going wrong with our filter statement - we have no rows of data. head(storms) ## # A tibble: 6 x 13 ## name year month day hour lat long status category wind pressure ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropical de… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropical de… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropical de… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropical de… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropical de… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropical de… -1 25 1012 ## # … with 2 more variables: ts_diameter &lt;dbl&gt;, hu_diameter &lt;dbl&gt; Whoops. I meant “category” when I typed “status.” for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(category == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) # print(plot) # Don&#39;t print plots - clutters up output at the moment } ## [1] &quot;Category 1 storms&quot; ## [1] &quot;subdata dims: nrow 13 ncol 13&quot; ## [1] &quot;Category 2 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 3 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 4 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; ## [1] &quot;Category 5 storms&quot; ## [1] &quot;subdata dims: nrow 0 ncol 13&quot; Ok, that’s something, at least. We now have some data for category 1 storms… filter(storms, year == 2013) %&gt;% # Get max category for each named storm group_by(name) %&gt;% filter(category == max(category)) %&gt;% ungroup() %&gt;% # See what categories exist select(name, category) %&gt;% unique() ## # A tibble: 14 x 2 ## name category ## &lt;chr&gt; &lt;ord&gt; ## 1 Andrea 0 ## 2 Barry 0 ## 3 Chantal 0 ## 4 Dorian 0 ## 5 Erin 0 ## 6 Fernand 0 ## 7 Gabrielle 0 ## 8 Eight -1 ## 9 Humberto 1 ## 10 Ingrid 1 ## 11 Jerry 0 ## 12 Karen 0 ## 13 Lorenzo 0 ## 14 Melissa 0 It looks like 2013 was just an incredibly quiet year for tropical activity. 2004, however, was not. So let’s just make sure our code works by checking out 2004. for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2004) %&gt;% filter(category == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) print(plot) # Don&#39;t print plots - clutters up output at the moment } ## [1] &quot;Category 1 storms&quot; ## [1] &quot;subdata dims: nrow 45 ncol 13&quot; ## [1] &quot;Category 2 storms&quot; ## [1] &quot;subdata dims: nrow 39 ncol 13&quot; ## [1] &quot;Category 3 storms&quot; ## [1] &quot;subdata dims: nrow 29 ncol 13&quot; ## [1] &quot;Category 4 storms&quot; ## [1] &quot;subdata dims: nrow 32 ncol 13&quot; ## [1] &quot;Category 5 storms&quot; ## [1] &quot;subdata dims: nrow 12 ncol 13&quot; If we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code). for (i in 1:5) { # print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(category == i) # print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # # str(subdata) works too, but produces more clutter. I started # # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) if (nrow(subdata) &gt; 0) print(plot) } 8.4.2 After an error has occurred - traceback() traceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This can help, but it can also be pretty arcane. traceback() example a &lt;- function(x) { b &lt;- function(y) { c &lt;- function(z) { stop(&#39;there was a problem&#39;) # This generates an error } c() } b() } a() ## Error in c(): there was a problem For more information, you could run traceback traceback() Which will provide the following output: 4: stop(&quot;there was a problem&quot;) at #4 3: c() at #6 2: b() at #8 1: a() Reading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code. If you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error. Figure 8.2: Both Show Traceback and Rerun with Debug are useful tools If you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. 8.4.3 browser() - debugging your own code, interactively The browser() function is useful for debugging your own code. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on. Example of using browser() Suppose that I want to write a function that will plot an xkcd comic in R. I start with library(png) library(xml2) # get the most current xkcd get_xkcd &lt;- function(id = NULL) { url &lt;- &quot;http://xkcd.com&quot; page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) readPNG(source = image) } Watch this live-coding video to see how I use browser() to figure out what’s going on in the function and how to fix it. Vidgrid link. Alternate YouTube Link Here’s the final function library(png) library(xml2) # get the most current xkcd get_xkcd &lt;- function(id = NULL) { url &lt;- &quot;http://xkcd.com&quot; page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) # Fix image address so that we can access the image image &lt;- substr(image, 3, nchar(image)) # Download the file to a temp file and read from there file_location &lt;- tempfile(fileext = &quot;.png&quot;) download.file(image, destfile = file_location, quiet = T) readPNG(source = file_location) } get_xkcd() %&gt;% as.raster() %&gt;% plot() Try it out Each xkcd has a corresponding ID number (ordered sequentially from 1 to 2328 at the time this was written). Modify the XKCD function above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic. Use browser() to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL. What things might you add to make this function “defensive programming” compatible? Solution # get the most current xkcd or the specified number get_xkcd &lt;- function(id = NULL) { if (is.null(id)) { # Have to get the location of the image ourselves url &lt;- &quot;http://xkcd.com&quot; } else if (is.numeric(id)) { url &lt;- paste0(&quot;http://xkcd.com/&quot;, id, &quot;/&quot;) } else { # only allow numeric or null input stop(&quot;To get current xkcd, pass in NULL, otherwise, pass in a valid comic number&quot;) } page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) # Fix image address so that we can access the image image &lt;- substr(image, 3, nchar(image)) # cut the first 2 characters off # make temp file location &lt;- tempfile(fileext = &quot;png&quot;) download.file(image, destfile = location, quiet = T) # This checks to make sure we saved the file correctly if (file.exists(location)) { readPNG(source = location) } else { # Give a good informative error message stop(paste(&quot;Something went wrong saving the image at &quot;, image, &quot; to &quot;, location)) } } get_xkcd(2259) %&gt;% as.raster() %&gt;% plot() 8.4.4 debug() - the general debugging workhorse In the traceback() Rstudio output, the other option is “rerun with debug.” In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow. debug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code. Example of using debug data(iris) tmp &lt;- lm(Species ~ ., data = iris) ## Warning in model.response(mf, &quot;numeric&quot;): using type = &quot;numeric&quot; with a factor ## response will be ignored ## Warning in Ops.factor(y, z$residuals): &#39;-&#39; not meaningful for factors summary(tmp) ## Warning in Ops.factor(r, 2): &#39;^&#39; not meaningful for factors ## ## Call: ## lm(formula = Species ~ ., data = iris) ## ## Residuals: ## Error in quantile.default(resid): (unordered) factors are not allowed We get this weird warning, and then an error about factors when we use summary() to look at the coefficients. debug(lm) # turn debugging on tmp &lt;- lm(Species ~ ., data = iris) summary(tmp) undebug(lm) # turn debugging off) The first thing I see when I run lm after turning on debug (screenshot) The variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function After pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function. We can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong. We probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened. I can hit “Stop” or type “Q” to exit the debug environment. But, until I run undebug(lm), every call to lm will take me into the debug window. undebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run. Try it out larger(x, y) is supposed to return the elementwise maximum of two vectors. larger &lt;- function(x, y) { y.is.bigger &lt;- y &gt; x x[y.is.bigger] &lt;- y[y.is.bigger] x } larger(c(1, 5, 10), c(2, 4, 11)) ## [1] 2 5 11 larger(c(1, 5, 10), 6) ## [1] 6 NA 10 Why is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it. Solution I’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined x &lt;- c(1, 5, 10) y &lt;- 6 # Inside of larger() with x = c(1, 5, 10), y = 6 (y.is.bigger &lt;- y &gt; x ) # putting something in () prints it out ## [1] TRUE TRUE FALSE y[y.is.bigger] # This isn&#39;t quite what we were going for, but it&#39;s what&#39;s causing the issue ## [1] 6 NA x[y.is.bigger] # What gets replaced ## [1] 1 5 # Better option larger &lt;- function(x, y) { y.is.bigger &lt;- y &gt; x ifelse(y.is.bigger, y, x) } 8.5 Debugging Tools in SAS In SAS, there are two stages that occur after you submit lines to the console. The Compilation Phase: code is parsed. In this step, SAS will catch the logic errors, misspellings, missing key words, etc. The Execution Phase: program is run. In this step, SAS will catch any wrong assignments, loop issues, etc. Sas recognizes four types of errors: Syntax errors - violations of the language structure Semantic errors - structure of the statement is incorrect, but the syntax is correct. e.g. trying to reference an index that doesn’t exist. Execution-time errors - errors that occur when the compiled function is run on data values – e.g. division by zero Data errors - errors that occur when statements are correct but data is invalid (taking the log of a negative number, etc.) SAS is built around enterprise users, as opposed to R’s open-source philosophy. SAS code also is more formulaic than R code, which means it is usually easier to figure out what is going wrong with the code. As a result, you may find that errors in your SAS code are much easier to diagnose than errors in your R code. Generally, it will tell you exactly where you are missing a semicolon, or exactly what word it thinks you’ve misspelled (and usually, it tries to correct that for you, but it doesn’t always succeed). In my experience with SAS (which is very limited and mostly contained in this book), SAS error messages are much easier to google and find solutions, right up until you’re working in Linux or some other not-well-supported system and the error is related to how the underlying OS handles some task. As a downside, though, trying to do a task SAS doesn’t think you need to do can be much more difficult than necessary. See the references section for a couple of good guides to SAS error statements and warnings. These guides are likely sufficient for most of your SAS debugging needs. There are certainly other errors which can occur in SAS – logic errors are not something SAS can protect you from . These errors can have dramatic consequences, as demonstrated in this twitter thread about a JAMA retraction due to a coding error. To debug these types of errors, you can use the same print() techniques demonstrated in R. For these types of errors, there’s nothing special about what language you’re using (outside of the usual quirks of every language) - the error is in the logic, not the encoding of that logic. 8.6 Minimal Working Examples If all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and SAS browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors. So, with that said, there are numerous resources for writing what’s called a “minimal working example,” “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example. The goal is to reproduce the error message with information that is minimal - as little code as possible to still reproduce the problem complete - everything necessary to reproduce the issue is contained in the description/question reproducible - test the code you provide to reproduce the problem. You should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working. 8.6.1 Example: MWEs You haven’t gotten to module 8 yet, but at one point I had issues with SAS graphs rendering in black and white most of the time. I started debugging the issue with the following code chunk: {r sas-cat-aes-map-07, engine=&quot;sashtml&quot;, engine.path=&quot;sas&quot;, fig.path = &quot;image/&quot;} libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled) group = Abb; /* maps to point color by default */ RUN; QUIT; PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */ SCATTER x = Population y = Assault / markercharattrs=(size=8) markerchar = Abb /* specify marker character variable */ group = Abb ; RUN; QUIT; (I moved the chunk header to the next line so that you can see the whole chunk) After running the code separately in SAS and getting a figure that looked like what I’d expected, I set out to construct a reproducible example so that I could post to the SASmarkdown github issues page and ask for help. The first thing I did was strip out all of the extra stuff that didn’t need to be in the chunk - this chunk generates 2 pictures; I only need one. This chunk requires the fbiwide data; I replaced it with a dataset in the sashelp library. When I was done, the chunk looked like this: PROC SGPLOT data=sashelp.snacks; SCATTER x = date y = QtySold / markerattrs=(size=8pt symbol=circlefilled) group = product; /* maps to point color by default */ RUN; QUIT; Then, I started constructing my reproducible example. I ran ?sas_enginesetup to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file). I copied the example from that page: indoc &lt;- &#39; --- title: &quot;Basic SASmarkdown Doc&quot; author: &quot;Doug Hemken&quot; output: html_document --- # I&#39;ve deleted the intermediate chunks because they screw # everything up when I print this chunk out &#39; knitr::knit(text=indoc, output=&quot;test.md&quot;) rmarkdown::render(&quot;test.md&quot;) Then, I created several chunks which would do the following: 1. Write the minimal example SAS code above to a file 2. Call that file in a SASmarkdown chunk using the %include macro, which dumps the listed file into the SAS program 3. Call the file using SAS batch mode Finally, I included the image generated from the batch mode call manually. You can see the resulting code here. I pasted my example into the issues page, and then included some additional information: A screenshot of the rendered page The image files themselves A description of what happened My suspicions (some obvious option I’m missing?) An additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks. This process took me about 45 minutes, but that was still much shorter than the time I’d spent rerunning code trying to get it to work with no success. In less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. I had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do). Then, I had to tinker with the book a bit to figure out if there were easier ways to get the same result. Hopefully, at this point, all of the SAS graphs are now in full color, as modern graphics intended. 8.6.2 Try It Out Use this list of StackOverflow posts to try out your new debugging techniques. Can you figure out what’s wrong? What information would you need from the poster in order to come up with a solution? References Stalking the elusive computer bug - the etymology and historical use of the term “debugging,” from Thomas Edison to Grace Hopper. Debugging (lecture materials from software construction class at MIT) - written with java, but mostly comprehensible for any language. Debugging with RStudio An Introduction to the Interactive Debugging Tools in R Stackoverflow: General Suggestions for Debugging in R WTF R - What They Forgot to Teach You about R Debugging chapter Debugging 101 in SAS Debugging SAS Programs - Ch. 1: The Basics of Debugging A webinar by Jenny Bryan/RStudio on Reproducible Examples the 0th step is from the 1st edition, the remaining steps are from the 2nd.↩︎ Some people use cats, but I find that they don’t meet the nonjudgemental criteria. Of course, they’re equally judgemental whether your code works or not, so maybe that works if you’re a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you’re upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.↩︎ "],["data-vis-intro.html", "Module 9 Introduction to Data Visualization Visualization: Module Objectives 9.1 Why do we create graphics? 9.2 General approaches to creating graphics 9.3 The Grammar of Graphics 9.4 Good charts 9.5 Other things worth exploring (and future expansions for this chapter) References", " Module 9 Introduction to Data Visualization Visualization: Module Objectives Create statistical charts in SAS and R Use the grammar of graphics to describe different types of charts and graphs Use the grammar of graphics to create layered graphics and highlight different features of a data set Modify or construct alternate charts showing the same material with better readability and accessibility There are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. This is one of the less serious schemes I’ve seen But, in my opinion, Randall missed the opportunity to put a pie chart as Neutral Evil. Hopefully by the end of this, you will be able to at least make the charts which are most commonly used to show data and statistical concepts. This is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. But, this is also the chapter where we really start to focus on R instead of SAS. Even the hardcore SAS users I know (in this department and others) go into R when they want to make a publication-quality chart. Visualization and statistical graphics are also my research area, so I’m probably going to be a bit more passionate about this chapter, which means there’s probably going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested. R package setup library(readr) library(dplyr) library(tidyr) library(ggplot2) library(stringr) 9.1 Why do we create graphics? The greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come Charts are easier to understand than raw data. (more details inside) When you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around.” Consider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options: Print out the data set Create some summary statistics of each variable and perhaps the covariance between the two variables Draw a scatter plot of the two variables Which one would you rather use? Why? Our brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel). It’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out. 9.2 General approaches to creating graphics There are two general approaches to generating statistical graphics computationally: Manually specify the plot that you want, doing the preprocessing and summarizing before you create the plot. Describe the relationship between the plot and the data, using sensible defaults that can be customized for common operations. In the introduction to The Grammar of Graphics, Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects.” His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics.” Base R graphics and the original SAS graphics engine fall firmly into the first camp. ggplot2 was designed using the philosophy of the grammar of graphics, and is still the primary place that people learn about the grammar of graphics in statistics. The SAS ODS Graphics Engine28 falls somewhere in between the two approaches - it provides some sensible defaults, but its design isn’t rooted in the philosophy of the grammar of graphics, so if (as some SAS manuals have claimed) there is a similarity between the two, it’s functional and not philosophical or because of a similarity in the design strategy. You’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while option 2 does have a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it. There are other graphics systems in R (namely, lattice, plus some web-based rendering engines) that you could explore, but it’s far more important that you know how to functionally create plots in R and/or SAS. I don’t recommend you try to become proficient in all of them. Pick one (two at most) and get to know that, then google for the rest. Before we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years, so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible at the end of this module. R base graphics example # Setup the data poke &lt;- read_csv(&quot;data/pokemon_ascii.csv&quot;, na = &#39;.&#39;) %&gt;% mutate(generation = factor(generation)) Let’s start with what we want: for each generation, we want the total number of pokemon. To get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation. # Create summary of pokemon by type tmp &lt;- poke %&gt;% group_by(generation) %&gt;% count() pie(tmp$n, labels = tmp$generation) # with(., &lt;base command&gt;) let you use the pipe easily with a base R command # that is otherwise not pipe friendly, e.g. # with(tmp, pie(n, labels = generation)) We could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion. # Create summary of pokemon by type tmp &lt;- poke %&gt;% group_by(generation) %&gt;% count() # Matrix is necessary for a stacked bar chart matrix(tmp$n, nrow = 8, ncol = 1, dimnames = list(tmp$generation)) %&gt;% barplot(beside = F, legend.text = T, main = &quot;Generations of Pokemon&quot;) R ggplot2 graphics In ggplot2, we start by specifying which variables we want to be mapped to which features of the data. In a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack.” So it’s easiest to specify our x variable as a constant, \"\". We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable. Then, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”). ggplot(aes(x = &quot;&quot;, fill = generation), data = poke) + geom_bar(position = &quot;stack&quot;) If we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\). ggplot(aes(x = &quot;&quot;, fill = generation), data = poke) + geom_bar(position = &quot;stack&quot;) + coord_polar(&quot;y&quot;, start = 0) Notice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and SAS, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation. SAS Examples Original SAS graphics engine Note: This code runs, but it causes every other sas chunk in this document to go haywire… so you’ll have to copy the code and run it on your own to see what it looks like. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; /* This step creates a constant variable in the data frame, so that all generations can be stacked in one bar */ DATA poketmp; SET classdat.poke; i = 1; RUN; PROC GCHART data=poketmp; pie generation / other = 0; RUN; QUIT; PROC GCHART data=poketmp; VBAR i / SUBGROUP = generation; RUN; QUIT; ODS Graphics Note: This is a terrible example in SAS because there isn’t an easy way to create a pie chart29. We have to resort to using SAS Graph Template Language. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; DATA poketmp; SET classdat.poke; i = 1; RUN; /* Define a pie chart template */ PROC TEMPLATE; DEFINE STATGRAPH WORK.simplepie; BEGINGRAPH; LAYOUT REGION; PIECHART category=generation; ENDLAYOUT; ENDGRAPH; END; RUN; /* Make the pie chart */ PROC SGRENDER data=classdat.poke template=WORK.simplepie; RUN; QUIT; /* Use SGPLOT to make a stacked bar chart */ PROC SGPLOT data=poketmp; VBAR i / GROUP = generation; RUN; QUIT; As in base R, the syntax between the two types of charts is different, even though the underlying operations required to make the plots are very similar. This is one example of why I don’t agree with the assertion that ODS graphics is like ggplot2 syntax - the functionality may be similar, but the structure is not. If only because it provides us with a place to start (because otherwise, we would just work through a graph gallery or two, and that’s boring), we’ll talk first about the general idea behind the grammar of graphics. For each concept, I’ll provide you first with the ggplot grammar of graphics code, and then, where it is possible to replicate the chart easily in base R or SAS ODS graphics, I will provide code for that as well - so that you can compare the approaches, but also so that you get a sense for what is easy and what is possible in each plotting system. 9.3 The Grammar of Graphics In the grammar of graphics, a plot consists of several mostly independent specifications: aesthetics - links between data variables and graphical features (position, color, shape, size) layers - geometric elements (points, lines, rectangles, text, …) transformations - transformations specify a functional link between the data and the displayed information (identity, count, bins, density, regression). Transformations act on the variables. scales - scales map values in data space to values in the aesthetic space. Scales change the coordinate space of an aesthetic, but don’t change the underlying value (so the change is at the visual level, not the mathematical level). coordinate system - e.g. polar or Cartesian faceting - facets allow you to split plots by other variables to produce many sub-plots. theme - formatting items, such as background color, fonts, margins… We can contrast this with other plotting systems (e.g. Base R), where transformations and scales must be handled manually, there may be separate plotting systems for different coordinate systems, etc. Functionally, the biggest difference between the two systems is that in the grammar of graphics system (as implemented in ggplot2), we work with a full tabular data set. So like the rest of the tidyverse, ggplot2 will allow you to reference bare column names as if they were variables, so long as you’ve passed in the data set to the data = argument.30 Building a masterpiece, by Allison Horst Let’s get the data set up in both SAS and R if (!&quot;classdata&quot; %in% installed.packages()) devtools::install_github(&quot;heike/classdata&quot;) # A package of data sets which are useful for class demonstrations library(classdata) library(tidyr) ## ## Attaching package: &#39;tidyr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## extract library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union data(happy) # you&#39;ll use this for try it out sections data(fbi) fbiwide &lt;- fbi %&gt;% select(-Violent.crime) %&gt;% pivot_wider(names_from = Type, values_from = Count) %&gt;% # Rename variables rename(Murder = Murder.and.nonnegligent.Manslaughter, Assault = Aggravated.assault, Larceny = Larceny.theft, Auto.theft = Motor.vehicle.theft) %&gt;% mutate(Rape = as.numeric(Rape)) # Write a csv out to a file to read into sas if(!file.exists(&quot;data/happy.csv&quot;)) write_csv(happy, &quot;data/happy.csv&quot;, na = &quot;.&quot;) if(!file.exists(&quot;data/fbi.csv&quot;)) write_csv(fbi, &quot;data/fbi.csv&quot;, na = &quot;.&quot;) if(!file.exists(&quot;data/fbiwide.csv&quot;)) write_csv(fbiwide, &quot;data/fbiwide.csv&quot;, na = &quot;.&quot;) libname classdat &quot;sas/&quot;; PROC IMPORT datafile = &quot;data/happy.csv&quot; out=classdat.happy DBMS = CSV REPLACE; GETNAMES = YES; GUESSINGROWS = 5000; RUN; PROC IMPORT datafile = &quot;data/fbi.csv&quot; out=classdat.fbi DBMS = CSV REPLACE; GETNAMES = YES; GUESSINGROWS = 5000; RUN; PROC IMPORT datafile = &quot;data/fbiwide.csv&quot; out=classdat.fbiwide DBMS = CSV REPLACE; GETNAMES = YES; GUESSINGROWS = 2000; RUN; 9.3.1 Demonstration: Aesthetic Mappings and Basic Plots 9.3.1.1 Basic Scatter plots A basic scatter plot is a combination of an aesthetic mapping for x and y (position), combined with the specification that the geometric object to be displayed is a point. Without any of the components (x, y, geom_point) you don’t have a scatter plot. ggplot(fbiwide, aes(x = Burglary, y = Murder)) + geom_point() Base R plot(x = Burglary, y = Murder, data = fbiwide) # this doesn&#39;t work ## Error in plot(x = Burglary, y = Murder, data = fbiwide): object &#39;Burglary&#39; not found plot(x = fbiwide$Burglary, y = fbiwide$Murder) # you can use numeric vector arguments # another option is to use formula notation, that is, y ~ x. # If you use formula notation, you can pass in a data frame using the # data argument and R will interpret things correctly plot(Murder ~ Assault, data = fbiwide) SAS ODS Graphics The workhorse of the ODS graphics engine is SGPLOT. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; SCATTER x = Burglary y = Murder; RUN; QUIT; 9.3.1.2 Adding labels and titles We can add a title to the plot ggplot(fbiwide, aes(x = Murder, y = Assault)) + geom_point() + ggtitle(&quot;Murders and Assaults&quot;) You can also add labels using xlab() and ylab(). We’ll talk more about how to make fine-grained modifications of axis scales later, but the shorthand commands for changing the title and labels are convenient. Base R plot(Murder ~ Assault, data = fbiwide) title(&quot;Murders and Assaults&quot;) SAS ODS Graphics ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; TITLE &quot;Murders and Assaults&quot;; SCATTER x = Assault y = Murder; RUN; QUIT; 9.3.1.3 Changing characteristics of plotted objects We can also modify the appearance of the plotted objects ggplot(fbiwide, aes(x = Population, y = Assault)) + geom_point(color = &quot;blue&quot;, alpha = .5) Alpha blending, or transparency, allows us to see the structure in over-plotted (crowded) charts. Here, I’ve specified that the points should have 50% opacity. Another point to note: inside the aes() function, variables are mapped to geometric object characteristics, but outside, those same parameters can be mapped to constant values. If something is inside aes(), it should be a variable. Base R Base R doesn’t support alpha blending by default, so we have to load the scales package in order to get that functionality. library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor # Using constant alpha: plot(Assault ~ Population, col = alpha(&quot;blue&quot;, .5), pch = 16, data = fbiwide) SAS ODS Graphics ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled color=&quot;blue&quot;) transparency=0.5; RUN; QUIT; 9.3.1.4 Mapping categorical variables to aesthetics like color If we want to get fancy, we can map other variables to aesthetics like color, size, etc. We could explore the relationship between Population and Assault, but we’d expect that over time, observations from each state would be closely related. We could explore that by coloring each point by state (note, this doesn’t allow us to really see which states belong to which points so much as it visually associates connected points.) ggplot(fbiwide, aes(x = Population, y = Assault, color = Abb)) + geom_point() Base R variable mapping In base R, you can also create scatter plots that have different plot aesthetics, but we have to get a bit more hands-on than we did with ggplot2. # This doesn&#39;t work because the values in Abb aren&#39;t actually colors # and plot() doesn&#39;t handle the mapping between color and abbreviation for us plot(Assault ~ Population, col = Abb, data = fbiwide) ## Error in plot.xy(xy, type, ...): invalid color name &#39;AL&#39; We have to specify the values of color manually for each point that we want to plot - essentially, where ggplot2 handled the scale for us, now we have to manually specify it ourselves. We could specify a legend, but in this particular case it’s unlikely to be that useful to us because the colors aren’t allowing us to identify the lines, they’re just differentiating the lines from other lines. # We need a vector of colors equal to the number of abbreviations we have state_colors &lt;- rainbow(length(unique(fbiwide$Abb))) # then we have to figure out which color goes with which abbreviation fbiwide$Abb_factor &lt;- factor(fbiwide$Abb) fbiwide$state_color &lt;- state_colors[fbiwide$Abb_factor] plot(Assault ~ Population, col = state_color, data = fbiwide) You can use ?points to get information on all of the graphical parameters for points. SAS ODS Graphics variable mapping ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled) group = Abb; /* maps to point color by default */ RUN; QUIT; PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */ SCATTER x = Population y = Assault / markercharattrs=(size=8) markerchar = Abb /* specify marker character variable */ group = Abb ; RUN; QUIT; 9.3.1.5 Line plots A better way to graphically examine this hypothesis (the patterns in the data are clusters of each state’s points) might be to change the geometric object - we could use lines instead of points to show this data. In that case, we’d want to show separate lines (groups) for each state. ggplot(fbiwide, aes(x = Population, y = Assault, group = Abb)) + geom_line() Or, we could be lazy and leave color specified; which will have the same effect. Color and most other aesthetics implicitly group the data. ggplot(fbiwide, aes(x = Population, y = Assault, color = Abb)) + geom_line() One of the best parts of ggplot is that you can get away with only changing one or two things and end up with a totally different plot. Base R line plots If we want to make a line plot, we can do it one of two ways: we can use a for loop to plot each state’s line separately, subsetting the data each time, or, we can transform the data to wide format and plot that using the matplot function # For loop method plot(Assault ~ Population, pch = NA, data = fbiwide) # generate a blank plot for (i in unique(fbiwide$Abb)) { subdata &lt;- filter(fbiwide, Abb == i) lines(Assault ~ Population, data = subdata) # add the line to the plot } # ggplot2 code # ggplot(fbiwide, aes(x = Population, y = Assault, group = Abb)) + geom_line() # matplot method subdata &lt;- fbiwide %&gt;% select(Assault, Population, Year, Abb) # get cols we need # Matrix of x values - Population popdat &lt;- pivot_wider(subdata, id_cols = Year, names_from = Abb, values_from = Population) %&gt;% arrange(Year) %&gt;% select(-Year) # Matrix of y values - Assaults assaultdat &lt;- pivot_wider(subdata, id_cols = Year, names_from = Abb, values_from = Assault) %&gt;% arrange(Year) %&gt;% select(-Year) matplot(popdat, assaultdat, type = &quot;l&quot;) # by default, matplot uses 5 different colors and linetypes # so that you can differentiate the lines in different columns If we want to add color in and have it mean something specific, it’s relatively straightforward because we already defined a new column in fbiwide that has state colors. plot(Assault ~ Population, pch = NA, data = fbiwide) # generate a blank plot for (i in unique(fbiwide$Abb)) { subdata &lt;- filter(fbiwide, Abb == i) lines(Assault ~ Population, col = state_color, data = subdata) # add the line to the plot } SAS ODS line plots ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; SERIES x = Population y = Assault / lineattrs=(pattern=solid) group = Abb; /* maps to color by default */ RUN; QUIT; SAS Line attributes and patterns options 9.3.1.6 Mapping numeric variables to aesthetics We can color our points (or lines) by a categorical variable, but that’s not all! We can also map numeric variables to aesthetics! There are perhaps too many data points in the FBI data to effectively show size variation, so let’s temporarily switch to pokemon and examine the relationship between attack and special attack points. Let’s also map alpha to weight, so that lighter points correspond to lighter pokemon. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, alpha = weight_kg)) + geom_point() See more numeric scales We could get fancier still and map the Pokemon’s height to size. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, size = height_m, alpha = weight_kg)) + geom_point() Note that ggplot2 will create separate legends for size and height by default. If we map size and alpha to the same variable, however, it will automatically combine the legends. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, size = weight_kg, alpha = weight_kg)) + geom_point() ## Warning: Removed 1 rows containing missing values (geom_point). Base R legends, numeric variables, and aesthetics If we use ?points and find out that the size parameter is cex in base R, we might think something like this would work: plot(sp_attack ~ attack, cex = weight_kg, data = poke) # cex controls the point size argument # note the lack of a legend # Also, we have points that are *way* too big. But, remember, in base graphics, we have to do the transformations ourselves. After some tinkering, I came up with this. plot(sp_attack ~ attack, cex = 0.5 + weight_kg/1000*3, data = poke) # we have to do the scale transformation ourselves # but there&#39;s no legend... We also have to make the legend ourselves. Sigh. plot(sp_attack ~ attack, cex = 0.5 + weight_kg/1000*3, data = poke) legend(&quot;topleft&quot;, legend = c(250, 500, 750, 1000), # labels pch = 1, # must specify point shape (pch = point character) pt.cex = 0.5 + c(250, 500, 750, 1000)/1000 * 3) # must specify cex value # for points specifically If we want to map a value to alpha, we have to make our own scale again… but it may be easier to just make a transformation function that we can pass stuff into. That way we can create the legend and the data transformation using the same function (and if we want to change the transformation, it will be much easier to do so if we only have to change things in one place). library(scales) # Define the transformation wt_trans_alpha &lt;- function(x) { # rescale function is from the scales package trans_x &lt;- rescale( x = x, to = c(.2, 1), # don&#39;t go to 0 because then we can&#39;t see the points from = range(poke$weight_kg, na.rm = T) # input domain ) # alpha values have to be between 0 and 1, so we need to make sure to # truncate the scale trans_x %&gt;% pmin(., 1) %&gt;% # don&#39;t go any higher than 1 pmax(., 0) # don&#39;t go any lower than 0 } range(poke$weight_kg, na.rm = T) # get the range ## [1] 0.1 999.9 # Make the legend variables leg_label &lt;- seq(0, 1000, length.out = 5) # cut weight range into 5 leg_value &lt;- alpha(&quot;black&quot;, alpha = wt_trans_alpha(leg_label)) select(poke, attack, sp_attack, weight_kg) %&gt;% mutate(alpha = wt_trans_alpha(weight_kg), color = alpha(&quot;black&quot;, alpha = alpha)) %&gt;% plot(sp_attack ~ attack, col = color, pch = 16, data = .) legend(&quot;topleft&quot;, title = &quot;Weight (kg)&quot;, legend = leg_label, # labels pch = 16, # must specify point shape (pch = point character) col = leg_value) # must specify point color using alpha() function If we want to do color and size simultaneously, we have to define another transformation, but we can re-use the same legend. wt_trans_size &lt;- function(x) { trans_x &lt;- rescale( x = x, to = c(0.5, 3), from = range(poke$weight_kg, na.rm = T) # input domain ) # cex values have to be &gt;0, so we need to make sure to truncate the scale trans_x %&gt;% pmax(., 0) # don&#39;t go any lower than 0 } range(poke$weight_kg, na.rm = T) # get the range ## [1] 0.1 999.9 # Make the legend variables leg_label &lt;- seq(0, 1000, length.out = 5) # cut weight range into 5 leg_color &lt;- alpha(&quot;black&quot;, alpha = wt_trans_alpha(leg_label)) leg_size &lt;- wt_trans_size(leg_label) select(poke, attack, sp_attack, weight_kg) %&gt;% mutate(alpha = wt_trans_alpha(weight_kg), color = alpha(&quot;black&quot;, alpha = alpha), size = wt_trans_size(weight_kg)) %&gt;% plot(sp_attack ~ attack, col = color, cex = size, pch = 16, data = .) legend(&quot;topleft&quot;, title = &quot;Weight (kg)&quot;, legend = leg_label, # labels pch = 16, # must specify point shape (pch = point character) col = leg_color, pt.cex = leg_size) SAS ODS graphics and numeric variable transformations In order to change point size to show a different variable value, we have to use BUBBLE instead of SCATTER. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.poke; BUBBLE x = attack y = sp_attack size = weight_kg / BRADIUSMAX=10 BRADIUSMIN =2 transparency=0.5; RUN; QUIT; As best as I can tell, there’s no similar plot option that would let us map a variable to transparency. There are attribute maps, but the options are discrete (which weight is not) or range (which would let us define ranges to map to a single value), and they don’t appear to support transparency. In theory, I think we could go to SAS Template graph language, but … ugh. To be honest, at this point, I’d move the data into R and graph things there. 9.3.1.7 Try it out Explore the happy data set, which is a selection of variables from the general social survey (?happy) related to happiness. What associations can you find between the variables? Can you use what you know about the graphics help resources to figure out how to create new plot types? 9.3.2 Syntax and General Structure: Layers One of the main advantages of ggplot2 is that the syntax is basically consistent across very different types of plots. In base R and SAS, this is not the case - you have to look up the available options for each different plot type. In ggplot2, I might have to look up what the aesthetic names are for a specific geom, but I can guess most of the time. So let’s look a bit more into what ggplot2’s approach to graph specification is and what it allows us to do. You’re fairly used to the syntax of the pipe by now; but ggplot works on a slightly different (but similar) concept that we’ve used implicitly up until this point. There is the initial plot statement, ggplot(), and successive layers are added using +. You can specify a data set and aesthetic variables in the ggplot() statement (which is what we’ll usually do), but you can also have a completely blank ggplot() statement and specify your aesthetic mappings and data sets for each layer separately. This approach is more useful when you start creating complex plots, because you may need to plot summary information and raw data, or e.g. separate tables with city information, geographic boundaries, and rivers, all of which need to be represented in the same map. In this extended example, we’ll examine the different features we need to make a map and how to add new layers to a map. We’ll also look at some new geoms: geom_polygon and geom_path. pkgs &lt;- installed.packages() # Install any of the packages that you don&#39;t have install.packages(setdiff(c(&quot;ggthemes&quot;, &quot;mapdata&quot;, &quot;maps&quot;), pkgs)) 9.3.2.1 Initial data set up and exploration of the data sets Initial data set up and exploration of the data sets library(ggplot2) library(maps) library(mapdata) library(ggthemes) # theme_map # Create a data frame of the outline of the US us_map &lt;- map_data(&quot;usa&quot;) states &lt;- map_data(&quot;state&quot;) # Read in some data about places of worship in the US (compiled from IRS filings) worship &lt;- read_csv(&quot;data/Places_of_Worship.csv&quot;, guess_max = 5000) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## EIN = col_double(), ## NAME = col_character(), ## STREET = col_character(), ## CITY = col_character(), ## STATE = col_character(), ## ZIP = col_double(), ## AFFILIATION = col_double(), ## FOUNDATION = col_double(), ## long = col_double(), ## lat = col_double() ## ) Let’s look at the map data first: head(us_map) ## long lat group order region subregion ## 1 -101.4078 29.74224 1 1 main &lt;NA&gt; ## 2 -101.3906 29.74224 1 2 main &lt;NA&gt; ## 3 -101.3620 29.65056 1 3 main &lt;NA&gt; ## 4 -101.3505 29.63911 1 4 main &lt;NA&gt; ## 5 -101.3219 29.63338 1 5 main &lt;NA&gt; ## 6 -101.3047 29.64484 1 6 main &lt;NA&gt; It looks like we have longitude and latitude, group, and point order. What does group mean? select(us_map, group, region) %&gt;% unique() ## group region ## 1 1 main ## 6888 2 martha&#39;s vineyard ## 6925 3 nantucket island ## 6956 4 manhattan ## 6973 5 staten island ## 6984 6 long island ## 7153 7 san juan island ## 7171 8 lopez island ## 7189 9 orcas island ## 7209 10 whidbey island The regions are all islands, and group is a number that corresponds to each region. This is important because if we plot polygons, we have to worry about what to do when we jump from one “island” to another (sometimes, regions might be contiguous areas, such as states or zip codes). As long as we pass in a group argument (so group = group) we should avoid most of the complications of plotting spatial data. The state data is similarly structured: head(states) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; Now, let’s look at our worship place data. The full data set was more extensive (and much larger), so I’ve reduced the number of columns so that I’m able to put it on GitHub. head(worship) ## # A tibble: 6 x 10 ## EIN NAME STREET CITY STATE ZIP AFFILIATION FOUNDATION long lat ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.92e4 ST GEORG… 523 E … SOUT… MA 2127 9 10 -71.0 42.3 ## 2 6.36e5 MINISTER… 454 ES… LAWR… MA 1840 3 10 -71.2 42.7 ## 3 2.03e6 CHURCH O… 569 BR… NEWA… NJ 7104 3 10 -74.2 40.8 ## 4 2.05e6 GENERAL … 3210 S… ORCH… NY 14127 9 10 -78.7 42.8 ## 5 1.02e7 CHILD EV… 431 CA… LIVE… ME 4254 9 15 -70.1 44.4 ## 6 1.02e7 BIBLE SO… 105 HA… PORT… ME 4103 3 16 -70.3 43.7 We have tax numbers, entity names, mailing address, affiliation, foundation, and latitude/longitude. A codebook for the affiliation and foundation information is available from the IRS. While I’m tempted to play around with the names of the organizations a bit (for instance, how many different “First XXX Church” exist in the country?) it’s probably for the best that we keep moving. Basic map geoms - geom_path and geom_polygon Let’s try plotting this out. Maybe we can start with just plotting the outline using geom_line? ggplot() + geom_line(aes(x = long, y = lat, group = group), data = us_map) A line geom is plotted sequentially with respect to the magnitude of x. That doesn’t really work for us - we want x and y to be plotted in order of the observations. To do that, we need geom_path. ggplot() + geom_path(aes(x = long, y = lat, group = group), data = us_map) Not too bad. What if we want to fill in the country, so that it looks different from the background? To do that, we’ll need geom_polygon ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map) # Let&#39;s make the fill white. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map, fill = &quot;white&quot;, color = &quot;black&quot;) # What happens if we get rid of group? ggplot() + geom_polygon(aes(x = long, y = lat), data = us_map, fill = &quot;white&quot;, color = &quot;black&quot;) If we get rid of the group argument, then all of the little islands that we have in the dataset are connected to each other and to the mainland by lines that don’t really belong on our map. Generally, your base map layer will be plotted with geom_polygon, and you will use geom_path and/or geom_polygon to plot additional layers on top of the base map. You may also add points or other geoms on top of those layers, depending on your application. Let’s add more layers! Let’s plot points for the location of every place of worship (according to the IRS) in the US. First, we can plot the points by themselves: ggplot() + geom_point(aes(x = long, y = lat), data = worship) Wow. One problem we’re going to have is Alaska – it isn’t on our base map of the contiguous states. Let’s go ahead and remove those points for now - we could make the map more complicated, but it won’t add much to the overall value of the example. worship_full &lt;- worship worship &lt;- filter(worship, STATE != &quot;AK&quot;) ggplot() + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) At this point it’s worth remembering the population density issue. With that said, we have quite a lot of churches here, and while some of the darker areas are dense populations, not all of the church clusters are in areas I’d call “dense.” Suppose we want to add our church data to our national map. We can plot our US map first, then add the points on top of it. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) The order of the geom statements determines the plotting order. It might help to add some state information in too. We could plot states on top of the full US map, but (hopefully) the states tile the national map, so it’s better just to plot the states. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) It might be helpful to figure out where there are major cities - some of them are obvious, but I suspect there are certain areas where there are more churches per capita31. The us.cities database has all US cities over 40k people, plus state capitals of any size. Let’s add major US cities as open circles (so that we don’t obscure the church data too much). large_cities &lt;- us.cities %&gt;% filter(country.etc != &quot;AK&quot;) %&gt;% filter(country.etc != &quot;HI&quot;) %&gt;% filter(pop &gt; 100000 | capital == 2) %&gt;% mutate(type = ifelse(capital == 2, &quot;State Capital&quot;, &quot;City&quot;)) layer_map &lt;- ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) + geom_point(aes(x = long, y = lat, color = type, size = pop), data = large_cities, shape = 1) + # put legend below the map theme(legend.position = &quot;bottom&quot;) layer_map Just to examine the South in a little more depth, because there’s an extremely dark area northeast of Atlanta, GA and southwest of Charlotte, NC that doesn’t correspond to any major cities. layer_map + # coord_fixed maintains the proper aspect ratio coord_fixed(xlim = c(-86.5, -78.5), ylim = c(31, 36)) It looks like most of the dark area I noticed corresponds to sprawling Atlanta suburbs, which may not be big enough individually to meet our criteria of 100k people or more. There are also several smaller clusters of churches along what I suspect is the interstate between Atlanta and Charlotte, corresponding to the Greenville-Spartanburg metro (the metro is over 100k people, but individual cities don’t meet the threshold). We could add a layer corresponding to the interstates to provide more geographic context, but I think the point here is pretty clear - you can do some very cool things with maps. Try it out Pick a few denominations or religious organizations (or even just a more generic pattern, like names starting with ST, or schools) and do some rough string matching. What spatial patterns do you detect? (This won’t be totally accurate, and doesn’t need to be, in order to paint a picture of the country’s religious affiliations and congregations) One solution First, let’s do a bit of string processing. common_words &lt;- worship %&gt;% pull(NAME) %&gt;% str_split(&quot;[- &amp;/%]&quot;, simplify = T) %&gt;% as.character() %&gt;% table() %&gt;% sort(decreasing = TRUE) common_words[1:40] ## . ## CHURCH OF INC BAPTIST ## 3427562 119645 79566 59809 25940 ## MINISTRIES CHRISTIAN THE GOD LUTHERAN ## 24719 21974 21039 17550 13004 ## CENTER CHRIST FELLOWSHIP COMMUNITY NEW ## 12385 12172 11256 10922 10915 ## FIRST ASSEMBLY INTERNATIONAL ST LIFE ## 10106 9367 9150 8992 8923 ## FAITH DE IN BIBLE UNITED ## 7780 6513 6101 5915 5682 ## IGLESIA METHODIST MINISTRY GRACE MISSIONARY ## 5610 5440 5191 5167 5141 ## AND TEMPLE PRESBYTERIAN GOSPEL HOPE ## 4873 4635 4155 4144 3915 ## CHAPEL OUTREACH LIVING FOR EPISCOPAL ## 3909 3634 3607 3491 3464 Let’s work with the following denominations: BAPTIST, LUTHERAN, METHODIST, PRESBYTERIAN, EPISCOPAL - I’ve left out CHRISTIAN, ASSEMBLY (because it could be ASSEMBLY OF GOD or a more generic term for a congregation), and TEMPLE because it could refer to any number of religions - Jewish, Hindu, etc. This will give us a very rough categorization (e.g. there are Methodist Episcopal churches which will be counted as one and not the other, and UMC is a common abbreviation for United Methodist Church but won’t be counted as one this way). worship &lt;- worship %&gt;% mutate(type = str_extract(NAME, &quot;(BAPTIST|LUTHERAN|METHODIST|PRESBYTERIAN|EPISCOPAL)&quot;)) denom_churches = worship %&gt;% filter(!is.na(type)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat, color = type), data = denom_churches, size = .1, alpha = .1) + # We want to see the legend, so we have to tell ggplot to override the aesthetics guides(color = guide_legend(override.aes = list(size = 2, alpha = 1))) + theme(legend.position = &quot;bottom&quot;) + theme_map() We could also look for other keywords: bahai &lt;- worship %&gt;% filter(str_detect(NAME, &quot;BAHAI&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = bahai, alpha = .1) + theme_map() + ggtitle(&quot;Baha&#39;i Organizations&quot;) zen &lt;- worship %&gt;% filter(str_detect(NAME, &quot;ZEN&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = zen, alpha = .1) + theme_map() + ggtitle(&quot;Zen Organizations&quot;) friends &lt;- worship %&gt;% filter(str_detect(NAME, &quot;(MEETING OF FRIENDS)|(FRIENDS MEETING)&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = friends, alpha = .1) + theme_map() + ggtitle(&quot;Quaker Religious Organizations&quot;) Try it out – Middle Earth edition Dedicated fans have re-created middle earth in digital format using ArcGIS files. These map file formats, called shape files, can be read into R and plotted. You may need to install a few spatial packages first (Mac and Windows, Linux) The sf package in R contains a special geom, geom_sf, which will plot map objects with an appropriate geom, whether they are points, lines, or polygons. In complicated maps with many layers, this is a really awesome feature. I’ve provided some code to get you started, but there are many other shapefiles in the dataset. Pick some layers which you think are interesting, and plot them with appropriate geoms to make a map of Middle Earth. Unfortunately, in this map there is not an underlying polygon (the coastline is a series of shorter segments). To resolve this, I have provided a theme statement that will have a white background, so that you can add useful layers without the grey grid background. library(ggplot2) library(ggthemes) library(sf) ## Linking to GEOS 3.9.0, GDAL 3.2.1, PROJ 7.2.1 if (!file.exists(&quot;data/MiddleEarthMap.zip&quot;)) { download.file(&quot;https://github.com/jvangeld/ME-GIS/archive/master.zip&quot;, &quot;data/MiddleEarthMap.zip&quot;, mode = &quot;wb&quot;) } if (!dir.exists(&quot;data/ME-GIS-master/&quot;)) { unzip(&quot;data/MiddleEarthMap.zip&quot;, exdir = &quot;data/&quot;) } coastline &lt;- read_sf(&quot;data/ME-GIS-master/Coastline2.shp&quot;) cities &lt;- read_sf(&quot;data/ME-GIS-master/Cities.shp&quot;) forests &lt;- read_sf(&quot;data/ME-GIS-master/Forests.shp&quot;) lakes &lt;- read_sf(&quot;data/ME-GIS-master/Lakes.shp&quot;) rivers &lt;- read_sf(&quot;data/ME-GIS-master/Rivers.shp&quot;) roads &lt;- read_sf(&quot;data/ME-GIS-master/Roads.shp&quot;) ggplot() + geom_sf(data = coastline) + geom_sf(data = forests, color = NA, fill = &quot;darkgreen&quot;, alpha = .2) + geom_sf(data = rivers, color = &quot;blue&quot;, alpha = .1) + geom_sf(data = lakes, fill = &quot;blue&quot;, color = NA, alpha = .2) + theme_map() 9.3.3 Statistics and Different Types of Plots At this point, we’ve primarily looked at charts which have two continuous variables - scatter plots, and line plots. There are a number of situations where these types of charts are inadequate. For one thing, we might want to only look at the distribution of a single variable. Or, we might want to look at how a continuous response variable changes when the level of a categorical variable changes. In this section, we’ll hit the most common types of plots, but there are almost infinite variations. Sites like the Data Viz Catalogue can be useful if you’re trying to accomplish a specific task and want to know what type of plot to use. In all of the plots which we discuss in this section, there is an implicit statistical function applied to the data before plotting. So while you may specify e.g. x = var1, what is plotted is f(var1), where f() might be the mean/median/quartiles, a binned count, or a computed kernel density. In ggplot2, you can formally specify a statistic by using stat_xxx functions, but many geoms implicitly call these same functions. Box Plots A box plot can show some summary information about the distribution of a single continuous variable, and usually is used to show differences in the level of a response variable at different levels of a categorical variable. Let’s look at the relative frequency of different types of crimes, putting all states on an equal scale of 100k residents so that the numbers are comparable and also meaningful. Box plots in ggplot2 fbi %&gt;% filter(Year == max(Year)) %&gt;% mutate(per_100k = Count/Population*100000) %&gt;% # make nicer names mutate(crime = str_replace_all(Type, c(&quot;Murder.*&quot; = &quot;Murder&quot;, &quot;.*[Rr]ape&quot; = &quot;Rape&quot;, &quot;.*vehicle.theft&quot; = &quot;Vehicle Theft&quot;, &quot;Larceny.theft&quot; = &quot;Theft&quot;, &quot;.*assault&quot; = &quot;Assault&quot;))) %&gt;% ggplot(aes(x = crime, y = per_100k)) + geom_boxplot() + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) ## Warning: Removed 52 rows containing non-finite values (stat_boxplot). We might want to sort crimes by incidence level, because it makes the plot prettier and also gives us an ordered list of crimes in descending frequency. tmp &lt;- fbi %&gt;% filter(Year == max(Year)) %&gt;% mutate(per_100k = Count/Population*100000) %&gt;% # make nicer names mutate(crime = str_replace_all(as.character(Type), c(&quot;Murder.*&quot; = &quot;Murder&quot;, &quot;.*[Rr]ape&quot; = &quot;Rape&quot;, &quot;.*vehicle.theft&quot; = &quot;GTA&quot;, &quot;Larceny.theft&quot; = &quot;Theft&quot;, &quot;.*assault&quot; = &quot;Assault&quot;))) tmpsum &lt;- tmp %&gt;% group_by(crime) %&gt;% summarize(ref = mean(per_100k, na.rm = T)) %&gt;% ungroup() %&gt;% arrange(desc(ref)) tmp &lt;- tmp %&gt;% mutate(crime = factor(crime, levels = tmpsum$crime)) tmp %&gt;% ggplot(aes(x = crime, y = per_100k, color = Violent.crime)) + geom_boxplot() + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) ## Warning: Removed 52 rows containing non-finite values (stat_boxplot). Additional ggplot2 details - stat_fivenumber(), geoms, and statistics Box plots implicitly call stat_fivenumber(), which computes the five-number summary used to construct the box plot lines and box. A statistic is an aggregation function that exists between the data and the geom. Statistics output values which are then plotted directly by the geom. You could, in theory, create a box plot by calling something like this (I stopped at the boxes, but you get the idea) tibble(x = rep(1:2, each = 50), y = rnorm(100)) %&gt;% ggplot(aes(x = x, y = y, group = x)) + # the box - actually make 2 half-boxes geom_rect(aes(xmin = x - after_stat(width)/2, xmax = x + after_stat(width)/2, ymin = after_stat(lower), ymax = after_stat(middle)), stat = &#39;fivenumber&#39;, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_rect(aes(xmin = x - after_stat(width)/2, xmax = x + after_stat(width)/2, ymin = after_stat(middle), ymax = after_stat(upper)), stat = &#39;fivenumber&#39;, fill = &quot;white&quot;, color = &quot;black&quot;) Box plots in base R boxplot(per_100k~crime, data = tmp) Box plots in SAS ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; DATA WORK.fbitmp; SET classdat.fbi; per_100k = Count/Population*100000; RUN; PROC SGPLOT data=fbitmp; hbox per_100k / group = Type; RUN; QUIT; There are other variants of box plots (and similar concepts). Here is a violin plot, which attempts to show some more information about the distribution of the continuous variable (instead of hiding it all in the summary statistics). tmp %&gt;% ggplot(aes(x = crime, y = per_100k, color = Violent.crime)) + geom_violin(scale = &quot;width&quot;, # This ensures all plots have the same max width # Otherwise, theft is too skinny to really see draw_quantiles = c(.25, .5, .75)) + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) ## Warning: Removed 52 rows containing non-finite values (stat_ydensity). ## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): ## collapsing to unique &#39;x&#39; values Histograms and Density Plots Box plots aren’t the only way to show distributions, though. If we want to, we can show distributions using histograms or density plots. A histogram is created by binning the variable, then counting the number of observations that fall within each specified range. Usually, these ranges have constant width (but not always). Histograms in ggplot2 poke %&gt;% ggplot(aes(x = hp)) + geom_histogram(color = &quot;black&quot;, fill = &quot;grey&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. poke %&gt;% ggplot(aes(x = hp)) + geom_histogram(color = &quot;black&quot;, fill = &quot;grey&quot;, binwidth = 20) You should always look at a histogram under multiple bin widths, because that parameter can change the visual appearance of the distribution quite a bit. Histograms and stat_bin in ggplot2 In a histogram, the statistic that is computed is stat_bin - we break the x range up into intervals and then count how many points lie in each interval. tibble(x = rnorm(100)) %&gt;% ggplot(aes(x = x)) + stat_bin(geom = &quot;bar&quot;) # this is equivalent to geom_histogram ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In base R, you can get histograms with the hist() function. Histograms in SAS ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; DATA WORK.fbitmp; SET classdat.fbi; per_100k = Count/Population*100000; RUN; PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); HISTOGRAM per_100k; RUN; QUIT; A kernel density is an empirical method of estimating the probability density function of a variable. In a density plot, instead of count, the y axis is labeled “density” and the continuous curve will be scaled so that the distribution integrates to 1. Density plots can be quite useful for continuous variables, but are also dependent on parameter selection - instead of a bin width, you may need to adjust a kernel width (though there are algorithms that will select this by default, so usually you only have to tweak it a bit). ggplot2 density plot poke %&gt;% ggplot(aes(x = hp)) + geom_density(color = &quot;black&quot;, fill = &quot;grey&quot;) If you try to create a density plot for a variable which has only a limited number of values, you’ll often get an extremely spiky distribution. If your variable is e.g. measured in imprecise intervals, even though it’s continuous, you can adjust the bandwidth, but you may be better off using a histogram and choosing your bin width to be a multiple of your measurement precision. A spiky density curve can alert you to other problems in your data - for instance, the presence of a large number of 0s might tell you a mixture distribution would be a more appropriate distribution when you’re modeling the data. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_density(color = &quot;black&quot;, fill = &quot;grey&quot;) ## Warning: Removed 104 rows containing non-finite values (stat_density). In base R, you can get a density plot of a single variable with plot(density(...)). Density plots in SAS ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; DATA WORK.fbitmp; SET classdat.fbi; per_100k = Count/Population*100000; RUN; PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); density per_100k / type = kernel; RUN; QUIT; Try It Out Read the help files for stat_density and stat_histogram and attempt to create your own histogram/density combination, before looking at the solution below If you’re careful to specify your mappings properly, you can plot a density curve on top of a histogram. Since both geoms compute statistics, you have to read the help files to see that they actually are computing multiple statistics. geom_histogram lists computed variables count, density, ncount, and ndensity. It will be easiest just to use the computed variable density, which we access by putting y = ..density.. in the aesthetic statement for the histogram geom. This moves the histogram to the same scale as the density - both will be scaled to integrate/sum to 1. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_histogram(aes(y = ..density..)) + geom_density(color = &quot;black&quot;, fill = &quot;white&quot;, alpha = .25) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 104 rows containing non-finite values (stat_bin). ## Warning: Removed 104 rows containing non-finite values (stat_density). We could similarly tell the density function we want a plot on the count scale, but this is harder to read and compare. Personally, I prefer the first version. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_histogram() + geom_density(aes(y = ..count..), color = &quot;black&quot;, fill = &quot;white&quot;, alpha = .25) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 104 rows containing non-finite values (stat_bin). ## Warning: Removed 104 rows containing non-finite values (stat_density). In SAS, this is actually easier than it is in ggplot2: As long as you put the histogram first in the SGPLOT statement, you’ll get the right scale. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; DATA WORK.fbitmp; SET classdat.fbi; per_100k = Count/Population*100000; RUN; PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); histogram per_100k; density per_100k / type = kernel; RUN; QUIT; Higher Dimensional Histograms and Density Plots You can use density plots and histograms in two dimensions to show the bivariate relationship between two variables, but at that point we have to represent the height of the density or number of points in the bin using another aesthetic (as x and y are taken by the two variables you’re computing a density for). One common aesthetic choice is to map density (or counts) to the fill color, leaving you to imagine a “height” associated with the color. poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_bin2d(aes(fill = ..count..), bins = 18, drop = F) + scale_x_log10() + scale_y_log10() ## Warning: Removed 1 rows containing non-finite values (stat_bin2d). poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_hex(aes(fill = ..count..), drop = F) + scale_x_log10() + scale_y_log10() ## Warning: Ignoring unknown parameters: drop ## Warning: Removed 1 rows containing non-finite values (stat_binhex). poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_density2d_filled() + scale_x_log10() + scale_y_log10() ## Warning: Removed 1 rows containing non-finite values (stat_density2d_filled). Alternately, it may be preferable to plot only the contour outlines. poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_density2d(bins = 15) + scale_x_log10() + scale_y_log10() ## Warning: Removed 1 rows containing non-finite values (stat_density2d). I found a blog post describing how to compute two-dimensional bins and make a heatmap bin plot in SAS. Of course, joint distributions can also be shown using a scatterplot. Another useful geom when examining distributions is geom_rug, which shows univariate distributions in the margins of the plot. Bar Charts A bar chart is a plot with a categorical variable on one axis and a summary statistic on the other (usually, this is a count). Note that a bar chart is NOT the same as a histogram (a histogram looks very similar, but has a binned numeric variable on one axis and counts on the other). Geometrically, bar charts are rectangles; typically each rectangle will have equal width and variable height. Bar Charts in ggplot2 ggplot(poke, aes(x = generation)) + geom_bar() As with other types of charts, we can add more information by adding aesthetic mappings: poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% ggplot(aes(x = generation, fill = status)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) We can get different visual information from the same data, if we switch which variable is on the axis and which is the fill variable. poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% ggplot(aes(x = status, fill = generation)) + geom_bar(position = &quot;dodge&quot;) We can also get useful information by changing the statistic: what if we use the proportion of responses in each category instead of the raw count? To do this, as far as I know, we have to do at least a little bit of pre-processing, because we need a nested order of grouping factors, and ggplot doesn’t do that very well. poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% group_by(status, generation) %&gt;% count() %&gt;% group_by(status) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes(x = status, fill = generation, weight = prop)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% group_by(generation, status) %&gt;% count() %&gt;% group_by(generation) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes(x = generation, fill = status, weight = prop)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) Bar Charts in SAS ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.poke; VBAR status; RUN; QUIT; PROC SGPLOT data=classdat.poke; HBAR type_1; RUN; QUIT; Other useful geoms There are a few other geoms which may be useful from time to time, especially when adding additional information to a plot. geom_hline draws horizontal lines geom_vline draws vertical lines geom_abline draws a line in slope-intercept form geom_function draws a function over the domain of the plot geom_smooth draws a smooth line over e.g. a scatterplot. You can use this to fit an implicit linear regression, but by default it uses loess or generalized additive models to produce a more flexible fit. 9.3.4 Small Multiples Sometimes, you want to show separate plots for each level of a factor (or a combination of factors). Often, this happens when you expect the gross shape of the plotted information to change - something relatively obvious. Other times, though, you might just have too much data and need to break it down somehow. These sub-plots within a plot are called small multiples, or facets, or panel plots, depending on the software or discipline. In ggplot2 fbi %&gt;% filter(Abb == &quot;NE&quot;) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_wrap(~Type) ## Warning: Removed 54 rows containing missing values (position_stack). The options to make panel plots in ggplot2 are facet_wrap, which wraps the specified panels into an overall plot with a reasonable aspect ratio, and facet_grid, which creates a grid of panels. We can control the scales on the panels - in some cases, it is advantageous to have the same scale for all panels, while at other times, such as in the example above, varying the scales at the panel level may allow you to see more detail. fbi %&gt;% filter(Abb == &quot;NE&quot;) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_wrap(~Type, scales = &quot;free_y&quot;) ## Warning: Removed 54 rows containing missing values (position_stack). Using a grid-style facet, we can make it easy to compare the relative trends in sub-populations. fbi %&gt;% filter(Abb %in% c(&quot;D.C.&quot;, &quot;NY&quot;), Type %in% c(&quot;Burglary&quot;, &quot;Robbery&quot;)) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_grid(Abb~Type) Panel plots in SAS Basically the same syntax as SGPLOT, but using SGPANEL and a PANELBY statement. This is very similar to ggplot2’s facet_ addition. ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */ libname classdat &quot;sas/&quot;; PROC SGPANEL data=classdat.poke; PANELBY status; HBAR type_1; RUN; QUIT; Try it out The classdata package contains the box dataset, which has weekly box office numbers. Conduct an exploratory data analysis of the dataset, using different types of plots appropriately. What is the most interesting aspect of the data you found in your exploration? One possible solution library(classdata) library(lubridate) # Work with dates and times ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union data(box) head(box) ## Rank Rank.Last.Week Movie Distributor Gross Change ## 1 1 1 Joker Warner Bros. 55861403 -42 ## 2 2 NA The Addams Family United Artists 30300007 NA ## 3 3 NA Gemini Man Paramount Pictures 20552372 NA ## 4 4 2 Abominable Universal 6072235 -49 ## 5 5 3 Downton Abbey Focus Features 4881075 -39 ## 6 6 4 Hustlers STX Entertainment 3887018 -39 ## Thtrs. Per.Thtr. Total.Gross Week Date ## 1 4374 12771 193590190 2 2019-10-11 ## 2 4007 7562 30300007 1 2019-10-11 ## 3 3642 5643 20552372 1 2019-10-11 ## 4 3496 1737 47873585 3 2019-10-11 ## 5 3019 1617 82668665 4 2019-10-11 ## 6 2357 1649 98052357 5 2019-10-11 ggplot(box, aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .1) # Ok, that&#39;s weird. Some movie was in theaters for 100,000 weeks? Or re-released? filter(box, Week &gt; 100) %&gt;% group_by(Movie) %&gt;% summarize(max_week = max(Week)) %&gt;% arrange(desc(max_week)) ## # A tibble: 120 x 2 ## Movie max_week ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ghanchakkar 105065 ## 2 The Captive 5186 ## 3 The Wizard of Oz 3869 ## 4 Detour (1945) (Re-Release) 3812 ## 5 Le Corbeau (1943) (Re-Release) 3681 ## 6 The Fallen Idol 3497 ## 7 Monsieur Verdoux 3456 ## 8 The Third Man 3434 ## 9 Olivia 3415 ## 10 Little Fugitive 3138 ## # … with 110 more rows box %&gt;% filter(Week &lt; 100) %&gt;% ggplot(aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .01) + scale_y_log10() ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 2 row(s) containing missing values (geom_path). To me, this looks like there are two main groups of movies - the less common movies that are hugely grossing initially, but decline relatively quickly in weekly gross, and the more common movies that show a gradual decline until they are removed from theaters. Most movies don’t seem to go beyond 25 weeks. # Still kind of a mess. Let&#39;s facet by year of initial release movie_summary &lt;- box %&gt;% group_by(Movie) %&gt;% summarize(release_year = min(year(Date)), weeks = max(Week), total.gross = Total.Gross[Week == max(Week)], .groups = &quot;drop_last&quot;) box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Week &lt; 25) %&gt;% ggplot(aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .05) + facet_wrap(~release_year) + scale_y_log10() ## Joining, by = &quot;Movie&quot; ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 2 row(s) containing missing values (geom_path). This pattern is relatively consistent year after year, but some years have many more high-grossing movies. Let’s look at the relationship between last week’s rank and this week’s rank. box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Week &lt; 25, Week &gt; 1) %&gt;% ggplot(aes(x = Rank.Last.Week, y = Rank, group = Movie)) + geom_point(alpha = .05) + scale_y_reverse() + facet_wrap(~Week) ## Joining, by = &quot;Movie&quot; ## Warning: Removed 1979 rows containing missing values (geom_point). It’s very strong, as expected, but this is extremely overplotted, so we might be missing some details. Let’s look at the 2d density to see if we get anything interesting. box %&gt;% filter(Week &lt; 15, Week &gt; 1) %&gt;% ggplot(aes(x = Rank.Last.Week, y = Rank)) + geom_density2d_filled() + scale_y_reverse() + facet_wrap(~Week) ## Warning: Removed 1586 rows containing non-finite values (stat_density2d_filled). As we go week by week, we see the mode of the distribution shift down in rank. By week 9, it’s extremely uncommon for a movie to be in the top 10. Next, lets look at the relationship between the number of theaters showing a movie and the weekly gross receipts. box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Rank &lt;= 25) %&gt;% ggplot(aes(x = Thtrs., y = Gross)) + geom_density2d_filled() + scale_y_log10() ## Joining, by = &quot;Movie&quot; It would be interesting to see if this relationship is the same for all distributors. distributors &lt;- box %&gt;% group_by(Distributor, Movie) %&gt;% filter(Week == max(Week)) %&gt;% group_by(Distributor) %&gt;% summarize(n = n(), median.gross = median(Total.Gross)) %&gt;% arrange(desc(median.gross)) head(distributors, 10) ## # A tibble: 10 x 3 ## Distributor n median.gross ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Walt Disney 76 185808558. ## 2 Columbia 1 169077585 ## 3 Universal 118 66197380 ## 4 20th Century Fox 105 65007045 ## 5 Warner Bros. 150 53930748. ## 6 Paramount Pictures 81 52822418 ## 7 Sony Pictures 130 37709109 ## 8 MGM 4 34693428 ## 9 Focus / Gramercy 1 26583369 ## 10 IMAX Films 4 26349032 distributors %&gt;% filter(n &gt;= 25) %&gt;% # want distributors with at least 25 movies slice(1:10) %&gt;% # keep top 10 rows left_join(box) %&gt;% mutate(Distributor = factor(Distributor) %&gt;% reorder(-median.gross)) %&gt;% filter(Week &lt;= 10) %&gt;% # Only look at the first 10 weeks of a movie ggplot(aes(x = Thtrs., y = Gross)) + geom_density2d_filled(contour_var = &quot;ndensity&quot;) + scale_y_log10() + facet_wrap(~Distributor) ## Joining, by = &quot;Distributor&quot; It’s rather obvious that Disney is the real outlier here - it has movies that are extremely widely released and very high grossing, and a limited-release movie is a relatively rare thing. Compare this to e.g. Lionsgate, where most movies are limited to a few theaters and it’s relatively rare to have a widely released movie. 9.4 Good charts Earlier, I mentioned that we’ve known pie charts suck for 100 years. But what makes a chart good? And how do we tell? This is my area of research, so I’m going to try to keep this to a broad overview, but I may not succeed. Sorry in advance. A chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate. Unfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms. 9.4.1 Perceptual and Cognitive Factors 9.4.1.1 Color Our eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum. Sensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons) Not everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances. You can take a test designed to screen for colorblindness here Your monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here. In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown. It is possible to simulate the effect of color blindness and color deficiency on an image. In addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context. Figure 9.1: The color constancy illusion. The squares marked A and B are actually the same color Figure 9.2: The color constancy illusion. The squares marked A and B are actually the same color Our brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade. Implications and Guidelines Do not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance. Avoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people. To “colorblind-proof” a graphic, you can use a couple of strategies: double encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out If you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it! If you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use. If you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly. Be conscious of what certain colors “mean” Leveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -&gt; blue gradients for showing rainfall totals) Some colors can can provoke emotional responses that may not be desirable.32 It is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women33. There are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes. 9.4.1.2 Short Term Memory We have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time. Without rehearsing information, short term memory lasts a few seconds. Try it out Click here, read the information, and then click to hide it. 1 4 2 2 3 9 8 0 7 8 Wait a few seconds, then expand this section What was the third number? Without rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information. In charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart. Where possible, try to keep your legends to 6 or 7 characteristics. Implications and Guidelines Limit the number of categories in your legends to minimize the short term memory demands on your reader. When using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude. Use colors and symbols which have implicit meaning to minimize the need to refer to the legend. Add annotations on the plot, where possible, to reduce the need to re-read captions. 9.4.1.3 Grouping and Sense-making Imposing order on visual chaos. What does the figure below look like to you? When faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options. Consider this image - what do you see? Did you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that?” In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience. Now, look at the logo for the Pittsburgh Zoo. Do you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background. The ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs. The catchphrase of Gestalt psychology is The whole is greater than the sum of the parts That is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene. The Gestalt Heuristics help us to impose order on ambiguous visual stimuli You can read about the gestalt rules here, but they are also demonstrated in the figure above. In graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders. Example: Suppose I want to emphasize the change in the murder rate between 1980 and 2010. I could use a bar chart fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) + geom_col(position = &quot;dodge&quot;) + coord_flip() + ylab(&quot;Murders per 100,000 residents&quot;) Or, I could use a line chart fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) + geom_line() + ylab(&quot;Murders per 100,000 residents&quot;) Or, I could use a box plot fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = factor(Year), y = Murder/Population*100000)) + geom_boxplot() + ylab(&quot;Murders per 100,000 residents&quot;) Which one best demonstrates that in every state and region, the murder rate decreeased? The line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships. The aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.34 9.4.2 General guidelines for accuracy There are certain tasks which are easier for us relative to other, similar tasks. Figure 9.3: Which of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels. When making judgements corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.35 Position (common scale) Position (non-aligned scale) Length, Direction, Angle, Slope Area Volume, Density, Curvature Shading, Color Saturation, Color Hue If we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts. When creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc. There is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area. . Ted ED: How to spot a misleading graph - Lea Gaslowitz Business Insider: The Worst Graphs Ever Extra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration). 9.4.3 How do we know? We do experiments on people (evil cackle)! No, seriously, we do. See this paper for a review of graphical testing in statistics, and this paper for one example of how we test competing designs to figure out that polar coordinates make everything harder. If you’re still curious after reading those, set up an appointment and let’s talk!. 9.5 Other things worth exploring (and future expansions for this chapter) The patchwork package lets you arrange ggplots (image by Allison Horst) (so do the gridExtra and cowplot packages) The rayshader package lets you create 3d maps and graphs (image by Allison Horst) People have made some very cool visualizations using rayrender: bending space time volcano topology (with map overlay) A super suspenseful animation whose only purpose is demonstrating solid object rendering 3D maps (which you can create STL files from and 3D print…) Rendering a table, on a laptop, on a table References 9.5.1 Motivation If you think this all sounds complicated, read this blog post about saving graphics from Excel. 9.5.2 R graphics ggplot2 cheat sheet ggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet ggplot2 reference guide R graph cookbook Data Visualization in R (@ramnathv) Base R plots - Notes by Dr. Bilder Combine multiple plots with the cowplot and gridExtra packages 9.5.3 SAS graphics SGPLOT cheat sheet Replicating ggplot2 in SAS SGPLOT ODS Graphics in SAS Periodic Table of SAS ODS graphics (code and actual graphic) Holiday Graphics in SAS, and, on that same theme, Captain America in SAS 9.5.4 Types of Charts and Chart Styling The Data Vis Project Data Visualization Catalogue The pros and cons of chart taxonomies Data Visualization Style Guidelines 9.5.5 Other Graphics packages plotnine in python (ggplot2 clone) matplotlib in python Tableau - student licenses are free d3 - javascript graphics I’ll fully admit my bias here - I think ODS graphics are better than the default SAS graphics, but I still prefer the syntax and logic behind ggplot2. But, if you prefer SAS Graphics, you do you. Better you than me, is all I’m saying.↩︎ It’s not often you’ll find me approving of SAS graphics, but making it hard to make pie charts is definitely a point in SAS’s favor↩︎ This can sometimes be a pain, though, depending on the set up, because you may end up with labels that are repeated many, many times. As with any system, you just have to make sure you’re formatting your data consistent with the underlying philosophy.↩︎ I’ve driven through enough small towns in the South, and I have relatives in NW Iowa. There are a LOT of churches in those places↩︎ When the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.↩︎ Lisa Charlotte Rost. What to consider when choosing colors for data visualization.↩︎ See this paper for more details. This is the last chapter of my dissertation, for what it’s worth. It was a lot of fun. (no sarcasm, seriously, it was fun!)↩︎ See this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.↩︎ "],["simulation.html", "Module 10 Simulation and Reproducibility Simulation: Module Objectives 10.1 Pseudorandom Number Generation 10.2 Built-in simulations from distributions 10.3 Simulation to test model assumptions 10.4 Monte Carlo methods 10.5 References", " Module 10 Simulation and Reproducibility Simulation: Module Objectives Understand how pseudorandom number generation works and necessary conditions for reproducibility Be able to implement a simulation for a specific task, process, or model Simulation is an extremely important part of computational statistics. Bayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models. In this module, we’re going to touch on a few foundational pieces of simulation in computing, and you will get more exposure to simulation-based methods in other courses down the line. 10.1 Pseudorandom Number Generation Computers are almost entirely deterministic, which makes it very difficult to come up with “random” numbers. In addition to the deterministic nature of computing, it’s also somewhat important to be able to run the same code and get the same results every time, which isn’t possible if you rely on truly random numbers. Historically, pseudorandom numbers were generated using linear congruential generators (LCGs). These algorithms aren’t typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem “random” but are actually deterministic. LCGs use modular arithmetic: \\(X_{n+1} = (aX_n + c) \\mod m\\) where \\(X_0\\) is the start value (the seed), \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. When using a LCG, the user generally specifies only the seed. LCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia) The important thing to note here is that if you specify the same generator values (a, c, m, and \\(X_0\\)), you will always get the same series of numbers. Since a, c, m are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time. It is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.36 Once you set your seed, the remaining results will only be reproducible if you generate the same set of random numbers every time. set.seed(342512) # Get 10 numbers after the seed is set sample(1:100, 10) ## [1] 65 51 64 21 45 53 3 6 43 8 # Compute something else that depends on randomness mean(rnorm(50)) ## [1] -0.1095366 # Get 10 more numbers sample(1:100, 10) ## [1] 4 57 69 10 76 15 67 1 3 91 Compare the results above to these results: set.seed(342512) # Get 10 numbers after the seed is set sample(1:100, 10) ## [1] 65 51 64 21 45 53 3 6 43 8 # Compute something else that depends on randomness mean(rnorm(30)) ## [1] -0.1936645 # Get 10 more numbers sample(1:100, 10) ## [1] 49 37 6 34 9 3 100 43 7 29 Notice how the results have changed? To make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I’ve already set the seed earlier. This introduces certain “fixed points” where results won’t change immediately after I’ve re-set the seed. This is particularly important when I’m generating bootstrap estimates, fitting models, or simulating data for graphics experiments. Pick your seed in any way you want. I tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people’s birthdays, the current time in hhmmss format… basically, you can use anything. 10.2 Built-in simulations from distributions Often, we can get away with just simulating data from a known distribution. As both R and SAS are meant for statistical computing, this is extremely easy by design. In R You can see the various distribution options using ?Distributions. In general, dxxx is the PDF/PMF, pxxx is the CDF, qxxx is the quantile function, and rxxx gives you random nubmers generated from the distribution. (xxx, obviously, is whatever distribution you’re looking to use.) library(tibble) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(tidyr) ## ## Attaching package: &#39;tidyr&#39; ## The following object is masked from &#39;package:magrittr&#39;: ## ## extract library(ggplot2) set.seed(109025879) tibble( norm = rnorm(500), gamma = rgamma(500, shape = 3, scale = 1), exp = rexp(500, rate = 1), # R uses a exp(-ax) t = rt(500, df = 5), chisq = rchisq(500, 5) ) %&gt;% pivot_longer(1:5, names_to = &quot;dist&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = &quot;free&quot;, nrow = 1) In SAS You can see the various distribution options in the RAND documentation. %let N=500; /* size of sample */ DATA sample; call streaminit(12532); DO i = 1 to &amp;N; /* &amp;N is the value of the macro variable defined above */ id = i; norm = rand(&quot;Normal&quot;, 0, 1); gamma = rand(&quot;Gamma&quot;, 3, 1); exp = rand(&quot;Exponential&quot;, 1); /* SAS uses 1/a exp(-x/a) */ t = rand(&quot;T&quot;, 5); chisq = rand(&quot;Chisq&quot;, 5); OUTPUT; END; RUN; PROC TRANSPOSE data=sample out=longsample (rename=(COL1 = value)) /* rename output variable (&#39;values_to&#39;) */ NAME = dist /* where the column names go (&#39;names_to&#39;) */ ; BY id; VAR norm gamma exp t chisq; RUN; PROC SGPANEL data=longsample; PANELBY dist / COLUMNS = 5 UNISCALE = ROW NOVARNAME; DENSITY value / TYPE = KERNEL; RUN; Try it out Generate variables x and y, where x is a sequence from -10 to 10 and y is equal to \\(x + \\epsilon\\), \\(\\epsilon \\sim N(0, 1)\\). Fit a linear regression to your simulated data (in R, lm, in SAS, PROC REG). In R set.seed(20572983) data &lt;- tibble(x = seq(-10, 10, .1), y = x + rnorm(length(x))) regression &lt;- lm(y ~ x, data = data) summary(regression) ## ## Call: ## lm(formula = y ~ x, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.14575 -0.70986 0.03186 0.65429 2.40305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.01876 0.06869 -0.273 0.785 ## x 0.99230 0.01184 83.823 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9738 on 199 degrees of freedom ## Multiple R-squared: 0.9725, Adjusted R-squared: 0.9723 ## F-statistic: 7026 on 1 and 199 DF, p-value: &lt; 2.2e-16 In SAS DATA tmp; call streaminit(20572983); DO i = -10 to 10 by .1; x = i; y = x + rand(&quot;Normal&quot;); OUTPUT; END; RUN; PROC REG data = tmp; MODEL y = x; RUN; Model: MODEL1 Dependent Variable: y Number of Observations Read 201 Number of Observations Used 201 Analysis of Variance Source DF Sum ofSquares MeanSquare F Value Pr &gt; F Model 1 6638.19224 6638.19224 6592.27 &lt;.0001 Error 199 200.38634 1.00697 Corrected Total 200 6838.57858 Root MSE 1.00348 R-Square 0.9707 Dependent Mean -0.09630 Adj R-Sq 0.9706 Coeff Var -1042.03435 Parameter Estimates Variable DF ParameterEstimate StandardError t Value Pr &gt; |t| Intercept 1 -0.09630 0.07078 -1.36 0.1752 x 1 0.99044 0.01220 81.19 &lt;.0001 Model: MODEL1 Dependent Variable: y 10.3 Simulation to test model assumptions One of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model. Suppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors. How bad will your results be? Where can you trust the results, and where should you be cautious? Example: Confidence Interval coverage rates Suppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our data. set.seed(40295023) sim &lt;- tibble( id = rep(1:100, each = 25), # generate 100 samples of 25 points each ln_x = rnorm(25*100), # generate the normal deviates x = exp(ln_x), # transform into lognormal deviates ) %&gt;% # this creates a 100-row data frame, with one row for each id. # the columns x, ln_x are stored in the data list-column as a tibble. nest(data = c(x, ln_x)) head(sim) ## # A tibble: 6 x 2 ## id data ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;tibble [25 × 2]&gt; ## 2 2 &lt;tibble [25 × 2]&gt; ## 3 3 &lt;tibble [25 × 2]&gt; ## 4 4 &lt;tibble [25 × 2]&gt; ## 5 5 &lt;tibble [25 × 2]&gt; ## 6 6 &lt;tibble [25 × 2]&gt; sim$data[[1]] ## # A tibble: 25 x 2 ## x ln_x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.310 -1.17 ## 2 0.622 -0.475 ## 3 0.303 -1.19 ## 4 1.05 0.0525 ## 5 0.529 -0.636 ## 6 1.09 0.0891 ## 7 1.97 0.676 ## 8 8.94 2.19 ## 9 0.598 -0.514 ## 10 0.183 -1.70 ## # … with 15 more rows You want to assess the coverage probability of a confidence interval computed under two different modeling scenarios: Working with the log-transformed values, ln(x), and then transform the computed interval back Working with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal. Under scenario 1, our theoretical interval should be exp((-1.96/5, 1.96/5)) (because \\(\\mu\\) is 0, and \\(\\sigma\\) is 1, so \\(SE(\\overline x) = 1/\\sqrt{25} = 1/5\\)). \\((0.6757041,1.4799377)\\) Under scenario 2, the expected value of the lognormal distribution is \\(\\exp(1/2) = 1.6487213\\), the variance is \\((\\exp(1) - 1)(\\exp(1)) = 4.6707743\\) and our theoretical interval should be \\((0.8015319, 2.4959107)\\). This interval contains 0, which is implausible for lognormally distributed data. Our expected values are different under scenario 1 and scenario 2: in scenario 1 we are computing an interval for \\(\\mu\\), in scenario 2, we are computing an interval for the population mean, which is \\(\\exp(\\mu + .5\\sigma^2)\\). Both are valid quantities we might be interested in, but they do not mean the same thing. The purrr::map notation specifies that we’re using the map function from the purrr package. When functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function. purrr::map takes an argument and for each “group” calls the compute_interval function, storing the results in res. So each row in res is a 1x2 tibble with columns lb and ub. This pattern is very useful in all sorts of applications. I wish we had time to cover purrr explicitly, but I at least want to expose you to how clean it makes your code. compute_interval &lt;- function(x) { s1 &lt;- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x))) s2 &lt;- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x)) tibble(scenario = c(&quot;scenario_1&quot;, &quot;scenario_2&quot;), mean = c(1, exp(1/2)), lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]), in_interval = (lb &lt; mean) &amp; (ub &gt; mean)) } sim_long &lt;- sim %&gt;% # This line takes each data entry and computes an interval for x. # .$x is code for take the argument you passed in to map and get the x column mutate(res = purrr::map(data, ~compute_interval(.$x))) %&gt;% # this &quot;frees&quot; res and we end up with two columns: lb and ub, for each scenario unnest(res) ci_df &lt;- tibble(scenario = c(&quot;scenario_1&quot;, &quot;scenario_2&quot;), mu = c(1, exp(1/2)), lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5), ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5)) ggplot() + geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), data = ci_df, fill = &quot;grey&quot;, alpha = .5, color = NA) + geom_vline(aes(xintercept = mu), data = ci_df) + geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval), data = sim_long) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme_bw() + facet_wrap(~scenario) From this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain. Example: Regression and high-leverage points What happens if we have one high-leverage point (e.g. a point which is an outlier in both x and y)? How pathological do our regression coefficient estimates get? The challenging part here is to design a data generating mechanism. gen_data &lt;- function(n = 30, o = 1, error_sd = 2) { # generate the main part of the regression data data &lt;- tibble(x = rnorm(n = n - o, mean = seq(-10, 10, length.out = n - o), sd = .1), y = x + rnorm(length(x), mean = 0, sd = error_sd)) # generate the outlier - make it at ~(-10, 5) outdata &lt;- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd)) bind_rows(data, outdata) } sim_data &lt;- tibble( id = 1:300, o = rep(0:2, each = 100), # call gen_data for each row in sim_data, but don&#39;t really use id as a parameter. data = purrr::map(o, ~gen_data(o = .)) ) head(sim_data) ## # A tibble: 6 x 3 ## id o data ## &lt;int&gt; &lt;int&gt; &lt;list&gt; ## 1 1 0 &lt;tibble [30 × 2]&gt; ## 2 2 0 &lt;tibble [30 × 2]&gt; ## 3 3 0 &lt;tibble [30 × 2]&gt; ## 4 4 0 &lt;tibble [30 × 2]&gt; ## 5 5 0 &lt;tibble [30 × 2]&gt; ## 6 6 0 &lt;tibble [30 × 2]&gt; # plot a few datasets just to check they look like we expect: sim_data %&gt;% filter(id %% 100 &lt; 3) %&gt;% unnest(data) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + facet_grid(id %% 100 ~ o ) library(broom) # the broom package cleans up model objects to tidy form sim_data &lt;- sim_data %&gt;% # fit linear regression mutate(model = purrr::map(data, ~lm(y ~ x, data = .))) %&gt;% mutate(tidy_model = purrr::map(model, tidy)) # Get the coefficients out tidy_coefs &lt;- select(sim_data, id, o, tidy_model) %&gt;% unnest(tidy_model) %&gt;% mutate(group = case_when(o == 0 ~ &quot;No HLPs&quot;, o == 1 ~ &quot;1 HLP&quot;, o == 2 ~ &quot;2 HLPs&quot;) %&gt;% factor(levels = c(&quot;No HLPs&quot;, &quot;1 HLP&quot;, &quot;2 HLPs&quot;))) ggplot(tidy_coefs, aes(x = estimate, color = group)) + facet_grid(term ~ .) + geom_density() Obviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0). Here is a similar example worked through in SAS with IML. Note the use of BY-group processing to analyze each group at once - this is very similar to the use of purrr::map() in the R code. Try it out Let’s explore what happens to estimates when certain observations are censored. Suppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102\\(^\\circ F\\); for these temperatures, the thermometer will record a value of 102.0. It is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness. Given that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer’s censoring. Hint If most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution? What if you assume that 101 and 102.5 are the 2SD bounds? Solution If 101 and 102.5 are the anchor points we have, let’s assume that 95% of normal pet temperatures fall in that range. So our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375. We can simulate 1000 observations from \\(N(101.75, 0.375)\\), create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are. DATA dogtemp; call streaminit(20572983); DO i = 1 to 1000; actual = rand(&quot;Normal&quot;, 101.75, 0.375); IF actual &gt; 102 THEN read = 102; IF actual &lt;= 102 THEN read = actual; OUTPUT; END; RUN; PROC MEANS DATA = dogtemp; VAR actual read; RUN; Variable N Mean Std Dev Minimum Maximum actual read 1000 1000 101.7455820 101.6862713 0.3850419 0.3031418 100.6693806 100.6693806 102.9749567 102.0000000 set.seed(204209527) dogtemp &lt;- tibble( actual = rnorm(1000, 101.75, 0.375), read = pmin(actual, 102) ) dogtemp %&gt;% summarize_all(mean) ## # A tibble: 1 x 2 ## actual read ## &lt;dbl&gt; &lt;dbl&gt; ## 1 102. 102. The effect of the thermometer’s censoring in both cases is around 0.06 degrees F. 10.4 Monte Carlo methods Monte carlo methods are methods which rely on repeated random sampling in order to solve numerical problems. Often, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically. In general, a MC problem involves these steps: Define the input domain Generate inputs randomly from an appropriate probability distribution Perform a computation using those inputs Aggregate the results. Let’s try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1. More precisely, if \\(u_i \\sim U(0,1)\\), where _{i=1}^k u_i &gt; 1, what is the expected value of \\(k\\)? In this simulation, our input domain is [0,1]. Our input is \\(u_i \\sim U(0,1)\\) We generate new \\(u_i\\) until \\(\\sum_{i=1}^k &gt; 1\\) and save the value of \\(k\\) We average the result of \\(N\\) such simulations. # It&#39;s easier to think through the code if we write it inefficiently first sim_fcn &lt;- function() { usum &lt;- 0 k &lt;- 0 # prevent infinite loops by monitoring the value of k as well while (usum &lt; 1 &amp; k &lt; 15) { usum &lt;- runif(1) + usum k &lt;- k + 1 } return(k) } set.seed(302497852) res &lt;- tibble(k = replicate(1000, sim_fcn(), simplify = T)) mean(res$k) ## [1] 2.717 If we want to see whether the result converges to something, we can increase the number of trials we run: set.seed(20417023) sim_res &lt;- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) sim_res &lt;- sim_res %&gt;% mutate(running_avg_est = cummean(samp), N = row_number()) ggplot(aes(x = N, y = running_avg_est), data = sim_res) + geom_hline(yintercept = exp(1), color = &quot;red&quot;) + geom_line() The expected number of uniform RV draws required to sum to 1 is \\(e\\)! Explanation of why this works Monte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods). Suppose you want to integrate \\[\\int_0^1 e^{-x^3}dx\\] You could set up Riemann integration and evaluate the integral using a sum over \\(K\\) points, but that approach only converges for smooth functions (and besides, that’s boring calc 2 stuff, right?). Instead, let’s observe that this is equivalent to \\(\\int_0^1 e^{-x^3}\\cdot 1 dx\\), where \\(p(x) = 1\\) for a uniform random variable. That is, this integral can be written as the expected value of the function over the interval \\([0,1]\\). What if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at that point, and average the result? Implementation set.seed(20491720) fn &lt;- function(x) exp(-x^3) sim_data &lt;- tibble(x = runif(100000), y = fn(x)) mean(sim_data$y) ## [1] 0.8076082 DATA tmp; CALL streaminit(20283492); DO i = 1 to 100000; x = RAND(&quot;Uniform&quot;, 0, 1); y = EXP(-x**3); OUTPUT; END; RUN; PROC MEANS data=tmp; VAR y; RUN; Analysis Variable : y N Mean Std Dev Minimum Maximum 100000 0.8075430 0.1965383 0.3678807 1.0000000 You can use the law of large numbers to prove that this approach will converge. Example stolen from this set of lecture notes Try it out Buffon’s needle is a mathematical problem which can be boiled down to a simple physical simulation. Read this science friday description of the problem and develop a monte carlo simulation method which estimates \\(\\pi\\) using the Buffon’s needle method. Your method should be a function which allows the user to specify how many sticks are dropped plots the result of the physical simulation prints out a numerical estimate of pi. Solution Let’s start out with horizontal lines at 0 and 1, and set our stick length to 1. We need to randomly generate a position (of one end of the stick) and an angle. The position in \\(x\\) doesn’t actually make much of a difference (since what we care about is the \\(y\\) coordinates), but we can draw a picture if we generate \\(x\\) as well. needle_sim &lt;- function(sticks = 100) { df &lt;- tibble(xstart = runif(sticks, 0, 10), ystart = runif(sticks, 0, 1), angle = runif(sticks, 0, 360), xend = xstart + cos(angle/180*pi), yend = ystart + sin(angle/180*pi) ) %&gt;% # We can see if a stick crosses a line if the floor() function of ystart is # different than floor(yend). Note this only works for integer line values... mutate(crosses_line = floor(ystart) != floor(yend)) gg &lt;- ggplot() + geom_hline(yintercept = c(0, 1)) + geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend, color = crosses_line), data = df) + coord_fixed() return(list(est = 2 * sticks/ sum(df$crosses_line), plot = gg)) } needle_sim(10) ## $est ## [1] 2.8571429 ## ## $plot needle_sim(100) ## $est ## [1] 2.8985507 ## ## $plot needle_sim(1000) ## $est ## [1] 3.1298905 ## ## $plot needle_sim(10000) ## $est ## [1] 3.1235358 ## ## $plot This blog post contains code for a SAS implementation 10.5 References Simulation (R programming for Data Science chapter) Simulation - R Studio lesson Simulation, focusing on statistical modeling (R) Simulating Data with SAS (Excerpt) Simulating a Drunkard’s Walk in 2D in SAS Simulation from a triangle distribution (SAS) Simulating the Monty Hall problem (SAS) When to use purrr (part of the ‘teaching the tidyverse’ series) - essentially, purrr is a great intro to functional programming, but there are other ways to solve iterative problems in R as well, and some of them are easier than purrr (but purrr is a general approach that is very powerful). I once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds). We got brilliant results, but couldn’t ever reproduce them, because I hadn’t set the seed first and we never could figure out what the original seed was. Learn from my mistakes.↩︎ "],["docs-reports.html", "Module 11 Documents and Reports Documents and Reports: Module Objectives 11.1 Literate Programming, knitr, and rmarkdown 11.2 A Very Brief Introduction to LaTeX 11.3 Slides 11.4 Posters 11.5 Resume/CV 11.6 Using Github Pages 11.7 References", " Module 11 Documents and Reports Documents and Reports: Module Objectives Be able to create presentation slides in LaTeX and rmarkdown posters in LaTeX and rmarkdown A CV in LaTeX and/or rmarkdown This chapter will be shorter in length than many of the rest, but you should not devote less time to it. Instead, you should spend the time playing with the different options presented here and deciding which one of each is your favorite. Rather than detailing all of the customization options in each package, I think you’ll have an easier time looking at examples, trying to customize them yourself to get the effect you want, and figuring out how to do that by reading the documentation, stackoverflow posts, and other help files – those are the skills you’ll need when you try to put this knowledge into action. At the end of this chapter there are a few extras – for instance, how to use GitHub to host your documents, how to create a blog with blogdown, and more. You should feel free to investigate, but as long as you are able to create presentation slides, posters, and a CV, you’re good to go. Reproducibility with Rmarkdown (by Allison Horst) 11.1 Literate Programming, knitr, and rmarkdown Literate programming is a programming method where you explain the code in natural language (e.g. English) in roughly the same space that you write the code (in a programming language). This solves two problems: code isn’t always clear as to what its goals are, and natural language descriptions of algorithms aren’t always clear enough to contain the details of how something is actually implemented. The knitr and Rmarkdown packages are both implementations of literate programming (and the two packages tend to overlap a bit, because both were written by the same author, Yihui Xie). knitr is primarily focused on the creation of Rnw (r no weave) files, which are essentially LaTeX files with R code inside. Rnw files are compiled into pdfs. rmarkdown uses Rmd or Rmarkdown files, which can then be compiled into many different formats: pdf, html, markdown, Microsoft Word. One major advantage of knitr and Rmarkdown from a practical perspective is that it largely removes the need to keep track of graphs and charts when you’re writing a paper, making a presentation, etc. The charts and tables based on your method automatically update when the document is recompiled. If you’re not reading this chapter early, you’ve been using Rmarkdown for the entire semester to submit your homework. Hopefully that’s been fairly easy - you’ve been creating Rmarkdown documents all semester. In this chapter, we’re going to explore some other applications of literate programming: creating slides, posters, and more. 11.2 A Very Brief Introduction to LaTeX LaTeX is a document preparation utility that attempts to take the focus off of layout (so you don’t have to spend 30 minutes trying to get the page break in the right place in e.g. Word) and bibliographic details. I’m not convinced LaTeX succeeds at freeing you from layout concerns, but it’s certainly true that it is much more powerful than Word for layout purposes. The philosophy of LaTeX is that presentation shouldn’t get in the way of content: you should be able to change the presentation formatting systematically, without having to mess with the content. This allows you to switch templates easily, make document-wide changes in a single command, and more. In Rstudio, copy the text in the document below, paste it into a text file in the editor window, and name it test.tex. You should see a Compile PDF button show up at the top of the document. Click that button to compile the document. \\documentclass{article} % this tells LaTeX what type of document to make % Note, comments are prefaced by a % sign. If you need to type the actual symbol % you will have to escape it with \\%. \\begin{document} Hello \\LaTeX! \\end{document} Most commonly, you’ll use the article document class for papers, and beamer for presentations and posters. Other useful classes include moderncv (for CVs) and book, as well as the LaTeX class maintained by the UNL math department for thesis formatting. Note that by changing the extension of any .tex file to .Rnw, you can easily add R code chunks to a LaTeX file. There are several types of latex commands: Declarations: statements like \\documentclass, \\usepackage or \\small, which are stated once and take effect until further notice. Environments: statements with matching \\begin{xxx} and \\end{xxx} clauses that define a block of the document which is treated differently. Common environments include figures and tables. Special characters: another type of command that don’t define formatting or structure, but may print special characters, e.g. \\% to print a literal % character. Both declarations and environments may come with both optional and required arguments. Required arguments are placed in {...} brackets, while optional arguments are placed in [...] brackets. You can, for instance, start your document with \\documentclass[12pt]{article} to specify the base font size. One of the most useful features in LaTeX is math mode, which you can enter by enclosing text in $...$ (for inline statements), $$...$$ (for statements on their own line), or using other environments like \\begin{array}...\\end{array} that come in math-specific packages. Once in math mode, you can use math symbol commands to get characters like \\(\\theta, \\pi, \\sum, \\int, \\infty\\), and more. Try it out With any document creation software, the easiest way to learn how to do it is to find a sample document, tinker with it, see if you can make things the way you want them to be, and then google the errors when you inevitably screw something up. Take the sample document up above and see if you can do the following tasks: (I’ve linked to documentation that may be useful) Add an image Add the quadratic formula and the PDF of a normal distribution to the document In extremely large text, print LaTeX using the \\LaTeX command In extremely small, italic text, print your name Solution \\documentclass{article} % this tells LaTeX what type of document to make % Add the graphicx package so that we can include images \\usepackage{graphicx} \\begin{document} Hello \\LaTeX! % Include a figure \\begin{figure}[h] \\centering \\includegraphics[width=.5\\textwidth]{../image/IllusoryContour.png} \\caption{Illusory contour image} \\end{figure} % Add the quadratic formula and the normal PDF to the document $y = ax^2 + bx + c$ can be solved to get $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$ The PDF of a normal distribution is $$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$ % In extremely large text, print \\LaTeX \\Huge\\LaTeX % In extremely small italic text, print your name \\tiny\\emph{Your name} \\end{document} You can see the compiled pdf here. 11.2.1 Knitr R code chunks are embedded in LaTeX documents using: % start of chunk &lt;&lt;chunk-name, ...options...&gt;&gt;= @ % end of chunk You can embed numerical results inline using \\Sexpr{...} where your R code goes in the .... Unfortunately, knitr does not work with SAS… for that, you’ll need Rmarkdown (or you can use a Jupyter notebook). How this works To compile a Rnw document, knitr first runs all of the R code, generating any figures or tables or text output for each chunk. For each chunk, knitr replaces the chunk code with LaTeX code to include the results; the result of this operation is saved to a tex file. Once the tex file is created, knitr compiles the tex file into a pdf. 11.3 Slides 11.3.1 Beamer (LaTeX) and knitr Beamer is a powerful LaTeX class which allows you to create slides. The only change necessary to turn a beamer slide deck into a knitr slide deck is to add fragile as an option to any slide with verbatim content. You can also create Beamer slides with Rmarkdown. Example presentation. Standard tradeoffs (formatting details vs. document complexity) apply. Try it out Download and compile beamer-demo.Rnw. What happens when you remove the [fragile] from each frame declaration? Can you change the theme of the presentation? Add another slide, and on that slide, show an appropriate style ggplot2 graph of the distribution of board game ratings, reading in the board game ratings using the following code: board_games &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv&quot;) Karl Broman has a set of slides that show how to use beamer + knitr to make reproducible slides with notes. You can also create Beamer slides using Rmarkdown, if you want, but you’ll probably have more control over the fine details if you go straight to the Rnw file without going through Rmd first. It’s a trade-off – the file will probably be simpler in Rmarkdown, but you won’t have nearly as much control. 11.3.2 HTML slides RStudio has a host of other options for html slide presentations. There are some definite advantages to HTML presentations: they’re easy to share (via URL), you can add gifs, emojis, and interactive graphics, and you can set up github to host the presentations as well37. The downside to HTML slides is that there are approximately 100000 different javascript libraries that create HTML slides, and all of them have different capabilities. Many of these libraries have R extensions that will let you create Rmarkdown slides, but they each have slightly different markdown syntax and capabilities. Slide options available by default in RStudio You can get the full details of any fully supported slide class in Rmarkdown by looking at the Rmarkdown book, which is freely available online. These guidelines will give you specifics about how to customize slides, add incremental information, change transitions, print your slides to PDF, and include speaker notes. It should be relatively straightforward to create an ioslides or slidy presentation, given that you’ve been using Rmarkdown all semester. From some reading, it seems as if slidy has more options, but ioslides is simpler to use. However, the library I prefer at the moment is xaringan, which is a package written by Yihui Xie (same guy that wrote rmarkdown/knitr). If you install the xaringan package, you can easily create a xaringan presentation by selecting the “From Template” option in the “New R markdown” window (shown above). Yihui has an excellent blog post describing the features of xaringan that aren’t found in other libraries. Rather than repeat the documentation for each slide package in this document, I think it is probably easier just to link you to the documentation and a sample presentation for each option. ioslides Example presentation slidy Example presentation xaringan Example presentation, Example presentation 2 using UNL CSS theme reveal.js Example presentation If you’re familiar with CSS (or happier tinkering to get the look of something exactly right) then xaringan is an excellent full-featured option. A nice feature of reveal.js presentations (my favorite option before xaringan) is support for 2D slide layouts, so you can have multiple sections in your presentation, and move vertically through each section, or horizontally between sections. That is useful for presentations where you may not plan on covering everything, but where you want to have all of the information available if necessary. I relied heavily on that during my PhD prelim and defense. Try it out Take a few minutes and try each of them out to see what feels right to you. Each one has a slightly different “flavor” of Rmarkdown, so read through the example to get a sense for what is different. 11.4 Posters Posters are another common vehicle for presenting academic project results. Because posters are typically printed on paper or fabric, the standard file format is still PDF. With that said, a number of HTML poster options exist and seem to be relatively well polished38, and some have PDF export capabilities so that you can have the best of both worlds - interactivity online, and static, stable PDF exports as well. 11.4.1 LaTeX Overleaf has a fantastic gallery of posters made in LaTeX. There are several LaTeX options for making scientific posters: baposter, beamerposter, tikzposter are among the most common. We’ll focus on beamerposter here, but you are free to explore the other poster classes at will. As with beamer, you can easily integrate knitr code chunks into a document, so that you are generating your images reproducibly. Basic code for a poster in beamer (along with the necessary style files) that I’ve minimally customized to meet UNL branding requirements can be found here. Try it out Download the beamer template and do the following: Change the 3-column span box to a 2-column span box. Make the “Block Colors” box purple Move the References block up to fill the 4th column. 11.4.2 Posterdown To start, install posterdown with install.packages(\"posterdown\"). Use the RStudio menu to create a posterdown presentation file – with a prefilled template I have provided an example posterdown theme here. You can also find the additional customization options here. As with other markdown items, you can customize things even more using CSS. The nice thing about HTML posters, though, is that you can directly link to them. You can also print a poster to PDF by running the following command: pagedown::chrome_print(\"myfile.Rmd\"). See the pdf version of my customized UNL-themed poster. 11.4.3 Pagedown The pagedown package also has a couple of poster templates, including poster-relaxed and poster-jacobs. There are also templates for letters, business cards, and more in pagedown, if you’re feeling ambitious. Try it out Download the pagedown template and do the following: Change the 3-column layout to 4 columns. Adjust the breaks ({.mybreak}) accordingly to make the poster look good. Make the 2nd-level headers #249ab5 (cerulean) Move the References block to the 4th column. Print your poster to a PDF 11.5 Resume/CV You can also create resumes and CVs in markdown and LaTeX. There is no real substitute for playing around with these classes, but I really like moderncv in LaTeX39 Pagedown also comes with a html resume template (Use the menu -&gt; Rmarkdown -&gt; From Template -&gt; HTML Resume) that can be printed to html and pdf simultaneously. There is also the vitae package, which has even more templates, integration with other packages/sites, and more.40 11.6 Using Github Pages Github will host HTML content for you using Github pages (case in point: this textbook). This means you can version control your content (for instance, presentations or your CV) and have GitHub do the hosting (so you don’t have to find a webserver, buy a domain name, etc). Create a new repository named username.github.io Clone your repository Add an index.html file (this can be anything, e.g. a text file that says “hello world,” so long as it has an extension of html) and push your changes Go to https://username.github.io (YouTube Link) Github will render any README.md file as actual HTML; it will also allow you to host straight HTML pages. By default, the README file is rendered first, but in subsequent directories, a file named index.html will be rendered as the “home page” for the subdirectory, if you have such a file. Otherwise you’ll have to know the file name. I tend to separate things out into separate repositories, but you can host HTML content on other repositories too, by enabling github pages in the repository settings. On my personal page, I have repositories for my CV, Presentations41, etc. Each repository that has pages enabled can be accessed via srvanderplas.github.io/&lt;repository name&gt;/&lt;repository file path&gt;. So, to see my unl-stat850 repository, you’d go to https://srvanderplas.github.io/unl-stat850/ (but you’re already there!). (YouTube Link) This mechanism provides a very convenient way to showcase your work, share information with collaborators, and more - instead of sending files, you can send a URL and no one has to download anything overtly. If you want to track your Rmarkdown code and then render the output to a separate folder, you can use the docs/ folder. Github has this as an option as well – where we selected “master” branch above, we would select “docs/” instead (it’s greyed out b/c there isn’t a docs folder in the repo). That is how this book is hosted - the book compiles to the docs/ folder, and that way the book is rendered in final form and you don’t have to see all of the other crud that is in the repository. 11.7 References There are many other XXXdown packages, including blogdown bookdown (what I’m using to make this book) pkgdown (to easily build documentation websites for R packages) ROpenSci tutorial: How to set up hosting on github liftr - use Docker to make persistently reproducible documents I have a repository for all of the presentations I’ve given, and I use github pages to render the html presentations. Very easy, convenient, and I never have to carry a flash drive around↩︎ See this list of Rmarkdown poster options.↩︎ You can see my highly customized version here, with timelines and numbered publications. It has to be compiled multiple times to get everything right, though.↩︎ At this point, the biggest reason I haven’t switched to HTML is that I really like my timeline CV and I don’t have enough time to fiddle with it more.↩︎ I’ve been putting my presentations on Github since 2014, so it has a pretty good record of every set of slides I’ve created for anything important (and many not-so-important things as well). I highly recommend this strategy - by storing everything online, you make it very easy to share your work with others, very easy to reference later, and more importantly, easy for you to find in 3 years when you need that one specific picture.↩︎ "],["animated-and-interactive-graphics.html", "Module 12 Animated and Interactive Graphics Animated and Interactive Graphics: Module Objectives 12.1 Plotly 12.2 Leaflet maps 12.3 Shiny 12.4 General References", " Module 12 Animated and Interactive Graphics Interactive and animated graphics are one of the major advantages of using the Rmarkdown ecosystem - because you can easily create web pages in markdown (without the pain of HTML), you aren’t limited by paper any more. We’ll cover two different technologies that allow you to create different types of interactive charts, graphs, and interfaces. It is helpful to think about interactivity in a couple of different ways: What does it require? Do you need to be doing statistical calculations in the background, or can you precompute all of the data ahead of time? What type of activity or interactivity do you need? Zoom in/out? Provide additional information in response to user actions (mouseover, click) Provide information over time (animation) Keep track of a data point over multiple plots? (linked plots) Keep track of one or more data points and change their appearance based on user interaction (brushing) Allow the user to change the underlying statistical model or data? (This is not a full list of all of the types of interactivity, just a few of the more common options) In this section, we’ll cover two ways to easily create interactive graphics or applets in R. There are, of course, many others – many javascript libraries have R extensions of one form or another. Animated and Interactive Graphics: Module Objectives Create interactive charts with appropriate tools Use Shiny to create interactive web applets 12.1 Plotly Plotly is a graphing library that uses javascript to add interactivity to graphics. There are several different ways to create plotly graphs in R, but by far the easiest is ggplotly, which converts a ggplot to a plotly plot automatically (so you don’t have to specify most of the details). 12.1.1 ggplotly: ggplot2 to plotly conversions Set up the data if (!&quot;plotly&quot; %in% installed.packages()) install.packages(&quot;plotly&quot;) if (!&quot;tidytuesdayR&quot; %in% installed.packages()) { devtools::install_github(&quot;thebioengineer/tidytuesdayR&quot;) } library(dplyr) library(tidyr) library(ggplot2) library(tibble) library(lubridate) # dates and times library(tidytuesdayR) # get interesting data library(plotly) library(stringr) # Load the data from TidyTuesday on May 12 full_data &lt;- tt_load(&#39;2020-05-12&#39;) volcano &lt;- full_data$volcano eruptions &lt;- full_data$eruptions events &lt;- full_data$events sulfur &lt;- full_data$sulfur trees &lt;- full_data$tree_rings Let’s try out plotly while doing a bit of exploratory data analysis on this dataset. Cleaning up volcano volcano &lt;- volcano %&gt;% filter(tectonic_settings != &quot;Unknown&quot;) %&gt;% separate(tectonic_settings, into = c(&quot;zone&quot;, &quot;crust&quot;), sep = &quot;/&quot;, remove = F) %&gt;% # Remove anything past the first punctuation character - that will catch (xx) and ? mutate(volcano_type = str_remove(primary_volcano_type, &quot;[[:punct:]].*$&quot;)) Let’s start by seeing whether the elevation of a volcano changes based on the type of zone it’s on - we might expect that Rift zone volcanos (where plates are pulling away from each other) might not be as high. p &lt;- volcano %&gt;% ggplot(aes(x = zone, y = elevation)) + geom_boxplot() + coord_flip() ggplotly(p) But it doesn’t really look like there’s much difference. Does volcano type makes a difference? p &lt;- volcano %&gt;% ggplot(aes(x = elevation, color = volcano_type)) + geom_density() + # Rug plots show each observation as a tick just below the x axis geom_rug() ggplotly(p) Here, the interactivity actually helps a bit: we don’t need to use the legend to see what each curve corresponds to. We can see that submarine volcanoes are typically much lower in elevation (ok, duh), but also that subglacial volcanoes are found in a very limited range. If we double-click on a legend entry, we can get rid of all other curves and examine each curve one by one. I added the rug layer after the initial bout because I was curious how much data each of these curves were based on. If we want only curves with n &gt; 10 observations, we can do that: p &lt;- volcano %&gt;% group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;% filter(n &gt; 10) %&gt;% ggplot(aes(x = elevation, color = volcano_type)) + geom_density() + # Rug plots show each observation as a tick just below the x axis geom_rug(aes(text = paste0(volcano_name, &quot;, &quot;, country))) ## Warning: Ignoring unknown aesthetics: text ggplotly(p) If we want to specify additional information that should show up in the tooltip, we can do that as well by adding the text aesthetic even though geom_rug doesn’t take a text aesthetic. You may notice that ggplot2 complains about the unknown aesthetic I’ve added to geom_rug: That allows us to mouse over each data point in the rug plot and see what volcano it belongs to. So we can tell from the rug plot that the tallest volcano is Ojas de Salvado, in Chile/Argentina (I believe that translates to Eyes of Salvation?). At any rate, there isn’t nearly as much variation as I was expecting in the elevation of different types of volcanoes. ggplotly makes it very easy to generate plots that have a ggplot2 equivalent; you can customize these plots further using plotly functions that we’ll see in the next section. But first, try the interface out on your own. Try it out Conduct an exploratory data analysis of the eruptions dataset. What do you find? My solution head(eruptions) ## # A tibble: 6 x 15 ## volcano_number volcano_name eruption_number eruption_catego… area_of_activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 266030 Soputan 22354 Confirmed Erupt… &lt;NA&gt; ## 2 343100 San Miguel 22355 Confirmed Erupt… &lt;NA&gt; ## 3 233020 Fournaise, P… 22343 Confirmed Erupt… &lt;NA&gt; ## 4 345020 Rincon de la… 22346 Confirmed Erupt… &lt;NA&gt; ## 5 353010 Fernandina 22347 Confirmed Erupt… &lt;NA&gt; ## 6 273070 Taal 22344 Confirmed Erupt… &lt;NA&gt; ## # … with 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;, ## # start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;, ## # end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt; summary(eruptions %&gt;% mutate(eruption_category = factor(eruption_category))) ## volcano_number volcano_name eruption_number ## Min. :210010 Length:11178 Min. :10001 ## 1st Qu.:263310 Class :character 1st Qu.:12817 ## Median :290050 Mode :character Median :15650 ## Mean :300284 Mean :15667 ## 3rd Qu.:343030 3rd Qu.:18464 ## Max. :600000 Max. :22355 ## ## eruption_category area_of_activity vei ## Confirmed Eruption :9900 Length:11178 Min. :0.000 ## Discredited Eruption: 166 Class :character 1st Qu.:1.000 ## Uncertain Eruption :1112 Mode :character Median :2.000 ## Mean :1.948 ## 3rd Qu.:2.000 ## Max. :7.000 ## NA&#39;s :2906 ## start_year start_month start_day evidence_method_dating ## Min. :-11345.0 Min. : 0.000 Min. : 0.000 Length:11178 ## 1st Qu.: 680.0 1st Qu.: 0.000 1st Qu.: 0.000 Class :character ## Median : 1847.0 Median : 1.000 Median : 0.000 Mode :character ## Mean : 622.8 Mean : 3.451 Mean : 7.015 ## 3rd Qu.: 1950.0 3rd Qu.: 7.000 3rd Qu.:15.000 ## Max. : 2020.0 Max. :12.000 Max. :31.000 ## NA&#39;s :1 NA&#39;s :193 NA&#39;s :196 ## end_year end_month end_day latitude ## Min. :-475 Min. : 0.000 Min. : 0.00 Min. :-77.530 ## 1st Qu.:1895 1st Qu.: 3.000 1st Qu.: 4.00 1st Qu.: -6.102 ## Median :1957 Median : 6.000 Median :15.00 Median : 17.600 ## Mean :1917 Mean : 6.221 Mean :13.32 Mean : 16.866 ## 3rd Qu.:1992 3rd Qu.: 9.000 3rd Qu.:21.00 3rd Qu.: 40.821 ## Max. :2020 Max. :12.000 Max. :31.00 Max. : 85.608 ## NA&#39;s :6846 NA&#39;s :6849 NA&#39;s :6852 ## longitude ## Min. :-179.97 ## 1st Qu.: -77.66 ## Median : 55.71 ## Mean : 31.57 ## 3rd Qu.: 139.39 ## Max. : 179.58 ## # Historical (very historical) dates are a bit of a pain to work with, so I # wrote a helper function which takes year, month, and day arguments and formats # them properly fix_date &lt;- function(yyyy, mm, dd) { # First, negative years (BCE) are a bit of a problem. neg &lt;- yyyy &lt; 0 subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later # for now, set to 0 year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0 # sometimes the day or month isn&#39;t known, so just use 1 for both. # recorded value may be NA or 0. day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1)) month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1)) # Need to format things precisely, so use sprintf # %0xd ensures that you have at least x digits, padding the left side with 0s # lubridate doesn&#39;t love having 3-digit years. date_str &lt;- sprintf(&quot;%04d/%02d/%02d&quot;, year_fixed, month_fixed, day_fixed) # Then we can convert the dates and subtract off the years for pre-CE dates date &lt;- ymd(date_str) - years(subtract_years) } erupt &lt;- eruptions %&gt;% # Don&#39;t work with discredited eruptions filter(eruption_category == &quot;Confirmed Eruption&quot;) %&gt;% # Create start and end dates mutate( start_date = fix_date(start_year, start_month, start_day), end_date = fix_date(end_year, end_month, end_day), # To get duration, we have to start with a time interval, # convert to duration, then convert to a numeric value duration = interval(start = start_date, end = end_date) %&gt;% as.duration() %&gt;% as.numeric(&quot;days&quot;)) ## Warning: 1 failed to parse. ## Warning: 5895 failed to parse. Let’s start out seeing what month most eruptions occur in… # Note, I&#39;m using the original month, so 0 = unknown p &lt;- ggplot(erupt, aes(x = factor(start_month))) + geom_bar() ggplotly(p) # I could rename some of the factors to make this pretty, but... nah Another numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released. # VEI is volcano explosivity index, p &lt;- ggplot(erupt, aes(x = vei)) + geom_bar() ggplotly(p) ## Warning: Removed 2270 rows containing non-finite values (stat_count). We can also look at the frequency of eruptions over time. We’ll expect some historical bias - we don’t have exact dates for some of these eruptions, and if no one was around to write the eruption down (or the records were destroyed) there’s not going to be a date listed here. p &lt;- erupt %&gt;% filter(!is.na(end_date)) %&gt;% filter(start_year &gt; 0) %&gt;% ggplot(aes(x = start_date, xend = start_date, y = 0, yend = duration, color = evidence_method_dating)) + geom_segment() + geom_point(size = .5, aes(text = volcano_name)) + xlab(&quot;Eruption Start&quot;) + ylab(&quot;Eruption Duration (days)&quot;) + facet_wrap(~vei, scales = &quot;free_y&quot;) ## Warning: Ignoring unknown aesthetics: text ggplotly(p) As expected, it’s pretty rare to see many eruptions before ~1800 AD, which is about when we have reliable historical records42 for most of the world (exceptions include e.g. Vestuvius, which we have extensive written information about). p &lt;- erupt %&gt;% filter(!is.na(end_date)) %&gt;% # Account for recency bias (sort of) filter(start_year &gt; 1800) %&gt;% ggplot(aes(x = factor(vei), y = duration)) + geom_violin() + xlab(&quot;VEI&quot;) + ylab(&quot;Eruption Duration (days)&quot;) + scale_y_sqrt() ggplotly(p) ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 3 rows containing non-finite values (stat_ydensity). It seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully). 12.1.2 plot_ly: Like base plotting, but interactive! You can also create plotly charts that aren’t limited by what you can do in ggplot2, using the plot_ly function. Plotly cheat sheet We can start with a scatterplot of volcanoes along the Earth’s surface: plot_ly(type = &quot;scattergeo&quot;, lon = volcano$longitude, lat = volcano$latitude) ## No scattergeo mode specifed: ## Setting the mode to markers ## Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode And then we can start customizing: plot_ly(type = &quot;scattergeo&quot;, lon = volcano$longitude, lat = volcano$latitude, mode = &quot;markers&quot;, # Add information to mouseover text = ~paste(volcano$volcano_name, &quot;\\n&quot;, &quot;Last Erupted: &quot;, volcano$last_eruption_year), # Change the markers because why not? marker = list(color = &quot;#d00000&quot;, opacity = 0.25) ) The plot_ly function is also pipe friendly. Variable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable. # Load RColorBrewer for palettes library(RColorBrewer) volcano %&gt;% group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;% filter(n &gt; 15) %&gt;% plot_ly(type = &quot;scattergeo&quot;, lon = ~longitude, lat = ~latitude, mode = &quot;markers&quot;, # Add information to mouseover text = ~paste(volcano_name, &quot;\\n&quot;, &quot;Last Erupted: &quot;, last_eruption_year), color = ~ volcano_type, # Specify a palette colors = brewer.pal(length(unique(.$volcano_type)), &quot;Paired&quot;), # Change the markers because why not? marker = list(opacity = 0.5) ) Plotly will handle some variable mappings for you, depending on which “trace” type (plot/geom) you’re using. The plotly documentation often uses plyr and reshape2 – which are older versions of dplyr and tidyr. If you load plyr and reshape2, it may seriously mess up your day – a lot of the function names are the same. So, instead, here’s a shortcut: cast is pivot_wider and melt is pivot_longer. That should at least help with understanding what the code is doing. If you do accidentally load plyr or reshape2, that’s fine: just restart your R session so that your loaded packages are cleared and you can start over. Or, if you must, you can reference a plyr function using plyr::function_name without loading the package – that’s a safe way to use the plotly demo code as-is. Let’s explore traces a bit. According to the plotly documentation, A trace is just the name we give a collection of data and the specifications of which we want that data plotted. Notice that a trace will also be an object itself, and these will be named according to how you want the data displayed on the plotting surface In ggplot2 terms, it seems that a trace is somewhat akin to a geom. trace0 &lt;- rnorm(100, mean = 5) trace1 &lt;- rnorm(100, mean = 0) trace2 &lt;- rnorm(100, mean = -5) data &lt;- tibble(x = 1:100, trace0, trace1, trace2) # Let&#39;s see how this goes with one trace plot_ly(data, x = ~x) %&gt;% add_trace(y = ~trace0, name = &#39;trace0&#39;, mode = &#39;lines&#39;) ## No trace type specified: ## Based on info supplied, a &#39;scatter&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter # Adding some more traces plot_ly(data, x = ~x) %>% add_trace(y = ~trace0, name = 'trace0', mode = 'lines') %>% add_trace(y = ~trace1, name = \"trace1\", mode = 'lines+markers') %>% add_trace(y = ~trace2, name = \"trace2\", mode = 'markers') ## No trace type specified: ## Based on info supplied, a 'scatter' trace seems appropriate. ## Read more about this trace type -> https://plotly.com/r/reference/#scatter ## No trace type specified: ## Based on info supplied, a 'scatter' trace seems appropriate. ## Read more about this trace type -> https://plotly.com/r/reference/#scatter ## No trace type specified: ## Based on info supplied, a 'scatter' trace seems appropriate. ## Read more about this trace type -> https://plotly.com/r/reference/#scatter But, if you want all of the variables to be shown with the same trace type, it’s probably easier to get to long form: data %&gt;% pivot_longer(matches(&quot;trace&quot;), names_to = &quot;trace&quot;, names_prefix = &quot;trace&quot;, values_to = &quot;y&quot;) %&gt;% plot_ly(x = ~x, y = ~y, color = ~trace, mode = &quot;lines+markers&quot;) ## No trace type specified: ## Based on info supplied, a &#39;scatter&#39; trace seems appropriate. ## Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter There are many different trace types in plotly, but your best bet is to check the documentation to see what is available. 12.1.3 Animation Plotly can also animate your plots for you. library(classdata) data(fbi) fbi %&gt;% mutate(State = factor(State), Rate_100k = Count/Population*100000) %&gt;% filter(Type == &quot;Aggravated.assault&quot;) %&gt;% arrange(Year, State, Type) %&gt;% plot_ly( x = ~State, y = ~Rate_100k, color = ~Type, frame = ~Year, type = &quot;scatter&quot;, mode = &quot;markers&quot; ) Sometimes the animations get a bit trippy, don’t they? You can even animate by something other than time, if you’re so inclined, though it’s not necessarily going to make sense if there isn’t any context shared between successive observations. So animating over space might make sense, but animating over a factor makes a lot less sense. fbi %&gt;% mutate(State = factor(State), Rate_100k = Count/Population*100000) %&gt;% arrange(Year, State, Type) %&gt;% plot_ly( x = ~Year, y = ~Rate_100k, color = ~Type, frame = ~State, type = &quot;scatter&quot;, mode = &quot;lines&quot; ) There are other types of animations as well, including the ability to change plot formats, trace types, and more. 12.2 Leaflet maps Leaflet is another javascript library that allows for interactive data visualization. We’re only going to briefly talk about it here, but there is extensive documentation that includes details of how to work with different types of geographical data, chloropleth maps, plugins, and more. To explore the leaflet package, we’ll start out playing with a dataset of Bigfoot sightings assembled from the Bigfoot Field Researchers Organization’s Google earth tool ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## observed = col_character(), ## location_details = col_character(), ## county = col_character(), ## state = col_character(), ## season = col_character(), ## title = col_character(), ## date = col_date(format = &quot;&quot;), ## classification = col_character(), ## geohash = col_character(), ## precip_type = col_character(), ## summary = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. if (!&quot;leaflet&quot; %in% installed.packages()) install.packages(&quot;leaflet&quot;) library(leaflet) library(readr) bigfoot_data &lt;- read_csv(&quot;https://query.data.world/s/egnaxxvegdkzzrhfhdh4izb6etmlms&quot;) We can start out by plotting a map with the location of each sighting. I’ve colored the points in a seasonal color scheme, and added the description of each incident as a mouseover label. bigfoot_data %&gt;% filter(classification == &quot;Class A&quot;) %&gt;% mutate(seasoncolor = str_replace_all(season, c(&quot;Fall&quot; = &quot;orange&quot;, &quot;Winter&quot; = &quot;skyblue&quot;, &quot;Spring&quot; = &quot;green&quot;, &quot;Summer&quot; = &quot;yellow&quot;)), # This code just wraps the description to the width of the R terminal # and inserts HTML for a line break into the text at appropriate points desc_wrap = purrr::map(observed, ~strwrap(.) %&gt;% paste(collapse = &quot;&lt;br/&gt;&quot;) %&gt;% htmltools::HTML())) %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(~longitude, ~latitude, color = ~seasoncolor, label = ~desc_wrap) ## Warning in validateCoords(lng, lat, funcName): Data contains 459 rows with ## either missing or invalid lat/lon values and will be ignored Of course, because this is an interactive map library, we aren’t limited to any one scale. We can also plot data at the city level: if(!&quot;nycsquirrels18&quot; %in% installed.packages()) { devtools::install_github(&quot;mine-cetinkaya-rundel/nycsquirrels18&quot;) } library(nycsquirrels18) data(squirrels) head(squirrels) ## # A tibble: 6 x 35 ## long lat unique_squirrel_… hectare shift date hectare_squirrel… age ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -74.0 40.8 13A-PM-1014-04 13A PM 2018-10-14 4 &lt;NA&gt; ## 2 -74.0 40.8 15F-PM-1010-06 15F PM 2018-10-10 6 Adult ## 3 -74.0 40.8 19C-PM-1018-02 19C PM 2018-10-18 2 Adult ## 4 -74.0 40.8 21B-AM-1019-04 21B AM 2018-10-19 4 &lt;NA&gt; ## 5 -74.0 40.8 23A-AM-1018-02 23A AM 2018-10-18 2 Juve… ## 6 -74.0 40.8 38H-PM-1012-01 38H PM 2018-10-12 1 Adult ## # … with 27 more variables: primary_fur_color &lt;chr&gt;, highlight_fur_color &lt;chr&gt;, ## # combination_of_primary_and_highlight_color &lt;chr&gt;, color_notes &lt;chr&gt;, ## # location &lt;chr&gt;, above_ground_sighter_measurement &lt;chr&gt;, ## # specific_location &lt;chr&gt;, running &lt;lgl&gt;, chasing &lt;lgl&gt;, climbing &lt;lgl&gt;, ## # eating &lt;lgl&gt;, foraging &lt;lgl&gt;, other_activities &lt;chr&gt;, kuks &lt;lgl&gt;, ## # quaas &lt;lgl&gt;, moans &lt;lgl&gt;, tail_flags &lt;lgl&gt;, tail_twitches &lt;lgl&gt;, ## # approaches &lt;lgl&gt;, indifferent &lt;lgl&gt;, runs_from &lt;lgl&gt;, ## # other_interactions &lt;chr&gt;, zip_codes &lt;dbl&gt;, community_districts &lt;dbl&gt;, ## # borough_boundaries &lt;dbl&gt;, city_council_districts &lt;dbl&gt;, ## # police_precincts &lt;dbl&gt; squirrels %&gt;% mutate(color = tolower(primary_fur_color)) %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat, color = ~color) We can also plot regions, instead of just points. I downloaded a dataset released by the US Forest Service, Bailey’s Ecoregions and Subregions dataset, which categorizes the US into different climate and ecological zones. To map colors to variables, we have to define a color palette and variable mapping ourselves, and pass that function into the leaflet object we’re adding. library(rgdal) ## Loading required package: sp ## rgdal: version: 1.5-23, (SVN revision 1121) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.2.1, released 2020/12/29 ## Path to GDAL shared files: /usr/share/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 7.2.1, January 1st, 2021, [PJ_VERSION: 721] ## Path to PROJ shared files: /home/susan/.local/share/proj:/usr/share/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.4-5 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading rgdal. ecoregions &lt;- readOGR(&quot;data/Bailey_s_Ecoregions_and_Subregions_Dataset.geojson&quot;) ## OGR data source with driver: GeoJSON ## Source: &quot;/home/susan/Projects/Class/unl-stat850/2020-stat850/data/Bailey_s_Ecoregions_and_Subregions_Dataset.geojson&quot;, layer: &quot;Bailey_s_Ecoregions_and_Subregions_Dataset&quot; ## with 3072 features ## It has 12 fields # Define a palette region_pal &lt;- colorFactor(c(&quot;#E67E22&quot;, &quot;#0B5345&quot;, &quot;#229954&quot;, &quot;#B3B6B7&quot;), ecoregions$DOMAIN) ecoregions %&gt;% leaflet() %&gt;% addTiles() %&gt;% addPolygons(stroke = TRUE, fillOpacity = 0.25, fillColor = ~region_pal(DOMAIN), color = ~region_pal(DOMAIN), label = ~SECTION) Try it out Download the Shapefiles for the 116th Congress Congressional Districts. Unzip the file and read it in using the code below (you’ll have to change the file path). Use the MIT Election Data and Science Lab’s US House election results43, and merge this data with the shapefiles to plot the results of the 2018 midterms in a way that you think is useful (you can use any of the available data). Some notes: - FIPS codes are used to identify the state and district, with 00 indicating at-large districts (one district for the state) and 98 indicating non-voting districts. - If you would like to add in the number of citizens of voting age, you can get that information here but you will have to do some cleaning in order to join the table with the others. - Minnesota’s Democratic-farmer-labor party caucuses with the Democrats but maintains its name for historical reasons. You can safely recode this if you want to. library(sf) ## Linking to GEOS 3.9.0, GDAL 3.2.1, PROJ 7.2.1 # Read in the districts congress_districts &lt;- st_read(&quot;data/116_congress/cb_2018_us_cd116_5m.shp&quot;) ## Reading layer `cb_2018_us_cd116_5m&#39; from data source `/home/susan/Projects/Class/unl-stat850/2020-stat850/data/116_congress/cb_2018_us_cd116_5m.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 441 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256 ## geographic CRS: NAD83 # Read in the results election_results &lt;- read_csv(&quot;data/1976-2018-house2.csv&quot;) %&gt;% filter(year == 2018) %&gt;% mutate(state_fips = sprintf(&quot;%02d&quot;, state_fips), district = sprintf(&quot;%02d&quot;, district)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## year = col_double(), ## state = col_character(), ## state_po = col_character(), ## state_fips = col_double(), ## state_cen = col_double(), ## state_ic = col_double(), ## office = col_character(), ## district = col_double(), ## stage = col_character(), ## runoff = col_logical(), ## special = col_logical(), ## candidate = col_character(), ## party = col_character(), ## writein = col_logical(), ## mode = col_character(), ## candidatevotes = col_double(), ## totalvotes = col_double(), ## unofficial = col_logical(), ## version = col_double() ## ) # Clean up congress districts congress_districts &lt;- congress_districts %&gt;% # Convert factors to characters mutate(across(where(is.factor), as.character)) %&gt;% # Handle at-large districts mutate(district = ifelse(CD116FP == &quot;00&quot;, &quot;01&quot;, CD116FP)) One solution library(sf) library(htmltools) # to mark labels as html code # Read in the results election_results &lt;- read_csv(&quot;data/1976-2018-house2.csv&quot;) %&gt;% filter(year == 2018) %&gt;% mutate(state_fips = sprintf(&quot;%02d&quot;, state_fips), district = sprintf(&quot;%02d&quot;, district)) %&gt;% group_by(state, state_fips, state_po, district, stage) %&gt;% arrange(candidatevotes) %&gt;% mutate(pct = candidatevotes/totalvotes) %&gt;% # Keep the winner only filter(pct == max(pct)) %&gt;% # Fix Minnesota mutate(party = ifelse(party == &quot;democratic-farmer-labor&quot;, &quot;democrat&quot;, party)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## year = col_double(), ## state = col_character(), ## state_po = col_character(), ## state_fips = col_double(), ## state_cen = col_double(), ## state_ic = col_double(), ## office = col_character(), ## district = col_double(), ## stage = col_character(), ## runoff = col_logical(), ## special = col_logical(), ## candidate = col_character(), ## party = col_character(), ## writein = col_logical(), ## mode = col_character(), ## candidatevotes = col_double(), ## totalvotes = col_double(), ## unofficial = col_logical(), ## version = col_double() ## ) # Read in the districts congress_districts &lt;- st_read(&quot;data/116_congress/cb_2018_us_cd116_5m.shp&quot;) %&gt;% mutate(geometry = st_transform(geometry, crs = st_crs(&quot;+proj=longlat +datum=WGS84&quot;))) ## Reading layer `cb_2018_us_cd116_5m&#39; from data source `/home/susan/Projects/Class/unl-stat850/2020-stat850/data/116_congress/cb_2018_us_cd116_5m.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 441 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256 ## geographic CRS: NAD83 # Clean up congress districts congress_districts &lt;- congress_districts %&gt;% # Convert factors to characters mutate(across(where(is.factor), as.character)) %&gt;% # Handle at-large districts mutate(district = ifelse(CD116FP == &quot;00&quot;, &quot;01&quot;, CD116FP)) # Merge congress_districts &lt;- congress_districts %&gt;% left_join(election_results, by = c(&quot;STATEFP&quot; = &quot;state_fips&quot;, &quot;CD116FP&quot; = &quot;district&quot;)) %&gt;% mutate(party = factor(party, levels = c(&quot;republican&quot;, &quot;democrat&quot;)), short_party = ifelse(party == &quot;republican&quot;, &quot;R&quot;, &quot;D&quot;), label = paste0(state_po, &quot;-&quot;, district, candidate, &quot; (&quot;, short_party, &quot;)&quot;)) # Define a palette region_pal &lt;- colorFactor(c(&quot;#e9141d&quot;, &quot;#0015bc&quot;), congress_districts$party) congress_districts %&gt;% leaflet() %&gt;% addTiles() %&gt;% addPolygons(stroke = TRUE, fillOpacity = ~pct/2, # still want to see what&#39;s underneath, even in safe districts fillColor = ~region_pal(party), color = ~region_pal(party), label = ~label) 12.3 Shiny https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/readme.md Before we get started on Shiny, take a few minutes and poke around the RStudio Shiny user showcase. It helps to have some motivation, and to get a sense of what is possible before you start learning something. One of the more amusing ones I found was an exploration of lego demographics. Shiny is a framework for building interactive web applications in R. Unlike plotly and other graphics engines, Shiny depends on an R instance on a server to do computations. This means Shiny is much more powerful and has more capabilities, but also that it’s harder to share and deploy - you have to have access to a web server with R installed on it. If you happen to have a server like that, though, Shiny is pretty awesome. RStudio runs a service called shinyapps.io that will provide some limited free hosting, as well as paid plans for apps that have more web traffic, but you can also create Shiny apps for local use - I often do this for model debugging when I’m using neural networks, because they’re so complicated. RStudio has a set of well produced video tutorials to introduce Shiny. I’d recommend you at least listen to the introduction if you’re a visual/audio learner (the whole tutorial is about 2 hours long). There is also a written tutorial if you prefer to learn in written form (7 lessons, each is about 20 minutes long). I generally think it’s better to send you to the source when there are well-produced resources, rather than trying to rehash something to put my own spin on it. One other interesting feature to keep in mind when using Shiny - you can integrate Shiny reactivity into Rmarkdown by adding runtime: shiny to the markdown header. 12.4 General References R graph gallery interactive charts points you in the right direction for which package to use for different tasks. Shiny articles Reactivity in Shiny Leaflet introduction for R 12.4.1 Other interactive tools htmlwidgets - a generic wrapper for any Javascript library (htmlwidgets is used under the hood in both Leaflet and Plotly R integration) dash - Another dashboard program supported by plotly. dash is the python equivalent of shiny, but also has R integration (though I’m not sure how well it’s supported). 12.4.2 Debugging Debugging with Dean - Shiny debugging - YouTube video with debugging in realtime. ShinyJS - Using Shiny and JavaScript together Using Shiny in Production - Joe Cheng There are obviously exceptions - we can figure out the exact date and approximate time that there was an earthquake along the Cascadia subduction zone based on a combination of oral histories of the indigenous people and records of a massive tsunami in Japan Excellent read, if you’re interested, and the Nature paper.↩︎ Alternately, you can find the rehosted file here.↩︎ "],["lists-nested-lists-and-functional-programming.html", "Module 13 Lists, Nested Lists, and Functional Programming 13.1 Review: Lists and Vectors 13.2 Introduction to map 13.3 Creating (and Using) List-columns 13.4 Ways to use map 13.5 Beyond map: Functions with multiple inputs Purrr References", " Module 13 Lists, Nested Lists, and Functional Programming 13.1 Review: Lists and Vectors A vector is a 1-dimensional R data structure that contains items of the same simple (‘atomic’) type (character, logical, integer, factor). (logical_vec &lt;- c(T, F, T, T)) ## [1] TRUE FALSE TRUE TRUE (numeric_vec &lt;- c(3, 1, 4, 5)) ## [1] 3 1 4 5 (char_vec &lt;- c(&quot;A&quot;, &quot;AB&quot;, &quot;ABC&quot;, &quot;ABCD&quot;)) ## [1] &quot;A&quot; &quot;AB&quot; &quot;ABC&quot; &quot;ABCD&quot; You index a vector using brackets: to get the 3rd element of the vector x, you would use x[3]. logical_vec[3] ## [1] TRUE numeric_vec[3] ## [1] 4 char_vec[3] ## [1] &quot;ABC&quot; You can also index a vector using a logical vector: numeric_vec[logical_vec] ## [1] 3 4 5 char_vec[logical_vec] ## [1] &quot;A&quot; &quot;ABC&quot; &quot;ABCD&quot; logical_vec[logical_vec] ## [1] TRUE TRUE TRUE A list is a 1-dimensional R data structure that has no restrictions on what type of content is stored within it. (mylist &lt;- list(logical_vec, numeric_vec, third_thing = char_vec[1:2])) ## [[1]] ## [1] TRUE FALSE TRUE TRUE ## ## [[2]] ## [1] 3 1 4 5 ## ## $third_thing ## [1] &quot;A&quot; &quot;AB&quot; A list is a vector, but it is not an atomic vector - that is, it does not necessarily contain things that are all the same type. List components may have names (or not), be homogeneous (or not), have the same length (or not). There are 3 ways to index a list: With single square brackets, just like we index atomic vectors. In this case, the return value is always a list. mylist[1] ## [[1]] ## [1] TRUE FALSE TRUE TRUE mylist[2] ## [[1]] ## [1] 3 1 4 5 mylist[c(T, F, T)] ## [[1]] ## [1] TRUE FALSE TRUE TRUE ## ## $third_thing ## [1] &quot;A&quot; &quot;AB&quot; With double square brackets. In this case, the return value is the thing inside the specified position in the list, but you also can only get one entry in the main list at a time. You can also get things by name. mylist[[1]] ## [1] TRUE FALSE TRUE TRUE mylist[[&quot;third_thing&quot;]] ## [1] &quot;A&quot; &quot;AB&quot; Using x$name. This is equivalent to using x[[\"name\"]]. Note that this does not work on unnamed entries in the list. mylist$third_thing ## [1] &quot;A&quot; &quot;AB&quot; You can get a more thorough review of vectors and lists from Jenny Bryan’s purrr tutorial. Operations in R are vectorized - that is, by default, they operate on vectors. This is primarily a feature that applies to atomic vectors (and we don’t even think about it): (rnorm(10) + rnorm(10, mean = 3)) ## [1] 2.172953 2.129084 2.265086 5.978046 5.472891 4.402667 2.468631 2.763687 ## [9] 4.506722 1.036752 We didn’t have to use a for loop to add these two vectors with 10 entries each together. In python (and SAS, and other languages), this might instead look like: a &lt;- rnorm(10) b &lt;- rnorm(10, mean = 3) result &lt;- rep(0, 10) for(i in 1:10) { result[i] &lt;- a[i] + b[i] } result ## [1] 4.520750 1.497100 4.696924 5.482570 1.420313 4.879504 2.137498 2.309878 ## [9] 2.457079 1.382129 That is, we would apply or map the + function to each entry of a and b. For atomic vectors, it’s easy to do this by default; with a list, however, we need to be a bit more explicit (because everything that’s passed into the function may not be the same type). This logic is the basis behind the purrr package (and similar base functions apply, lapply, sapply, tapply, and mapply - I find the purrr package easier to work with, but you may use the base package versions if you want, and you can find a side-by-side comparison in the purrr tutorial). 13.2 Introduction to map library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.0 ✓ dplyr 1.0.5 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x tidyr::extract() masks magrittr::extract() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x purrr::set_names() masks magrittr::set_names() library(purrr) # list functions library(repurrrsive) # examples We’ll use one of the datasets in repurrsive, got_chars, to start playing with the map_ series of functions. data(got_chars) length(got_chars) ## [1] 30 got_chars[[1]] ## $url ## [1] &quot;https://www.anapioficeandfire.com/api/characters/1022&quot; ## ## $id ## [1] 1022 ## ## $name ## [1] &quot;Theon Greyjoy&quot; ## ## $gender ## [1] &quot;Male&quot; ## ## $culture ## [1] &quot;Ironborn&quot; ## ## $born ## [1] &quot;In 278 AC or 279 AC, at Pyke&quot; ## ## $died ## [1] &quot;&quot; ## ## $alive ## [1] TRUE ## ## $titles ## [1] &quot;Prince of Winterfell&quot; ## [2] &quot;Captain of Sea Bitch&quot; ## [3] &quot;Lord of the Iron Islands (by law of the green lands)&quot; ## ## $aliases ## [1] &quot;Prince of Fools&quot; &quot;Theon Turncloak&quot; &quot;Reek&quot; &quot;Theon Kinslayer&quot; ## ## $father ## [1] &quot;&quot; ## ## $mother ## [1] &quot;&quot; ## ## $spouse ## [1] &quot;&quot; ## ## $allegiances ## [1] &quot;House Greyjoy of Pyke&quot; ## ## $books ## [1] &quot;A Game of Thrones&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## ## $povBooks ## [1] &quot;A Clash of Kings&quot; &quot;A Dance with Dragons&quot; ## ## $tvSeries ## [1] &quot;Season 1&quot; &quot;Season 2&quot; &quot;Season 3&quot; &quot;Season 4&quot; &quot;Season 5&quot; &quot;Season 6&quot; ## ## $playedBy ## [1] &quot;Alfie Allen&quot; It appears that each entry in this 30-item list is a character from Game of Thrones, and there are several sub-fields for each character. What characters do we have? We can use purrr::map(x, \"name\") to get a list of all characters’ names. Since they are all the same type, we could also use an extension of map, map_chr, which will coerce the returned list into a character vector (which may be simpler to operate on). There are several packages with map() functions including functions that are meant to actually plot maps; it generally saves time and effort to just type the function name with the package you want; you don’t have to do so, but if you have a lot of other (non tidyverse, in particular) packages loaded, it will save you a lot of grief. purrr::map(got_chars, &quot;name&quot;) ## [[1]] ## [1] &quot;Theon Greyjoy&quot; ## ## [[2]] ## [1] &quot;Tyrion Lannister&quot; ## ## [[3]] ## [1] &quot;Victarion Greyjoy&quot; ## ## [[4]] ## [1] &quot;Will&quot; ## ## [[5]] ## [1] &quot;Areo Hotah&quot; ## ## [[6]] ## [1] &quot;Chett&quot; ## ## [[7]] ## [1] &quot;Cressen&quot; ## ## [[8]] ## [1] &quot;Arianne Martell&quot; ## ## [[9]] ## [1] &quot;Daenerys Targaryen&quot; ## ## [[10]] ## [1] &quot;Davos Seaworth&quot; ## ## [[11]] ## [1] &quot;Arya Stark&quot; ## ## [[12]] ## [1] &quot;Arys Oakheart&quot; ## ## [[13]] ## [1] &quot;Asha Greyjoy&quot; ## ## [[14]] ## [1] &quot;Barristan Selmy&quot; ## ## [[15]] ## [1] &quot;Varamyr&quot; ## ## [[16]] ## [1] &quot;Brandon Stark&quot; ## ## [[17]] ## [1] &quot;Brienne of Tarth&quot; ## ## [[18]] ## [1] &quot;Catelyn Stark&quot; ## ## [[19]] ## [1] &quot;Cersei Lannister&quot; ## ## [[20]] ## [1] &quot;Eddard Stark&quot; ## ## [[21]] ## [1] &quot;Jaime Lannister&quot; ## ## [[22]] ## [1] &quot;Jon Connington&quot; ## ## [[23]] ## [1] &quot;Jon Snow&quot; ## ## [[24]] ## [1] &quot;Aeron Greyjoy&quot; ## ## [[25]] ## [1] &quot;Kevan Lannister&quot; ## ## [[26]] ## [1] &quot;Melisandre&quot; ## ## [[27]] ## [1] &quot;Merrett Frey&quot; ## ## [[28]] ## [1] &quot;Quentyn Martell&quot; ## ## [[29]] ## [1] &quot;Samwell Tarly&quot; ## ## [[30]] ## [1] &quot;Sansa Stark&quot; purrr::map_chr(got_chars, &quot;name&quot;) ## [1] &quot;Theon Greyjoy&quot; &quot;Tyrion Lannister&quot; &quot;Victarion Greyjoy&quot; ## [4] &quot;Will&quot; &quot;Areo Hotah&quot; &quot;Chett&quot; ## [7] &quot;Cressen&quot; &quot;Arianne Martell&quot; &quot;Daenerys Targaryen&quot; ## [10] &quot;Davos Seaworth&quot; &quot;Arya Stark&quot; &quot;Arys Oakheart&quot; ## [13] &quot;Asha Greyjoy&quot; &quot;Barristan Selmy&quot; &quot;Varamyr&quot; ## [16] &quot;Brandon Stark&quot; &quot;Brienne of Tarth&quot; &quot;Catelyn Stark&quot; ## [19] &quot;Cersei Lannister&quot; &quot;Eddard Stark&quot; &quot;Jaime Lannister&quot; ## [22] &quot;Jon Connington&quot; &quot;Jon Snow&quot; &quot;Aeron Greyjoy&quot; ## [25] &quot;Kevan Lannister&quot; &quot;Melisandre&quot; &quot;Merrett Frey&quot; ## [28] &quot;Quentyn Martell&quot; &quot;Samwell Tarly&quot; &quot;Sansa Stark&quot; Similar shortcuts work to get the nth item in each sub list: purrr::map_chr(got_chars, 4) ## [1] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; ## [9] &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ## [17] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ## [25] &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; Specifying the output type using e.g. map_chr works if each item in the list is an atomic vector of length 1. If the list is more complicated, though, these shortcuts will issue an error: purrr::map(got_chars, &quot;books&quot;) ## [[1]] ## [1] &quot;A Game of Thrones&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## ## [[2]] ## [1] &quot;A Feast for Crows&quot; &quot;The World of Ice and Fire&quot; ## ## [[3]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## ## [[4]] ## [1] &quot;A Clash of Kings&quot; ## ## [[5]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## ## [[6]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; ## ## [[7]] ## [1] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## ## [[8]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [4] &quot;A Dance with Dragons&quot; ## ## [[9]] ## [1] &quot;A Feast for Crows&quot; ## ## [[10]] ## [1] &quot;A Feast for Crows&quot; ## ## [[11]] ## NULL ## ## [[12]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [4] &quot;A Dance with Dragons&quot; ## ## [[13]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; ## ## [[14]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; ## [3] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## [5] &quot;The World of Ice and Fire&quot; ## ## [[15]] ## [1] &quot;A Storm of Swords&quot; ## ## [[16]] ## [1] &quot;A Feast for Crows&quot; ## ## [[17]] ## [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; &quot;A Dance with Dragons&quot; ## ## [[18]] ## [1] &quot;A Feast for Crows&quot; &quot;A Dance with Dragons&quot; ## ## [[19]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## ## [[20]] ## [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [3] &quot;A Feast for Crows&quot; &quot;A Dance with Dragons&quot; ## [5] &quot;The World of Ice and Fire&quot; ## ## [[21]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; ## ## [[22]] ## [1] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## [3] &quot;The World of Ice and Fire&quot; ## ## [[23]] ## [1] &quot;A Feast for Crows&quot; ## ## [[24]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [4] &quot;A Dance with Dragons&quot; ## ## [[25]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [4] &quot;A Feast for Crows&quot; ## ## [[26]] ## [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; ## ## [[27]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Feast for Crows&quot; ## [4] &quot;A Dance with Dragons&quot; ## ## [[28]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; ## [4] &quot;A Feast for Crows&quot; ## ## [[29]] ## [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Dance with Dragons&quot; ## ## [[30]] ## [1] &quot;A Dance with Dragons&quot; purrr::map_chr(got_chars, &quot;books&quot;) ## Error: Result 1 must be a single string, not a character vector of length 3 What if we want to extract several things? This trick works off of the idea that [ is a function: that is, the single brackets we used before are actually a special type of function. In R functions, there is often the argument ..., which is a convention that allows us to pass arguments to other functions that are called within the main function we are using (you’ll see … used in plotting and regression functions frequently as well). Here, we use ... to pass in our list of 3 things we want to pull from each item in the list. purrr::map(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) ## [[1]] ## [[1]]$name ## [1] &quot;Theon Greyjoy&quot; ## ## [[1]]$gender ## [1] &quot;Male&quot; ## ## [[1]]$born ## [1] &quot;In 278 AC or 279 AC, at Pyke&quot; ## ## ## [[2]] ## [[2]]$name ## [1] &quot;Tyrion Lannister&quot; ## ## [[2]]$gender ## [1] &quot;Male&quot; ## ## [[2]]$born ## [1] &quot;In 273 AC, at Casterly Rock&quot; ## ## ## [[3]] ## [[3]]$name ## [1] &quot;Victarion Greyjoy&quot; ## ## [[3]]$gender ## [1] &quot;Male&quot; ## ## [[3]]$born ## [1] &quot;In 268 AC or before, at Pyke&quot; ## ## ## [[4]] ## [[4]]$name ## [1] &quot;Will&quot; ## ## [[4]]$gender ## [1] &quot;Male&quot; ## ## [[4]]$born ## [1] &quot;&quot; ## ## ## [[5]] ## [[5]]$name ## [1] &quot;Areo Hotah&quot; ## ## [[5]]$gender ## [1] &quot;Male&quot; ## ## [[5]]$born ## [1] &quot;In 257 AC or before, at Norvos&quot; ## ## ## [[6]] ## [[6]]$name ## [1] &quot;Chett&quot; ## ## [[6]]$gender ## [1] &quot;Male&quot; ## ## [[6]]$born ## [1] &quot;At Hag&#39;s Mire&quot; ## ## ## [[7]] ## [[7]]$name ## [1] &quot;Cressen&quot; ## ## [[7]]$gender ## [1] &quot;Male&quot; ## ## [[7]]$born ## [1] &quot;In 219 AC or 220 AC&quot; ## ## ## [[8]] ## [[8]]$name ## [1] &quot;Arianne Martell&quot; ## ## [[8]]$gender ## [1] &quot;Female&quot; ## ## [[8]]$born ## [1] &quot;In 276 AC, at Sunspear&quot; ## ## ## [[9]] ## [[9]]$name ## [1] &quot;Daenerys Targaryen&quot; ## ## [[9]]$gender ## [1] &quot;Female&quot; ## ## [[9]]$born ## [1] &quot;In 284 AC, at Dragonstone&quot; ## ## ## [[10]] ## [[10]]$name ## [1] &quot;Davos Seaworth&quot; ## ## [[10]]$gender ## [1] &quot;Male&quot; ## ## [[10]]$born ## [1] &quot;In 260 AC or before, at King&#39;s Landing&quot; ## ## ## [[11]] ## [[11]]$name ## [1] &quot;Arya Stark&quot; ## ## [[11]]$gender ## [1] &quot;Female&quot; ## ## [[11]]$born ## [1] &quot;In 289 AC, at Winterfell&quot; ## ## ## [[12]] ## [[12]]$name ## [1] &quot;Arys Oakheart&quot; ## ## [[12]]$gender ## [1] &quot;Male&quot; ## ## [[12]]$born ## [1] &quot;At Old Oak&quot; ## ## ## [[13]] ## [[13]]$name ## [1] &quot;Asha Greyjoy&quot; ## ## [[13]]$gender ## [1] &quot;Female&quot; ## ## [[13]]$born ## [1] &quot;In 275 AC or 276 AC, at Pyke&quot; ## ## ## [[14]] ## [[14]]$name ## [1] &quot;Barristan Selmy&quot; ## ## [[14]]$gender ## [1] &quot;Male&quot; ## ## [[14]]$born ## [1] &quot;In 237 AC&quot; ## ## ## [[15]] ## [[15]]$name ## [1] &quot;Varamyr&quot; ## ## [[15]]$gender ## [1] &quot;Male&quot; ## ## [[15]]$born ## [1] &quot;At a village Beyond the Wall&quot; ## ## ## [[16]] ## [[16]]$name ## [1] &quot;Brandon Stark&quot; ## ## [[16]]$gender ## [1] &quot;Male&quot; ## ## [[16]]$born ## [1] &quot;In 290 AC, at Winterfell&quot; ## ## ## [[17]] ## [[17]]$name ## [1] &quot;Brienne of Tarth&quot; ## ## [[17]]$gender ## [1] &quot;Female&quot; ## ## [[17]]$born ## [1] &quot;In 280 AC&quot; ## ## ## [[18]] ## [[18]]$name ## [1] &quot;Catelyn Stark&quot; ## ## [[18]]$gender ## [1] &quot;Female&quot; ## ## [[18]]$born ## [1] &quot;In 264 AC, at Riverrun&quot; ## ## ## [[19]] ## [[19]]$name ## [1] &quot;Cersei Lannister&quot; ## ## [[19]]$gender ## [1] &quot;Female&quot; ## ## [[19]]$born ## [1] &quot;In 266 AC, at Casterly Rock&quot; ## ## ## [[20]] ## [[20]]$name ## [1] &quot;Eddard Stark&quot; ## ## [[20]]$gender ## [1] &quot;Male&quot; ## ## [[20]]$born ## [1] &quot;In 263 AC, at Winterfell&quot; ## ## ## [[21]] ## [[21]]$name ## [1] &quot;Jaime Lannister&quot; ## ## [[21]]$gender ## [1] &quot;Male&quot; ## ## [[21]]$born ## [1] &quot;In 266 AC, at Casterly Rock&quot; ## ## ## [[22]] ## [[22]]$name ## [1] &quot;Jon Connington&quot; ## ## [[22]]$gender ## [1] &quot;Male&quot; ## ## [[22]]$born ## [1] &quot;In or between 263 AC and 265 AC&quot; ## ## ## [[23]] ## [[23]]$name ## [1] &quot;Jon Snow&quot; ## ## [[23]]$gender ## [1] &quot;Male&quot; ## ## [[23]]$born ## [1] &quot;In 283 AC&quot; ## ## ## [[24]] ## [[24]]$name ## [1] &quot;Aeron Greyjoy&quot; ## ## [[24]]$gender ## [1] &quot;Male&quot; ## ## [[24]]$born ## [1] &quot;In or between 269 AC and 273 AC, at Pyke&quot; ## ## ## [[25]] ## [[25]]$name ## [1] &quot;Kevan Lannister&quot; ## ## [[25]]$gender ## [1] &quot;Male&quot; ## ## [[25]]$born ## [1] &quot;In 244 AC&quot; ## ## ## [[26]] ## [[26]]$name ## [1] &quot;Melisandre&quot; ## ## [[26]]$gender ## [1] &quot;Female&quot; ## ## [[26]]$born ## [1] &quot;At Unknown&quot; ## ## ## [[27]] ## [[27]]$name ## [1] &quot;Merrett Frey&quot; ## ## [[27]]$gender ## [1] &quot;Male&quot; ## ## [[27]]$born ## [1] &quot;In 262 AC&quot; ## ## ## [[28]] ## [[28]]$name ## [1] &quot;Quentyn Martell&quot; ## ## [[28]]$gender ## [1] &quot;Male&quot; ## ## [[28]]$born ## [1] &quot;In 281 AC, at Sunspear, Dorne&quot; ## ## ## [[29]] ## [[29]]$name ## [1] &quot;Samwell Tarly&quot; ## ## [[29]]$gender ## [1] &quot;Male&quot; ## ## [[29]]$born ## [1] &quot;In 283 AC, at Horn Hill&quot; ## ## ## [[30]] ## [[30]]$name ## [1] &quot;Sansa Stark&quot; ## ## [[30]]$gender ## [1] &quot;Female&quot; ## ## [[30]]$born ## [1] &quot;In 286 AC, at Winterfell&quot; If this is ugly syntax to you, that’s fine - the magrittr package also includes an extract function that works the same way. purrr::map(got_chars, magrittr::extract, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) ## [[1]] ## [[1]]$name ## [1] &quot;Theon Greyjoy&quot; ## ## [[1]]$gender ## [1] &quot;Male&quot; ## ## [[1]]$born ## [1] &quot;In 278 AC or 279 AC, at Pyke&quot; ## ## ## [[2]] ## [[2]]$name ## [1] &quot;Tyrion Lannister&quot; ## ## [[2]]$gender ## [1] &quot;Male&quot; ## ## [[2]]$born ## [1] &quot;In 273 AC, at Casterly Rock&quot; ## ## ## [[3]] ## [[3]]$name ## [1] &quot;Victarion Greyjoy&quot; ## ## [[3]]$gender ## [1] &quot;Male&quot; ## ## [[3]]$born ## [1] &quot;In 268 AC or before, at Pyke&quot; ## ## ## [[4]] ## [[4]]$name ## [1] &quot;Will&quot; ## ## [[4]]$gender ## [1] &quot;Male&quot; ## ## [[4]]$born ## [1] &quot;&quot; ## ## ## [[5]] ## [[5]]$name ## [1] &quot;Areo Hotah&quot; ## ## [[5]]$gender ## [1] &quot;Male&quot; ## ## [[5]]$born ## [1] &quot;In 257 AC or before, at Norvos&quot; ## ## ## [[6]] ## [[6]]$name ## [1] &quot;Chett&quot; ## ## [[6]]$gender ## [1] &quot;Male&quot; ## ## [[6]]$born ## [1] &quot;At Hag&#39;s Mire&quot; ## ## ## [[7]] ## [[7]]$name ## [1] &quot;Cressen&quot; ## ## [[7]]$gender ## [1] &quot;Male&quot; ## ## [[7]]$born ## [1] &quot;In 219 AC or 220 AC&quot; ## ## ## [[8]] ## [[8]]$name ## [1] &quot;Arianne Martell&quot; ## ## [[8]]$gender ## [1] &quot;Female&quot; ## ## [[8]]$born ## [1] &quot;In 276 AC, at Sunspear&quot; ## ## ## [[9]] ## [[9]]$name ## [1] &quot;Daenerys Targaryen&quot; ## ## [[9]]$gender ## [1] &quot;Female&quot; ## ## [[9]]$born ## [1] &quot;In 284 AC, at Dragonstone&quot; ## ## ## [[10]] ## [[10]]$name ## [1] &quot;Davos Seaworth&quot; ## ## [[10]]$gender ## [1] &quot;Male&quot; ## ## [[10]]$born ## [1] &quot;In 260 AC or before, at King&#39;s Landing&quot; ## ## ## [[11]] ## [[11]]$name ## [1] &quot;Arya Stark&quot; ## ## [[11]]$gender ## [1] &quot;Female&quot; ## ## [[11]]$born ## [1] &quot;In 289 AC, at Winterfell&quot; ## ## ## [[12]] ## [[12]]$name ## [1] &quot;Arys Oakheart&quot; ## ## [[12]]$gender ## [1] &quot;Male&quot; ## ## [[12]]$born ## [1] &quot;At Old Oak&quot; ## ## ## [[13]] ## [[13]]$name ## [1] &quot;Asha Greyjoy&quot; ## ## [[13]]$gender ## [1] &quot;Female&quot; ## ## [[13]]$born ## [1] &quot;In 275 AC or 276 AC, at Pyke&quot; ## ## ## [[14]] ## [[14]]$name ## [1] &quot;Barristan Selmy&quot; ## ## [[14]]$gender ## [1] &quot;Male&quot; ## ## [[14]]$born ## [1] &quot;In 237 AC&quot; ## ## ## [[15]] ## [[15]]$name ## [1] &quot;Varamyr&quot; ## ## [[15]]$gender ## [1] &quot;Male&quot; ## ## [[15]]$born ## [1] &quot;At a village Beyond the Wall&quot; ## ## ## [[16]] ## [[16]]$name ## [1] &quot;Brandon Stark&quot; ## ## [[16]]$gender ## [1] &quot;Male&quot; ## ## [[16]]$born ## [1] &quot;In 290 AC, at Winterfell&quot; ## ## ## [[17]] ## [[17]]$name ## [1] &quot;Brienne of Tarth&quot; ## ## [[17]]$gender ## [1] &quot;Female&quot; ## ## [[17]]$born ## [1] &quot;In 280 AC&quot; ## ## ## [[18]] ## [[18]]$name ## [1] &quot;Catelyn Stark&quot; ## ## [[18]]$gender ## [1] &quot;Female&quot; ## ## [[18]]$born ## [1] &quot;In 264 AC, at Riverrun&quot; ## ## ## [[19]] ## [[19]]$name ## [1] &quot;Cersei Lannister&quot; ## ## [[19]]$gender ## [1] &quot;Female&quot; ## ## [[19]]$born ## [1] &quot;In 266 AC, at Casterly Rock&quot; ## ## ## [[20]] ## [[20]]$name ## [1] &quot;Eddard Stark&quot; ## ## [[20]]$gender ## [1] &quot;Male&quot; ## ## [[20]]$born ## [1] &quot;In 263 AC, at Winterfell&quot; ## ## ## [[21]] ## [[21]]$name ## [1] &quot;Jaime Lannister&quot; ## ## [[21]]$gender ## [1] &quot;Male&quot; ## ## [[21]]$born ## [1] &quot;In 266 AC, at Casterly Rock&quot; ## ## ## [[22]] ## [[22]]$name ## [1] &quot;Jon Connington&quot; ## ## [[22]]$gender ## [1] &quot;Male&quot; ## ## [[22]]$born ## [1] &quot;In or between 263 AC and 265 AC&quot; ## ## ## [[23]] ## [[23]]$name ## [1] &quot;Jon Snow&quot; ## ## [[23]]$gender ## [1] &quot;Male&quot; ## ## [[23]]$born ## [1] &quot;In 283 AC&quot; ## ## ## [[24]] ## [[24]]$name ## [1] &quot;Aeron Greyjoy&quot; ## ## [[24]]$gender ## [1] &quot;Male&quot; ## ## [[24]]$born ## [1] &quot;In or between 269 AC and 273 AC, at Pyke&quot; ## ## ## [[25]] ## [[25]]$name ## [1] &quot;Kevan Lannister&quot; ## ## [[25]]$gender ## [1] &quot;Male&quot; ## ## [[25]]$born ## [1] &quot;In 244 AC&quot; ## ## ## [[26]] ## [[26]]$name ## [1] &quot;Melisandre&quot; ## ## [[26]]$gender ## [1] &quot;Female&quot; ## ## [[26]]$born ## [1] &quot;At Unknown&quot; ## ## ## [[27]] ## [[27]]$name ## [1] &quot;Merrett Frey&quot; ## ## [[27]]$gender ## [1] &quot;Male&quot; ## ## [[27]]$born ## [1] &quot;In 262 AC&quot; ## ## ## [[28]] ## [[28]]$name ## [1] &quot;Quentyn Martell&quot; ## ## [[28]]$gender ## [1] &quot;Male&quot; ## ## [[28]]$born ## [1] &quot;In 281 AC, at Sunspear, Dorne&quot; ## ## ## [[29]] ## [[29]]$name ## [1] &quot;Samwell Tarly&quot; ## ## [[29]]$gender ## [1] &quot;Male&quot; ## ## [[29]]$born ## [1] &quot;In 283 AC, at Horn Hill&quot; ## ## ## [[30]] ## [[30]]$name ## [1] &quot;Sansa Stark&quot; ## ## [[30]]$gender ## [1] &quot;Female&quot; ## ## [[30]]$born ## [1] &quot;In 286 AC, at Winterfell&quot; What if we want this to be a data frame instead? We can use map_dfr to get a data frame that is formed by row-binding each element in the list. purrr::map_dfr(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) ## # A tibble: 30 x 3 ## name gender born ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Theon Greyjoy Male &quot;In 278 AC or 279 AC, at Pyke&quot; ## 2 Tyrion Lannister Male &quot;In 273 AC, at Casterly Rock&quot; ## 3 Victarion Greyjoy Male &quot;In 268 AC or before, at Pyke&quot; ## 4 Will Male &quot;&quot; ## 5 Areo Hotah Male &quot;In 257 AC or before, at Norvos&quot; ## 6 Chett Male &quot;At Hag&#39;s Mire&quot; ## 7 Cressen Male &quot;In 219 AC or 220 AC&quot; ## 8 Arianne Martell Female &quot;In 276 AC, at Sunspear&quot; ## 9 Daenerys Targaryen Female &quot;In 284 AC, at Dragonstone&quot; ## 10 Davos Seaworth Male &quot;In 260 AC or before, at King&#39;s Landing&quot; ## # … with 20 more rows # Equivalent to purrr::map(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) %&gt;% dplyr::bind_rows() ## # A tibble: 30 x 3 ## name gender born ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Theon Greyjoy Male &quot;In 278 AC or 279 AC, at Pyke&quot; ## 2 Tyrion Lannister Male &quot;In 273 AC, at Casterly Rock&quot; ## 3 Victarion Greyjoy Male &quot;In 268 AC or before, at Pyke&quot; ## 4 Will Male &quot;&quot; ## 5 Areo Hotah Male &quot;In 257 AC or before, at Norvos&quot; ## 6 Chett Male &quot;At Hag&#39;s Mire&quot; ## 7 Cressen Male &quot;In 219 AC or 220 AC&quot; ## 8 Arianne Martell Female &quot;In 276 AC, at Sunspear&quot; ## 9 Daenerys Targaryen Female &quot;In 284 AC, at Dragonstone&quot; ## 10 Davos Seaworth Male &quot;In 260 AC or before, at King&#39;s Landing&quot; ## # … with 20 more rows 13.3 Creating (and Using) List-columns Data structures in R are typically list-based in one way or another. Sometimes, more complicated data structures are actually lists of lists, or tibbles with a list-column, or other variations on “list within a ____.” In combination with purrr, this is an incredibly powerful setup that can make working with simulations and data very easy. Suppose, for instance, I want to simulate some data for modeling purposes, where I can control the number of outliers in the dataset: data_sim &lt;- function(n_outliers = 0) { tmp &lt;- tibble(x = seq(-10, 10, .1), y = rnorm(length(x), mean = x, sd = 1)) outlier_sample &lt;- c(NULL, sample(tmp$x, n_outliers)) # Create outliers tmp %&gt;% mutate( is_outlier = x %in% outlier_sample, y = y + is_outlier * sample(c(-1, 1), n(), replace = T) * runif(n(), 5, 10) ) } data_sim() ## # A tibble: 201 x 3 ## x y is_outlier ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 -10 -11.6 FALSE ## 2 -9.9 -9.15 FALSE ## 3 -9.8 -10.6 FALSE ## 4 -9.7 -8.29 FALSE ## 5 -9.6 -10.2 FALSE ## 6 -9.5 -11.4 FALSE ## 7 -9.4 -9.68 FALSE ## 8 -9.3 -8.69 FALSE ## 9 -9.2 -7.26 FALSE ## 10 -9.1 -8.56 FALSE ## # … with 191 more rows Now, lets suppose that I want 100 replicates of each of 0, 5, 10, and 20 outliers. sim &lt;- crossing(rep = 1:100, n_outliers = c(0, 5, 10, 20)) %&gt;% mutate(sim_data = purrr::map(n_outliers, data_sim)) I could use unnest(sim_data) if I wanted to expand my data a bit to see what I have, but in this case, it’s more useful to leave it in its current, compact form. Instead, suppose I fit a linear regression to each of the simulated data sets, and store the fitted linear regression object in a new list-column? sim &lt;- sim %&gt;% mutate(reg = purrr::map(sim_data, ~lm(data = ., y ~ x))) Here, we use an anonymous function in purrr: by using ~{expression}, we have defined a function that takes the argument . (which is just a placeholder). So in our case, we’re saying “use the data that I pass in to fit a linear regression of y using x as a predictor.” Let’s play around a bit with this: We might want to look at our regression coefficients or standard errors to see how much the additional outliers affect us. We could use a fancy package for tidy modeling, such as broom, but for now, lets do something a bit simpler and apply the purrr name extraction functions we used earlier. It can be helpful to examine one of the objects just to see what you’re dealing with: str(sim$reg[[1]]) ## List of 12 ## $ coefficients : Named num [1:2] 0.0538 0.9818 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;x&quot; ## $ residuals : Named num [1:201] -1.33 0.225 -1.668 0.91 0.452 ... ## ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:201] -0.762 80.767 -1.612 0.967 0.509 ... ## ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;(Intercept)&quot; &quot;x&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:201] -9.76 -9.67 -9.57 -9.47 -9.37 ... ## ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:201, 1:2] -14.1774 0.0705 0.0705 0.0705 0.0705 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;x&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.07 1.11 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 199 ## $ xlevels : Named list() ## $ call : language lm(formula = y ~ x, data = .) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language y ~ x ## .. ..- attr(*, &quot;variables&quot;)= language list(y, x) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;y&quot; &quot;x&quot; ## .. .. .. ..$ : chr &quot;x&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;x&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x564d58199478&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(y, x) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;y&quot; &quot;x&quot; ## $ model :&#39;data.frame&#39;: 201 obs. of 2 variables: ## ..$ y: num [1:201] -11.09 -9.44 -11.24 -8.56 -8.92 ... ## ..$ x: num [1:201] -10 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language y ~ x ## .. .. ..- attr(*, &quot;variables&quot;)= language list(y, x) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;y&quot; &quot;x&quot; ## .. .. .. .. ..$ : chr &quot;x&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;x&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x564d58199478&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(y, x) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;y&quot; &quot;x&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; If we pull out the coefficients by name we get a vector of length two. So before we unnest, we need to change that so that R formats it as a row of a data frame. sim$reg[[1]]$coefficients %&gt;% as_tibble_row() ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0538 0.982 This will make our formatting a lot easier and prevent any duplication that might occur if we unnest a vector that has length &gt; 1. sim &lt;- sim %&gt;% mutate(coefs = purrr::map(reg, &quot;coefficients&quot;) %&gt;% purrr::map(as_tibble_row)) sim$coefs[1:5] ## [[1]] ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0538 0.982 ## ## [[2]] ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.152 1.01 ## ## [[3]] ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0912 1.01 ## ## [[4]] ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.00991 0.973 ## ## [[5]] ## # A tibble: 1 x 2 ## `(Intercept)` x ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.0764 0.992 Then, we can plot our results: sim %&gt;% unnest(coefs) %&gt;% select(rep, n_outliers, `(Intercept)`, x) %&gt;% pivot_longer(-c(rep, n_outliers), names_to = &quot;coef&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value, color = factor(n_outliers))) + geom_density() + facet_wrap(~coef, scales = &quot;free_x&quot;) So as there are more and more outliers, the coefficient estimates get a wider distribution, but remain (relatively) centered on the “true” values of 0 and 1, respectively. Notice that we keep our data in list column form right up until it is time to actually unnest it - which means that we have at the ready the simulated data, the simulated model, and the conditions under which it was simulated, all in the same data structure. It’s a really nice, organized system. 13.4 Ways to use map There are 3 main use cases for map (and its cousins pmap, map2, etc.): Use with an existing function Use with an anonymous function, defined on the fly Use with a formula (which is just a concise way to define an anonymous function) I’ll use a trivial example to show the difference between these options: # An existing function res &lt;- tibble(x = 1:10, y1 = map_dbl(x, log10)) # An anonymous function res &lt;- res %&gt;% mutate(y2 = map_dbl(x, function(z) z^2/10)) # A formula equivalent to function(z) z^5/(z + 10) # the . is the variable you&#39;re manipulating res &lt;- res %&gt;% mutate(y3 = map_dbl(x, ~.^5/(.+10))) It can be a bit tricky to differentiate between options 2 and 3 in practice - the biggest difference is that you’re not using the keyword function and your variable is the default placeholder variable . used in the tidyverse. .reset() library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.0 ✓ dplyr 1.0.5 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x tidyr::extract() masks magrittr::extract() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x purrr::set_names() masks magrittr::set_names() library(repurrrsive) Try it out Use each of the 3 options for defining a method in purrr to pull out a single string of all of the books each character was in. To do this, you’ll need to collapse the list of books for each character into a single string, which you can do with the paste function and the collapse argument. letters[1:10] %&gt;% paste(collapse = &quot;|&quot;) ## [1] &quot;a|b|c|d|e|f|g|h|i|j&quot; Start with this data frame of character names and book list-columns: data(got_chars) got_df &lt;- tibble(name = map_chr(got_chars, &quot;name&quot;), id = map_int(got_chars, &quot;id&quot;), books = map(got_chars, &quot;books&quot;)) Solution # Define a function my_collapse &lt;- function(x) paste(x, collapse = &quot; | &quot;) data(got_chars) got_df &lt;- tibble(name = map_chr(got_chars, &quot;name&quot;), id = map_int(got_chars, &quot;id&quot;), books = map(got_chars, &quot;books&quot;)) got_df &lt;- got_df %&gt;% mutate( fun_def_res = map_chr(books, my_collapse), # Here, I don&#39;t have to define a function, I just pass my additional # argument in after the fact... fun_base_res = map_chr(books, paste, collapse = &quot; | &quot;), # Here, I can just define a new function without a name and apply it to # each entry fun_anon_res = map_chr(books, function(x) paste(x, collapse = &quot; | &quot;)), # And here, I don&#39;t even bother to specifically say that I&#39;m defining a # function, I just apply a formula to each entry fun_formula_res = map_chr(books, ~paste(., collapse = &quot; | &quot;)) ) head(got_df) ## # A tibble: 6 x 7 ## name id books fun_def_res fun_base_res fun_anon_res fun_formula_res ## &lt;chr&gt; &lt;int&gt; &lt;list&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Theon… 1022 &lt;chr … A Game of Th… A Game of Th… A Game of Th… A Game of Thron… ## 2 Tyrio… 1052 &lt;chr … A Feast for … A Feast for … A Feast for … A Feast for Cro… ## 3 Victa… 1074 &lt;chr … A Game of Th… A Game of Th… A Game of Th… A Game of Thron… ## 4 Will 1109 &lt;chr … A Clash of K… A Clash of K… A Clash of K… A Clash of Kings ## 5 Areo … 1166 &lt;chr … A Game of Th… A Game of Th… A Game of Th… A Game of Thron… ## 6 Chett 1267 &lt;chr … A Game of Th… A Game of Th… A Game of Th… A Game of Thron… 13.5 Beyond map: Functions with multiple inputs Sometimes, you might need to map a function over two vectors/lists in parallel. purrr has you covered with the map2 function. As with map, the syntax is map2(thing1, thing2, function, other.args); the big difference is that function takes two arguments. Let’s create a simple times-table: crossing(x = 1:10, y = 1:10) %&gt;% mutate(times = map2_int(x, y, `*`)) %&gt;% pivot_wider(names_from = y, names_prefix = &#39;y=&#39;, values_from = times) ## # A tibble: 10 x 11 ## x `y=1` `y=2` `y=3` `y=4` `y=5` `y=6` `y=7` `y=8` `y=9` `y=10` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 2 3 4 5 6 7 8 9 10 ## 2 2 2 4 6 8 10 12 14 16 18 20 ## 3 3 3 6 9 12 15 18 21 24 27 30 ## 4 4 4 8 12 16 20 24 28 32 36 40 ## 5 5 5 10 15 20 25 30 35 40 45 50 ## 6 6 6 12 18 24 30 36 42 48 54 60 ## 7 7 7 14 21 28 35 42 49 56 63 70 ## 8 8 8 16 24 32 40 48 56 64 72 80 ## 9 9 9 18 27 36 45 54 63 72 81 90 ## 10 10 10 20 30 40 50 60 70 80 90 100 # we could use `multiply_by` instead of `*` if we wanted to If you are using formula notation to define functions with map2, you will need to refer to your two arguments as .x and .y. You can determine this from the Usage section when you run map2, which shows you map2(.x, .y, .f, ...) - that is, the first argument is .x, the second is .y, and the third is the function. Try it out Use map2 to determine if each Game of Thrones character has more titles than aliases. Start with this code: library(repurrrsive) library(tidyverse) data(got_chars) got_names &lt;- tibble(name = purrr::map_chr(got_chars, &quot;name&quot;), titles = purrr::map(got_chars, &quot;titles&quot;), aliases = purrr::map(got_chars, &quot;aliases&quot;)) Solution got_names %&gt;% mutate(more_titles = map2_lgl(titles, aliases, ~length(.x) &gt; length(.y))) ## # A tibble: 30 x 4 ## name titles aliases more_titles ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;lgl&gt; ## 1 Theon Greyjoy &lt;chr [3]&gt; &lt;chr [4]&gt; FALSE ## 2 Tyrion Lannister &lt;chr [2]&gt; &lt;chr [11]&gt; FALSE ## 3 Victarion Greyjoy &lt;chr [2]&gt; &lt;chr [1]&gt; TRUE ## 4 Will &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE ## 5 Areo Hotah &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE ## 6 Chett &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE ## 7 Cressen &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE ## 8 Arianne Martell &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE ## 9 Daenerys Targaryen &lt;chr [5]&gt; &lt;chr [11]&gt; FALSE ## 10 Davos Seaworth &lt;chr [4]&gt; &lt;chr [5]&gt; FALSE ## # … with 20 more rows Like map, you can specify the type of the output response using map2. This makes it very easy to format the output appropriately for your application. You can use functions with many arguments with map by using the pmap variant; here, you pass in a list of functions, which are identified by position (..1, ..2, ..3, etc). Note the .. - you are referencing the list first, and the index within the list argument 2nd. Purrr References The Joy of Functional Programming (for Data Science): Hadley Wickham’s talk on purrr and functional programming. ~1h video and slides. (The Joy of Cooking meets Data Science, with illustrations by Allison Horst) Pirating Web Content Responsibly with R and purrr (a blog post in honor of international talk like a pirate day) Happy R Development with purrr Web mining with purrr Text Wrangling with purrr Setting NAs with purrr (uses the naniar package) Mappers with purrr - handy ways to make your code simpler if you’re reusing functions a lot. Function factories - code optimization with purrr Stats and Machine Learning examples with purrr "],["high-performance-computing.html", "Module 14 High Performance Computing HPC References", " Module 14 High Performance Computing The material for this week will be provided in a lecture given by the Holland Computing Center. Here, I’m going to list off a few of the useful resources at the Holland Computing Center. An introduction to High Performance Computing Creating an account Connecting to the HCC clusters Basic Linux Commands (and a video tutorial) Using X11 Forwarding (how to get minimal graphical application support via SSH) Using R software on the cluster HPC References Working with the Unix Shell Software Carpentry lesson "],["tidy-models.html", "Module 15 Tidy Models", " Module 15 Tidy Models Tidymodels workflow Tidymodels is a set of packages designed to help with the modeling part of the tidy workflow shown above. We’ll be working off of the Tidymodels introduction. There are 5 articles in the sequence; we’ll work through them together in class. Please make sure you have the tidymodels package installed as well as the broom.mixed package. install.packages(&quot;tidymodels&quot;) install.packages(&quot;broom.mixed&quot;) "],["spatial-packages.html", "Module 16 Spatial packages References", " Module 16 Spatial packages Map Projections Matter! We’ll work off of this set of 3 tutorials: Drawing Beautiful maps with R, sf, and ggplot2: Basics Layers Layouts References Geocomputation with R - a whole textbook on spatial work in R, including ecology, marketing, transportation, and connecting to GIS software. Analyzing Honeybee Permits in Minneapolis "],["common-problems-and-general-computing-topics.html", "A Common Problems and General Computing Topics A.1 File Paths, Working Directories, and Reproducibility", " A Common Problems and General Computing Topics A.1 File Paths, Working Directories, and Reproducibility A.1.1 File paths A file path is how you tell the computer where to find a file. You might be familiar with the “C:\\Program Files\\” structure - that’s a file path telling Windows to look at the C: drive, in the Program Files folder. There are two types of file paths: absolute and relative. - An absolute file path is a file path that tells the computer the location of the file, regardless of where the computer is currently working from (regardless of the current working directory). An absolute file path is something like an address - no matter where you’re currently located, an address will give you the information necessary to get to the correct house. Examples: - C:\\USER\\DOCS\\LETTER.TXT (Windows) - \\\\SERVER01\\USER\\DOCS\\LETTER.TXT (Windows, for e.g. remote file servers) - /home/user/docs/Letter.txt (UNIX/macOS) A relative file path is a file path that tells the computer how to get to a file from its current location. To continue the analogy, a relative file path is like “Go down the hallway, turn left, take a right at the next hallway, and go to the third door on the right.” - it gives you information on how to get from your current location to the destination, but that information wouldn’t necessarily work for someone who’s in a different location. Examples: ../../Letter.txt\"` (this says go up two directories and look for Letter.txt) ./Letter.txt (this says look for Letter.txt in your current directory) ./data/Letter.txt (this says look for Letter.txt in the data folder in the current directory) If you are using a UNIX-like environment (Linux, macOS), you have an additional shortcut available: ~/ is the shortcut for the user’s home directory. So ~/ is equivalent to /home/ted/ as long as you’re logged in as ted. If you’re logged in as theodora, though, ~/ is equivalent to /home/theodora/. This YouTube video has a good explanation as well. A.1.2 Working Directory When you start a program on a computer, it starts with a working directory. In many cases, this may be the user’s home directory, but that’s not always the case. For instance, if you are working in an RStudio project, your working directory is the folder containing the .Rproj file. If you are compiling an Rmarkdown document, your working directory is the folder where the document is saved. Your working directory determines what relative file path you should be using. Here is a video showing how to change your working directory in SAS, and in R. A.1.3 Reproducibility Reproducibility is the idea that I should be able to run your code and get the same results you got. Ideally, to do this, I wouldn’t have to configure my computer in exactly the same way your computer is set up. Instead, ideally, your code will use relative file paths, with a working directory that is appropriate to the project set-up. Since we’re storing everything on GitHub for this class, a natural file setup is to have the project working directory be the same directory the git repository is based in (the directory should contain a .git folder, but you may have to view hidden files to see it). You can store your data in a data/ subfolder, your extra scripts in a code/ folder, etc., but all of the files you need to run the code should be included in the git repository unless they are too large for git. Then, when someone else clones your repository, they will have access to the data they need to run the code, and the code will be written with relative file paths that match the file structure. This also has the advantage of saving you tons of time trying to help your PI figure out how to run your code… you can direct them to GitHub, have them download the folder (or clone the repo, if they’re tech-savvy), and everything should just work. "],["other-resources-articles-and-food-for-thought.html", "B Other Resources, Articles, and Food for Thought B.1 Comparing Languages", " B Other Resources, Articles, and Food for Thought B.1 Comparing Languages Data Scientist’s Analysis Toolbox: Comparison of Python, R, and SAS Performance `r if (knitr::is_html_output()) ’ "],["references-7.html", "References", " References "]]
