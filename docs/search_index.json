[["index.html", "Stat 850: Computing Tools for Statisticians Introduction Forward Course Description Course Goals Course Objectives", " Stat 850: Computing Tools for Statisticians Susan Vanderplas Last Updated: 2021-04-22 Introduction Forward This textbook is intended to be a substitute for hours and hours of video lectures wherein I read code to you. That’s awful, and boring1, and honestly, as painful for me to have to listen to myself while editing the things as it is for you to watch them. Instead, I hope that you’ll be able to work through this book, week by week, over the course of Stat 850. I’ve included comics, snark, gifs, YouTube videos, and more, with the goal of making this a collection of the best information I could find on learning R and SAS and statistical programming. Course Description Introductions to statistical computing packages and document preparation software. Topics include: graphical techniques, data management, Monte Carlo simulation, dynamic document preparation, presentation software. Course Goals (Broad, amorphous conceptual things that are hard to measure) Learn how to use R and SAS for data analysis, data processing, and data visualization. Become familiar with the process, techniques, and goals of exploratory data analysis. Create, assess, and debug code effectively. Use online resources to find software to perform a task, comparing approaches taken by competing programs. Read error messages, find related problems in online forums, and isolate the conditions necessary to generate the error. Generate minimum working examples or reproducible examples of errors in order to ask for help effectively. Communicate statistical results using reproducible, dynamic tools. Understand the importance of reproducibility in scientific computation. Course Objectives (what you should be able to do at the end of this course) A. Clean and format the data appropriately for the intended analysis or visualization method. (Goals: 1) B. Explore a data set using numerical and visual summaries, developing questions which can be answered using statistics. (Goals: 1, 2) C. Evaluate methods or software to assess relevance to a problem. Compare similar options to determine which are more appropriate for a given application (Goals: 1, 3) D. Test and debug software, using the following sequence: (Goals: 3, 4) Reproduce the error in a new environment, Create a minimal reproducible example, Research the error message and evaluate online resources for relevance, Ask for help, describing the error or problem appropriately. E. Document the data, methods, and results of an analysis using reproducible methods. (Goals: 1, 2, 4) If you want boring, or have insomnia, check out this Microsoft Word tutorial from 1989. Youtube says it’s the most boring video ever made.↩︎ "],["tools.html", "Module 1 Tools for Statistical Computing Tools: Module Objectives 1.1 R and RStudio 1.2 SAS 1.3 Version Control with Git 1.4 LaTeX 1.5 Easy Extra Setup Script 1.6 Topic Sequencing 1.7 Using Version Control 1.8 References", " Module 1 Tools for Statistical Computing The goal of this class is to expose you to basic computing skills in R and SAS, which are two of the more common languages for statistical computing (python is the 3rd most common, and is particularly popular in data science and machine learning, but will not be explicitly taught in this class.) Since we’ll be learning how to use a bunch of different software, the first step is to get everything set up on your machine: git SAS 9.4 (or later) R (4.0 or higher) RStudio LaTeX You will also need to sign up for a GitHub account Tools: Module Objectives Set up R, Rstudio, SAS, LaTeX, and git on personal machines Use version control to track changes to documents and code 1.1 R and RStudio What is R? With a bonus short history lesson. R is a statistical computing language which originated as an open-source clone of Bell labs S computing language. S was inspired by Scheme, but also has features which are similar to Lisp. It is a scripting language (you don’t have to compile the code before it runs) and is natively accessed using a command-line prompt. One feature of R that is relatively unique is that it uses vector-based math, which means that mathematical operations on vectors occur for the entire vector without having to use loops to iterate through the vector line-by-line (this feature is more common in languages designed for data manipulation, like Matlab and Julia; it is rare in more general-purpose computing languages). R is optimized for working on data: unlike more general programming languages such as Python, R was built with the idea of facilitating data analysis. As a result, data structures in R tend to be more natural for statistical work than similar structures in Python or C, which can feel unwieldy. From a computer science perspective, though, R seems like an extremely odd language because the design choices that make data analysis easier are unconventional for more general-purpose languages. What is RStudio? Is it the same as R? RStudio is an integrated development environment(IDE) for R. Basically, it adds a pretty graphical layer on top of R, providing an easy way to develop R code, evaluate that code, and keep track of all of the variables which are available in the computing environment. RStudio contains integrations which provide syntax highlighting, code folding, basic checks (missing parentheses, etc.), debugging tools, and many other features. RStudio was designed around the idea of making R easier to use and making it easy to develop statistical software reproducibly. RStudio (the company) is responsible for adding many features to the R ecosystem which facilitate running statistical analyses and presenting the results in user-friendly ways. RStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question. Another useful explanation of R and RStudio can be found in Section 1 of ModernDive’s book 1.1.1 Getting Set up: R Note that the tutorial videos use versions of R that are old. You should be installing at least R 4.0 (if you have an older version, please update.) The basic installation process is the same, though, so the videos are still useful. R on Windows installation R on Mac installation R on Linux installation On Linux, instead of a YouTube video, you get a text-based tutorial. One of the Debian maintainers, Dirk Eddelbuettel, is also on R core, which means that R tends to work extremely well with Debian-based distributions, like Ubuntu and Linux Mint. R does work on RPM based distros, and you can of course also compile it from source for Arch/Gentoo, but I’ve stuck with Deb-based distributions for approximately 7 years because it seems to be a bit less hassle. Additional troubleshooting can be found here. Once you have R installed, try to run the following code, which will install (most of) the packages you need to run the code in this book. # Read in a list of all packages that are required pkgs &lt;- readLines(&quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/packages&quot;) # Remove any already installed packages from the list pkgs &lt;- setdiff(pkgs, installed.packages()) # The following code will not make a lot of sense... yet. Come back to it in a # few weeks and see how much you understand (or can decode) # This ensures that if the installation fails, the code will keep running try_install_pkg &lt;- function(...) try(install.packages(..., dependencies = T)) lapply(pkgs, try_install_pkg) list() pkgs &lt;- setdiff(pkgs, installed.packages()) if (length(pkgs) &gt; 0) { paste(&quot;The following packages did not install: \\n&quot;, paste(pkgs, collapse = &quot;\\n&quot;)) } If some packages did not install, feel free to post in Yellowdig with the error message(s) received and the list of packages you’re having trouble installing (or contact me and we’ll set up a time to debug). 1.1.2 Getting Set up: RStudio You can find RStudio at https://rstudio.com/products/rstudio/download/. You want the open source edition of RStudio Desktop. If you’re feeling brave, you can install the preview release - this may have a few bugs, but tends to be relatively stable and has the latest features. Please install RStudio 1.3.9+ for this class. If you’re on Linux, go ahead and import RStudio’s public key so that software validation works. (All of the commands are provided in the linked page) 1.1.3 Exploring RStudio The RStudio window will look something like this. In the top-left pane is the text editor. This is where you’ll do most of your work. In the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections. On the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab. On the bottom right, there are a set of tabs: - files (to give you an idea of where you are working, and what files are present), - plots (which will be self-explanatory), - packages (which extensions to R are installed and loaded), - the help window (where documentation will show up), and - the viewer window, which is used for interactive graphics or previewing HTML documents. To get started, type 2+2 into the console window and hit enter. Now, type 2+2 into the text editor and press the run button that is on the pane’s shortcut bar (or, you can hit Ctrl-Enter/CMD-Enter to send a single line to the console). If both of those things worked, you’re probably set up correctly! Next, try typing this into the text editor, then run the line. Look in the environment tab and see if you can see what has changed. a &lt;- 3 # store 3 in the variable a Your environment window should now look something like this (the .Last.value entry may not be there, and that’s ok) You can use the environment window to preview your data, check on the status of variables, and more. Note that while R is running, the window doesn’t update (so you can’t check on the status of a loop while the loop is running using the window). 1.2 SAS SAS is another extensively used statistical programming language. It is primarily used for mixed models and in the biostatistics community (for e.g. drug trials). For SAS installation, see Steve Westerholt. He manages UNL SAS installations. You can pay a small fee to get the full SAS installation, or you can use SAS community edition (but if you take this route, it may be more difficult to manage your paths, etc., and you may not be able to use SAS Markdown on your machine). You can also use the HCC environment (we will talk more about this in week 2 or thereabouts) set up for you to be able to do your homework. As long as you can run SAS code on a machine you have regular access to, I’m fine. Note SAS looks different on different machines. On Linux, SAS looks like you took a time machine back to the early 1990s. Screenshots from SAS will likely look very different on your machine than on mine. I will try to integrate screenshots from other OS’s where it matters.^[As I write this in late July 2020, I do not yet have access to a Windows or Mac machine. Sigh. XXX TODO: Get Windows/Mac screenshots XXX. I still don’t have access to a Windows or Mac machine, and it’s now April 2021. Sigh.] 1.2.1 SAS Windows/Panes The SAS toolbox has options to create new files, submit code, and more (but I mostly use keyboard shortcuts in the individual windows). This is probably one of the biggest things that’s different on Linux… The SAS Log gives you lots of detailed information about how your code ran – look here for errors and warnings. The SAS program editor is where you’ll be writing your code. If you want, you can write code in a more advanced text editor and copy/paste it into the log when you’re ready to run it. There are two different places your output may end up: if you’re using the old output system, you’ll get text output in the output pane. The old output system output pane. The new output system uses HTML and will output to your browser. You can navigate through your results using the results window The explorer window lets you access documentation, data, and more If you click on libraries, you get to this list: This area of the SAS explorer shows all of the libraries you have access to. Clicking on any one library will show you a list of datasets in that library You can then click on a dataset and you will get a spreadsheet-like view. 1.2.2 SAS Modules SAS is a very large set of programs. In this class, we’re primarily working with base SAS (the underlying language and interpreter), SAS/STAT (the statistical procedures, like PROC GLM), and SAS/IML, which is SAS’s version of a scripting language. IML allows you to implement your own procedures “from scratch.” Initially, we’ll primarily focus on SAS/IML, because it contains information parallel to what you need to know to start programming in R. It’s easier to teach general programming concepts at the same time, even if your typical SAS course would probably introduce you to the DATA step and simple PROC statements first. 1.3 Version Control with Git This section shows you how to install/set up Git. For information on how to actually USE git, see this subsection. Git is a program whose primary purpose is version control. Git tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history. When writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently overwrite your files, so long as you remember to commit frequently. The git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better. Go read Chapter 1. 1.3.1 Getting set up: GitHub See the instructions here 1.3.2 Getting set up: git See the instructions here Write down, or keep track of, the git installation path. This will make your life much easier. There is a troubleshooting guide that has some common problems which occur during git installation. 1.3.3 Introduce yourself to git You need to tell git what your name and email address are, because every “commit” you make will be signed. Follow the instructions here, or just wait and run the R script to install and configure R, LaTeX, and packages. 1.3.4 Optional: Install a git client Instructions I don’t personally use a git client other than RStudio, but you may prefer to have a client, especially if you anticipate doing lots of work in SAS. 1.3.5 Git and Github Slightly crude (but memorable) analogy (don’t click if you’re offended by PG13/R rated stuff) Git is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories. Git is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git. If you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else. Remember: any data you don’t have in 3 different places is data you don’t care about.2 1.4 LaTeX LaTeX is a typesetting program, which makes it different from most other document creation software, such as MS Word, which is “WYSIWYG” - what you see is what you get. In LaTeX, you’ll type in code to create a document, and LaTeX will compile the document into something pretty. The goal is that you have to think less about formatting and what goes on which page - LaTeX will handle that part for you - so that you can think more about the content. Why use LaTeX? (shamelessly stolen from FB, which apparently stole it from Tumblr) Latex is meant for you to focus on the content and not worry about the formatting - the computer optimizes figure placement according to directives you’ve given. In practice, it doesn’t usually work out like that, so there are programs like markdown which aim to simplify document creation even more to free you from the formatting that LaTeX requires. LaTeX is often used for typesetting statistical and mathematical papers because its equation editor is top notch. (It was actually written by Donald Knuth because he got so annoyed trying to write his dissertation that he took some time off to write TeX first, and then used it to write his dissertation).3 If you want to install LaTeX so that you can use it outside of R/RStudio, go here for operating system specific instructions. If you don’t care about being able to access it outside of R, then you can put it off until you run the code in the next section - one of the packages that is installed is tinytex, which will make it easy to install LaTeX within R. We’ll work with LaTeX later in the semester, but for now, we’ll be primarily working with Markdown, which is much simpler. Here’s a quick cheatsheet. 1.5 Easy Extra Setup Script Copy this script into a text editor in RStudio on your machine. It will ask you for some input - your name, email address, etc. – Make those changes in the text document, and then source the document. It will run for a while, installing most of the packages you’ll need for this course. Read the code file if you want to know what it’s doing - I’ve provided short explanations of what each package does (and anyways, you shouldn’t run code on your machine if you don’t understand what it does). source(&quot;code/01_setup_install.R&quot;) This script will set up git for you (e.g. the introduce yourself to git section above) and then will install tinytex, which is a version of LaTeX that is very simple and easy to install. If you want to install a different version of LaTeX instead (like MikTex or TexLive) feel free to do so, and feel free to comment out the lines that install tinytex and the local tex installation. 1.6 Topic Sequencing In several places in this class, you’ll have to use material that you haven’t been formally taught yet. I will do my absolute best to provide thorough instructions, help you along as much as I can, and generally provide enough support that you can muddle through. But it’s going to be hard to teach you everything you need to e.g. analyze some data, before providing you the opportunity to SEE that data using visualization packages. And it’s silly to teach you plotting before you know how to read data in. But to teach you how to read data in, you need to be able to take a look at the data, and plots are the best way to do that. To do any of this stuff, you need to know about functions, but it can be easier to figure out how to run a function than to write a function. You see my problem. So instead, what I’m going to do is to leave you lots of comments as to what a piece of code does when I’m using things you haven’t been formally shown yet. Then, you can copy/paste/modify those pieces of code, and if they break, you can ask why and we’ll dig into it (breaking code is usually a good thing, because it means you’re learning how to program). For each chapter, focus on learning how to write code that accomplishes that chapter’s objectives. If you understand some of the code you’re modifying that covers other topics not in that chapter, so much the better. But it’s not an expectation or a requirement. If you’re confused, please post on the class message boards so that those who have seen this material before can help you out. SAS quick start guide R quick start guide 1.7 Using Version Control I’ve set this class up so that you’ll be using version control from the very beginning. Not only will this help you to learn good habits, it will also give you a platform for collaboration, hosting your work online, and more. If that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything. In this class, we’ll be using Github Classroom. Basically, this allows me to set up a template repository for each assignment. You’ll accept the assignment, which will create a copy of the repository on your GitHub account, and then your work will be saved to your repository using a fairly standard workflow which will be discussed below. When you submit your assignment, you’ll copy the link to the commit you want to be graded, and upload that to Canvas. I will clone your repository, compile your files (I’ll change the SAS path if necessary), and grade the compiled result. So, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is primarily my higher-level understanding of git - I do not have any idea how it works on the backend, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion. 1.7.1 Create a Repository Repositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. To create a repository, you can start with your local computer first, or you can start with the online repository first. Both methods are relatively simple, but the options you choose depend on which method you’re using, so be careful not to get them confused. 1.7.1.1 Local repository first Let’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it. If you want, you can use the following R code to set this up: dir &lt;- &quot;./hello-world-1&quot; if (!dir.exists(dir)) { dir.create(dir) } file &lt;- file.path(dir, &quot;README&quot;) if (!file.exists(file)) { writeLines(&quot;hello world&quot;, con = file) } To create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in git init Alternately, if you prefer a GUI (graphical user interface) approach, that will work too: Open Rstudio Project (upper right corner) -&gt; New Project -&gt; Existing Directory. Navigate to the directory. (In your new project) Tools -&gt; Project options -&gt; Git/SVN -&gt; select git from the dropdown, initialize new repository. RStudio will need to restart. Navigate to your new Git tab on the top right. Screen capture video of the steps described above to create a git repository in RStudio The next step is to add our file to the repository. Using the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\". Using the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button. Screen capture video of the steps described above to add and commit a file in RStudio The final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you’re logged in. Create a new repository using the “New” button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time). You’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go. Connect your local git repository to GitHub 1.7.1.2 GitHub repository first In the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally). GUI method: Log into GitHub and create a new repository Initialize your repository with a README Copy the repository location by clicking on the “Code” button on the repo homepage Open RStudio -&gt; Project -&gt; New Project -&gt; From version control. Paste your repository URL into the box. Hit enter. Make a change to the README file Click commit, then push your changes Check that the remote repository (Github) updated Video of the GUI method for creating a repository starting with GitHub Command line method: Log into GitHub and create a new repository Initialize your repository with a README Copy the repository location by clicking on the “Code” button on the repo homepage Navigate to the location you want your repository to live on your machine. Clone the repository by using the git shell or terminal: git clone &lt;your repo url here&gt;. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git Make a change to your README file and save the change Commit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking). Push your changes to the remote (GitHub) repository and check that the repo has updated: git push 1.7.2 Adding files git add tells git that you want it to track a particular file. git add diagram: add tells git to add the file to the index of files git monitors. You don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about,” and what it thinks belongs in each commit. If you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (sort-of, kind-of) equivalent to clicking the check box. 1.7.2.1 What files should I add to git? Git is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB4. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free. In general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository). You should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good. I highly recommend that you make a point to only add and commit files which you consciously want to track. 1.7.3 Staging your changes In RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file. More formally, staging is saying “I’d like these changes to be added to the current version, I think.” Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out. Individually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do). 1.7.4 Committing your changes A git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache. Here, we commit the red line as a change to our file. In general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things: You’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing) When you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred. 1.7.5 Pushing and Pulling When you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document. If you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document. For this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible. Pulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up). Pushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine. In general, your workflow will be Clone the project or create a new repository Make some changes Stage the changes with git add Commit the changes with git commit Pull any changes from the remote repository Resolve any merge conflicts Push the changes (and merged files) with git push If you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways. 1.8 References Git “Hello World” Tutorial on GitHub Crash course on git (30 minute YouTube video) Git and GitHub for poets YouTube playlist (this is supposed to be the best introduction to Git out there…) More advanced git concepts, in comic form, by Erika Heidi Yes, I’m aware that this sounds paranoid. It’s been a very rare occasion that I’ve needed to restore something from another backup. You don’t want to take chances. I knew a guy who had to retype his entire masters thesis from the printed out version the night before it was due because he had stored it on a network drive that was decommissioned. You don’t want to be that person.↩︎ Amusingly, knitr was written in much the same manner. Yihui Xie had to substitute-teach ISU’s version of 850 on the day we covered Sweave (a predecessor to knitr). He got so frustrated teaching the class that he went home and started writing knitr. Later, he developed Rmarkdown, bookdown, blogdown, and several other packages aimed at making writing documents + code easier to handle. Moral of the story - if you get frustrated with the tools you have, you’re in good company. Use it as fuel to make better tools.↩︎ Yes, I’m seriously pushing it with this book; several of the datasets are ~30 MB↩︎ "],["intro-prog.html", "Module 2 Introduction to Statistical Programming A quick note on Rmarkdown Intro to Programming: Module Objectives Definitions Statistical Programming Languages 2.1 Variable types 2.2 Data structures 2.3 Control structures 2.4 Overgrown Calculators References and Links", " Module 2 Introduction to Statistical Programming The only way to learn how to program effectively is to take something that works, break it, and then fix it again. There’s plenty of theory and you should definitely learn that, but fundamentally, if you are not regularly breaking code, you’re probably not programming. Figure 2.1: This is basically the class summarized. The goal for this chapter (and several chapters to come) is that you can modify example code and adapt it to the problem at hand. This is the best way to learn how to program, but it means you may break the code and not know how to fix it. If that happens, please try the following steps: Google the error and see if you can understand why it happened. Consult with a classmate to see if they can understand where things broke. Post to the discussion board and see if anyone in the class can understand where things broke. (When you do this, post all of the code relevant to the problem, and the error you’re getting, so that your classmates can replicate the problem) If you do not hopelessly break code during this chapter, then please do your best to help others who may not have previously programmed (or previously programmed in these languages). While writing this chapter, I came across about 10 errors in SAS that I’d never encountered before. If all else fails, while you’re waiting for someone to help you figure out what an error message means… try this approach. A quick note on Rmarkdown In this module, you’ll be submitting homework in Rmarkdown using SASmarkdown chunks. I’ve structured the assignment so that you shouldn’t have to do much configuring of the markdown document itself, aside from providing the location of the SAS executable file on your hard drive (look for the SASHome directory). However, if you find yourself having issues with SASmarkdown, feel free to check out the section on markdown in next week’s module. The use of markdown for this week’s assignment is a “trust me, and you’ll get comfortable with this eventually” sort of thing. Intro to Programming: Module Objectives Identify variable types, data structures, and common reserved words in R and SAS Use control structures (loops and conditionals) effectively Use matrix algebra functionality in R and SAS to perform basic calculations More informally, the goal is to get familiar with the basics of each programming language, and to show you where to find references for how to use each command – because (at least) half of programming is knowing where to look something up. I’m teaching SAS very differently I’m definitely teaching SAS differently than it is normally taught. This is so that we don’t have to do half the semester in SAS and half in R - I’d rather teach the concepts and show you how they’re implemented than split them up by language. BUT, this means that some of the things we’re doing first in SAS are things you wouldn’t normally do until you were already proficient in SAS. It also means that SAS is probably going to seem even more oddly organized when taught this way than it actually is (and it is oddly organized, in my opinion). We’re going to start with SAS IML (programming concepts) and then talk about the DATA step. We’ll use some procedures implicitly along the way, but hopefully that will make sense in context. Then, we’ll work on the PROCs (SQL, Transpose, and graphing) - in greater detail. If you’ve used R or SAS a lot in the past Note: If you’ve programmed before, this chapter is going to seem very … boring. Sorry, there’s no help for that. Some of your classmates haven’t ever so much as written “Hello World,” and we have to get them up to speed. If you’re bored, or feel like you know this material, skim through it anyways just to confirm (and if I’m doing something that’s really out there, or there’s an easier way to do it, tell me!). Then you can either find something in the references that you don’t know already (the book Advanced R is always a great place to start if you want to be quickly confused), or help your classmates that are less experienced. Definitions Many programming resources talk about 3, or 5, or 10 core concepts in any programming language. In this module, we’re going to discuss the generic concepts, and then how these concepts are implemented in R and SAS. Interestingly, the “core concepts” aren’t necessarily the same across lists. So, here is a consensus list of concepts which are generic across languages and usually important Variables - a symbolic name or reference to some kind of information. In the expression a + b &gt; a, both a and b are variables. Variables may have a specific type (what data can be stored in the variable), scope (where the variable can be accessed), location (in memory). Here is a nice explanation of the difference between variables in programming and variables in math. Conditional statements (if statements) - These statements allow the program to handle information adaptively - if a statement is true, one set of instructions will be used, and if the statement is false, a different set of instructions will be used. Looping and iteration - An iteration is any time a sequence of steps is executed. Most languages have several different types of loops or iteration: for loops, which allow for the sequence of steps to be executed a specific number of times, while loops, which allow for the sequence of steps to be executed while a conditional statement is true, recursion, where a block of code calls itself. Data types and data structures - these concepts determine what information a variable can hold. Data types are lower-level, simple objects (floating-point numbers, integers, boolean T/F, characters, strings). Data structures may include lists (sequences of many objects) and vectors (sequences of many objects of the same type), dictionaries (a list of key-value pairs), objects (data structures which may hold multiple related pieces of information). Functions, or self-contained modules of code that accomplish a particular task. Syntax, the set of rules that define which combinations of symbols consist of correctly structured and interpretable commands in the language. Tools, the set of external programs which may help with development and writing code. Some common tools are IDEs (Integrated Development Environments), which may correct syntax and typos, organize files for you, allow you to keep track of which variables you have defined, and assist you with code organization and navigation. Other tools include compilers (which take human-written code and translate it into efficient machine code), version control systems (which help you track changes to code over time), debuggers, and documentation generators. Not all of these tools are necessary for all languages - scripting languages such as python and R do not require compilers by default, for instance. Sequence of commands: It’s important to have the right commands in the right order. Some recipes, like bread dough, are flexible, and you can add the ingredients in almost any order, but in other recipes, the order matters as much as the correct quantity of ingredients (try putting the cheese powder in before the noodles are boiled when making macaroni and cheese. Yuck.). Programming tends to be like these less flexible recipes. Statistical Programming Languages Having established the generic definitions of the concepts which apply to almost any programming language, we now must examine how R and SAS implement these concepts. R and SAS are both statistical programming languages - they are specifically designed to work with data, which means that they make compromises that other languages do not in order to make it easier to write code where the data (rather than the functions, classes, methods, or objects) are the primary concern. Both R and SAS have long histories. SAS in particular dates back to the 1960s, and has syntax which is unique compared to more modern languages such as C, python, Java, and R. R’s predecessor, S, dates back to 1976 and was designed for internal use at Bell Labs. The histories of both languages are useful in understanding why they are optimized for their respective tasks, but are not essential for this course (so read them at your leisure). The biggest difference between R and SAS (at a fundamental level) is that R is a functional language - it consists mainly of functions, which can (and do) manipulate objects, including other functions. SAS, on the other hand, is a procedural language - most SAS programs follow a specific series of steps, known as “proc”s. Procs are essentially functions (or compositions of multiple functions), but in SAS, it is simpler to think of an analysis as a series of procedural steps; in R, there are steps, but they may be implemented in a more flexible way (depending on the analysis). Another interesting feature of SAS is that it’s really several languages - some commands work in PROC IML (interactive matrix language) but not in a DATA step. When looking for help in SAS, make sure you’re referencing the correct part of the language documentation. Basic Syntax and Cheatsheets SAS Cheatsheet (from another class like this) SAS Cheatsheet (by SAS) R Cheatsheet - this is a simplified cheat sheet offered by RStudio. R Cheatsheet (classic) SAS Programming for R Users (free book) I kept the classic R reference card by my computer for about 5 years, and referenced it at least once or twice a day for that entire period. There will be other cheat sheets and reference cards scattered through this book because if you can’t remember something’s name, you might be able to remember where it is on the reference card (or at least, that’s how I learned R). 2.1 Variable types Variable types are sufficiently different in R and SAS that we will cover R first, then SAS. For a general overview, though, this video, titled ‘Why TRUE + TRUE = 2’ is an excellent introduction. 2.1.1 R In R, there are 4 commonly-used types: Type Description character holds text-based information: “abcd” or “3.24a” are examples of values which would be stored as characters in R logical holds binary information: 0/1, or FALSE/TRUE. Logical variables are stored as single bit information (e.g. either a 0 or 1), but display as TRUE and FALSE (which are reserved words and constants). integer holds (as you might expect) integers. Note that integers are handled differently than doubles (floating point numbers), but in general, R will implicitly convert integers to doubles to avoid common pitfalls with integer divison (which does not allow for decimals). double holds floating point numbers. By default, most numeric variables in R are doubles. You can test to see whether a variable holds a value of a specific type using the is.xxx() functions, which are demonstrated below. You can convert a variable of one type to another with as.xxx() functions. You can test what type a variable is using typeof(). Note that &lt;- is used for assigning a value to a variable. So x &lt;- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’.” Character variables x &lt;- &quot;R is awesome&quot; typeof(x) [1] &quot;character&quot; is.character(x) [1] TRUE is.logical(x) [1] FALSE is.integer(x) [1] FALSE is.double(x) [1] FALSE Logical Variables x &lt;- FALSE typeof(x) [1] &quot;logical&quot; is.character(x) [1] FALSE is.logical(x) [1] TRUE is.integer(x) [1] FALSE is.double(x) [1] FALSE It is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false.5 Integer Variables x &lt;- 2 typeof(x) [1] &quot;double&quot; is.character(x) [1] FALSE is.logical(x) [1] FALSE is.integer(x) [1] FALSE is.double(x) [1] TRUE Wait, 2 is an integer, right? 2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically. x &lt;- 2L # The L immediately after the 2 indicates that it is an integer. typeof(x) [1] &quot;integer&quot; is.character(x) [1] FALSE is.logical(x) [1] FALSE is.integer(x) [1] TRUE is.double(x) [1] FALSE is.numeric(x) [1] TRUE Double Variables x &lt;- 2.45 typeof(x) [1] &quot;double&quot; is.character(x) [1] FALSE is.logical(x) [1] FALSE is.integer(x) [1] FALSE is.double(x) [1] TRUE is.numeric(x) [1] TRUE Numeric Variables A fifth common “type”6, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing. The numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work. The general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double. 2.1.1.1 Type Conversions R will generally work hard to seamlessly convert variables to different types. So, for instance, TRUE + 2 [1] 3 2L + 3.1415 [1] 5.1415 &quot;abcd&quot; + 3 Error in &quot;abcd&quot; + 3: non-numeric argument to binary operator This conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops. When you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be: as.double(TRUE) + 2 [1] 3 as.double(2L) + 3.1415 [1] 5.1415 as.numeric(&quot;abcd&quot;) + 3 Warning: NAs introduced by coercion [1] NA When we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value. There’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA. If you are unsure what the type of a variable is, use the typeof() function to find out. w &lt;- &quot;a string&quot; x &lt;- 3L y &lt;- 3.1415 z &lt;- FALSE typeof(w) [1] &quot;character&quot; typeof(x) [1] &quot;integer&quot; typeof(y) [1] &quot;double&quot; typeof(z) [1] &quot;logical&quot; 2.1.1.2 Factors In R, there is one other type of variable to know about, and that is a factor. Factors are basically labeled integers. Instead of storing the data as a string or character, R instead stores the data as a series of integers, and then stores a separate table mapping the integers to labels. This is technically more efficient (which was important when computers had extremely limited memory), but it is also a pain in the rear (that’s a technical term). Factors are the default way to store characters for most base R functions. Or rather, they were. In R 4.0, the default way to read data in will change from stringsAsFactors = T to stringsAsFactors = F. You can read about why factors aren’t ideal here, which helps explain why this change was made. Depending on what version of R you have installed, you may run into errors related to factors, or not. Because R 4.0 is so new (released in May 2020) most of the tutorials online will probably have behavior that isn’t matched by your R installation. I’m new enough to R 4.0 that I’m not sure when factor related errors will pop up. Other reasons to learn factors besides for debugging purposes: They allow you to control the order of things in graphs, tables, and models They allow you to easily change category labels without having to sort through an entire data table … I’m sure there are more, but I’m drawing a blank at the moment Factors example In this example, we’ll use a data.frame, which you can think of as a spreadsheet-type table. We’ll work with data frames later in much more detail, but for now, I’m mostly trying to show you a real-life situation that happens ALL the time, with the hopes that you’ll recognize the error when/if you encounter it. The data frame isn’t the important part. Let’s look at the names of the months: month.name [1] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; &quot;May&quot; &quot;June&quot; [7] &quot;July&quot; &quot;August&quot; &quot;September&quot; &quot;October&quot; &quot;November&quot; &quot;December&quot; df &lt;- data.frame(num = 1:12, name = month.name, stringsAsFactors = T) # I&#39;m putting the argument in so that this is still relevant when everyone # switches to R 4.0. Even with stringsAsFactors = F, factors are still useful # and we still need to work with them. # # Any time you create a data frame in base R, you should be watchful for errors # that are based on strings being converted to factors. str(df) &#39;data.frame&#39;: 12 obs. of 2 variables: $ num : int 1 2 3 4 5 6 7 8 9 10 ... $ name: Factor w/ 12 levels &quot;April&quot;,&quot;August&quot;,..: 5 4 8 1 9 7 6 2 12 11 ... Notice that as soon as we make that data.frame, the months are converted into a factor variable? The other big problem is that the order of the factor levels is … not what we’d normally want. We don’t want alphabetical ordering of month names - they have a different, implicit, and natural order. We could get this same behavior without the data.frame, but this is where it shows up most often. month_fct &lt;- factor(month.name) # the order is still not exactly what we&#39;d want it to be To fix this, we can explicitly specify that we’re dealing with a factor, and what we want the levels to be. If you specify the levels manually (instead of letting R do the work for you) then you get to determine the order. month_fct &lt;- factor(month.name, levels = month.name) str(month_fct) Factor w/ 12 levels &quot;January&quot;,&quot;February&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... We can even be more explicit: month_fct &lt;- factor(month.name, levels = month.name, ordered = T) str(month_fct) Ord.factor w/ 12 levels &quot;January&quot;&lt;&quot;February&quot;&lt;..: 1 2 3 4 5 6 7 8 9 10 ... Making the factor ordered lets us explicitly say which levels are less than other levels. Factors are technically integers, with labels that are stored as an attribute. That doesn’t mean you can do math with them, though. month_fct[1] + month_fct[2] Warning in Ops.ordered(month_fct[1], month_fct[2]): &#39;+&#39; is not meaningful for ordered factors [1] NA Often, years or dates or other numeric-like information will end up as factor variables. When this happens, you need to be a little bit careful. # This works pretty naturally for months, right? as.numeric(month_fct) [1] 1 2 3 4 5 6 7 8 9 10 11 12 yfact &lt;- factor(2000:2020, levels = 2000:2020) yfact [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 [16] 2015 2016 2017 2018 2019 2020 21 Levels: 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 ... 2020 # But, this does not... as.numeric(yfact) [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 as.character(yfact) # gets the labels [1] &quot;2000&quot; &quot;2001&quot; &quot;2002&quot; &quot;2003&quot; &quot;2004&quot; &quot;2005&quot; &quot;2006&quot; &quot;2007&quot; &quot;2008&quot; &quot;2009&quot; [11] &quot;2010&quot; &quot;2011&quot; &quot;2012&quot; &quot;2013&quot; &quot;2014&quot; &quot;2015&quot; &quot;2016&quot; &quot;2017&quot; &quot;2018&quot; &quot;2019&quot; [21] &quot;2020&quot; as.numeric(as.character(yfact)) # gets the info we want [1] 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 [16] 2015 2016 2017 2018 2019 2020 When converting factors with numeric labels, you need to first convert the factor to a character, and then to a numeric variable. That will get the information you actually want back out. Try It Out Create variables string, integer, decimal, and logical, with types that match the relevant variable names. string &lt;- integer &lt;- decimal &lt;- logical &lt;- Can you get rid of the error that occurs when this chunk is run? logical + decimal integer + decimal string + integer What happens when you add string to string? logical to logical? Solutions string &lt;- &quot;hi, I&#39;m a string&quot; integer &lt;- 4L decimal &lt;- 5.412 logical &lt;- TRUE logical + decimal [1] 6.412 integer + decimal [1] 9.412 as.numeric(string) + integer Warning: NAs introduced by coercion [1] NA &quot;abcd&quot; + &quot;defg&quot; Error in &quot;abcd&quot; + &quot;defg&quot;: non-numeric argument to binary operator TRUE + TRUE [1] 2 In R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value. 2.1.2 SAS In SAS, there are two basic variable types: numeric and character variables. SAS does not differentiate between integers and floats and doubles. Functionally, though, the same basic operations can be performed in SAS. As with R, SAS does attempt to implicitly convert variable types, and will notify you that the conversion has taken place in the log file. 2.1.2.1 Type Conversions SAS will attempt to implicitly convert variables when: a character value is assigned to a previously defined numeric variable a character value is used in arithmetic operations a character value is compared to a numeric value using a comparison operator (&lt;, &gt;, &lt;=, &gt;=) a character value is specified in a function that takes numeric arguments Implicit conversion does not occur in WHERE statements. (This will make more sense later, but is here for reference) Manual type conversions If you want to manually convert a value, use the INPUT statement. Unlike in R, the INPUT statement has the ability to read numbers which are formatted differently. For instance data set1; x = 3; y = &#39;3.1415&#39;; z = x * y; put z; run; data set2; x = 3; y = &#39;3.1415&#39;; z = x * y; put z; /* print to log */ x = &#39;3.14159&#39;; /* x previously had a number in it, so it will be converted to a number here */ put x; /* print to log */ zz = y &lt;= 2; /* comparison operator: y will be converted */ put zz; /* print to log */ run; Notice that in SAS, zz, which is the result of the logical statement y&lt;=2, is a numeric variable. The value 0 signifies that the comparison was false. SAS does not have a logical data type, it uses the numeric variable with 0:=FALSE, 1:=TRUE. Try it out Create variables string1 and string2 that each have text/character values. “Bob” and “Jane” might be good options. How does logical operation work with actual character values? What happens if you use string1 and add 3 to it? Solutions 6 data set1; 7 string1 = &#39;Bob&#39;; 8 string2 = &#39;Jane&#39;; 9 x = string1 &lt; string2; 10 put x=; /* This prints the result to the log */ 11 run; x=1 NOTE: The data set WORK.SET1 has 1 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 12 SAS will actually compare strings based on the first letter: Bob comes before Jane, so Bob &lt; Jane. 6 data set2; 7 string1 = &#39;Bob&#39;; 8 y = string1 + 3; 9 put y=; 10 run; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 8:7 NOTE: Invalid numeric data, string1=&#39;Bob&#39; , at line 8 column 7. y=. string1=Bob y=. _ERROR_=1 _N_=1 NOTE: Missing values were generated as a result of performing an operation on missing values. Each place is given by: (Number of times) at (Line):(Column). 1 at 8:15 NOTE: The data set WORK.SET2 has 1 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds The . in SAS is a missing value (like NA in R). So SAS is behaving basically like R does: it complains about the fact that you asked it to add a string to a number, and then it stores the result as a missing value. 2.2 Data structures Data structures are more complex arrangements of information than single variables. Of primary interest in statistical programming are the following types of structures:   Homogeneous Heterogeneous 1d vector list 2d matrix data frame (R) or data set (SAS) nd array (R) In the table above, homogeneous means that all entries in the structure must be of the same type. Heterogeneous means that the entries are allowed to be of different types. Figuring out what to call these types with two languages is hard - in SAS, an array is a group of columns of a data set, but in R, it’s a multi-dimensional matrix. In this section, we’ll discuss the generic concepts relevant to both languages. The differences between the two languages will be discussed as appropriate. As there are more similarities than differences, it’s easier to do this in a single section rather than duplicating half of the content. 2.2.1 Homogeneous data structures (R and SAS) R does not have scalar types - even single-value variables are technically vectors of length 1. SAS does have scalar types. If we try to create a heterogeneous vector in R, using the concatenate function, c(), which combines scalar entries into a vector, what happens? c(1, 2, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) [1] &quot;1&quot; &quot;2&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; Because there were 3 character entries, the entire vector is now a character vector. This behavior can cause some errors - for instance, R does not read in numeric data formatted with commas as numeric data. You may thus get the result x &lt;- c(356, 452, &quot;1,325&quot;) mean(x) Warning in mean.default(x): argument is not numeric or logical: returning NA [1] NA If you are reading in data from a file, this will cause some issues - the whole column of data will be formatted as characters. Keep an eye out for errors of this type. Jenny Bryan has an excellent set of images to demonstrate R data types as legos. She’s released them under an open license, so I am shamelessly stealing them. Logical vector Factor vector Integer and Numeric vectors These correspond to the 1-dimensional homogeneous data structures. Similarly, models can be made for 2-dimensional and 3-dimensional homogeneous data structures7: Vector (1D) Matrix (2D) Array (3D) 2.2.2 Heterogeneous data structures The heterogeneous data types are not much harder to grasp, as they’re mostly different ways to combine various homogeneous data types. 2.2.2.1 Lists A list is, well, a list - a sequence of potentially different-typed values. Unlike when concatenating values, the list() command in R allows each value to keep its natural type. You can access elements of a list using [] (this will extract a subset of the list items) or [[]], which will extract a single item from the list. (there will be more on this in the Indexing section below) Basic List Syntax in R x &lt;- list(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, 1, 2, 3) x [[1]] [1] &quot;a&quot; [[2]] [1] &quot;b&quot; [[3]] [1] &quot;c&quot; [[4]] [1] 1 [[5]] [1] 2 [[6]] [1] 3 x[[3]] [1] &quot;c&quot; x[[4]] + x[[5]] [1] 3 x[1:2] # This will work [[1]] [1] &quot;a&quot; [[2]] [1] &quot;b&quot; x[[1:2]] # This won&#39;t work Error in x[[1:2]]: subscript out of bounds The lego version of a list looks like this: Figure 2.2: A list of 4 vectors. Even though the vectors in the list are all the same size in this case, they don’t have to be, because they’re not organized in any sort of cohesive rectangle shape. A data frame (see below) is essentially a list where all of the components are vectors or lists of the same length. Indexing in Lists Some lists (and data frames) consist of named variables. These list components can be accessed either by index (as above) or by name, using the $ operator. Names which have spaces or special characters must be enclosed in backticks (next to the 1 on the keyboard). Named components can also be accessed using the [[ ]] operator. dog &lt;- list(name = &quot;Edison Vanderplas&quot;, age = 8, breed = &quot;Jack Russell Terrorist&quot;, `favorite toy` = &quot;a blue and orange stuffed duck. Or rawhide.&quot;, `link(video)` = &quot;https://youtu.be/zVeoQTOTIuQ&quot;) dog $name [1] &quot;Edison Vanderplas&quot; $age [1] 8 $breed [1] &quot;Jack Russell Terrorist&quot; $`favorite toy` [1] &quot;a blue and orange stuffed duck. Or rawhide.&quot; $`link(video)` [1] &quot;https://youtu.be/zVeoQTOTIuQ&quot; dog$name [1] &quot;Edison Vanderplas&quot; dog$breed [1] &quot;Jack Russell Terrorist&quot; dog$`favorite toy` [1] &quot;a blue and orange stuffed duck. Or rawhide.&quot; dog[[&quot;link(video)&quot;]] [1] &quot;https://youtu.be/zVeoQTOTIuQ&quot; You can get a sense of the structure of a list (or any other object) in R using the str() command. str(dog) List of 5 $ name : chr &quot;Edison Vanderplas&quot; $ age : num 8 $ breed : chr &quot;Jack Russell Terrorist&quot; $ favorite toy: chr &quot;a blue and orange stuffed duck. Or rawhide.&quot; $ link(video) : chr &quot;https://youtu.be/zVeoQTOTIuQ&quot; Recursive lists Lists can also contain other lists. When accessing a list-within-a-list, just add another index or name reference (see below). grocery_list &lt;- list( dairy = list(&quot;asiago&quot;, &quot;fontina&quot;, &quot;mozzarella&quot;, &quot;blue cheese&quot;), baking = list(&quot;flour&quot;, &quot;yeast&quot;, &quot;salt&quot;), canned_goods = list(&quot;pepperoni&quot;, &quot;pizza sauce&quot;, &quot;olives&quot;), meat = list(&quot;bacon&quot;, &quot;sausage&quot;, &quot;anchovies&quot;), veggies = list(&quot;bell pepper&quot;, &quot;onion&quot;, &quot;scallions&quot;, &quot;tomatoes&quot;, &quot;basil&quot;) ) ick &lt;- c(grocery_list[[4]][2:3], grocery_list$canned_goods[[3]]) ick [[1]] [1] &quot;sausage&quot; [[2]] [1] &quot;anchovies&quot; [[3]] [1] &quot;olives&quot; crust_ingredients &lt;- c(grocery_list$baking, &quot;water&quot;) crust_ingredients [[1]] [1] &quot;flour&quot; [[2]] [1] &quot;yeast&quot; [[3]] [1] &quot;salt&quot; [[4]] [1] &quot;water&quot; essential_toppings &lt;- c(grocery_list$dairy[3], grocery_list$canned_goods[2]) essential_toppings [[1]] [1] &quot;mozzarella&quot; [[2]] [1] &quot;pizza sauce&quot; yummy_toppings &lt;- c(grocery_list$dairy[c(1, 2, 4)], grocery_list$meat[1], grocery_list[[5]][c(3, 5)]) yummy_toppings [[1]] [1] &quot;asiago&quot; [[2]] [1] &quot;fontina&quot; [[3]] [1] &quot;blue cheese&quot; [[4]] [1] &quot;bacon&quot; [[5]] [1] &quot;scallions&quot; [[6]] [1] &quot;basil&quot; Basic List Syntax in SAS There are also lists in SAS IML which function similarly to lists in R. To create a named object in a list, precede the name with #. In SAS, the $ operator can be used to get items from a list, using either name or numeric references. 6 7 proc iml; NOTE: IML Ready 8 grocery_list = [ 9 #dairy = [&quot;asiago&quot;, &quot;fontina&quot;, &quot;mozzarella&quot;, &quot;blue 9 ! cheese&quot;], 10 #baking = [&quot;flour&quot;, &quot;yeast&quot;, &quot;salt&quot;], 11 #canned = [&quot;pepperoni&quot;, &quot;pizza sauce&quot;, &quot;olives&quot;], 12 #meat = [&quot;bacon&quot;, &quot;sausage&quot;, &quot;anchovies&quot;], 13 #veggies= [&quot;bell pepper&quot;, &quot;onion&quot;, &quot;scallions&quot;, &quot;tomatoes&quot;, 13 ! &quot;basil&quot;] 14 ]; 15 16 /* print only works on matrices and vectors */ 17 /* so we&#39;ll cheat and load another library to print lists */ 18 19 package load ListUtil; NOTE: Module LISTPRINT defined. NOTE: Module _SUBLISTPRINT1 defined. NOTE: Module _SUBLISTPRINT2 defined. NOTE: Module _STRUCTMATRIX defined. NOTE: Module _STRUCTTABLE defined. NOTE: Module _STRUCTLIST defined. NOTE: Module _STRUCTLIST1 defined. NOTE: Module STRUCTGET defined. NOTE: Module STRUCT defined. 20 21 /* run ListPrint(grocery_list); */ 22 /* This would print the thing, but it&#39;s long */ 23 24 ick = [grocery_list$&quot;canned&quot;$3, grocery_list$4$2, 24 ! grocery_list$4$3]; 25 crust = grocery_list$&quot;baking&quot;; 26 call ListAddItem(crust, &quot;water&quot;); 26 ! /* add an item to a list */ 27 essential_toppings = [grocery_list$&quot;dairy&quot;$3, 27 ! grocery_list$&quot;canned&quot;$2]; 28 yummy_toppings = [grocery_list$&quot;dairy&quot;$1, 28 ! grocery_list$&quot;dairy&quot;$2, 29 grocery_list$&quot;dairy&quot;$4, grocery_list$&quot;meat&quot;$1, 29 ! grocery_list$5$3] ; 30 /* The || is a concatenation operator, like c(). */ 31 /* It is inefficient for large data sets */ 32 33 run ListPrint(ick); NOTE: Module ROWVEC loaded from the storage SASHELP.IMLMLIB. 34 run ListPrint(crust); 35 run ListPrint(yummy_toppings); 36 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 37 ——— List = ick——— Item 1 olives Item 2 sausage Item 3 anchovies ——— List = crust——— Item 1 flour Item 2 yeast Item 3 salt Item 4 water ——— List = yummy_toppings——— Item 1 asiago Item 2 fontina Item 3 blue cheese Item 4 bacon Item 5 scallions Try it out Using the list of pizza toppings above as a starting point, make your own list of pizza toppings organized by grocery store section (approximately). Create your own vectors of yummy, essential, and ick toppings, using R and SAS. 2.2.2.2 Data frames (R only) A data frame is a special type of list - one in which each element in the list is a vector of the same length. If you put these vectors side-by-side, you get a table of data that looks like a spreadsheet. The lego version of a data frame looks like this: Figure 2.3: A data frame with data frame 4 columns. A data frame is essentially a list where all of the components are vectors or lists, and are constrained to have the same length. Basic Data Frame Syntax When you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head() command shows the first 6 rows of a data frame (enough to see what’s there, not enough to overflow your screen). head(mtcars) ## A data frame included in base R mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... You can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. mtcars$gpm &lt;- 1/mtcars$mpg # gpm is sometimes used to assess efficiency summary(mtcars$gpm) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615 summary(mtcars$mpg) Min. 1st Qu. Median Mean 3rd Qu. Max. 10.40 15.43 19.20 20.09 22.80 33.90 Often, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary(). summary(mtcars) mpg cyl disp hp Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 Median :19.20 Median :6.000 Median :196.3 Median :123.0 Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 drat wt qsec vs Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 am gear carb gpm Min. :0.0000 Min. :3.000 Min. :1.000 Min. :0.02950 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 1st Qu.:0.04386 Median :0.0000 Median :4.000 Median :2.000 Median :0.05208 Mean :0.4062 Mean :3.688 Mean :2.812 Mean :0.05423 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:0.06483 Max. :1.0000 Max. :5.000 Max. :8.000 Max. :0.09615 dim(mtcars) [1] 32 12 nrow(mtcars) [1] 32 ncol(mtcars) [1] 12 Missing variables in an R data frame are indicated with NA. Creating an R data frame math_and_lsd &lt;- data.frame(lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41), test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97)) math_and_lsd lsd_conc test_score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 # add a column - character vector math_and_lsd$subjective &lt;- c(&quot;finally coming back&quot;, &quot;getting better&quot;, &quot;it&#39;s totally better&quot;, &quot;really tripping out&quot;, &quot;is it over?&quot;, &quot;whoa, man&quot;, &quot;I can taste color, but I can&#39;t do math&quot;) math_and_lsd lsd_conc test_score subjective 1 1.17 78.93 finally coming back 2 2.97 58.20 getting better 3 3.26 67.47 it&#39;s totally better 4 4.69 37.47 really tripping out 5 5.83 45.65 is it over? 6 6.00 32.92 whoa, man 7 6.41 29.97 I can taste color, but I can&#39;t do math Try it out The dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below. data(state) state_facts &lt;- data.frame(state.x77) state_facts &lt;- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) # State names were stored as row labels # Store them in a variable instead, and add it to the data frame row.names(state_facts) &lt;- NULL # get rid of row names head(state_facts) state Population Income Illiteracy Life.Exp Murder HS.Grad Frost Area 1 Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 2 Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 3 Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 4 Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 5 California 21198 5114 1.1 71.71 10.3 62.6 20 156361 6 Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 How many rows and columns does it have? Can you find at least 3 ways to get that information? The Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands. Calculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density? Solutions # 3 ways to get rows and columns str(state_facts) &#39;data.frame&#39;: 50 obs. of 9 variables: $ state : chr &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... $ Population: num 3615 365 2212 2110 21198 ... $ Income : num 3624 6315 4530 3378 5114 ... $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... $ Life.Exp : num 69 69.3 70.5 70.7 71.7 ... $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... $ HS.Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... $ Area : num 50708 566432 113417 51945 156361 ... dim(state_facts) [1] 50 9 nrow(state_facts) [1] 50 ncol(state_facts) [1] 9 # Illiteracy state_facts$TotalNumIlliterate &lt;- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) # Population Density state_facts$PopDensity &lt;- state_facts$Population * 1e3/state_facts$Area # in people per square mile # minimum population state_facts$state[which.min(state_facts$PopDensity)] [1] &quot;Alaska&quot; Advanced Data Frames: Tibbles and List-columns If at this point you’re bored because you’ve seen this material before, keep reading to find out about tibbles, list columns and other ways to make data frames even more powerful. A tibble is a fancy data frame that is optimized to work with the tidyverse, which is a collection of R packages that make data wrangling (getting the data clean and ready for analysis) easier. You can read about tibbles here. You like data frames? Lists? Let’s put some lists inside a data frame! (All about list columns) Let’s start with the lego picture: (The full explanation is available in slide form here). A list is just another object that could be stored in a data frame! It is a “generalized vector” in that each entry in a list can be thought of as another list - so a list is really a vector of lists. List-columns make it possible to store e.g. whole data sets in a nested, organized way. Another useful feature is that each entry in a list-column doesn’t have to be the same length, which makes it easier to store “ragged” data. You can see a couple of examples here (but they assume that you know things that you’ll only learn in a few modules). It is worth coming back to this link later in the book. I will try to remind you. Data Sets (SAS) The SAS data set structure is similar to a R data frame. In SAS, missing values are indicated with . SAS datasets also come with a description which is attached to the table. The descriptor portion of the data set records names of variables (and attributes), numbers of observations, and date/time stamps of creation and updates. Creating a SAS data set In the next code chunk, we’ll create a data set using a SAS Data step. We’ll talk more about the anatomy of a SAS command later, but for now, notice that I’m specifying some metadata (the title), telling SAS what the variable names are (Drugs, Score), and then providing some data (indicated by the datalines statement). 6 /* Data source: http://blog.yhat.com/posts/7-funny-datasets.html 6 ! */ 7 8 data mathLSD; 9 title &#39;Average math test scores under the influence of LSD&#39;; 10 input Drugs Score; 11 datalines; NOTE: The data set WORK.MATHLSD has 7 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 ; 20 21 /* Describe the dataset */ 22 proc datasets; 23 contents data = mathLSD; 24 run; 25 NOTE: PROCEDURE DATASETS used (Total process time): real time 0.01 seconds cpu time 0.02 seconds 26 proc print data = mathLSD; 27 run; NOTE: There were 7 observations read from the data set WORK.MATHLSD. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds Average math test scores under the influence of LSD Directory Libref WORK Engine V9 Physical Name /tmp/SAS_workE53E0004A1B8_silicon Filename /tmp/SAS_workE53E0004A1B8_silicon Inode Number 11278091 Access Permission rwx—— Owner Name susan File Size 4KB File Size (bytes) 4096 # Name Member Type File Size Last Modified 1 MATHLSD DATA 128KB 04/22/2021 08:58:29 2 SASMAC3 CATALOG 20KB 04/22/2021 08:58:29 3 SET1 DATA 128KB 04/22/2021 08:58:29 4 SET2 DATA 128KB 04/22/2021 08:58:29 Average math test scores under the influence of LSD Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:30 Observation Length 16 Last Modified 04/22/2021 08:58:30 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workE53E0004A1B8_silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278103 Access Permission rw-rw-r– Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 The last two blocks are SAS procedures (PROCs). In the first block, I’m asking SAS to describe the contents of the mathMJ dataset. In the second block, I’m telling SAS to print the whole mathMJ dataset out. 2.2.3 Indexing The 1, 2, and multi-dimensional homogeneous data types should be familiar from e.g. linear algebra and calculus. Single elements of a vector can be extracted using single square brackets, e.g. x[1] will get the first element of the vector x. In a matrix, elements are indexed as row, column, so to get the (2, 2) entry of a matrix x, you would use x[2,2]. This is extended for multi-dimensional arrays in R, with each dimension added, e.g. x[3,1,2] or x[4, 3, 2, 1]. To get a full row or column from a matrix (in both SAS and R) you would use x[1,] (get the first row) or x[,3] (get the 3rd column). To select multiple rows or columns from a matrix, you would use x[, c(1, 3)] in R or x[,{1 3}] in SAS - both options get the first and third column of the matrix, with all rows of data included. In both R and SAS, a:b where a and b are numbers will form a sequence from a to b by 1s. So 1:4 is 1, 2, 3, 4. This is often used to get a set of rows or columns: x[3:4, 1:2]. R matrix example x &lt;- matrix(1:20, nrow = 5, byrow = T) # Create a matrix with values 1 to 20, 5 rows, and fill by row x [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 [4,] 13 14 15 16 [5,] 17 18 19 20 x[3:4, 1:2] [,1] [,2] [1,] 9 10 [2,] 13 14 # Gets a submatrix SAS matrix example In SAS, the same basic code works (though matrix definition is a bit more manual). proc iml; /* Interactive Matrix Language */ x = {1 2 3 4 5, 6 7 8 9 10, 11 12 13 14 15, 16 17 18 19 20}; y = x[3:4, 1:2]; print x; /* Here, print is used instead of put */ print y; quit; /* exit proc IML */ The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:30 Observation Length 16 Last Modified 04/22/2021 08:58:30 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_work1E450004A1E7_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Both R and SAS are 1-indexed languages, so the elements of a list or vector are indexed as 1, 2, 3, 4, …8 As R has logical vectors, it is possible to index a vector using a logical vector of the same length. Try it out (From project Euler) If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000. Hint: The modulo operator, %%, gives the integer remainder of one number divided by another. So a %% b gives the integer remainder when dividing a by b. Modular division is often used to find multiples of a number. R solution x &lt;- 1:999 # all nums below 1000 m3 &lt;- (x %% 3) == 0 # multiple of 3 m5 &lt;- (x %% 5) == 0 # multiple of 5 m3or5 &lt;- m3 | m5 sum(x[m3or5]) [1] 233168 SAS solution 6 data tmp; 7 do x = 1 to 999; 8 output; 9 end; 10 run; NOTE: The data set WORK.TMP has 999 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 11 12 proc summary data=tmp; /* Summarize data */ 13 where (mod(x, 3) = 0) | (mod(x, 5) = 0); 14 /* Keep only obs where x is divisible by 3 or 5 */ 15 16 var x; /* what variable we want the summary for */ 17 18 output out=sum_x sum=; /* output sum_x to a new dataset */ 19 run; NOTE: There were 466 observations read from the data set WORK.TMP. WHERE (MOD(x, 3)=0) or (MOD(x, 5)=0); NOTE: The data set WORK.SUM_X has 1 observations and 3 variables. NOTE: PROCEDURE SUMMARY used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 21 proc print data = sum_x; /* print our sum_x dataset */ 22 run; NOTE: There were 1 observations read from the data set WORK.SUM_X. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds Obs TYPE FREQ x 1 0 466 233168 Note on the SAS code: where statements allow you to select part of the data for further processing. There was a note earlier about the fact that type conversion doesn’t happen in where clauses… this is one of those clauses. We’ll get into where clauses in more detail later, in module 5. Most complicated structures in R are actually lists underneath. You should be able to access any of the pieces of a list using a combination of named references and indexing. If you have trouble distinguishing between $, [, and [[, you’re not alone. The R for Data Science book has an excellent illustration, which I will summarize for you here in abbreviated form (pictures directly lifted from the book). R4DS indexing illustration x x[1] x[[1]] x[[1]][[1]] 2.3 Control structures 2.3.1 If statements If statements are just about as simple in programming as they are in real life. Figure 2.4: Source. I’ve actually met some programmers who talk like this in real life. General structure of an if statement In general, the structure of an if statement is if (condition) then { # do something here } If the condition is true, the inner code will be executed. Otherwise, nothing happens. You can add an else statement that will execute if the condition is not true if (condition) then { # do something } else { # do a different thing } And in some languages, you can even have many sets of if statements: if (condition) { # do something } else if (condition 2) { # do something else } else { # do a third thing } Note that this could also be written (perhaps more clearly) as: if (condition) { # do something } else { if (condition 2) { # do something else } else { # do a third thing } } That is, condition 2 is only checked once it is known that condition is false. Often, programmers use logic flow maps, like the one shown below, to map out a logical sequence and ensure that every possible value is handled appropriately. Example: If/then logic in SAS and R The syntax for conditional statements using if/then logic is shown below using an example where Santa must determine which members of a household will receive a toy for Christmas and which members will receive coal.9 In R tmp &lt;- data.frame(name = c(&quot;Alex&quot;, &quot;Edison&quot;, &quot;Susan&quot;, &quot;Ryan&quot;), status = c(&quot;naughty&quot;, &quot;nice&quot;, NA, &quot;neutral&quot;), stringsAsFactors = F) # Santa&#39;s decision process if (tmp$status == &quot;naughty&quot;) { tmp$present &lt;- &quot;coal&quot; } else { tmp$present &lt;- &quot;toy&quot; } Warning in if (tmp$status == &quot;naughty&quot;) {: the condition has length &gt; 1 and only the first element will be used tmp name status present 1 Alex naughty coal 2 Edison nice coal 3 Susan &lt;NA&gt; coal 4 Ryan neutral coal What happened? When evaluating if statements, R does not evaluate each entry in the vector tmp$status separately. Instead, it takes the first value and issues a warning message. One option would be to use a loop, and examine each row in the data set separately. We’ll talk about loops in the next subsection. Another option is to use the ifelse() function, which is ifelse(condition, thing to do if condition is true, thing to do if condition is false) tmp$present &lt;- ifelse(tmp$status == &quot;naughty&quot;, &quot;coal&quot;, &quot;toy&quot;) tmp name status present 1 Alex naughty coal 2 Edison nice toy 3 Susan &lt;NA&gt; &lt;NA&gt; 4 Ryan neutral toy When R evaluates a missing value, (so ? NA == “naughty”), the result is NA. This is fine for us - if we don’t have data on whether someone is naughty or nice, maybe we don’t need to give them a present at all. But “neutral” is evaluated as getting a toy. Do we want that to happen? Maybe not. We might have to nest ifelse statements to solve this issue… tmp$present &lt;- ifelse(tmp$status == &quot;naughty&quot;, &quot;coal&quot;, ifelse(tmp$status == &quot;nice&quot;, &quot;toy&quot;, NA)) tmp name status present 1 Alex naughty coal 2 Edison nice toy 3 Susan &lt;NA&gt; &lt;NA&gt; 4 Ryan neutral &lt;NA&gt; In SAS In a data step: 6 data santa; 7 input name $ status $; 8 datalines; NOTE: The data set WORK.SANTA has 4 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 13 ; 14 15 /* Modify santa_list and make a new dataset, present_list */ 16 data presents; 17 set santa; 18 if status = &quot;naughty&quot; then present = &quot;coal&quot;; 19 else present = &quot;toy&quot;; 20 run; NOTE: There were 4 observations read from the data set WORK.SANTA. NOTE: The data set WORK.PRESENTS has 4 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ! /* must end with run if no datalines option */ 21 22 proc print data=presents; 23 run; NOTE: There were 4 observations read from the data set WORK.PRESENTS. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy Note that ., or missing data is handled the same as ‘nice.’ That might not be what we wanted… this is the natural thing to do, right? 6 data santa; 7 input name $ status $; 8 datalines; NOTE: The data set WORK.SANTA has 4 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 13 ; 14 15 /* Modify santa_list and make a new dataset, present_list */ 16 data presents; 17 set santa; 18 if status = &quot;naughty&quot; then present = &quot;coal&quot;; 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ______ 22 19 ! .); ERROR: Undeclared array referenced: else. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 388 19 ! .); ERROR 22-322: Syntax error, expecting one of the following: !, !!, &amp;, (, *, **, +, &#39;,&#39;, -, /, &lt;, &lt;=, &lt;&gt;, =, &gt;, &gt;&lt;, &gt;=, AND, EQ, GE, GT, IN, LE, LT, MAX, MIN, NE, NG, NL, NOTIN, OR, [, ^=, {, |, ||, ~=. ERROR 388-185: Expecting an arithmetic operator. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 202 19 ! .); ERROR 202-322: The option or parameter is not recognized and will be ignored. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 388 19 ! .); ERROR 388-185: Expecting an arithmetic operator. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = ____ 202 19 ! .); ERROR 202-322: The option or parameter is not recognized and will be ignored. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = 19 ! .); _ 22 ERROR 22-322: Syntax error, expecting one of the following: +, =. 19 else (if status = &quot;nice&quot; then present = &quot;toy&quot; else present = 19 ! .); _ 76 ERROR 76-322: Syntax error, statement will be ignored. 20 run; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 19:54 NOTE: The SAS System stopped processing this step because of errors. NOTE: Due to ERROR(s) above, SAS set option OBS=0, enabling syntax check mode. This prevents execution of subsequent data modification statements. WARNING: The data set WORK.PRESENTS may be incomplete. When this step was stopped there were 0 observations and 4 variables. WARNING: Data set WORK.PRESENTS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ! /* must end with run if no datalines option */ 21 22 proc print data=presents; 23 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on page 4. SAS doesn’t handle nested if statements very well - they can be ambiguous. Instead, SAS documentation suggests using do; and end; to denote the start and end points of each if statement (like the {} in R). 6 data santa; 7 input name $ status $; 8 datalines; NOTE: The data set WORK.SANTA has 0 observations and 2 variables. WARNING: Data set WORK.SANTA was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ; 14 data presents; 15 set santa; 16 if status = &quot;naughty&quot; then 17 do; 18 present = &quot;coal&quot;; 19 end; 20 else if status = &quot;nice&quot; then 21 do; 22 present = &quot;toy&quot;; 23 end; 24 else 25 do; 26 present = .; 27 end; 28 run; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 26:17 NOTE: The data set WORK.PRESENTS has 0 observations and 3 variables. WARNING: Data set WORK.PRESENTS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 proc print data=presents; 31 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5. Interestingly, if you set a character variable to be missing, SAS converts it to ‘.’ So, if we actually want to have the value be missing, we can set it to an empty string. 6 data santa; 7 input name $ status $; 8 datalines; NOTE: The data set WORK.SANTA has 0 observations and 2 variables. WARNING: Data set WORK.SANTA was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ; 14 data presents; 15 set santa; 16 if status = &quot;naughty&quot; then 17 do; 18 present = &quot;coal&quot;; 19 end; 20 else if status = &quot;nice&quot; then 21 do; 22 present = &quot;toy&quot;; 23 end; 24 else 25 do; 26 present = &#39;&#39;; 27 end; 28 run; NOTE: The data set WORK.PRESENTS has 0 observations and 3 variables. WARNING: Data set WORK.PRESENTS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 proc print data=presents; 31 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5. Now things work the way we expected them to work. There are more complicated if-statement like control structures, such as switch statements, which can save time and typing. In the interests of simplicity, we will skip these for now, as any conditional can be implemented with sequences of if statements in the proper order. If you would like to read about switch statements, here are links to SAS case statement documentation and base R switch statement explanation and documentation. Try it out The sample() function selects a random sample of entries from a vector. Suppose we sample a random vector \\(x\\) with 10 entries. Write one or more if statements to fulfill the following conditions if \\(x\\) is divisible by 2, \\(y\\) should be positive; otherwise, it should be negative. if \\(x\\) is divisible by 3, \\(y\\) should have a magnitude of 2; otherwise, it should have a magnitude of 1. It may be helpful to define separate variables y_mag and y_sign and then multiply them afterwards. Once you have found the value of \\(y\\) compute \\(\\text{sum}(x * y)\\). You may use the following R and SAS code skeletons to set the problem up. set.seed(342502837) x &lt;- sample(1:50, size = 20, replace = F) # Conditional statements go here sum(x * y) [1] 1567.609 proc iml; call randseed(342502837); x = sample(1:50, 20)`; create sampledata from x [colname = &quot;x&quot;]; append from x; close; quit; data xy; set sampledata; /* Conditional statements go here */ /* Leave this so that the code below works */ res = x * y; run; proc summary data=xy; /* Summarize data */ var res; /* what variable we want the summary for */ output out=tmpsum sum=; /* output tmpsum to a new dataset */ run; proc print data = xy; /* print our original dataset to check result */ var x y res; sum res; run; proc print data = tmpsum; /* print our tmpsum dataset */ run; R Solution set.seed(342502837) x &lt;- sample(1:50, size = 20, replace = F) y_sign &lt;- ifelse(x %% 2 == 0, 1, -1) y_mag &lt;- ifelse(x %% 3 == 0, 2, 1) y &lt;- y_sign * y_mag sum(x * y) [1] 157 SAS Solution 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 call randseed(342502837); 8 x = sample(1:50, 20)`; 9 create sampledata from x [colname = &quot;x&quot;]; 10 append from x; 11 close; 12 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 data xy; 15 set sampledata; ERROR: File WORK.SAMPLEDATA.DATA does not exist. 16 17 y_sign = 0 * x; 18 y_mag = 0 * x; 19 20 /* Conditional statements go here */ 21 if MOD(x, 2) = 0 then y_sign = 1; 22 else y_sign = -1; 23 if MOD(x, 3) = 0 then y_mag = 2; 24 else y_mag = 1; 25 26 y = y_sign * y_mag; 27 res = x * y; 28 run; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.XY may be incomplete. When this step was stopped there were 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 proc summary data=xy; /* Summarize data */ 31 var res; /* what variable we want the summary for */ 32 33 output out=tmpsum sum=; /* output tmpsum to a new dataset */ 34 run; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TMPSUM may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE SUMMARY used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 35 36 37 proc print data = xy; /* print our original dataset to check 37 ! result */ 38 var x y res; 39 sum res; 40 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 41 42 proc print data = tmpsum; /* print our tmpsum dataset */ 43 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. See this to understand how the print statement works and how to add column summary values. 2.3.2 Loops Often, we need to do a single task many times - for instance, we may need to calculate the average data value for each week, using daily data. Rather than typing out 52 different iterations of the same code, it is likely easier to type out one single block of code which contains the steps necessary to complete one instance of the task, and then leverage variables to ensure that each task is completed the correct number of times, using the correct inputs. Let us start with the most generic loop written in pseudocode (code that won’t work, but provides the general idea of the steps which are taken) loop_invocation(iteration variable, exit condition) { # Steps to repeat } We use the loop_invocation function to indicate what type of loop we use. We have at least one iteration variable that indicates where in the looping process we currently are. This may be an index (if we want to do something 500 times, it would take values from 1 to 500), or it may take a more complicated sequence of values (for instance, if we are testing convergence, we might put some sort of delta variable as the iteration variable). Most loops also have an explicit exit condition that is part of the loop invocation; more rarely, a loop may depend on break statements that cause the control flow of the code to exit. Without some sort of exit condition, our program would run forever, which is… not optimal. 2.3.2.1 Count controlled loops (FOR loops) In a for loop, the steps in the loop body repeat a specified number of times. That is, for each value in a sequence, the steps within the loop are repeated. Example: Santa and if/else + loops in R (plus some debugging strategies) For instance, suppose we want to revisit our R Santa example from the previous section. The original if/else code we wrote in R didn’t work, because R evaluates if statements using a single (scalar or vector of length 1) condition. If we add a loop around that code, we can evaluate only one row at a time. We need to check every row, so we’ll iterate over 1:nrow(tmp) - it’s better to get the upper bound from the data frame, rather than just using 4 - if we add another entry, the code will still work if we’re using nrow(tmp) to define how many iterations we need. We start by defining our data frame: tmp &lt;- data.frame(name = c(&quot;Alex&quot;, &quot;Edison&quot;, &quot;Susan&quot;, &quot;Ryan&quot;), status = c(&quot;naughty&quot;, &quot;nice&quot;, NA, &quot;neutral&quot;), stringsAsFactors = F) And then we add the basic loop syntax: for (i in 1:nrow(tmp)) { } For some reason, i is often used as the iteration variable (with j and k for nested loops). What this loop says is that i will first take on the value 1, then 2, then 3, then 4. On each iteration, i will advance to the next value in the vector of options we have provided. Now we need to add the middle part by adapting the conditional statement we used before so that it looks at only the ith row. I’ve also added the catch-all else condition that assigns NA for any value that isn’t “naughty” or “nice.” It’s good practice to initialize your variable (create a column for it) ahead of time and set the variable to a default value. tmp$present &lt;- NA # Initialize column and set to NA by default for (i in 1:nrow(tmp)) { # Santa&#39;s decision process if (tmp$status[i] == &quot;naughty&quot;) { tmp$present[i] &lt;- &quot;coal&quot; } else if (tmp$status[i] == &quot;nice&quot;) { tmp$present[i] &lt;- &quot;toy&quot; } else { tmp$present[i] &lt;- NA_character_ # use a special NA value that has # character type to avoid any issues } } Error in if (tmp$status[i] == &quot;naughty&quot;) {: missing value where TRUE/FALSE needed Well, that didn’t work! We can see that the loop stopped at i = 3 by printing out the value of i - because the loop failed, i will still contain the value which caused the loop to stop. i [1] 3 tmp[i,] # print tmp at that point name status present 3 Susan &lt;NA&gt; &lt;NA&gt; Combining this information with the error above, we can guess that R stopped evaluating the loop because the if statement returned NA (missing) instead of TRUE or FALSE. if/else statements in R can’t evaluate to NA, so we need to restructure our conditional statement - first, we’ll test for NA values, then, we can test for naughty and nice, and we’ll keep the catch-all statement at the bottom. We’ll test for an NA value using the function is.na(). tmp$present &lt;- NA # Initialize column and set to NA by default for (i in 1:nrow(tmp)) { # Santa&#39;s decision process if (is.na(tmp$status[i])) { tmp$present[i] &lt;- NA_character_ } else if (tmp$status[i] == &quot;naughty&quot;) { tmp$present[i] &lt;- &quot;coal&quot; } else if (tmp$status[i] == &quot;nice&quot;) { tmp$present[i] &lt;- &quot;toy&quot; } else { tmp$present[i] &lt;- NA_character_ } } tmp name status present 1 Alex naughty coal 2 Edison nice toy 3 Susan &lt;NA&gt; &lt;NA&gt; 4 Ryan neutral &lt;NA&gt; Now the if/else logic works exactly as intended. This is longer than the version using ifelse(), but it is perhaps more readable. In most cases in R and SAS, it is possible to write code without needing loops at all, because both languages are vector-based - they will often use vectorized functions which implicitly loop over each row without having to write a loop to do so. ifelse() is a vectorized version of if() {} else {}. Here is an example of the most basic for loop logic - printing the numbers 1 through 10 - in both R and SAS. SAS code is provided for both PROC IML and DATA steps. For loops in R # R Example loop for (i in 1:10) { print(i) } [1] 1 [1] 2 [1] 3 [1] 4 [1] 5 [1] 6 [1] 7 [1] 8 [1] 9 [1] 10 “For loops” in SAS IML (using do) 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 8 do i = 1 to 10; 9 print i; 10 end; 10 ! /* This ends the loop definition */ 11 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. “For loops” in a SAS DATA step 6 data A; 7 do i = 1 to 10; 8 put i=; 9 end; /* This ends the loop definition */ 10 run; NOTE: The data set WORK.A has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. Another explanation of for loops is available at Khan Academy. While the most straighforward (and common) case of for-loop use in practice is to count from 1 to N, both R and SAS allow for loops to use other sequence structures. Other sequences in loops in R R allows loops to occur over any vector… even randomly generated numbers, or nonnumeric vectors (say, a character vector of URLs). x &lt;- rnorm(5) # Generate 5 normal (0,1) samples for (i in x) { print(i^2) } [1] 4.500349 [1] 3.820737 [1] 2.656216 [1] 1.120775 [1] 2.10086 We can also iterate by non-integer values using seq(from = , to = , by = ) # This loop counts down in 1/2 units from 5 to 0 for (i in seq(5, 0, -.5)) { # do nothing } Other sequence structures in SAS for loops We can iterate by non-integer values: 6 data A; 7 y = 0; 8 do i = 5 to 0 by -0.5; 9 put i=; 10 end; 11 run; NOTE: The data set WORK.A has 0 observations and 2 variables. WARNING: Data set WORK.A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 4,5,7. We can even add additional conditions: 6 data A; 7 y = 0; 8 do i = 5 to 0 by -0.5 while (i**2 &gt; 1); 9 put i=; 10 end; 11 run; NOTE: The data set WORK.A has 0 observations and 2 variables. WARNING: Data set WORK.A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. Try it out (in R) The beepr package plays sounds in R to alert you when your code has finished running (or just to annoy your friends and classmates). (Documentation) We’ll learn more about packages in the next chapter, but for now, just go with it. You can install the package using the following command: install.packages(&quot;beepr&quot;) (if you are using Linux you will also need to make sure one of paplay, aplay, or vlc is installed) Load the library and write a for loop which plays the 10 different sounds corresponding to integers 1 through 10. library(beepr) # load the beepr library beep(sound = 1) # sound is any integer between 1 and 10. It may be helpful to add the command Sys.sleep(5) into your loop to space out the noises so that they can be heard individually. Solution library(beepr) for (i in 1:10) { beep(sound = i) Sys.sleep(5) } Try it out (in SAS) Write a for loop which will output the first 30 fibbonacci numbers. You can use the following code as a starting point: 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 8 current = 1; 9 prev = 0; 10 11 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ERROR: Errors printed on pages 4,5,7. Solution 6 /* SAS IML example loop */ 7 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 8 current = 1; 9 prev = 0; 10 11 do i = 1 to 30; 12 new = current + prev; 13 prev = current; 14 current = new; 15 print current; 16 end; 16 ! /* This ends the loop definition */ 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 ERROR: Errors printed on pages 4,5,7. 2.3.2.2 Condition-controlled loops (WHILE, DO WHILE) Frequently, we do not know how many times a loop will need to execute a priori. We might be converging on a value, and want to repeat the calculation until the new value is within an acceptably epsilon of the previous iteration. In these cases, it can be helpful to use a WHILE loop, which loops while the condition is true (another variant, the do-while loop, is similar, except that a do-while loop will always execute once, and checks the condition at the end of the iteration). If a WHILE loop condition is never falsified, the loop will continue forever. Thus, it is usually wise to include a loop counter as well, and a condition to terminate the loop if the counter value is greater than a certain threshold. Example: Let’s solve the Basel problem in R and SAS using WHILE loops - we’ll repeat the calculation until the value changes by less than 0.000001. The Basel problem is the problem of calculating the precise infinite summation \\[\\sum_{n=1}^\\infty \\frac{1}{n^2}\\] We’ll stick to calculating it computationally. In R # Start out by defining your starting values outside of the loop i &lt;- 1 basel_value &lt;- 0 # initial guess prev_basel_value &lt;- -Inf # previous value while (abs(basel_value - prev_basel_value) &gt; 0.000001) { prev_basel_value &lt;- basel_value # update condition basel_value &lt;- basel_value + 1/i^2 i &lt;- i + 1 # Prevent infinite loops if (i &gt; 1e6) { break } # Monitor the loop to know that it&#39;s behaving if (i %% 200 == 0) { print(c(&#39;i = &#39; = i, &#39;prev&#39; = prev_basel_value, &#39;current&#39; = basel_value, diff = basel_value - prev_basel_value)) } } i = prev current diff 2.000000e+02 1.639896e+00 1.639922e+00 2.525189e-05 i = prev current diff 4.000000e+02 1.642425e+00 1.642431e+00 6.281368e-06 i = prev current diff 6.000000e+02 1.643263e+00 1.643266e+00 2.787060e-06 i = prev current diff 8.000000e+02 1.643682e+00 1.643683e+00 1.566414e-06 i = prev current diff 1.000000e+03 1.643933e+00 1.643934e+00 1.002003e-06 i [1] 1001 basel_value [1] 1.643935 prev_basel_value [1] 1.643934 In SAS 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 i = 1; 8 basel = 0; 9 prev = -1; 10 do while((basel - prev) &gt; 1e-6); 11 prev = basel; 12 basel = basel + 1/i**2; 12 ! /* ** is the exponent operator */ 13 i = i + 1; 14 15 if i &gt; 1e6 then 16 do; 17 leave; 18 end; 19 20 if MOD(i, 200) = 0 then 21 do; 22 print i, prev, basel; 23 end; 24 end; 25 26 print i, basel; 27 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 28 ERROR: Errors printed on pages 4,5,7. Another explanation of while loops is available at Khan Academy. Try it out Write a while loop in R and in SAS to calculate \\(\\displaystyle \\lim_{x \\rightarrow 4} \\frac{2 - \\sqrt{x}}{4-x}\\) by starting at 3 and halving the distance to 4 with each iteration. Exit the loop when you are within 1e-6 of the value computed on the previous iteration, or when you are within 1e-6 from 4. Which exit condition did you hit first? How do you know? Solutions x &lt;- 3 dist &lt;- 4 - x current_value &lt;- 0 prev_value &lt;- -Inf while (abs(current_value - prev_value) &gt; 1e-6 &amp; dist &gt; 1e-6) { prev_value &lt;- current_value dist &lt;- dist/2 x &lt;- 4 - dist current_value &lt;- (2 - sqrt(x))/(4-x) } c(x = x, dist = dist, current_value = current_value, d_value = abs(current_value - prev_value)) x dist current_value d_value 3.999939e+00 6.103516e-05 2.500010e-01 9.536961e-07 Before \\(x\\) got to 4 - 1e-6, the change in f(x) became less than 1e-6. 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 x = 3; 8 dist = 4 - x; 9 fx = 0; 10 prev_fx = 1; 11 dfx = abs(fx - prev_fx); 12 do while(dfx &gt; 1e-6 &amp; dist &gt; 1e-6); 13 prev_fx = fx; 14 dist = dist/2; 15 x = 4 - dist; 16 fx = (2 - sqrt(x))/(4 - x); 17 dfx = abs(fx - prev_fx); 18 end; 19 20 print x, dist, fx, dfx; 21 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 ERROR: Errors printed on pages 4,5,7. 2.3.2.3 Other Loops and Interative Structures There are many different ways to implement iteration in any language, including very low-level controls like repeat (in R). Higher level iteration may include a FOREACH loop, where a series of commands is applied to a list or vector (the *apply commands in R are examples of this). An additional method of iteration that requires functions is the recursion (where a function calls itself). In every case, these alternative loop structures can be translated to for or while loops. 2.4 Overgrown Calculators While R and SAS are both extremely powerful statistical programming languages, the core of both languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in R and SAS. In this section, we will essentially be using both R and SAS as overgrown calculators. Operation R SAS Addition + + Subtraction - - Elementwise Multiplication * # Matrix/Vector Multiplication %*% * Division \\ \\ Elementwise Exponentiation ^ ## Matrix Exponentiation ^ ** Matrix Transpose t(A) `A\\`` R basic mathematical operators # transpose these to make row vectors to match SAS x &lt;- t(1:10) y &lt;- t(seq(3, 30, by = 3)) x + y [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 4 8 12 16 20 24 28 32 36 40 x - y [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] -2 -4 -6 -8 -10 -12 -14 -16 -18 -20 x * y [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 3 12 27 48 75 108 147 192 243 300 x / y [,1] [,2] [,3] [,4] [,5] [,6] [,7] [1,] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 [,8] [,9] [,10] [1,] 0.3333333 0.3333333 0.3333333 x^2 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 1 4 9 16 25 36 49 64 81 100 t(x) %*% y [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 3 6 9 12 15 18 21 24 27 30 [2,] 6 12 18 24 30 36 42 48 54 60 [3,] 9 18 27 36 45 54 63 72 81 90 [4,] 12 24 36 48 60 72 84 96 108 120 [5,] 15 30 45 60 75 90 105 120 135 150 [6,] 18 36 54 72 90 108 126 144 162 180 [7,] 21 42 63 84 105 126 147 168 189 210 [8,] 24 48 72 96 120 144 168 192 216 240 [9,] 27 54 81 108 135 162 189 216 243 270 [10,] 30 60 90 120 150 180 210 240 270 300 SAS basic mathematical operators By default, SAS creates row vectors with do(a, b, by = c) syntax. The transpose operator (a single backtick) can be used to transform A into A`. 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 x = do(1, 10, 1); 8 y = do(3, 30, 3); 9 10 z = x + y; 11 z2 = x - y; 12 z3 = x # y; 13 z4 = x/y; 14 z5 = x##2; 15 z6 = x` * y; 16 print z, z2, z3, z4, z5, z6; 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. Other matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy: R matrix operations mat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T) t(mat) # transpose [,1] [,2] [,3] [1,] 1 6 7 [2,] 2 4 8 [3,] 3 5 9 det(mat) # get the determinant [1] 18 diag(mat) # get the diagonal [1] 1 4 9 diag(diag(mat)) # get a square matrix with off-diag 0s [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 4 0 [3,] 0 0 9 diag(1:3) # diag() also will create a diagonal matrix if given a vector [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 2 0 [3,] 0 0 3 SAS matrix operations 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 mat = {1 2 3, 6 4 5, 7 8 9}; 8 tmat = mat`; 8 ! /* transpose */ 9 determinant = det(mat); 9 ! /* get the determinant */ 10 diagonal_vector = vecdiag(mat); 10 ! /* get the diagonal as a 10 ! vector */ 11 diagonal_mat = diag(mat); 11 ! /* get the diagonal as a square 11 ! matrix */ 12 /* with 0 on off-diagonal entries */ 13 14 dm = diag({1 2 3}); 14 ! /* make a square matrix with vector as the 14 ! diagonal */ 15 16 print tmat, determinant, diagonal_vector, diagonal_mat, dm; 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. The other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, in both languages, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. Invert a matrix in R mat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T) minv &lt;- solve(mat) # get the inverse minv [,1] [,2] [,3] [1,] -0.2222222 0.3333333 -0.1111111 [2,] -1.0555556 -0.6666667 0.7222222 [3,] 1.1111111 0.3333333 -0.4444444 mat %*% minv [,1] [,2] [,3] [1,] 1 0 0 [2,] 0 1 0 [3,] 0 0 1 Invert a matrix in SAS Documentation 6 proc iml; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 mat = {1 2 3, 6 4 5, 7 8 9}; 8 9 mat_inv = solve(mat, diag({1 1 1})); 9 ! /* get the inverse */ 10 mat_inv2 = inv(mat); 10 ! /* less efficient and less accurate */ 11 print mat_inv, mat_inv2; 12 13 id = mat * mat_inv; 14 id2 = mat * mat_inv2; 15 print id, id2; 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. References and Links Non-exhaustive list of general R and SAS references used in this chapter: SAS Matrix reference SAS Data set documentation Creating SAS Data Sets from IML (also this friendly guide and this blog post) SAS Data Step options SAS Mathematical Operators Lists and Data Structures in SAS Loops in SAS and SAS documentation for DO WHILE loops Random number generation in SAS SAS and R compared (by SAS) Repeatable random number generation in R Data structures in Advanced R 2.4.1 Cheat Sheets and Reference Cards SAS Cheatsheet (from another class like this) SAS Cheatsheet (by SAS) R Cheatsheet - this is a simplified cheat sheet offered by RStudio. R Cheatsheet (classic) SAS Programming for R Users (free book) and github site with training materials R programming for SAS users - site for the book, plus link to a free early version of the book (the book is now published) 2.4.2 SAS (as taught in other places) Introduction to SAS - Undergraduate course at Penn State Intermediate SAS - Undergraduate course at Penn State Advanced SAS - Undergraduate course at Penn State 2.4.3 R courses (as taught elsewhere) and Textbooks Stat 579 at Iowa State (as taught by Heike Hofmann) Stat 545 at Univ. British Columbia (developed by Jenny Bryan) R for Data Science - R textbook (free) by Hadley Wickham and Garret Grolemund Advanced R by Hadley Wickham 2.4.4 Combination of R and SAS courses Stat 850 as taught by Chris Bilder at UNL 2.4.5 Non-Exhaustive List of Sources used to aggregate “core programming concepts”: https://blog.upperlinecode.com/computer-language-fundamentals-five-core-concepts-1aa43e929f40 https://howtoprogramwithjava.com/programming-101-the-5-basic-concepts-of-any-programming-language/ https://dev.to/lucpattyn/basic-programming-concepts-for-beginners-2o73 http://livecode.byu.edu/programmingconcepts/ControlStruct.php http://holowczak.com/programming-concepts-tutorial-programmers/ There is also an R package dedicated to pure evil that will set F and T randomly on startup. Use this information wisely.↩︎ numeric is not really a type, it’s a mode. Run ?mode for more information.↩︎ While there are both Duplo and Lego in my house, my toddler is a lot more willing to share than my husband, so Duplo will have to do.↩︎ Most languages are 0-indexed languages: C, C++, python, Java, javascript. Vectors in these languages are indexed as 0, 1, 2, 3. Other 1-indexed languages include FORTRAN, Matlab, Julia, Mathematica, and Lua, many of which were intended for mathematical processing or data analysis.↩︎ Traditionally, naughty children get coal, while nice children get toys or candy.↩︎ "],["organization.html", "Module 3 Organization: Packages, Functions, Scripts, and Documents Organization: Module Objectives 3.1 Reproducibility 3.2 Functions and Modules 3.3 Procs and Data steps 3.4 Scripts 3.5 Packages References and Links", " Module 3 Organization: Packages, Functions, Scripts, and Documents Organization: Module Objectives Use markdown to create dynamic documents which show code and results reproducibly Create functions and scripts to accomplish simple tasks Compare the structure and syntax of R and SAS programs Know where to find external software packages or modules to provide additional functionality 3.1 Reproducibility The concepts of replication and reproducibility is central to science - we do not trust studies whose results cannot be replicated by additional repetitions of the experiment, and we do not trust statistical analyses whose results are not backed up by valid methods that can be re-run to verify the reported results. While replication covers the lab methods, experimental design, and data collection procedures, reproducibility is concerned with the code and the data from an experiment which has already been run. Specifically, the idea is that the research paper is basically an advertisement - by exposing the code and data used in the analysis, readers can engage with the core of the analysis methods, leading to better peer feedback as well as easier adoption of the research for future work. Reproducibility has several advantages: It allows you to show the correctness of your results Trying to reproduce an analysis from the data and the description in the journal article is… challenging, if not impossible, in many cases. By providing the raw data and code to take the data from raw form to analysis results, readers can verify the legitimacy of each step in the analysis. This allows researchers to review each others methods, finding mistakes due to bugs in the software used or due to implementation errors. In one particularly prominent failure of reproducibility, a study used to support macroeconomic theories that shaped the response to the 2008-2009 recession negatively correlating national debt with gdp growth was found to be flawed due to an excel indexing mistake. Use of GUI-based (graphical user interface) statistical analysis software may make it harder to identify these mistakes, because the formulas and code are not visually displayed. It allows others to use your results more easily By sharing your code and raw data, you provide the wider scientific community with the ability to use your results to build new scientific studies. This increases the relevance of your work, the number of citations your papers get, and you also benefit from the community adopting a culture of openness and reproducibility. In 2 years, when you need to find the code you used for that analysis in XXX paper, you’ll be able to find the code (and the data) to see how it worked and what you did. The code may or may not run as-is (depending on software versioning, package updates, etc.), but you will have the methods clearly documented along with the data (so it’s easy to replicate the data format needed, etc.) There are other advantages (personal and public) described in an issue of Biostatistics dedicated to reproducibility. David Donoho’s response is particularly useful. As you might expect, there are many different types of reproducibility. Code reproducibility - allows replication of the computing aspects of the study. Includes code, software, hardware, and implementation details. Data reproducibility - allows replication of the non-computational parts of the study (e.g. experiment and data collection). This may include making protocols and data available. Statistical reproducibility - allows replication of the statistical methods. Includes details about model parameters, thresholds, etc. and may also include study pre-registration to prevent p-hacking. There are also many levels of reproducibility. Much of the computer code written in the 1960s is no longer runnable today, because the computer architecture it was written for is not available anymore. Code which depends on URLs is vulnerable to website rearrangements or the content no longer being hosted. Archiving projects on GitHub is nice, but what happens if GitHub goes down? It’s important to decide what type of reproducibility is important for a particular project, and then design the project’s workflow around that process. For most of my projects, I don’t worry about software versioning (I may archive my sessionInfo() so that package versions are documented) and storing the software packages alongside the code and data. As much as possible, I keep the code and the data (if it’s small) on GitHub in a public repository for people to access, along with any manuscripts or presentations related to the project. Manuscripts are written in knitr or r markdown, so that the code is documented by the context of the project, and every image in the article generated by R has corresponding code available. This ensures that my code (and data) is stored somewhere off-site (backed up in the cloud) my code is available if others want to use it I can track my contributions to a project relative to any collaborators I can undo changes that I make if something in the code breaks. I can undo changes my collaborators inadvertently make because all changes are recorded. I can reuse blocks of code easily (and find them easily on GitHub) In situations where I run experiments, I also make sure that any experimental stimuli or other code that would contribute to the execution and data collection part of the experiment is also included in the repository. This may involve archiving intermediate results that would not normally be archived so that exact stimuli can be regenerated “just in case.” The github reproducibility work flow is convenient - it allows for me to easily collaborate with others, without emailing versions of code and documents back and forth or dealing with Dropbox version conflicts. I can revert changes that are made that had unintentional effects fairly easily. I can sync my files across multiple machines effortlessly. And if necessary, I can look back at the changes I’ve made and see why I made them, or what I’ve already tried. Reproducibility References and Reading I highly recommend scanning these resources to get a good sense of the different ways the word “reproducibility” is used in the literature. Advanced R’s reproducibility guide A reproducible R workflow ROpenSci’s guide to reproducibility Roger Peng’s Biostatistics editorial on reproducibility The Biostatistics reproducibility issue w/ responses to the editorial and associated commentary 3.1.1 Markdown and R In this class, we’re primarily going to use rmarkdown to create dynamic documents. Markdown itself is a special style of text that is intended to allow you to do basic formatting without having to pause to actually click the buttons (if you were writing in word). It integrates equation functionality (so you can type mathematical equations using LaTeX syntax) and also allows for the use of templates (so you can write whole journal articles in a text editor). Markdown is also program agnostic - it will allow you to compile your work into HTML, word, or PDF form. Rmarkdown is magic. (image by Allison Horst) Markdown documents must be compiled - a computer program runs and transforms the text file into a full document. RStudio has markdown functionality built-in, and also supports rmarkdown, which is a markdown variant designed to make it easy to integrate R code with document creation (so-called literate programming). There are other markdown programs which extend markdown’s functionality so that you can write a book (like this one), create presentations or posters, or maintain a blog in markdown. Rmarkdown, despite the name, also allows you to integrate the results from code in other languages. As you saw in the last chapter, SAS code can be integrated into markdown as well. Other languages commonly used include python, julia, SQL, Bash, C++, and Stan. There is a full set of Rmarkdown tutorials from RStudio. There is also a handy cheatsheet. If you run into trouble or want to do something more complicated, there is an Rmarkdown cookbook which contains a number of useful tricks. Or, you can take the “jump right in” approach - open RStudio, File -&gt; New -&gt; Rmarkdown document. To compile it, click the knit button in the bar at the top of the text editor window. Make changes to the text and the R code, compile it, and see what happens. Voila! You’re a markdow expert! Rmarkdown documents may contain code used to support an analysis, but they are usually not the best way to develop an analysis method - they are better for documentation, writing tutorials, and other scenarios where you need both text explanations and code/analysis/results. There are other “containers” for code, though, including functions, scripts, and packages. Each has their own advantages and disadvantages, and can be used together. 3.1.2 Rmarkdown with… SAS? You may have noticed that I’ve been including SAS chunks throughout this book, and even in your homework assignments. Here’s how that’s set up. SAS in Rmarkdown – guide. It’s really fairly easy, which surprised me – I was expecting it to be a lot more involved to set SASmarkdown up. SASmarkdown will not work if you don’t have the following setup(s): Windows + SAS (not Community Edition) + R Mac + Parallels (with SAS and RStudio both installed in parallels) Linux + SAS + R If you’re using Community Edition, you cannot use SASMarkdown. Instead, you’ll have to submit SAS files separately from your Rmarkdown code, or use the server set up by HCC that has the correct configuration. If you are using SAS Community Edition, you are still expected to ensure your code is fully reproducible on other computers. 3.2 Functions and Modules A function (or a module, in SAS) is a block of code which is only run when it is called. It takes arguments (known as parameters) and returns data or some other value. There is some extensive material on this subject in R for Data Science on functions. If you aren’t familiar with functions, you should read that material before proceeding. Let’s look at the structure of a generic function in pseudocode (code that isn’t really part of any language, but describes the steps of a program): my_function_name = function(param1, param2 = 3) { step1 // do something step2 // do something else return step_1/step2 } The first part of a function declaration (storing information in a named object) is the function’s intended name, my_function_name. Then, we indicate that we are defining a function, and what parameters our function requires. For param1, we do not provide a default value, but for param2, we indicate that the default value is 3. Thus, if we call the function (tell the program to run this function with certain arguments and provide the result), we could either say my_function_name(param1 = value1, param2 = value2) or my_function_name(param1 = value1) (which is equivalent to my_function_name(param1 = value1, param2 = 3)). In R, you can even say my_function_name(value1, value2) and the assumption is that you’ve supplied the parameters in the correct order.10 Inside the function block (indicated by {} here, but some languages may use do ... end;), we perform whatever steps we’ve decided to include in the function, and then at the end of the function, we return a value - the function exits, and leaves behind some information. In R, functions look like this: function( arglist ) { expr return(value) } In SAS, functions are called modules. The documentation for a module is below: Statements That Define and Execute Modules Modules are used to create a user-defined subroutine or function. A module definition begins with a START statement, which has the following general form: START &lt;name&gt; &lt;( arguments )&gt; &lt;GLOBAL( arguments )&gt;; A module definition ends with a FINISH statement, which has the following general form: FINISH &lt;name&gt;; To execute a module, you can use either a RUN statement or a CALL statement. The general forms of these statements are as follows: `RUN &lt;name&gt; &lt;( arguments)&gt;; `CALL &lt;name&gt; &lt;( arguments)&gt;;` The only difference between the RUN and CALL statements is the order of resolution. Source: SAS function reference Let’s try functions out by writing a simple function that takes two numbers as arguments and adds them together. adder &lt;- function(a, b) { return(a + b) } adder(3, 4) [1] 7 2 proc IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 4 start adder(a, b); 5 return(a + b); 6 finish; NOTE: Module ADDER defined. 7 8 c = adder(3, 4); 8 ! /* In IML, you can use the function like this 8 ! as well */ 9 print c; 10 11 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:38 Observation Length 16 Last Modified 04/22/2021 08:58:38 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_work90320004A7DD_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy Of course, it’s not just important to be able to write your own functions. It’s also helpful to be able to see how functions are written, both to explore how a method is implemented and for debugging purposes. In SAS, this is generally not an option, because SAS is closed source, but in R, you can see the code behind any function which is implemented in R (it is harder to see functions implemented in C or C++, but not impossible) by typing the function name (no parentheses) into the command prompt. Let’s examine how the colSums() function is implemented colSums function (x, na.rm = FALSE, dims = 1L) { if (is.data.frame(x)) x &lt;- as.matrix(x) if (!is.array(x) || length(dn &lt;- dim(x)) &lt; 2L) stop(&quot;&#39;x&#39; must be an array of at least two dimensions&quot;) if (dims &lt; 1L || dims &gt; length(dn) - 1L) stop(&quot;invalid &#39;dims&#39;&quot;) n &lt;- prod(dn[id &lt;- seq_len(dims)]) dn &lt;- dn[-id] z &lt;- if (is.complex(x)) .Internal(colSums(Re(x), n, prod(dn), na.rm)) + (0+1i) * .Internal(colSums(Im(x), n, prod(dn), na.rm)) else .Internal(colSums(x, n, prod(dn), na.rm)) if (length(dn) &gt; 1L) { dim(z) &lt;- dn dimnames(z) &lt;- dimnames(x)[-id] } else names(z) &lt;- dimnames(x)[[dims + 1L]] z } &lt;bytecode: 0x56038117d2d8&gt; &lt;environment: namespace:base&gt; You can see that the first 3 steps in the function are if statements to test whether the inputs are acceptable - x must be a data frame, a matrix, or an array (with 2+ dimensions). The next couple of lines test to see whether there are additional “column” dimensions (don’t worry if you don’t understand what’s going on in this code - it’s highly optimized and a bit arcane). Then, the function checks to see if x is real-valued or complex, and if it’s complex, computes the real and imaginary sums separately. The .Internal(colSums(x...)) part is calling a C function - basically, functions written in C are faster than R because they’re compiled, so this speeds basic operations up in R. Then there are statements that transfer dimension names over to the summed object. At the end of the function, the last value computed is returned automatically (in this case, z). Try it out Write a function named circle_area which computes the area of a circle given the radius. Make sure to use reasonable parameter names! (Note: in R, pi is conveniently stored in the variable of the same name - it can be overwritten if you want to do so, but why would you want to do that? In SAS, you can get the value of pi using constant(\"pi\")) Solution circle_area &lt;- function(r) { r^2*pi # automatically returned as the last computed value } circle_area(5) [1] 78.53982 A more complete and robust answer might include a test for numeric r: circle_area &lt;- function(r) { if (!is.numeric(r)) { stop(&quot;Supplied radius must be numeric&quot;) # This issues an error } r^2*pi # automatically returned as the last computed value } circle_area(5) [1] 78.53982 6 proc IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 8 start circle_area(r); 9 pi = constant(&quot;pi&quot;); 10 return(pi*r**2); 11 finish; NOTE: Module CIRCLE_AREA defined. 12 13 c = circle_area(5); 14 print c; 15 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 ERROR: Errors printed on pages 4,5,7. One last trick to note: functions generally can only return one object. If you need to return more than one thing, put the objects into a list or another data structure, and return that - then you can take the list/structure apart outside the function to use the returned values separately. 3.2.1 Scope The scope of a variable is the space in the program where a variable is defined and can be accessed. A local variable is one which can only be accessed within a function or block of code - it does not exist outside of that code. A global variable is one which is available to the entire program. In R, every function is defined in a certain environment, and once it is defined, executed in a specific environment. Think of an environment as a space full of available variables, functions, and objects. Any defined object or variable that a function has access to is in scope. When you are inside of a function block, you have access to values defined within the function, plus any other values outside the function. When there are two variables with the same name, the object in the environment which is “closest” is used. Demonstration of scoping in R a &lt;- 3 myfun &lt;- function(a, b) { a + b + 2 } myfun(5, 6) # a is 5 inside the function, so that overrides the [1] 13 # a defined outside the function myfun(a, 3) # this references the a outside the function [1] 8 Scope diagram. When myfun() is called, the calling environment contains the two parameters a and b. a &lt;- 3 myfun2 &lt;- function(d) { myfun(a, d) } myfun2(3) # the only a in scope inside fun2 is the a defined at the top of the chunk [1] 8 Scope diagram. When myfun2() is called, the calling environment contains only a parameter \\(d\\). \\(a\\) is pulled from the global environment, as there is no parameter \\(a\\) in the myfun2 calling environment. a &lt;- 3 myfun3 &lt;- function(a, d) { b &lt;- a; # make a copy of the value a &lt;- 250; myfun(b, d) } a [1] 3 myfun3(5, 3) # now, a is defined inside fun3 as a = 5, so there is an a in [1] 10 # fun3&#39;s scope that isn&#39;t in the global environment. a # value of a hasn&#39;t changed [1] 3 Scope diagram. When myfun3() is called, the calling environment contains parameters a and d, which are then copied into the calling environment of myfun as \\(a\\) and \\(b\\). The variable \\(a\\) in the global environment is ignored. If you want to avoid too many issues with scoping (because scoping rules are complicated), the simplest way is to not reuse variable names inside of a function if you’ve already used those names outside the function (this holds for all languages, really). R does have global variables and a global assignment operator, &lt;&lt;-, but the use of global variables is strongly discouraged, and global variables are not permitted in e.g. CRAN packages. In SAS, scoping rules are more like those in other programming languages - you have to keep track of how arguments are made available to the function. Demonstration of scoping in SAS. Environments do not inherit variables from the calling environment. 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 a = 3; 4 5 start myfun(a, b); 6 return a + b + 2; 7 finish; NOTE: Module MYFUN defined. 8 9 r1 = myfun(5, 6); 10 /* a is 5 inside the function, so that overrides the 11 a defined outside the function */ 12 13 r2 = myfun(a, 3); 14 /* this references the a outside the function */ 15 16 print r1 r2; 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:38 Observation Length 16 Last Modified 04/22/2021 08:58:38 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_work2C380004A898_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy In SAS, the equivalent version of the program used to demonstrate lexical scoping in R produces an error. In SAS, you cannot assume the function has access to values defined outside of that function that are not passed into the function as arguments. 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 a = 3; 4 5 start myfun(a, b); 6 return a + b + 2; 7 finish; NOTE: Module MYFUN defined. 8 9 start myfun2(d); 10 return myfun(a, d); 10 ! /* SAS complains because a is not defined 10 ! */ 11 finish; NOTE: Module MYFUN2 defined. 12 13 r1 = myfun2(3); 14 15 print r1; 16 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:39 Observation Length 16 Last Modified 04/22/2021 08:58:39 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workB8180004A8FD_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy First, let’s consider the similarities: like R, functions have a local scope, and changing a similarly named value inside the function doesn’t change the value outside the function. 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 start myfun(x); 4 y = 2 * x; 5 print y[label=&quot;y inside function (local)&quot;]; 6 return 1; 7 finish; NOTE: Module MYFUN defined. 8 9 y = 0; 10 x = 1:5; 11 res = myfun(x); 12 print y[label=&quot;y outside function&quot;]; 13 14 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 15 ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:39 Observation Length 16 Last Modified 04/22/2021 08:58:39 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workFFA40004A959_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy However, R’s environment feature and lexical scoping is not common to many other programming languages. When values are passed into the function as arguments, the behavior in SAS deviates from the equivalent behavior in R. In SAS, arguments to functions are passed by reference. An argument that is passed by value makes a copy of the value (to a new memory location) for the local function scope (this is what R does). When an argument is passed by reference, the address of the argument is passed in instead11. This is faster and more efficient (because you aren’t making a new copy of the data), but it does mean that changes inside the function persist outside that function. This can cause problems when reusing modules. Argument passing in SAS If we make a slight modification to myfun(a, b), though, we see some interesting behavior in SAS that we wouldn’t see in the equivalent R program. 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 a = 3; 4 5 start myfun(a, b); 6 a = a + 2; 7 return a + b; 8 finish; NOTE: Module MYFUN defined. 9 10 b = a; 11 12 r1 = myfun(5, 6); 13 14 c = a; 15 16 r2 = myfun(a, 3); 17 18 d = a; 19 20 print b[label=&quot;a before r1&quot;] r1 c[label=&quot;a after r1&quot;] r2 20 ! d[label=&quot;a after r2&quot;]; 21 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:40 Observation Length 16 Last Modified 04/22/2021 08:58:40 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_work523A0004A9B8_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy In SAS, be sure that if you are using a logical operator AND, &amp;, you put spaces around it (e.g. &amp;) - &amp;var2 is a way to specify that you are passing an argument (var2) by reference, and SAS can get confused and issue weird warnings if you actually intended this to be a logical statement of var1 &amp;var2. I ran across this once and it thoroughly confused me until I remembered that &amp;var can mean “pass by reference” in some languages. Another example of SAS passing arguments by reference (and the unexpected effects that can have on a program’s state) 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 start myfun(x); 4 call sort(x, 1); 4 ! /* sort the values */ 5 return (cusum(x)); 5 ! /* cusum() = cumulative sum */ 6 finish; NOTE: Module MYFUN defined. 7 8 y = {3, 1, 4, 1, 5, 9, 2, 6, 5, 4}; 9 z = y; 10 11 print y[label=&quot;y before function is called&quot;]; 12 13 cs = myfun(y); 14 15 print z[label=&quot;original y&quot;] 16 y[label=&quot;y after function is called&quot;] 17 cs[label=&quot;cumulative sum of sorted y&quot;]; 18 19 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:40 Observation Length 16 Last Modified 04/22/2021 08:58:40 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workB89F0004AA1B_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy Because arguments in SAS are passed by reference, you can “trick” a function into returning multiple values by passing the variables in as arguments to the function, changing their values in the function, and returning. This is not necessarily a good practice - it can make code very difficult to debug, and may lead to non-obvious dependencies - but for short, simple programs, you can probably get away with it. 2 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 3 start getDescriptive(Mean, SD, /* output args */ 4 x /* input arg */); 5 Mean = x[:]; 5 ! /* this is shorthand for compute the mean of 5 ! the column */ 6 SD = sqrt( ssq(x - Mean)/(nrow(x) - 1)); 7 finish; NOTE: Module GETDESCRIPTIVE defined. 8 9 m = 0; 10 s = 0; 11 y = {3, 1, 4, 1, 5, 9, 2, 6, 5, 4}; 12 13 run GetDescriptive(m, s, y); 14 print m s; 15 16 17 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 ERROR: Errors printed on pages 4,5,7. The SAS System 1 08:58 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 08:58 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 08:58 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 08:58:41 Observation Length 16 Last Modified 04/22/2021 08:58:41 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workD97F0004AA81_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278101 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 08:58 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 08:58 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 08:58 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 08:58 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy If you want to avoid any of these side-effects of SAS’s pass-by-reference behavior, you can very easily do so: just don’t write any modules that modify input arguments. Always modify a copy of the variable instead. Argument passing in R In R, variables inside a function don’t modify variables which are outside a function (generally speaking). In SAS, this is not necessarily the case. In the first call to myfun(), we pass in two numerical arguments, and we see that even though the value of a changes inside the function, that change doesn’t affect the variable defined outside of the function. In the second call to myfun(), we pass a variable in as an argument, and we see that the variable changes after the function’s execution! In the R chunk (below), you can see that the behavior of what is essentially the same code is different. a &lt;- 3 myfun &lt;- function(a, b) { a &lt;- a + 2 a + b } a [1] 3 myfun(5, 6) [1] 13 a [1] 3 myfun(a, 3) [1] 8 a [1] 3 Try it out Can you predict what the output of this chunk will be? f &lt;- function(x) { f &lt;- function(x) { f &lt;- function(x) { x ^ 2 } f(x) + 1 } f(x) * 2 } f(10) Run it - were you right? What happens when you run a similar program in SAS? (I only nested two functions this time, but you get the idea) proc IML; start f(x); start f(x); return x**x; finish; return f(x)+1; finish; quit; Solution Working through the R program: This gets much less confusing if you rename the functions following R’s scoping rules. f &lt;- function(x) { f1 &lt;- function(x) { # because inside of f(), the new definition will dominate f2 &lt;- function(x) { # because inside of f1(), the new definition will dominate x ^ 2 } f2(x) + 1 } f1(x) * 2 } f(10) Once this has been renamed, it is relatively easy to write out as a series of mathematical substitutions: \\[f(10) = f1(10) * 2 = (f2(10) + 1) * 2 = (10^2 + 1)*2 = 202\\] Running the similar program in SAS results in SAS complaining about a recursive function definition. What will this SAS program output? 6 proc IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 8 start funwithSAS(x, y); 9 a = x; 10 x = x + 3; 11 return x*y; 12 finish; NOTE: Module FUNWITHSAS defined. 13 14 a = 0; 15 res1 = funwithSAS(a, 5); 16 a1 = a; 17 18 x = 3; 19 res2 = funwithSAS(x, 5); 20 a2 = a; 21 x2 = x; 22 23 y = 3; 24 res3 = funwithSAS(x, y); 25 26 /* print res1 a1 res2 a2 x2 res3 a x y; */ /* uncomment this 26 ! when you&#39;re ready */ 27 28 quit; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 ERROR: Errors printed on pages 4,5,7. Solution This is easiest if we step through the program and list what is passed around inside and outside of the function evaluation. a = 0 res1 = ...? (inside funwithsas res1) x = (location of a), y = 5 (inside funwithsas res1) a = x = (location of a), x = (location of a) + 3 = 3, so (location of a) = 3 and x = 3. res1 returns x = 3 * y = 5 = 15 a1 = 3 res2 = ...? (inside funwithsas res2) x = (location of x), y = 5 (inside funwithsas res2) a = (location of x), x = (location of x) + 3 = 6 (a outside the function is unaffected because a is not a parameter) res2 returns x = 6 * y = 5 = 30 a2 = 3 x2 = 6 y = 3 res3 = ...? (inside funwithsas res3) x = (location of x), y = (location of y) (inside funwithsas res3) a = (location of x), x = (location of x) + 3 = 9 (a outside the function is unaffected; y is unchanged) res3 returns x = 9 * y = 3 = 27 at this point, x = 9 and y = 3. So the output is res1 = 15 a1 = 3 res2 = 30 a2 = 3 x2 = 6 res3 = 27 a = 3 x = 9 y = 3 The trick to this problem is to realize that inside of the function, when a variable is passed in (by reference) the thing that is assigned is the pointer to the value in memory where the original variable lives. So when a is passed in as x, x is assigned (location of a), which is then assigned to the local copy of a. So both variables now point at the outside (a), and any changes to x also affect both a and the local copy of a. 3.2.2 The Pipe You do not need to understand the collapsed section below. If you’re feeling comfortable with the material, go ahead and read it now. I’ve left it in the functions section, because it’s a topic relating to functions, but it’s not essential to understand to use R (even at an advanced level)12. Infix operators Most functions take arguments written after the function name. Can you think of any functions which work differently? Infix functions are functions that take arguments on both sides of the function name. Say, a + b: technically, a and b are arguments to the function +, so we could think of this as +(a, b). R has a number of infix functions, but you can also create your own. User-defined infix functions start and end with %. So %&gt;%, %in%, %dosomething% would all be valid infix operators. To define one, you need to enclose the name in back-ticks (`). `%dosomething%` &lt;- function(a, b) { a^b - a + b } 3 %dosomething% 4 [1] 82 You can also call default infix operators using this syntax: `+`(3, 4) = 7 One of the most useful infix operators is the pipe, %&gt;%, which is a part of the magrittr library and is commonly included in other packages, such as dplyr, and tidyr (we will talk about packages later in this chapter, but pipes are useful to discuss now, so roll with it for a few minutes). The one part of this section that is important to at least be able to use is the pipe: %&gt;%. There is an entire chapter dedicated to discussing the pipe in R4DS, including a discussion of when not to use the pipe. library(magrittr) 3 %&gt;% exp() [1] 20.08554 exp(3) [1] 20.08554 The pipe takes the left hand side and a function, and puts the left hand side as an argument to the function on the right hand side. It doesn’t sound very impressive, but it allows you to do a very cool thing: “chain” operations. Consider 3 functions: f1 &lt;- function(x, y = 3) { x * y } f2 &lt;- function(z) { z^2 } f3 &lt;- function(z2) { log10(z2) } Normally, you’d write f3(f2(f1(4))), which you have to read from the inside to the outside if you want to describe what this function call is doing. With pipes, you can write the same operation as: 4 %&gt;% f1() %&gt;% f2() %&gt;% f3() [1] 2.158362 This is much simpler to read - it’s like a recipe. “Take 4, do f1, then f2, then f3.” You don’t need to know how to define your own infix operators, but you will want to become familiar with the pipe. It’s a central component of writing “tidy” R code. Incidentally, SAS also has a use for the pipe: leveraging the operating system to work with files. In SAS, you just use the keyword pipe, which is hopefully pretty obvious. 3.3 Procs and Data steps In R, the primary unit of code is a function, and nearly every operation in R is a function call, but the functions themselves are malleable and can be easily re-written. SAS is not that flexible - its procedures have been checked and double-checked and maintained for 30 years (in many cases). That level of validation explains its popularity in e.g. the pharmaceutical industry or government, but it does lead to a certain rigidity in the “flow” of a SAS data analysis. Personally, I find that using SAS generally doesn’t require much thought, but I quickly get frustrated when it’s not possible to do the exact thing I want to do in a direct or relatively easy way. The primary units of code in SAS are “steps,” and come in two main flavors: data steps, and proc steps. Data steps are written by the user and customized extensively to the dataset you’re creating or reading in. Proc steps, on the other hand, execute mainly pre-defined procedures that are built into SAS. Thus far, you’ve seen DATA steps and PROC PRINT and IML. In the next module, you’ll see PROC IMPORT, PROC MEANS, PROC CORR, PROC SUMMARY, PROC FREQ, and PROC UNIVARIATE. There is a handy overview of the SAS language that may be useful in understanding the code that you’ve only been reading and copying up to this point. The bare SAS procedure with no options looks like this: PROC MEANS; RUN; By default, SAS will use the last dataset created to run this procedure. SAS procedures can also be run with options that modify the statement. 6 PROC PRINT DATA=SASHELP.CARS (obs=10); 7 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 8 9 PROC MEANS DATA=SASHELP.CARS; 10 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. In the code above, both DATA=SASHELP.CARS and (obs=10) are options that modify their respective statements. Some procs also support additional statements: for instance, we can use the VAR statement to tell SAS which variables we want to work with. 6 PROC MEANS DATA=SASHELP.CARS; 7 VAR msrp cylinders; 8 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. Another statement, CLASS, tells SAS to run the procedure for each level of a named categorical variable. 6 PROC MEANS DATA=SASHELP.CARS; 7 CLASS type; 8 VAR msrp cylinders; 9 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7. Some statements may have additional options that further modify the statement. The SAS documentation contains full lists of all statements (and all options for those statements). Try It Out Take a look at the Dictionary of SAS DATA step statements. Find the RENAME statement and read up on its syntax. Can you rename the variable msrp in the SASHELP.CARS dataset? DATA tmp; /* Create a temporary dataset */ SET SASHELP.CARS; /* use the CARS data */ /* Your RENAME statement goes here */ RUN; There is an overview of many common SAS procedures here. 3.4 Scripts Up until this point, you may have been writing code in an R Studio or SAS text editor window, or, you may have been typing commands into the command line without preserving them in a separate file. You might even have been working in R markdown documents, where you had code and non-code chunks of the file. A script is a file which contains only code and comments. It is intended to run from start to finish, and usually completes one or more tasks - for instance, cleaning your data, or loading a series of custom functions into your R environment. Scripts are useful because they preserve code so that it can be re-run… and in some cases, they can even be re-run autonomously - I have several scripts which automatically run at specific times every day to complete various tasks (scraping data off the internet, mostly). I find that when doing data analysis, it is often easier to write a script as opposed to working in R markdown or typing commands into the console. Scripts are a record of what I’ve done, and ensure that commands are executed in the right order. As with any tool, it is important to know where to use the tool and where the tool is usually not the best option. You can source (run) an R script using the source() command with the file path of the script as the argument. Scripts in R end in .r or .R, while scripts in SAS end in .sas. Scripts can be run in one of (at least) two modes: batch mode, or interactive mode. In batch mode, the entire script is run without human intervention or monitoring. This is useful for repetitive jobs – for instance, to record the weather at 6h intervals throughout the day. In interactive mode, scripts may be run line-by-line or block-wise, with small tweaks made to the code as you proceed through the file. I find that in some cases, what starts out as an interactive mode script can become a batch script as I work the kinks out. If you ever need to use the high-performance computing resources on campus, you will need to write code to run in batch mode, because these jobs are generally not friendly to interactive programming. Try it out I maintain a list of packages that I find to be useful so that when I install R on a new machine (or update R), I don’t have to spend 3 weeks realizing that I need to install X package. Instead of many repeated 5 minute pauses for package installation, I can just let this script run once and walk away. I’ve pared down my list of packages a bit for this class (you don’t need the packages for analysis of 3D bullet scan data, for instance), but this step should help populate your R installation with a few new packages. Read the script (located here) and try to understand what it is doing. Once you think you understand what it is doing, run the following command to run the script and install the packages on your machine. Were you right? url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/code/03_setup.R&quot; source(url) Try it out (SAS) Use this blog post to create a SAS script that draws a Koch Snowflake in SAS. Save it as “Snowflake.SAS” and open the file in SAS to run it. Snowflake.SAS 6 PROC IML; NOTE: IML Ready WARNING: IML is now in syntax-check-only mode due to errors in previous steps. No statements will be executed. 7 start KochDivide(A, E); 7 ! /* (x,y) coords of 7 ! endpoints */ 8 segs = j(5, 2, .); 8 ! /* matrix to hold 4 8 ! shorter segs */ 9 v = (E-A) / 3; 9 ! /* vector 1/3 as 9 ! long as orig */ 10 segs[{1 2 4 5}, ] = A + v @ T(0:3); 10 ! /* endpoints of new 10 ! segs */ 11 /* Now compute middle point. Use ATAN2 to find direction 11 ! angle. */ 12 theta = -constant(&quot;pi&quot;)/3 + atan2(v[2], v[1]); 12 ! /* change 12 ! angle by pi/3 */ 13 w = cos(theta) || sin(theta); 13 ! /* vector to middle 13 ! point */ 14 segs[3,] = segs[2,] + norm(v)*w; 15 return segs; 16 finish; NOTE: Module KOCHDIVIDE defined. 17 18 19 /* create Koch Snowflake from an equilateral triangle */ 20 start KochPoly(P0, iters=5); 21 P = P0; 22 do j=1 to iters; 23 N = nrow(P) - 1; 23 ! /* old number of segments */ 24 newP = j(4*N+1, 2); 24 ! /* new number of segments + 24 ! 1 */ 25 do i=1 to N; 25 ! /* for each segment... */ 26 idx = (4*i-3):(4*i+1); 26 ! /* rows for 26 ! 4 new segments */ 27 newP[idx, ] = KochDivide(P[i,], P[i+1,]); 27 ! /* generate 27 ! new segments */ 28 end; 29 P = newP; 29 ! /* update polygon and 29 ! iterate */ 30 end; 31 return P; 32 finish; NOTE: Module KOCHPOLY defined. 33 34 /* create equilateral triangle as base for snowflake */ 35 pi = constant(&quot;pi&quot;); 36 angles = -pi/6 // pi/2 // 7*pi/6; 36 ! /* angles for equilateral 36 ! triangle */ 37 P = cos(angles) || sin(angles); 37 ! /* vertices of equilateral 37 ! triangle */ 38 P = P // P[1,]; 38 ! /* append first vertex to 38 ! close triangle */ 39 K = KochPoly(P); 39 ! /* create the Koch 39 ! snowflake */ 40 41 S = j(nrow(K), 3, 1); 41 ! /* add ID variable with constant 41 ! value 1 */ 42 S[ ,1:2] = K; 43 create Koch from S[colname={X Y ID}]; 44 append from S; 45 close; 46 47 48 /* test KochDivide on line segment from (0,0) to (1,0) */ 49 s = KochDivide({0 0}, {1 0}); 50 title &quot;Fundamental Step in the Koch Snowflake Construction&quot;; 51 ods graphics / width=600 height=300; 52 call series(s[,1], s[,2]) procopt=&quot;aspect=0.333&quot;; 53 54 QUIT; NOTE: Exiting IML. NOTE: PROCEDURE IML used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 55 56 ods graphics / width=400px height=400px; 57 footnote justify=Center &quot;Koch Snowflake&quot;; 58 proc sgplot data=Koch; ERROR: File WORK.KOCH.DATA does not exist. 59 styleattrs wallcolor=CXD6EAF8; /* light blue */ 60 polygon x=x y=y ID=ID / outline fill fillattrs=(color=white); ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 61 xaxis display=none; 62 yaxis display=none; 63 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11. 6 dm log &quot;clear&quot;; 7 dm output &quot;clear&quot;; 8 ods html close; 9 ods html; NOTE: Writing HTML Body file: sashtml3.htm ERROR: Errors printed on pages 4,5,7,11. 3.5 Packages Both SAS and R have systems for extending the base system/language with additional functionality. In R, these extensions are called packages. In SAS, language extensions are called modules (for things sold by SAS), macros (for functions distributed by users), and in packages (which exist but are rarely used).13 SAS packages In SAS, there are packages of code that encapsulate scripts. However, unlike R (and many other languages), there is no centralized repository for SAS packages. Papers may include SAS packages to demonstrate new methods, and other packages may be found on GitHub or various SAS forums. A more common way of distributing code in SAS seems to be through the use of single-file macros. (Even longtime SAS users don’t necessarily know about the package system). These macros often need to be slightly customized to your environment (or you need to customize your environment to match the assumptions made in the macro, which can be harder). R packages In R, there are two main sources for packages: CRAN (the Comprehensive R Archive Network) (and related archives, such as MRAN, which is Microsoft’s version of CRAN) and github. R packages published on CRAN go through a basic verification process that makes sure that the package meets certain standards (for instance, packages must have proper dependencies specified, cannot conflict with previous package names, must have a software license, and cannot contain malicious code). Note that CRAN does not check the packages for statistical correctness! On github, packages go through less verification. Many packages use a system where the version in development is on github and available for installation, but the version on CRAN is considered “stable.” Sometimes, packages are never put on CRAN: I contribute to several packages which are too large for CRAN, and it’s not worth the hassle to get an exception or figure out a workaround. Currently, the CRAN package repository features 1.7471^{4} available packages. How do you navigate them to find the one you need? Sometimes, the CRAN Task Views may be helpful - for instance, if you want to see all of the packages which are useful for Meta-Analysis, Finance, Bayesian statistics, etc. Other times, it’s useful to let Google help you navigate: searching for “R CRAN” + “what you want the package to do” can often narrow things down (I recommend adding CRAN in because Google results for “R” are not particularly useful.) Once you find and install a package (install.packages() for CRAN packages, devtools::install_github() for GitHub packages), you have to figure out how to use it. Many R packages come with vignettes, which are short articles that demonstrate how a package is used. You can browse the available vignettes using browseVignettes() (if you provide a package name as an argument, you will get only vignettes from that package). Another way to get help on a package is to use ?&lt;package name&gt;, e.g. ?ggplot2. That will take you to the main package description page, and there are often links to documentation. At the bottom of the description page, you can click on a link to get to the Index, which is a list of all functions that are provided in that package. From there, you can find the documentation for each function in the package. Try it out Install the R package tibble if it is not already installed. Pull up the vignettes for the tibble package, and read about tibbles. What is the equivalent base R object? How do tibbles differ from that object? Get to the package index for the tibble package, either by navigating through the Packages tab in RStudio, or using ?tibble. Access the documentation for the tribble function, and try loading the package and creating your own tibble using tribble. Solution ## install.packages(&quot;tibble&quot;) library(tibble) ## ?tibble ## ?tribble tribble(~var1, ~var2, 1, 2, 3, 4, 5, 6) # A tibble: 3 x 2 var1 var2 &lt;dbl&gt; &lt;dbl&gt; 1 1 2 2 3 4 3 5 6 References and Links A more advanced take on functions in R can be found here (Advanced R chapter) There is also a handy cheat sheet style summary of PROC IML in SAS here and some useful demonstrations of simple tasks in IML here. A helpful blog post for scoping in SAS is here Here is another explanation of pass-by-value vs. pass-by-reference (intended for C programming) A comparison of R, SAS, and python - source of information about the SAS package distribution system. Another explanation of pipes (lots of examples) It’s a good idea to specify your parameter names when you’re using functions you’re unfamiliar with, which at this point, is probably all of them.↩︎ if you’re familiar with C, the argument passed in is just a pointer to the original memory↩︎ I had been using R for ~8 years before I ever heard the term “infix operator” - and I’d been using infix operators for a long time at that point, just without thinking about what they were.↩︎ Note that the various components of SAS can be extremely confusing to separate. I found this guide to be somewhat helpful.↩︎ "],["reading-data.html", "Module 4 External Data Reading Data: Module Objectives 4.1 External Data Formats 4.2 Text Files 4.3 Spreadsheets 4.4 Binary Files 4.5 Databases 4.6 Exploratory Data Analysis References and Links", " Module 4 External Data Reading Data: Module Objectives Import data from text and excel files into R and SAS for analysis Conduct exploratory data analysis to determine how the data is structured, what cleaning must be done, and what (if any) interesting artifacts are in the dataset 4.1 External Data Formats In order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways: as raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable. in a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet. There is a collection of horror stories here and a series of even more horrifying tweets here as a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected. R, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. To read data from R into SAS, the easiest way is probably to call R from PROC IML. Here is a very thorough explanation of why binary file formats exist, and why they’re not necessarily optimal. in a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with. There are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common: Web related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. Spatial files: Shapefiles are by far the most common version of spatial files14. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. We’ll explore these a bit more when we talk about maps. To be minimally functional in R and SAS, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly cover binary files and databases, but it is less critical to remember how to read these in without consulting one or more online references. 4.2 Text Files There are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say? 4.2.1 Fixed-width files In a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory). Col1 Col2 Col3 3.4 4.2 5.4 27.3 -2.4 15.9 In base R (no extra packages), you can read fwf files in using read.fwf, but you must specify the column breaks yourself. ## url &lt;- &quot;https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/&quot; data &lt;- read.fwf(url, skip = 3, # Skip the first 2 lines (useless) + header line widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8)) # There is a row with the column names specified Warning in readLines(file, n = thisblock): incomplete final line found on &#39;data/ mesodata.txt&#39; data[1:6,] # first 6 rows V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 6.9 0 964.79 272 31.8 4.0 2 ADAX 1 0 55 32.4 1.0 0.8 108 36.5 0.4 2.2 0 976.20 245 32.0 0.2 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 11.5 0 960.94 296 34.7 6.8 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 9.0 0 957.45 298 35.5 5.4 5 ANT2 135 0 73 27.8 0.0 0.0 0 0.0 0.0 0.0 0 990.11 213 27.8 0.0 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 7.9 0 959.54 277 31.9 4.6 V17 V18 V19 V20 V21 V22 V23 V24 1 29.2 36.2 31.6 25.2 21.7 3.09 2.22 1.48 2 28.8 38.2 29.6 26.8 -998.0 2.61 1.88 -998.00 3 29.3 34.1 30.7 26.1 -998.0 3.39 2.47 -998.00 4 24.7 34.7 25.6 22.6 -998.0 2.70 1.60 -998.00 5 29.5 31.1 30.2 26.8 23.8 1.96 1.73 1.33 6 30.4 35.2 34.7 28.2 22.8 1.79 1.53 1.78 You can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you. # I like to cheat a bit.... # Read the first few lines in tmp &lt;- readLines(url, n = 20)[-c(1:2)] # split each line into a series of single characters tmp_chars &lt;- strsplit(tmp, &#39;&#39;) # Bind the lines together into a character matrix # do.call applies a function to an entire list - so instead of doing 18 rbinds, # one command will put all 18 rows together tmp_chars &lt;- do.call(&quot;rbind&quot;, tmp_chars) # (it&#39;s ok if you don&#39;t get this line) # Make into a logical matrix where T = space, F = not space tmp_chars_space &lt;- tmp_chars == &quot; &quot; # Add up the number of rows where there is a non-space character # space columns would have 0s/FALSE tmp_space &lt;- colSums(!tmp_chars_space) # We need a nonzero column followed by a zero column breaks &lt;- which(tmp_space != 0 &amp; c(tmp_space[-1], 0) == 0) # Then, we need to get the widths between the columns widths &lt;- diff(c(0, breaks)) # Now we&#39;re ready to go mesodata &lt;- read.fwf(url, skip = 3, widths = widths, header = F) Warning in readLines(file, n = thisblock): incomplete final line found on &#39;data/ mesodata.txt&#39; # read header separately - if you use header = T, it errors for some reason. # It&#39;s easier just to work around the error than to fix it :) mesodata_names &lt;- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, stringsAsFactors = F) names(mesodata) &lt;- as.character(mesodata_names) mesodata[1:6,] # first 6 rows STID STNM TIME RELH TAIR WSPD WVEC WDIR WDSD WSSD 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 2 ADAX 1 0 55 32.4 1.0 0.8 108 36.5 0.4 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 5 ANT2 135 0 73 27.8 0.0 0.0 0 0.0 0.0 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 WMAX RAIN PRES SRAD TA9M WS2M TS10 TB10 TS05 1 6.9 0 964.79 272 31.8 4.0 29.2 36.2 31.6 2 2.2 0 976.20 245 32.0 0.2 28.8 38.2 29.6 3 11.5 0 960.94 296 34.7 6.8 29.3 34.1 30.7 4 9.0 0 957.45 298 35.5 5.4 24.7 34.7 25.6 5 0.0 0 990.11 213 27.8 0.0 29.5 31.1 30.2 6 7.9 0 959.54 277 31.9 4.6 30.4 35.2 34.7 TS25 TS60 TR05 TR25 TR60 1 25.2 21.7 3.09 2.22 1.48 2 26.8 -998.0 2.61 1.88 -998.00 3 26.1 -998.0 3.39 2.47 -998.00 4 22.6 -998.0 2.70 1.60 -998.00 5 26.8 23.8 1.96 1.73 1.33 6 28.2 22.8 1.79 1.53 1.78 But, there’s an even simpler way… The readr package creates data-frame like objects called tibbles (really, they’re a souped-up data frame), but it is much friendlier to use. Tibbles also do not have the problems with factors (see the introduction to factors) - they will always read characters in as characters. library(readr) # Better data importing in R read_table(url, skip = 2) # Gosh, that was much easier! # A tibble: 121 x 24 STID STNM TIME RELH TAIR WSPD WVEC WDIR WDSD WSSD WMAX RAIN PRES &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 ACME 110 0 53 31.8 5.2 5.1 146 8.5 0.7 6.9 0 965. 2 ADAX 1 0 55 32.4 1 0.8 108 36.5 0.4 2.2 0 976. 3 ALTU 2 0 31 35.6 8.9 8.7 147 10.9 1.1 11.5 0 961. 4 ALV2 116 0 27 35.8 6.7 6.7 145 8.2 1.2 9 0 957. 5 ANT2 135 0 73 27.8 0 0 0 0 0 0 0 990. 6 APAC 111 0 52 32.3 6.2 6.1 133 9.8 0.8 7.9 0 960. 7 ARD2 126 0 46 32.9 2.6 2.5 150 11.8 0.5 3.6 0 979. 8 ARNE 6 0 28 33.5 6.5 6.3 163 11.9 1.5 10 0 927. 9 BEAV 8 0 23 34.9 11.2 11.1 165 7.3 1.4 15.2 0 921. 10 BESS 9 0 37 33.8 8.3 8.3 156 6.6 1.3 11.2 0 951. # … with 111 more rows, and 11 more variables: SRAD &lt;dbl&gt;, TA9M &lt;dbl&gt;, # WS2M &lt;dbl&gt;, TS10 &lt;dbl&gt;, TB10 &lt;dbl&gt;, TS05 &lt;dbl&gt;, TS25 &lt;dbl&gt;, TS60 &lt;dbl&gt;, # TR05 &lt;dbl&gt;, TR25 &lt;dbl&gt;, TR60 &lt;dbl&gt; You can also write fixed-width files if you really want to: if (!&quot;gdata&quot; %in% installed.packages()) install.packages(&quot;gdata&quot;) library(gdata) write.fwf(mtcars, file = &quot;data/04_mtcars-fixed-width.txt&quot;) In SAS, it’s a bit more complicated, but not that much - the biggest difference is that you generally have to specify the column names for SAS. For complicated data, as in R, you may also have to specify the column widths. 6 /* This downloads the file to my machine */ 7 /* x &quot;curl 7 ! https://www.mesonet.org/index.php/dataMdfMts/dataController/getF 7 ! ile/202006070000/mdf/TEXT/ 8 &gt; data/mesodata.txt&quot; */ 9 /* only run this once */ 10 11 /* Specifying WORK.mesodata means the dataset will cease to 11 ! exist after this chunk exits */ 12 data WORK.mesodata; 13 14 infile &quot;data/mesodata.txt&quot; firstobs = 4; 15 /* Skip the first 3 rows */ 16 length STID $ 4; /* define ID length */ 17 input STID $ STNM TIME RELH TAIR 18 WSPD WVEC WDIR WDSD WSSD WMAX 19 RAIN PRES SRAD TA9M WS2M TS10 20 TB10 TS05 TS25 TS60 TR05 TR25 TR60; 21 run; NOTE: The data set WORK.MESODATA has 0 observations and 24 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 23 proc print data=mesodata (obs=10); /* print the first 10 23 ! observations */ 24 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 4,5,7,11. In SAS data statements, you generally need to specify the data names explicitly. In theory you can also get SAS to write out a fixed-width file, but it’s much easier to just… not. You can generally use a CSV or format of your choice – and you should definitely do that, because delimited files are much easier to work with. 4.2.2 Delimited Text Files Delimited text files are files where fields are separated by a specific character, such as \" “,”\", tab, etc. Often, delimited text files will have the column names as the first row in the file. As long as you know the delimiter, it’s pretty easy to read in data from these files in R using the readr package. url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; pokemon_info &lt;- read_csv(url) Warning: Missing column names filled in: &#39;X1&#39; [1] ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_double(), name = col_character(), german_name = col_character(), japanese_name = col_character(), status = col_character(), species = col_character(), type_1 = col_character(), type_2 = col_character(), ability_1 = col_character(), ability_2 = col_character(), ability_hidden = col_character(), growth_rate = col_character(), egg_type_1 = col_character(), egg_type_2 = col_character() ) ℹ Use `spec()` for the full column specifications. pokemon_info[1:6, 1:6] # Show only the first 6 lines &amp; cols # A tibble: 6 x 6 X1 pokedex_number name german_name japanese_name generation &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 0 1 Bulbasaur Bisasam フシギダネ (Fushigid… 1 2 1 2 Ivysaur Bisaknosp フシギソウ (Fushigis… 1 3 2 3 Venusaur Bisaflor フシギバナ (Fushigib… 1 4 3 3 Mega Venusa… Bisaflor フシギバナ (Fushigib… 1 5 4 4 Charmander Glumanda ヒトカゲ (Hitokage) 1 6 5 5 Charmeleon Glutexo リザード (Lizardo) 1 # a file delimited with | url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt&quot; nebraska_locations &lt;- read_delim(url, delim = &quot;|&quot;) ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_character(), FEATURE_ID = col_double(), PRIM_LAT_DEC = col_double(), PRIM_LONG_DEC = col_double(), SOURCE_LAT_DEC = col_double(), SOURCE_LONG_DEC = col_double(), ELEV_IN_M = col_double(), ELEV_IN_FT = col_double() ) ℹ Use `spec()` for the full column specifications. nebraska_locations[1:6, 1:6] # A tibble: 6 x 6 FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_NAME &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 171013 Peetz Table Area CO 08 Logan 2 171029 Sidney Draw Valley NE 31 Cheyenne 3 182687 Highline Canal Canal CO 08 Sedgwick 4 182688 Cottonwood Cre… Stream CO 08 Sedgwick 5 182689 Sand Draw Valley CO 08 Sedgwick 6 182690 Sedgwick Draw Valley CO 08 Sedgwick You can also read in the same files using read.csv and read.delim, which are the equivalent base R functions. url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; pokemon_info &lt;- read.csv(url, header = T, stringsAsFactors = F) pokemon_info[1:6, 1:6] # Show only the first 6 lines &amp; cols X pokedex_number name german_name japanese_name 1 0 1 Bulbasaur Bisasam フシギダネ (Fushigidane) 2 1 2 Ivysaur Bisaknosp フシギソウ (Fushigisou) 3 2 3 Venusaur Bisaflor フシギバナ (Fushigibana) 4 3 3 Mega Venusaur Bisaflor フシギバナ (Fushigibana) 5 4 4 Charmander Glumanda ヒトカゲ (Hitokage) 6 5 5 Charmeleon Glutexo リザード (Lizardo) generation 1 1 2 1 3 1 4 1 5 1 6 1 # a file delimited with | url &lt;- &quot;https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt&quot; nebraska_locations &lt;- read.delim(url, sep = &quot;|&quot;, header = T) nebraska_locations[1:6, 1:6] FEATURE_ID FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC 1 171013 Peetz Table Area CO 8 2 171029 Sidney Draw Valley NE 31 3 182687 Highline Canal Canal CO 8 4 182688 Cottonwood Creek Stream CO 8 5 182689 Sand Draw Valley CO 8 6 182690 Sedgwick Draw Valley CO 8 COUNTY_NAME 1 Logan 2 Cheyenne 3 Sedgwick 4 Sedgwick 5 Sedgwick 6 Sedgwick SAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters). Don’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding! While writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS. CSV Import in SAS 6 /* x &quot;curl 6 ! https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/ 6 ! master/pokemon_gen_1_to_8.csv &gt; data/pokemon.csv&quot;; 7 only run this once to download the file... */ 8 filename pokeloc &#39;data/pokemon.csv&#39; encoding=&quot;utf-8&quot;; 9 10 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 11 proc import datafile = pokeloc out=poke 12 DBMS = csv; /* comma delimited file */ 13 GETNAMES = YES 14 ; 15 proc print data=poke (obs=10); /* print the first 10 15 ! observations */ ERROR: File WORK.POKE.DATA does not exist. 16 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12. The only abnormal thing is that on my computer, the japanese characters don’t render. Here is the output from SAS running the above code interactively Alternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in. CSVs in SAS (via R) If I fix the file in R (because I know how to fix it there… another option is to fix it manually), library(readr) library(dplyr) tmp &lt;- read_csv(&quot;data/pokemon.csv&quot;)[,-1] Warning: Missing column names filled in: &#39;X1&#39; [1] ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_double(), name = col_character(), german_name = col_character(), japanese_name = col_character(), status = col_character(), species = col_character(), type_1 = col_character(), type_2 = col_character(), ability_1 = col_character(), ability_2 = col_character(), ability_hidden = col_character(), growth_rate = col_character(), egg_type_1 = col_character(), egg_type_2 = col_character() ) ℹ Use `spec()` for the full column specifications. # You&#39;ll learn how to do this later tmp &lt;- select(tmp, -japanese_name) %&gt;% mutate_all(iconv, from=&quot;UTF-8&quot;, to = &quot;ASCII//TRANSLIT&quot;) write_csv(tmp, &quot;data/pokemon_ascii.csv&quot;, na=&#39;.&#39;) Then, reading in the new file allows us to actually see the output. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 /* Create a library of class data */ 8 9 filename pokeloc &quot;data/pokemon_ascii.csv&quot;; 10 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 11 proc import datafile = pokeloc out=classdat.poke 12 DBMS = csv /* comma delimited file */ 13 replace; 14 GETNAMES = YES; 15 GUESSINGROWS = 1028 /* use all data for guessing the variable 15 ! type */ 16 ; 17 proc print data=classdat.poke (obs=10); /* print the first 10 17 ! observations */ 18 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12. This trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate. Non-comma delimited files in SAS (via R) To read in a pipe delimited file (the ‘|’ character), we have to make some changes. Here is the proc import code. Note that I am reading in a version of the file that I’ve converted to ASCII (see details below) because while the import works with the original file, it causes the SAS -&gt; R pipeline that the book is built on to break. tmp &lt;- readLines(&quot;data/NE_Features_20200501.txt&quot;) tmp_ascii &lt;- iconv(tmp, to = &quot;ASCII//TRANSLIT&quot;) writeLines(tmp_ascii, &quot;data/NE_Features_ascii.txt&quot;) 6 /* Without specifying the library to store the data in, it is 6 ! stored in WORK */ NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 7 proc import datafile = &quot;data/NE_Features_ascii.txt&quot; 7 ! out=nefeatures 8 DBMS = DLM /* delimited file */ 9 replace; 10 GETNAMES = YES; 11 DELIMITER = &#39;|&#39;; 12 GUESSINGROWS = 31582; 13 run; 14 15 proc print data=nefeatures (obs=10); /* print the first 10 15 ! observations */ ERROR: File WORK.NEFEATURES.DATA does not exist. 16 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 ERROR: Errors printed on pages 4,5,7,11,12. Under the hood, proc import is just writing code for a data step. So when proc import doesn’t work, we can just write the code ourselves. It requires a bit more work (specifying column names, for example) but it also doesn’t fail nearly as often. 6 /* x &quot;curl 6 ! https://raw.githubusercontent.com/srvanderplas/unl-stat850/maste 6 ! r/data/NE_Features_20200501.txt 7 &gt; data/NE_Features_20200501.txt&quot;; */ 8 /* only run this once... */ 9 10 data nefeatures; 11 /*infile &quot;data/NE_Features_20200501.txt&quot;*/ 12 infile &quot;data/NE_Features_ascii.txt&quot; 13 dlm=&#39;|&#39; /* specify delimiter */ 14 encoding=&quot;utf-8&quot; /* specify encoding */ 15 DSD /* delimiter sensitive data */ 16 missover /* keep going if missing obs encountered */ 17 firstobs=2; /* skip header row */ 18 input FEATURE_ID $ 19 FEATURE_NAME $ 20 FEATURE_CLASS $ 21 STATE_ALPHA $ 22 STATE_NUMERIC 23 COUNTY_NAME $ 24 COUNTY_NUMERIC $ 25 PRIMARY_LAT_DMS $ 26 PRIM_LONG_DMS $ 27 PRIM_LAT_DEC 28 PRIM_LONG_DEC 29 SOURCE_LAT_DMS $ 30 SOURCE_LONG_DMS $ 31 SOURCE_LAT_DEC 32 SOURCE_LONG_DEC 33 ELEV_IN_M 34 ELEV_IN_FT 35 MAP_NAME $ 36 DATE_CREATED $ 37 DATE_EDITED $ 38 ; 39 run; NOTE: The data set WORK.NEFEATURES has 0 observations and 20 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 40 41 proc print data=nefeatures (obs=10); /* print the first 10 41 ! observations */ 42 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. Try it out Rebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveninently available at their download page. Because these datasets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV. The readr package can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv? Solution library(readr) legosets &lt;- read_csv(&quot;https://cdn.rebrickable.com/media/downloads/sets.csv.gz&quot;) write_csv(legosets, &quot;data/lego_sets.csv&quot;) In SAS, it is also possible to read gzip files directly; however, it is tricky to get PROC IMPORT to work with gzip files. The code below will 1) download the file (uncomment that part), 2) create a link to the gzip file, 3) create a link to where the unzipped file will go, and 4) unzip the file to the link specified in (3). Can you write two different statements (one using proc import on the unzipped file, one using a datastep on the zipped file) to read the data in? Note that you may have to specify the length of character fields in the data step version using length var_name $ 100; before the input statement to set variable var_name to have maximum length of 100 characters. /* x &quot;curl https://cdn.rebrickable.com/media/downloads/sets.csv.gz &gt; \\ data/lego_sets.csv.gz&quot;; only run this once... */ filename legofile ZIP &quot;data/lego_sets.csv.gz&quot; GZIP; filename target &quot;data/lego_sets.csv&quot;; data _null_; infile legofile; file target; input; put _infile_; run; Solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 /* Work with the library of class data */ 8 9 filename legofile ZIP &quot;data/lego_sets.csv.gz&quot; GZIP; 10 filename target &quot;data/lego_sets.csv&quot;; 11 12 data _null_; 13 infile legofile; 14 file target; 15 input; 16 put _infile_; 17 run; NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 19 proc import datafile = target out=classdat.legoset DBMS=csv 19 ! replace; 20 GETNAMES=YES; 21 GUESSINGROWS=15424; 22 run; 23 24 /* This dataset will be stored in WORK */ 25 data legoset2; 26 infile legofile dsd firstobs=2 27 dlm=&quot;,&quot;; 28 length set_num $20; 29 length name $100; 30 input set_num $ name $ year theme_id num_parts; 31 run; NOTE: The data set WORK.LEGOSET2 has 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 32 33 proc print data=classdat.legoset (obs=10); 34 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 35 36 proc print data=legoset2 (obs=10); 37 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. 4.3 Spreadsheets In R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in Excel or LibreOffice first to make sure the formatting is as you expected it to be. if (!&quot;readxl&quot; %in% installed.packages()) install.packages(&quot;readxl&quot;) library(readxl) path &lt;- &quot;data/police_violence.xlsx&quot; if (!file.exists(path)) download.file(&quot;https://mappingpoliceviolence.org/s/MPVDatasetDownload.xlsx&quot;, path, mode = &quot;wb&quot;) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1) police_violence[1:10, 1:6] # A tibble: 10 x 6 `Victim&#39;s name` `Victim&#39;s age` `Victim&#39;s gender` `Victim&#39;s race` &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Eric M. Tellez 28 Male White 2 Name withheld by police NA Male Unknown race 3 Terry Hudson 57 Male Black 4 Malik Williams 23 Male Black 5 Frederick Perkins 37 Male Black 6 Michael Vincent Davis 49 Male White 7 Brian Elkins 47 Male Unknown race 8 Debra D. Arbuckle 51 Female White 9 Name withheld by police NA Male Unknown race 10 Cody McCaulou 27 Male White # … with 2 more variables: URL of image of victim &lt;chr&gt;, # Date of Incident (month/day/year) &lt;dttm&gt; In SAS, PROC IMPORT is one easy way to read in xlsx files. In this code chunk, we have to handle the fact that one of the columns in the spreadsheet contains dates. SAS and Excel handle dates a bit differently, so we have to transform the date variable – and we may as well relabel it at the same time. To do this, we use a DATA statement that outputs to the same dataset it references. We define a new variable date, adjust the Excel dates so that they conform to SAS’s standard, and tell SAS how to format the date. (We’ll talk more about dates and times later) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 8 PROC IMPORT OUT=classdat.police 9 DATAFILE=&quot;data/police_violence.xlsx&quot; 10 DBMS=xlsx /* Tell SAS what type of file it&#39;s reading */ 11 REPLACE; /* replace the dataset if it already exists */ 12 SHEET=&quot;2013-2019 Police Killings&quot;; /* SAS reads the first 12 ! sheet by default */ 13 GETNAMES=yes; 14 informat VAR6 mmddyy10.; /* tell SAS what format the date is 14 ! in */ 15 RUN; 16 17 DATA classdat.police; 18 SET classdat.police; /* modify the dataset and write back out 18 ! to it */ 19 20 date = VAR6 - 21916; /* Conversion to SAS date standard from 20 ! Excel */ 21 FORMAT date MMDDYY10.; /* Tell SAS how to format the data when 21 ! printing it */ 22 DROP VAR6; /* Get rid of the original data */ 23 24 num_age = INPUT(Victim_s_age, 3.); /* create numeric age 24 ! variable */ 25 26 DROP 27 A_brief_description_of_the_circu 28 URL_of_image_of_victim 29 Link_to_news_article_or_photo_of; 30 /* drop longer variable to save space so the file fits on 30 ! GitHub */ 31 /* Size went from 100 MB to 6.7 MB without these 3 vars */ 32 RUN; WARNING: The variable A_brief_description_of_the_circu in the DROP, KEEP, or RENAME list has never been referenced. WARNING: The variable URL_of_image_of_victim in the DROP, KEEP, or RENAME list has never been referenced. WARNING: The variable Link_to_news_article_or_photo_of in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The data set CLASSDAT.POLICE has 0 observations and 25 variables. WARNING: Data set CLASSDAT.POLICE was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 33 34 35 PROC PRINT DATA=classdat.police (obs=10); /* print the first 10 35 ! observations */ 36 VAR Victim_s_name Victim_s_age num_age Victim_s_gender 36 ! Victim_s_race date; 37 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,40. Here is some additional information about reading and writing Excel files in SAS. In general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier. Try it out The Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link Read that data in using both R and SAS. Be sure to look at the structure of the excel file, so that you can read the data in properly! Solution url &lt;- &quot;https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/2019_Veh_Reg_by_Plate_Type.xlsx&quot; download.file(url, destfile = &quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot;, mode = &quot;wb&quot;) library(readxl) ne_plates &lt;- read_xlsx(path = &quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot;, skip = 1) New names: * County -&gt; County...1 * County -&gt; County...34 ne_plates[1:10,1:6] Warning in fansi::strwrap_ctl(x, width = max(width, 0), indent = indent, : Encountered a C0 control character, see `?unhandled_ctl`; you can use `warn=FALSE` to turn off these warnings. # A tibble: 10 x 6 County...1 `Amateur\\r\\nRadio` `Apport-\\r\\nioned` `Apport\\r\\nTrlr` AC &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 C01 - DOUGLAS 148 0 0 10 2 C02 - LANCASTER 244 0 0 2 3 C03 - GAGE 10 0 0 1 4 C04 - CUSTER 6 0 0 0 5 C05 - DODGE 28 0 0 0 6 C06 - SAUNDERS 18 0 0 0 7 C07 - MADISON 19 0 0 0 8 C08 - HALL 16 0 0 0 9 C09 - BUFFALO 26 0 0 2 10 C10 - PLATTE 12 0 0 2 # … with 1 more variable: Breast Cancer &lt;dbl&gt; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 6 PROC IMPORT OUT=WORK.licplate 7 DATAFILE=&quot;data/2019_Vehicle_Registration_Plates_NE.xlsx&quot; 8 DBMS=xlsx /* Tell SAS what type of file it&#39;s reading */ 9 REPLACE; /* replace the dataset if it already exists */ 10 RANGE=&quot;&#39;Reg By Plate Type&#39;$A2:0&quot; 11 GETNAMES=yes; 12 RUN; 13 14 /* just a few columns... way too many to handle */ 15 PROC PRINT DATA=WORK.licplate (obs=10); /* print the first 10 15 ! observations */ ERROR: File WORK.LICPLATE.DATA does not exist. 16 Var County Amateur__Radio Breast__Cancer Choose__Life 16 ! County__Gov Comm__Truck Passenger Total; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 17 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41. 4.4 Binary Files Both R and SAS have binary data files that store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Luckily, it is possible to read the binary data files in both programs. Let’s read in the data from the 2009 National Household Travel Survey: 6 libname classdat &quot;data&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/data 7 /* this tells SAS where to look for (a bunch of) data files */ 8 9 proc contents data=classdat.cen10pub; /* This tells sas to 9 ! access the specific file */ NOTE: Data file CLASSDAT.CEN10PUB.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 10 run; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.01 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41. Data Set Name CLASSDAT.CEN10PUB Observations 150147 Member Type DATA Variables 8 Engine V9 Indexes 0 Created 06/30/2014 12:35:56 Observation Length 25 Last Modified 06/30/2014 12:35:56 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation WINDOWS_32 Encoding wlatin1 Western (Windows) Engine/Host Dependent Information Data Set Page Size 4096 Number of Data Set Pages 928 First Data Page 1 Max Obs per Page 162 Obs in First Data Page 75 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/stat850-textbook/data/cen10pub.sas7bdat Release Created 9.0202M2 Host Created W32_VSPRO Inode Number 39064453 Access Permission rw-rw-r– Owner Name susan File Size 4MB File Size (bytes) 3802112 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat Label 5 CBSACAT10 Char 2 CBSA category for the HH home address 4 CBSASIZE10 Char 2 CBSA (2010) population size for the HH home address 2 HH_CBSA10 Char 5 \\(CHAR5.&lt;/td&gt; &lt;td class=&quot;l data&quot;&gt;\\)CHAR5. HH CBSA location, 2013 CBSA definitions based on 2010 Census 1 HOUSEID Char 8 $8. $8. HH eight-digit ID number 3 RAIL10 Char 2 CBSA (2010) heavy rail status for HH 6 URBAN10 Char 2 Home address in urbanized area 8 URBRUR10 Char 2 Household in urban/rural area (2010 Urban definition) 7 URBSIZE10 Char 2 Size of urban area in which home address is located Koch Snowflake We can read the same file into R using the sas7bdat library: if (!&quot;sas7bdat&quot; %in% installed.packages()) install.packages(&quot;sas7bdat&quot;) library(sas7bdat) data &lt;- read.sas7bdat(&quot;https://github.com/srvanderplas/unl-stat850/raw/master/data/cen10pub.sas7bdat&quot;) head(data) HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10 1 20000017 XXXXX 02 02 03 04 06 02 2 20000231 XXXXX 02 03 03 01 03 01 3 20000521 XXXXX 02 03 03 01 03 01 4 20001283 35620 01 05 01 01 05 01 5 20001603 -1 02 06 04 04 06 02 6 20001649 XXXXX 02 03 03 01 02 01 If you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured. There are theoretically ways to read R data into SAS via the R subsystem. Feel free to do that on your own machine.15 In R, there are a couple of different binary files. .Rdata is perhaps the most common, and can store several objects (along with their names) in the same file. legos &lt;- read_csv(&quot;data/lego_sets.csv&quot;) ── Column specification ──────────────────────────────────────────────────────── cols( set_num = col_character(), name = col_character(), year = col_double(), theme_id = col_double(), num_parts = col_double() ) my_var &lt;- &quot;This variable contains a string&quot; save(legos, my_var, file = &quot;data/R_binary.Rdata&quot;) If we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious. We can load the R binary file back in using the load() function. rm(legos, my_var) # clear the files out ls() # all objects in the working environment [1] &quot;%dosomething%&quot; &quot;a&quot; &quot;abab_regex&quot; [4] &quot;adder&quot; &quot;addr_regex&quot; &quot;address&quot; [7] &quot;alert&quot; &quot;all_files&quot; &quot;all_lines&quot; [10] &quot;all_pkgs&quot; &quot;bank_balance&quot; &quot;basel_value&quot; [13] &quot;breaks&quot; &quot;ChickWeight&quot; &quot;circle_area&quot; [16] &quot;city&quot; &quot;college&quot; &quot;college_data&quot; [19] &quot;college_data2&quot; &quot;college_fips&quot; &quot;content&quot; [22] &quot;crimtab&quot; &quot;crust_ingredients&quot; &quot;current_value&quot; [25] &quot;data&quot; &quot;decimal&quot; &quot;dep_list&quot; [28] &quot;df&quot; &quot;disease_incidence&quot; &quot;dist&quot; [31] &quot;dn&quot; &quot;dog&quot; &quot;dog_hears&quot; [34] &quot;dog_regex&quot; &quot;essential_toppings&quot; &quot;f1&quot; [37] &quot;f2&quot; &quot;f3&quot; &quot;fun&quot; [40] &quot;fun2&quot; &quot;fun3&quot; &quot;gapminder&quot; [43] &quot;git_location&quot; &quot;grocery_list&quot; &quot;hiv&quot; [46] &quot;house_num&quot; &quot;human_talk&quot; &quot;i&quot; [49] &quot;ick&quot; &quot;id&quot; &quot;integer&quot; [52] &quot;line&quot; &quot;logical&quot; &quot;m3&quot; [55] &quot;m3or5&quot; &quot;m5&quot; &quot;mat&quot; [58] &quot;math_and_lsd&quot; &quot;mesodata&quot; &quot;mesodata_names&quot; [61] &quot;minv&quot; &quot;month_fct&quot; &quot;mtcars&quot; [64] &quot;myfun&quot; &quot;myfun2&quot; &quot;myfun3&quot; [67] &quot;ne_plates&quot; &quot;nebraska_locations&quot; &quot;newnames&quot; [70] &quot;npkgs&quot; &quot;nuid&quot; &quot;num_string&quot; [73] &quot;page&quot; &quot;path&quot; &quot;pattern&quot; [76] &quot;pattern2&quot; &quot;phone&quot; &quot;phone_num_variants&quot; [79] &quot;phone_regex&quot; &quot;pkgs&quot; &quot;pkgs2&quot; [82] &quot;pokemon_info&quot; &quot;police_violence&quot; &quot;prev_basel_value&quot; [85] &quot;prev_value&quot; &quot;r_files&quot; &quot;res&quot; [88] &quot;rmd_files&quot; &quot;sasexe&quot; &quot;sasopts&quot; [91] &quot;saspath&quot; &quot;sheets&quot; &quot;squirrels&quot; [94] &quot;squirrels_colorfix&quot; &quot;ssn&quot; &quot;state_facts&quot; [97] &quot;state.abb&quot; &quot;state.area&quot; &quot;state.center&quot; [100] &quot;state.division&quot; &quot;state.fips&quot; &quot;state.name&quot; [103] &quot;state.region&quot; &quot;state.x77&quot; &quot;street&quot; [106] &quot;string&quot; &quot;strings&quot; &quot;titles&quot; [109] &quot;tmp&quot; &quot;tmp_ascii&quot; &quot;tmp_chars&quot; [112] &quot;tmp_chars_space&quot; &quot;tmp_space&quot; &quot;try_install_pkg&quot; [115] &quot;txt&quot; &quot;url&quot; &quot;w&quot; [118] &quot;who_disease&quot; &quot;who_disease_long&quot; &quot;widths&quot; [121] &quot;x&quot; &quot;X&quot; &quot;X1&quot; [124] &quot;Xorig&quot; &quot;y&quot; &quot;Y&quot; [127] &quot;y_mag&quot; &quot;y_sign&quot; &quot;yfact&quot; [130] &quot;yummy_toppings&quot; &quot;z&quot; &quot;zip&quot; load(&quot;data/R_binary.Rdata&quot;) ls() # all objects in the working environment [1] &quot;%dosomething%&quot; &quot;a&quot; &quot;abab_regex&quot; [4] &quot;adder&quot; &quot;addr_regex&quot; &quot;address&quot; [7] &quot;alert&quot; &quot;all_files&quot; &quot;all_lines&quot; [10] &quot;all_pkgs&quot; &quot;bank_balance&quot; &quot;basel_value&quot; [13] &quot;breaks&quot; &quot;ChickWeight&quot; &quot;circle_area&quot; [16] &quot;city&quot; &quot;college&quot; &quot;college_data&quot; [19] &quot;college_data2&quot; &quot;college_fips&quot; &quot;content&quot; [22] &quot;crimtab&quot; &quot;crust_ingredients&quot; &quot;current_value&quot; [25] &quot;data&quot; &quot;decimal&quot; &quot;dep_list&quot; [28] &quot;df&quot; &quot;disease_incidence&quot; &quot;dist&quot; [31] &quot;dn&quot; &quot;dog&quot; &quot;dog_hears&quot; [34] &quot;dog_regex&quot; &quot;essential_toppings&quot; &quot;f1&quot; [37] &quot;f2&quot; &quot;f3&quot; &quot;fun&quot; [40] &quot;fun2&quot; &quot;fun3&quot; &quot;gapminder&quot; [43] &quot;git_location&quot; &quot;grocery_list&quot; &quot;hiv&quot; [46] &quot;house_num&quot; &quot;human_talk&quot; &quot;i&quot; [49] &quot;ick&quot; &quot;id&quot; &quot;integer&quot; [52] &quot;legos&quot; &quot;line&quot; &quot;logical&quot; [55] &quot;m3&quot; &quot;m3or5&quot; &quot;m5&quot; [58] &quot;mat&quot; &quot;math_and_lsd&quot; &quot;mesodata&quot; [61] &quot;mesodata_names&quot; &quot;minv&quot; &quot;month_fct&quot; [64] &quot;mtcars&quot; &quot;my_var&quot; &quot;myfun&quot; [67] &quot;myfun2&quot; &quot;myfun3&quot; &quot;ne_plates&quot; [70] &quot;nebraska_locations&quot; &quot;newnames&quot; &quot;npkgs&quot; [73] &quot;nuid&quot; &quot;num_string&quot; &quot;page&quot; [76] &quot;path&quot; &quot;pattern&quot; &quot;pattern2&quot; [79] &quot;phone&quot; &quot;phone_num_variants&quot; &quot;phone_regex&quot; [82] &quot;pkgs&quot; &quot;pkgs2&quot; &quot;pokemon_info&quot; [85] &quot;police_violence&quot; &quot;prev_basel_value&quot; &quot;prev_value&quot; [88] &quot;r_files&quot; &quot;res&quot; &quot;rmd_files&quot; [91] &quot;sasexe&quot; &quot;sasopts&quot; &quot;saspath&quot; [94] &quot;sheets&quot; &quot;squirrels&quot; &quot;squirrels_colorfix&quot; [97] &quot;ssn&quot; &quot;state_facts&quot; &quot;state.abb&quot; [100] &quot;state.area&quot; &quot;state.center&quot; &quot;state.division&quot; [103] &quot;state.fips&quot; &quot;state.name&quot; &quot;state.region&quot; [106] &quot;state.x77&quot; &quot;street&quot; &quot;string&quot; [109] &quot;strings&quot; &quot;titles&quot; &quot;tmp&quot; [112] &quot;tmp_ascii&quot; &quot;tmp_chars&quot; &quot;tmp_chars_space&quot; [115] &quot;tmp_space&quot; &quot;try_install_pkg&quot; &quot;txt&quot; [118] &quot;url&quot; &quot;w&quot; &quot;who_disease&quot; [121] &quot;who_disease_long&quot; &quot;widths&quot; &quot;x&quot; [124] &quot;X&quot; &quot;X1&quot; &quot;Xorig&quot; [127] &quot;y&quot; &quot;Y&quot; &quot;y_mag&quot; [130] &quot;y_sign&quot; &quot;yfact&quot; &quot;yummy_toppings&quot; [133] &quot;z&quot; &quot;zip&quot; RDS format (another binary file format in R) Another (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents. As a result, when you read from an RDS file, you need to store the result of that function into a variable. saveRDS(legos, &quot;data/RDSlego.rds&quot;) other_lego &lt;- readRDS(&quot;data/RDSlego.rds&quot;) Because RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately. Try it out Read in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. In RStudio, go to Session -&gt; Clear Workspace. (This will clear your environment) Now, using your RDS files, load the objects back into R with different names. Finally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious) Solution library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1, guess_max = 7000) police_violence2 &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 2, guess_max = 7000) save(police_violence, police_violence2, file = &quot;data/04_Try_Binary.Rdata&quot;) saveRDS(police_violence, &quot;data/04_Try_Binary1.rds&quot;) saveRDS(police_violence2, &quot;data/04_Try_Binary2.rds&quot;) rm(police_violence, police_violence2) # Limited clearing of workspace... pv1 &lt;- readRDS(&quot;data/04_Try_Binary1.rds&quot;) pv2 &lt;- readRDS(&quot;data/04_Try_Binary2.rds&quot;) load(&quot;data/04_Try_Binary.Rdata&quot;) all.equal(police_violence, pv1) [1] TRUE all.equal(police_violence2, pv2) [1] TRUE 4.5 Databases There are many different database formats. Some of the most common databases are SQL* related formats and Microsoft Access files. Strictly speaking you can get through this class without this section. Feel free to skip it and come back when/if you need it. This excellent GitHub repo contains code to connect to multiple types of databases in R, python, PHP, Java, SAS, and VBA 4.5.1 Microsoft Access In R, we can read in MS Access files using the Hmisc package, as long as the mdbtools library is available on your computer16. For this demo, we’ll be using the Scottish Witchcraft Database, which you can download from their website, or acquire from the course data folder. if (!&quot;Hmisc&quot; %in% installed.packages()) install.packages(&quot;Hmisc&quot;) library(Hmisc) Loading required package: lattice Loading required package: survival Loading required package: Formula Attaching package: &#39;Hmisc&#39; The following objects are masked from &#39;package:dplyr&#39;: src, summarize The following objects are masked from &#39;package:base&#39;: format.pval, units db_loc &lt;- &quot;data/Witchcraftsurvey_download.mdb&quot; mdb.get(db_loc, tables = TRUE) # get table list [1] &quot;WDB_Accused&quot; &quot;WDB_Accused_family&quot; [3] &quot;WDB_Appeal&quot; &quot;WDB_CalendarCustom&quot; [5] &quot;WDB_Case&quot; &quot;WDB_Case_person&quot; [7] &quot;WDB_Commission&quot; &quot;WDB_Complaint&quot; [9] &quot;WDB_Confession&quot; &quot;WDB_CounterStrategy&quot; [11] &quot;WDB_DemonicPact&quot; &quot;WDB_Denunciation&quot; [13] &quot;WDB_DevilAppearance&quot; &quot;WDB_Elf_FairyElements&quot; [15] &quot;WDB_Imprisonment&quot; &quot;WDB_LinkedTrial&quot; [17] &quot;WDB_Malice&quot; &quot;WDB_MentionedAsWitch&quot; [19] &quot;WDB_MovestoHLA&quot; &quot;WDB_MusicalInstrument&quot; [21] &quot;WDB_Ordeal&quot; &quot;WDB_OtherCharges&quot; [23] &quot;WDB_OtherNamedwitch&quot; &quot;WDB_Person&quot; [25] &quot;WDB_PrevCommission&quot; &quot;WDB_PropertyDamage&quot; [27] &quot;WDB_Ref_Parish&quot; &quot;WDB_Reference&quot; [29] &quot;WDB_ReligiousMotif&quot; &quot;WDB_RitualObject&quot; [31] &quot;WDB_ShapeChanging&quot; &quot;WDB_Source&quot; [33] &quot;WDB_Torture&quot; &quot;WDB_Trial&quot; [35] &quot;WDB_Trial_Person&quot; &quot;WDB_WeatherModification&quot; [37] &quot;WDB_WhiteMagic&quot; &quot;WDB_WitchesMeetingPlace&quot; mdb.get(db_loc, tables = &quot;WDB_Trial&quot;)[1:6,1:10] # get table of trials, print first 6 rows and 10 cols Trialref TrialId TrialSystemId CaseRef TrialType Trial.settlement 1 T/JO/1 1 JO C/EGD/2120 2 2 T/JO/100 100 JO C/JO/2669 2 3 T/JO/1000 1000 JO C/EGD/1474 2 4 T/JO/1001 1001 JO C/EGD/1558 2 5 T/JO/1002 1002 JO C/EGD/1681 2 6 T/JO/1003 1003 JO C/EGD/1680 2 Trial.parish Trial.presbytery Trial.county Trial.burgh 1 Aberdeen Aberdeen Aberdeen 2 3 4 5 6 Many databases have multiple tables with keys that connect information in each table. We’ll spend more time on databases later in the semester - for now, it’s enough to be able to get data out of one. Unfortunately, it appears that SAS on Linux doesn’t allow you to read in Access files. So I can’t demonstrate that for you. But, since you know how to do it in R, worst case you can open up R and export all of the tables to separate CSV files, then read those into SAS. 😢 4.5.2 SQLite SQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. Let’s try working with a sqlite file that has only one table: if (!&quot;RSQLite&quot; %in% installed.packages()) install.packages(&quot;RSQLite&quot;) if (!&quot;DBI&quot; %in% installed.packages()) install.packages(&quot;DBI&quot;) library(RSQLite) library(DBI) con &lt;- dbConnect(RSQLite::SQLite(), &quot;data/ssa-babynames-for-2015.sqlite&quot;) dbListTables(con) # List all the tables [1] &quot;babynames&quot; babyname &lt;- dbReadTable(con, &quot;babynames&quot;) head(babyname, 10) # show the first 10 obs state year name sex count rank_within_sex per_100k_within_sex 1 AK 2015 Olivia F 56 1 2367.9 2 AK 2015 Liam M 53 1 1590.6 3 AK 2015 Emma F 49 2 2071.9 4 AK 2015 Noah M 46 2 1380.6 5 AK 2015 Aurora F 46 3 1945.0 6 AK 2015 James M 45 3 1350.5 7 AK 2015 Amelia F 39 4 1649.0 8 AK 2015 Ava F 39 4 1649.0 9 AK 2015 William M 44 4 1320.5 10 AK 2015 Oliver M 41 5 1230.5 You can of course write formal queries using the DBI package, but for many databases, it’s easier to do the querying in R. We’ll cover both options later - the R version will be in the next module. In SAS, you can theoretically connect to SQLite databases, but there are very specific instructions for how to do that for each operating system. You’ll need to acquire the SQLite ODBC Driver for your operating system. You may also need to set up a DSN (Data Source Name) (Windows, Mac and Linux).17 Here is my .odbc.ini file as I’ve configured it for my Ubuntu 18.04 machine. A similar file should work for any Mac or Linux machine. In windows, you’ll need to use the ODBC Data Source Administrator to set this up. [babyname] Description = 2015 SSA baby names Driver = SQLite3 Database = data/ssa-babynames-for-2015.sqlite 6 /* This code requires that I&#39;ve set up a DSN connecting the 6 ! sqlite file to */ 7 /* a specific driver on my computer. You&#39;ll have to set up your 7 ! machine to */ 8 /* have a configuration that is appropriate for your setup */ 9 10 libname mydata odbc complete = 10 ! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX; ERROR: CLI error trying to establish connection: [unixODBC][Driver Manager]Can&#39;t open lib &#39;libsqlite3odbc.so&#39; : file not found ERROR: Error in the LIBNAME statement. 11 12 proc print data=mydata.babynames (obs=10); ERROR: Libref MYDATA is not assigned. 13 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42. 4.6 Exploratory Data Analysis Once your data has been read in, we can do some basic exploratory data analysis. EDA is important because it helps us to know what challenges a particular data set might bring. Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence. While in many classes you’ll be given cleaner data, you do need to know how to clean your own data up so that you can use more interesting datasets for projects (and for fun!). The EDA chapter in R for Data Science is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. It is so good that I am not going to try to completely reproduce it here. Both R and SAS make it relatively easy to get summary statistics from a dataset, but the “flow” of EDA is somewhat different between the two programs, so this section will cover SAS first, and then R. Major components of EDA: - tables - summary statistics - basic plots - unique values 4.6.1 SAS Proc Freq generates frequency tables for variables or interactions of variables. This can help you to see whether there is missing information. Using those frequency tables, you can create frequency plots and set up chi squared tests. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 ODS GRAPHICS ON; 9 PROC FREQ DATA=classdat.poke ORDER=FORMATTED; 10 TABLES generation / CHISQ PLOTS=freqplot(type=dotplot); 11 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 PROC FREQ DATA=classdat.poke ORDER=FREQ; 13 TABLES type_1 status / MAXLEVELS=10 13 ! PLOTS=freqplot(type=dotplot scale=percent); 14 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 ODS GRAPHICS OFF; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42. Proc Means can be used to get more useful summary statistics for numeric variables. Note that the Class statement identifies a categorical variable; the summary statistics are computed for each level of this variable. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC MEANS DATA = classdat.poke; 9 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 10 11 proc means data = classdat.poke; 12 class status; 13 run; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42. For even higher levels of detail, Proc Univariate will provide variability, tests for location, quantiles, skewness, and will identify the extreme observations for you. You can also get histograms for variables, even specifying distributions you’d like to be fit to the data (if that’s something you want). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 ODS GRAPHICS ON; 9 PROC UNIVARIATE DATA = classdat.poke; 10 VAR attack defense sp_attack sp_defense speed; 11 HISTOGRAM attack defense sp_attack sp_defense speed; 12 RUN; NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ODS GRAPHICS OFF; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42. Proc Corr allows you to examine the relationship between two quantitative variables. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 ODS GRAPHICS ON; 9 PROC CORR DATA = classdat.poke PLOTS( 9 ! MAXPOINTS=200000)=MATRIX(HISTOGRAM); 10 VAR attack defense sp_attack sp_defense speed ; 11 RUN; NOTE: PROCEDURE CORR used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ODS GRAPHICS OFF; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42,43. The plot here is called a scatterplot matrix. It contains histograms on the diagonal, and pairwise scatterplots on off-diagonals. It can be useful for spotting strong correlations among multiple variables which may affect the way you build a model. Try it out One of the datasets we read in above records incidents of police violence around the country. Explore the variables present in this dataset (see code in the spreadsheets section to read it in). Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis. It is useful to memorize the SAS PROC options you use most frequently, but it’s also a good idea to reference the SAS documentation - it provides a list of all viable options for each procedure, and generally has decent examples to show how those options are used. Solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 ODS GRAPHICS ON; 9 PROC CONTENTS DATA = classdat.police; /* see what&#39;s in the 9 ! dataset */ 10 RUN; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 11 12 PROC FREQ DATA = classdat.police ORDER=FREQ; /* Examine Freq of 12 ! common vars */ 13 TABLES Victim_s_gender Victim_s_race State Cause_of_death 14 Unarmed Geography__via_Trulia_methodolog / MAXLEVELS = 14 ! 10; 15 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 17 PROC FREQ DATA = classdat.police ORDER=FREQ; /* Combinations of 17 ! vars */ 18 TABLES Unarmed * Criminal_Charges_ / NOCUM NOPERCENT NOCOL NOROW 18 ! MAXLEVELS=10; 19 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 21 PROC MEANS DATA = classdat.police; /* Numeric variable 21 ! exploration */ 22 VAR num_age; /* Only numeric variable in this set */ 23 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC UNIVARIATE DATA = classdat.police; /* Investigating 25 ! age/date info */ 26 HISTOGRAM num_age date; 27 RUN; NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 ODS GRAPHICS OFF; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41,42,43. Data Set Name CLASSDAT.POLICE Observations 7663 Member Type DATA Variables 25 Engine V9 Indexes 0 Created 04/21/2021 11:33:07 Observation Length 896 Last Modified 04/21/2021 11:33:07 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 73728 Number of Data Set Pages 94 First Data Page 1 Max Obs per Page 82 Obs in First Data Page 75 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas/police.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 39065012 Access Permission rw-rw-r– Owner Name susan File Size 7MB File Size (bytes) 7004160 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat Label 10 Agency_responsible_for_death Char 177 $177. $177. Agency responsible for death 17 Alleged_Threat_Level__Source__Wa Char 12 $12. $12. Alleged Threat Level (Source: WaPo) 11 Cause_of_death Char 39 $39. $39. Cause of death 6 City Char 29 $29. $29. City 9 County Char 80 $80. $80. County 13 Criminal_Charges_ Char 77 $77. $77. Criminal Charges? 22 Geography__via_Trulia_methodolog Char 8 $8. $8. Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) 23 ID Num 8 BEST. ID 21 Off_Duty_Killing_ Char 8 $8. $8. Off-Duty Killing? 12 Official_disposition_of_death__j Char 176 $176. $176. Official disposition of death (justified or other) 7 State Char 2 $2. $2. State 5 Street_Address_of_Incident Char 73 $73. $73. Street Address of Incident 14 Symptoms_of_mental_illness_ Char 19 $19. $19. Symptoms of mental illness? 15 Unarmed Char 15 $15. $15. Unarmed 16 VAR20 Char 32 $32. $32. Alleged Weapon (Source: WaPo) 18 VAR22 Char 11 $11. $11. Fleeing (Source: WaPo) 19 VAR23 Char 18 $18. $18. Body Camera (Source: WaPo) 2 Victim_s_age Char 7 $7. $7. Victim's age 3 Victim_s_gender Char 11 $11. $11. Victim's gender 1 Victim_s_name Char 49 $49. $49. Victim's name 4 Victim_s_race Char 16 $16. $16. Victim's race 20 WaPo_ID__If_included_in_WaPo_dat Num 8 BEST. WaPo ID (If included in WaPo database) 8 Zipcode Char 5 $5. $5. Zipcode 24 date Num 8 MMDDYY10. 25 num_age Num 8 Koch Snowflake Oddities to note: - Gender - Unknown should be recoded as missing (’ ’) - Victim_s_race - Unknown race and Unknown Race should be recoded as missing - State - might need to check to make sure all states are valid (but top 10 are, at least) - Cause of death - sometimes, there are multiple causes. Also, varying capitalizations… - Geography - #N/A should be recoded as missing - Criminal_Charges_ - What does No/NO mean? (would need to look up in the codebook) - Age - the maximum age recorded is 107, which bears some investigation… other extreme observations between 89 and 95 are also fairly interesting and could be investigated further. There are also several infants/young children included, which is horribly sad, but believable. - Date - PROC UNIVARIATE doesn’t display date results with a meaningful format, even though format is specified. - Conclusions (ok, probably obvious before this analysis): - It’s much more likely for charges to be filed if the suspect was unarmed (but still very rare) - Data is relatively evenly distributed between 2013 and 2019. - It’s fairly rare for police to kill female or transgender individuals - around 5% of all victims - California, Texas, and Florida, while populous, seem to have a disproportionate number of killings, especially compared to e.g. NY, which is also a high population state. To really make the state numbers meaningful, though, we’d need to know population counts. There’s also an issue of accurate comparisons - some states may not report police killings with the same standards as other states. 4.6.2 R In SAS, EDA is fairly straightforward - you use specific procedures for each data type, and the plots which may be most useful come along with those procedures. It’s something like ordering off of a menu of pre-defined meals, and then slightly customizing your order. In R, you put your whole order together from the a la carte menu. That is, R will give you all of the same summary information (and possibly more), but you have to assemble a series of commands to get each portion. This can be more efficient (since you don’t have to wade through pages of output to get the piece you want) but may take a bit more coding as well. In this section, I will mostly be using the plot commands that come with base R and require no extra packages. The R for Data Science book shows plot commands which use the ggplot2 library. We will learn this library later in this class - it produces beautiful plots - and if you want to use it at this point, you may. It requires a bit more thought as to how to specify the plot, though, which may not be desireable. The first, and most basic EDA command in R is summary(). For numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all “other” category. library(readr) url &lt;- &quot;https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv&quot; poke &lt;- read_csv(url) Warning: Missing column names filled in: &#39;X1&#39; [1] ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_double(), name = col_character(), german_name = col_character(), japanese_name = col_character(), status = col_character(), species = col_character(), type_1 = col_character(), type_2 = col_character(), ability_1 = col_character(), ability_2 = col_character(), ability_hidden = col_character(), growth_rate = col_character(), egg_type_1 = col_character(), egg_type_2 = col_character() ) ℹ Use `spec()` for the full column specifications. # Make types into factors to demonstrate the difference poke$type_1 &lt;- factor(poke$type_1) poke$type_2 &lt;- factor(poke$type_2) summary(poke) X1 pokedex_number name german_name Min. : 0.0 Min. : 1.0 Length:1028 Length:1028 1st Qu.: 256.8 1st Qu.:213.8 Class :character Class :character Median : 513.5 Median :433.5 Mode :character Mode :character Mean : 513.5 Mean :437.7 3rd Qu.: 770.2 3rd Qu.:663.2 Max. :1027.0 Max. :890.0 japanese_name generation status species Length:1028 Min. :1.000 Length:1028 Length:1028 Class :character 1st Qu.:2.000 Class :character Class :character Mode :character Median :4.000 Mode :character Mode :character Mean :4.034 3rd Qu.:6.000 Max. :8.000 type_number type_1 type_2 height_m weight_kg Min. :1.000 Water :134 Flying :109 Min. : 0.100 Min. : 0.10 1st Qu.:1.000 Normal :115 Fairy : 41 1st Qu.: 0.600 1st Qu.: 8.80 Median :2.000 Grass : 91 Ground : 39 Median : 1.000 Median : 28.50 Mean :1.527 Bug : 81 Poison : 38 Mean : 1.368 Mean : 69.75 3rd Qu.:2.000 Psychic: 76 Psychic: 38 3rd Qu.: 1.500 3rd Qu.: 69.10 Max. :2.000 Fire : 65 (Other):277 Max. :100.000 Max. :999.90 (Other):466 NA&#39;s :486 NA&#39;s :1 abilities_number ability_1 ability_2 ability_hidden Min. :0.000 Length:1028 Length:1028 Length:1028 1st Qu.:2.000 Class :character Class :character Class :character Median :2.000 Mode :character Mode :character Mode :character Mean :2.284 3rd Qu.:3.000 Max. :3.000 total_points hp attack defense Min. : 175.0 Min. : 1.00 Min. : 5.00 Min. : 5.00 1st Qu.: 330.0 1st Qu.: 50.00 1st Qu.: 55.00 1st Qu.: 50.00 Median : 455.0 Median : 66.50 Median : 76.00 Median : 70.00 Mean : 437.6 Mean : 69.58 Mean : 80.12 Mean : 74.48 3rd Qu.: 510.0 3rd Qu.: 80.00 3rd Qu.:100.00 3rd Qu.: 90.00 Max. :1125.0 Max. :255.00 Max. :190.00 Max. :250.00 sp_attack sp_defense speed catch_rate Min. : 10.00 Min. : 20.00 Min. : 5.00 Min. : 3.00 1st Qu.: 50.00 1st Qu.: 50.00 1st Qu.: 45.00 1st Qu.: 45.00 Median : 65.00 Median : 70.00 Median : 65.00 Median : 60.00 Mean : 72.73 Mean : 72.13 Mean : 68.53 Mean : 93.17 3rd Qu.: 95.00 3rd Qu.: 90.00 3rd Qu.: 90.00 3rd Qu.:127.00 Max. :194.00 Max. :250.00 Max. :180.00 Max. :255.00 NA&#39;s :104 base_friendship base_experience growth_rate egg_type_number Min. : 0.00 Min. : 36.0 Length:1028 Min. :0.000 1st Qu.: 70.00 1st Qu.: 67.0 Class :character 1st Qu.:1.000 Median : 70.00 Median :159.0 Mode :character Median :1.000 Mean : 64.14 Mean :153.8 Mean :1.271 3rd Qu.: 70.00 3rd Qu.:201.5 3rd Qu.:2.000 Max. :140.00 Max. :608.0 Max. :2.000 NA&#39;s :104 NA&#39;s :104 egg_type_1 egg_type_2 percentage_male egg_cycles Length:1028 Length:1028 Min. : 0 Min. : 5.00 Class :character Class :character 1st Qu.: 50 1st Qu.: 20.00 Mode :character Mode :character Median : 50 Median : 20.00 Mean : 55 Mean : 30.32 3rd Qu.: 50 3rd Qu.: 25.00 Max. :100 Max. :120.00 NA&#39;s :236 NA&#39;s :1 against_normal against_fire against_water against_electric Min. :0.0000 Min. :0.000 Min. :0.000 Min. :0.000 1st Qu.:1.0000 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.500 Median :1.0000 Median :1.000 Median :1.000 Median :1.000 Mean :0.8684 Mean :1.125 Mean :1.054 Mean :1.034 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:1.000 3rd Qu.:1.000 Max. :1.0000 Max. :4.000 Max. :4.000 Max. :4.000 against_grass against_ice against_fight against_poison Min. :0.000 Min. :0.000 Min. :0.000 Min. :0.0000 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.500 1st Qu.:0.5000 Median :1.000 Median :1.000 Median :1.000 Median :1.0000 Mean :1.004 Mean :1.196 Mean :1.079 Mean :0.9523 3rd Qu.:1.000 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:1.0000 Max. :4.000 Max. :4.000 Max. :4.000 Max. :4.0000 against_ground against_flying against_psychic against_bug Min. :0.000 Min. :0.250 Min. :0.0000 Min. :0.0000 1st Qu.:0.500 1st Qu.:1.000 1st Qu.:1.0000 1st Qu.:0.5000 Median :1.000 Median :1.000 Median :1.0000 Median :1.0000 Mean :1.085 Mean :1.166 Mean :0.9793 Mean :0.9925 3rd Qu.:1.625 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :4.000 Max. :4.000 Max. :4.0000 Max. :4.0000 against_rock against_ghost against_dragon against_dark Min. :0.25 Min. :0.000 Min. :0.0000 Min. :0.250 1st Qu.:1.00 1st Qu.:1.000 1st Qu.:1.0000 1st Qu.:1.000 Median :1.00 Median :1.000 Median :1.0000 Median :1.000 Mean :1.24 Mean :1.011 Mean :0.9757 Mean :1.066 3rd Qu.:2.00 3rd Qu.:1.000 3rd Qu.:1.0000 3rd Qu.:1.000 Max. :4.00 Max. :4.000 Max. :2.0000 Max. :4.000 against_steel against_fairy Min. :0.0000 Min. :0.000 1st Qu.:0.5000 1st Qu.:1.000 Median :1.0000 Median :1.000 Mean :0.9803 Mean :1.085 3rd Qu.:1.0000 3rd Qu.:1.000 Max. :4.0000 Max. :4.000 One common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis. There is one pokemon who appears to not have a weight specified. Let’s investigate further: poke[is.na(poke$weight_kg),] # Show any rows where weight.kg is NA # A tibble: 1 x 51 X1 pokedex_number name german_name japanese_name generation status species &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 1027 890 Eter… &lt;NA&gt; &lt;NA&gt; 8 Legen… Gigant… # … with 43 more variables: type_number &lt;dbl&gt;, type_1 &lt;fct&gt;, type_2 &lt;fct&gt;, # height_m &lt;dbl&gt;, weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, # ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, # attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, # speed &lt;dbl&gt;, catch_rate &lt;dbl&gt;, base_friendship &lt;dbl&gt;, # base_experience &lt;dbl&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, # egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;dbl&gt;, # egg_cycles &lt;dbl&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, # against_water &lt;dbl&gt;, against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, # against_ice &lt;dbl&gt;, against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, # against_ground &lt;dbl&gt;, against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, # against_bug &lt;dbl&gt;, against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, # against_dragon &lt;dbl&gt;, against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, # against_fairy &lt;dbl&gt; This is the last row of our data frame, and this pokemon appears to have many missing values. We are often also interested in the distribution of values. We can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). table(poke$generation) 1 2 3 4 5 6 7 8 192 107 165 121 171 85 99 88 plot(table(poke$generation)) # bar plot table(poke$type_1, poke$type_2) Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass Bug 0 0 0 4 2 4 2 14 1 6 Dark 0 0 4 0 3 2 3 5 2 0 Dragon 0 0 0 1 1 2 1 6 3 0 Electric 0 2 2 0 2 0 1 6 1 1 Fairy 0 0 0 0 0 0 0 2 0 0 Fighting 0 1 0 0 0 0 0 1 1 0 Fire 2 1 2 0 0 7 0 7 2 0 Flying 0 0 2 0 0 0 0 0 0 0 Ghost 0 1 2 0 1 0 3 3 0 11 Grass 0 3 5 0 5 3 0 7 1 0 Ground 0 3 2 1 0 0 1 4 4 0 Ice 2 0 0 0 1 0 1 2 1 0 Normal 0 0 1 0 5 4 0 27 0 2 Poison 1 5 4 0 1 2 2 3 0 0 Psychic 0 1 1 0 9 3 1 7 3 1 Rock 2 2 2 3 3 1 2 6 0 2 Steel 0 0 2 0 4 1 0 2 4 0 Water 2 7 3 2 4 3 0 7 2 3 Ground Ice Normal Poison Psychic Rock Steel Water Bug 2 0 0 12 2 3 7 3 Dark 0 2 5 0 2 0 2 0 Dragon 7 3 0 0 4 0 0 0 Electric 0 2 2 3 1 0 4 1 Fairy 0 0 0 0 0 0 1 0 Fighting 0 1 0 0 3 0 3 0 Fire 3 0 2 0 2 1 1 1 Flying 0 0 0 0 0 0 1 1 Ghost 2 0 0 4 0 0 0 0 Grass 1 3 0 15 2 0 3 0 Ground 0 0 0 0 2 3 4 0 Ice 3 0 0 0 2 0 2 3 Normal 1 0 0 0 3 0 0 1 Poison 2 0 0 0 0 0 0 3 Psychic 0 2 2 0 0 0 2 0 Rock 6 2 0 1 2 0 4 6 Steel 2 0 0 0 7 3 0 0 Water 10 4 0 3 6 5 1 0 plot(table(poke$type_1, poke$type_2)) # mosaic plot - hard to read b/c too many categories There are better options for examining this data, but they are easier to get in ggplot2. library(ggplot2) # define the x and y axis variables first ggplot(data = poke, aes(x = type_1, y = type_2)) + # define what will be plotted (points) # and what aesthetics will be used (size, color) # and how those aesthetics will be mapped to values # (proportional to the count in a 2d bin) geom_point(aes(size = ..count.., color = ..count..), stat = &quot;bin2d&quot;) We can also generate histograms or bar charts18 By default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desireable result. If we do not specify breaks, R will pick them for us. hist(poke$generation) # This isn&#39;t really optimal... we only have whole numbers. hist(poke$generation, breaks = 0:8) # Much better. For continuous variables, we can use histograms, or we can examine kernel density plots. Remember that %&gt;% is the “pipe” and takes the left side of the pipe to pass as an argument to the right side. This makes code easier to read because it becomes a step-wise “recipe.” library(magrittr) # This provides the pipe command, %&gt;% hist(poke$weight_kg) poke$weight_kg %&gt;% log10() %&gt;% # Take the log - will transformation be useful w/ modeling? hist() # create a histogram poke$weight_kg %&gt;% density(na.rm = T) %&gt;% # First, we compute the kernel density # (na.rm = T says to ignore NA values) plot() # Then, we plot the result poke$weight_kg %&gt;% log10() %&gt;% # Transform the variable density(na.rm = T) %&gt;% # Compute the density ignoring missing values plot(main = &quot;Density of Log10 pokemon weight in Kg&quot;) # Plot the result, # changing the title of the plot to a meaningful value We may also want to look at correlations between variables. In R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms. To examine the relationship between a categorical variable and a continuous variable, we might look at boxplots: boxplot(log10(height_m) ~ status, data = poke) boxplot(total_points ~ species, data = poke) In the second boxplot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn’t usually about creating pretty plots (or we’d be using ggplot right now) but rather about identifying things which may come up in the analysis later. To look at the relationship between numeric variables, we could compute a numeric correlation, but a plot is more useful. plot(defense ~ attack, data = poke, type = &quot;p&quot;) cor(poke$defense, poke$attack) [1] 0.4507656 Sometimes, we discover that a variable which appears to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset. plot(x = poke$base_experience, y = poke$base_friendship, type = &quot;p&quot;) A scatterplot matrix can also be a useful way to visualize relationships between several variables. pairs(poke[,19:23]) # hp - sp_defense columns There’s more information on how to customize these plots here. Try it out Explore the variables present in the police violence data (see code in the spreadsheets section to read it in). Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis. How does your analysis in R differ from the way that you approached the data in SAS? Solution if (!&quot;readxl&quot; %in% installed.packages()) install.packages(&quot;readxl&quot;) library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, sheet = 1, guess_max = 7000) police_violence$`Victim&#39;s age` &lt;- as.numeric(police_violence$`Victim&#39;s age`) Warning: NAs introduced by coercion summary(police_violence) Victim&#39;s name Victim&#39;s age Victim&#39;s gender Victim&#39;s race Length:7663 Min. : 1.0 Length:7663 Length:7663 Class :character 1st Qu.: 27.0 Class :character Class :character Mode :character Median : 34.0 Mode :character Mode :character Mean : 36.8 3rd Qu.: 45.0 Max. :107.0 NA&#39;s :206 URL of image of victim Date of Incident (month/day/year) Length:7663 Min. :2013-01-01 00:00:00 Class :character 1st Qu.:2014-10-07 00:00:00 Mode :character Median :2016-07-07 00:00:00 Mean :2016-07-06 23:33:18 3rd Qu.:2018-04-04 00:00:00 Max. :2019-12-31 00:00:00 Street Address of Incident City State Length:7663 Length:7663 Length:7663 Class :character Class :character Class :character Mode :character Mode :character Mode :character Zipcode County Agency responsible for death Length:7663 Length:7663 Length:7663 Class :character Class :character Class :character Mode :character Mode :character Mode :character Cause of death Length:7663 Class :character Mode :character A brief description of the circumstances surrounding the death Length:7663 Class :character Mode :character Official disposition of death (justified or other) Criminal Charges? Length:7663 Length:7663 Class :character Class :character Mode :character Mode :character Link to news article or photo of official document Symptoms of mental illness? Length:7663 Length:7663 Class :character Class :character Mode :character Mode :character Unarmed Alleged Weapon (Source: WaPo) Length:7663 Length:7663 Class :character Class :character Mode :character Mode :character Alleged Threat Level (Source: WaPo) Fleeing (Source: WaPo) Length:7663 Length:7663 Class :character Class :character Mode :character Mode :character Body Camera (Source: WaPo) WaPo ID (If included in WaPo database) Length:7663 Min. : 3 Class :character 1st Qu.:1402 Mode :character Median :2722 Mean :2724 3rd Qu.:4051 Max. :5439 NA&#39;s :2785 Off-Duty Killing? Length:7663 Class :character Mode :character Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) Length:7663 Class :character Mode :character ID Min. : 1 1st Qu.:1916 Median :3832 Mean :3833 3rd Qu.:5750 Max. :7667 Let’s examine the numeric and date variables first: hist(police_violence$`Victim&#39;s age`) # hist(police_violence$`Date of Incident (month/day/year)`) # This didn&#39;t work - it wants me to specify breaks # Instead, lets see if ggplot handles it better - from R4DS library(ggplot2) ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) + geom_density() Let’s look at the victims’ gender and race: table(police_violence$`Victim&#39;s race`, useNA = &#39;ifany&#39;) Asian Black Hispanic Native American 118 1944 1335 112 Pacific Islander Unknown race Unknown Race White 42 670 64 3378 table(police_violence$`Victim&#39;s gender`) Female Male Transgender Unknown 391 7253 7 4 table(police_violence$`Victim&#39;s race`, police_violence$`Victim&#39;s gender`) Female Male Transgender Unknown Asian 6 111 0 1 Black 69 1871 2 1 Hispanic 49 1283 1 0 Native American 7 104 0 0 Pacific Islander 2 40 0 0 Unknown race 33 632 1 2 Unknown Race 2 62 0 0 White 223 3150 3 0 plot(table(police_violence$`Victim&#39;s race`, police_violence$`Victim&#39;s gender`), main = &quot;Police Killing by Race, Gender&quot;) We can also look at the age range for each race: police_violence %&gt;% # get groups with at least 100 observations that aren&#39;t unknown subset(`Victim&#39;s race` %in% c(&quot;Asian&quot;, &quot;Black&quot;, &quot;Native American&quot;, &quot;Hispanic&quot;, &quot;White&quot;)) %&gt;% boxplot(`Victim&#39;s age` ~ `Victim&#39;s race`, data = .) And examine the age range for each gender as well: police_violence %&gt;% boxplot(`Victim&#39;s age` ~ `Victim&#39;s gender`, data = .) The thing I’m honestly most surprised at with this plot is that there are so many elderly individuals (of both genders) shot. That’s not a realization I’d normally construct this plot for, but the visual emphasis on the outliers in a boxplot makes it much easier to focus on that aspect of the data. My analysis in R was a bit more free-form than in SAS - in SAS, I proceeded fairly directly through each procedure, while in R, I could investigate things that caught my eye along the way more easily. I didn’t focus as much on what we’d need to clean up in R (because the same problems exist that we identified when using SAS). 4.6.2.1 skimr package I discovered this package while looking over the material in this chapter a second time (so this is new as of 2020/09/07). if (!&quot;skimr&quot; %in% installed.packages()) install.packages(&quot;skimr&quot;) library(skimr) skim(police_violence) Table 4.1: Data summary Name police_violence Number of rows 7663 Number of columns 27 _______________________ Column type frequency: character 23 numeric 3 POSIXct 1 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace Victim’s name 0 1.00 7 49 0 7411 0 Victim’s gender 8 1.00 4 11 0 4 0 Victim’s race 0 1.00 5 16 0 8 0 URL of image of victim 3462 0.55 27 10527 0 4192 0 Street Address of Incident 83 0.99 3 73 0 7487 0 City 6 1.00 3 29 0 2884 0 State 0 1.00 2 2 0 51 0 Zipcode 39 0.99 4 5 0 4996 0 County 15 1.00 3 80 0 1110 0 Agency responsible for death 16 1.00 7 177 0 2853 0 Cause of death 0 1.00 4 39 0 30 0 A brief description of the circumstances surrounding the death 20 1.00 30 1631 0 7581 0 Official disposition of death (justified or other) 256 0.97 7 176 0 97 0 Criminal Charges? 0 1.00 2 77 0 29 0 Link to news article or photo of official document 12 1.00 21 312 0 7560 0 Symptoms of mental illness? 11 1.00 2 19 0 6 0 Unarmed 0 1.00 7 15 0 4 0 Alleged Weapon (Source: WaPo) 0 1.00 2 32 0 169 0 Alleged Threat Level (Source: WaPo) 2382 0.69 5 12 0 3 0 Fleeing (Source: WaPo) 2616 0.66 1 11 0 8 0 Body Camera (Source: WaPo) 2869 0.63 2 18 0 5 0 Off-Duty Killing? 7437 0.03 8 8 0 1 0 Geography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx ) 67 0.99 5 8 0 3 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Victim’s age 206 0.97 36.80 13.21 1 27.00 34 45.00 107 ▂▇▃▁▁ WaPo ID (If included in WaPo database) 2785 0.64 2723.53 1534.33 3 1402.25 2722 4050.75 5439 ▇▇▇▇▇ ID 0 1.00 3832.89 2213.34 1 1916.50 3832 5749.50 7667 ▇▇▇▇▇ Variable type: POSIXct skim_variable n_missing complete_rate min max median n_unique Date of Incident (month/day/year) 0 1 2013-01-01 2019-12-31 2016-07-07 2404 You may find the summary tables given by the skimr package to be more appealing - it separates the variables out by type, provides histograms of numeric variables, and is compatible with rmarkdown/knitr. If you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I’m cheating a bit by mentioning it now, but it’s just so useful!) library(dplyr) police_violence %&gt;% # get variables which are important select(matches(&quot;age$|race|gender|Cause|Symptoms|Unarmed&quot;)) %&gt;% group_by(Unarmed) %&gt;% skim() Table 4.2: Data summary Name Piped data Number of rows 7663 Number of columns 6 _______________________ Column type frequency: character 4 numeric 1 ________________________ Group variables Unarmed Variable type: character skim_variable Unarmed n_missing complete_rate min max empty n_unique whitespace Victim’s gender Allegedly Armed 3 1.00 4 11 0 4 0 Victim’s gender Unarmed 0 1.00 4 11 0 4 0 Victim’s gender Unclear 2 1.00 4 7 0 3 0 Victim’s gender Vehicle 3 0.99 4 11 0 3 0 Victim’s race Allegedly Armed 0 1.00 5 16 0 8 0 Victim’s race Unarmed 0 1.00 5 16 0 8 0 Victim’s race Unclear 0 1.00 5 16 0 8 0 Victim’s race Vehicle 0 1.00 5 16 0 8 0 Cause of death Allegedly Armed 0 1.00 4 39 0 21 0 Cause of death Unarmed 0 1.00 5 39 0 17 0 Cause of death Unclear 0 1.00 5 25 0 8 0 Cause of death Vehicle 0 1.00 5 14 0 4 0 Symptoms of mental illness? Allegedly Armed 10 1.00 2 19 0 6 0 Symptoms of mental illness? Unarmed 0 1.00 2 19 0 4 0 Symptoms of mental illness? Unclear 1 1.00 2 19 0 4 0 Symptoms of mental illness? Vehicle 0 1.00 2 19 0 4 0 Variable type: numeric skim_variable Unarmed n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Victim’s age Allegedly Armed 139 0.97 37.82 13.64 14 27 35 47 107 ▇▇▃▁▁ Victim’s age Unarmed 16 0.99 34.36 12.18 1 25 33 41 89 ▁▇▅▁▁ Victim’s age Unclear 34 0.95 35.36 11.60 15 26 34 43 76 ▇▇▆▂▁ Victim’s age Vehicle 17 0.97 32.82 10.75 15 25 31 38 77 ▆▇▃▁▁ This summary allows us to see very quickly that there is a difference in the age distribution of unarmed individuals who died during an encounter with police - unarmed individuals are likely to be significantly older on average. If you are using skimr in knitr/rmarkdown, your data frame will automatically render as a custom-printed table if the last line in the code chunk is a skim_df object. There are many ways to customize the summary statistics detailed in the package that I’m not going to go into here, but you are free to investigate if you like the way these summaries look. I mention this package now because it is appropriate for EDA, but it may not be intuitive or easy to use in the way you might want to use it until after we cover the dplyr package in the manipulating data module and the tidyr package in the transforming data module. 4.6.2.2 janitor package The janitor package has functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing. janitor and clean_names() by Allison Horst 4.6.3 Comparison You must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of “give me every possible statistic that could be calculated from this model, whether or not it makes sense.” The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. – Douglas Bates References and Links Reading JSON in SAS – You know SAS documentation is getting weird when they advertise a method as “the sexiest way to import JSON data into SAS.” Reading Rdata files in SAS Common problems with SAS data files U.S. Department of Transportation, Federal Highway Administration, 2009 National Household Travel Survey. URL: http://nhts.ornl.gov. Data acquired from data.world. RSQLite vignette Slides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.) The vroom package works like read_csv but allows you to read in and write to many files at incredible speeds. though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times↩︎ I tried, and it crashed SAS on my machine.↩︎ A currently maintained version of the library is here and should work for UNIX platforms. It may be possible to install the library on Windows using the UNIX subsystem, per this thread↩︎ On one of my machines, I also had to make sure the file libodbc.so existed - it was named libodbc.so.1 on my laptop, so a symbolic link fixed the issue.↩︎ A histogram is a chart which breaks up a continuous variable into ranges, where the height of the bar is proportional to the number of items in the range. A bar chart is similar, but shows the number of occurrences of a discrete variable.↩︎ "],["manipulating-data.html", "Module 5 Manipulating Data Manipulating Data: Module Objectives 5.1 Tidy Data 5.2 Filter: Pick cases (rows) based on their values 5.3 Select: Pick columns 5.4 Mutate: Add and transform variables 5.5 Summarize 5.6 Group By + (?) = Power! 5.7 Other dplyr functions: across, relocate Try it out References", " Module 5 Manipulating Data In this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis19 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on. In R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy20 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to the tidyverse - specifically, readr. In SAS, there is no tidyverse, but there is a relatively consistent structure for how to accomplish each task. Most data cleaning in SAS is accomplished in data steps. In the interests of not confusing terms too much between languages, I’m going to use the tidyverse “verbs” to describe operations in both SAS and R. dplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in SAS (without the underlying framework). Each verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone. This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy. There is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter. Manipulating Data: Module Objectives Filter, subset, and clean data to prepare a dataset for analysis Describe and document operations performed on a data set transparently, and implement the operations using reproducible steps. Create summaries of data appropriate for additional analysis or display 5.1 Tidy Data There are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes: Each variable has its own column Each observation has its own row Each value has its own cell These attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your dirty clothes are folded, but they aren’t clean just because they’re folded). We’ll get more into how to work with different “messy” data configurations in the next module, but it’s worth keeping rules 1 and 3 in mind while working through this module. 5.2 Filter: Pick cases (rows) based on their values Filter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original. dplyr filter() by Allison Horst Let’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies. Data set up This data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows. library(dplyr) data(starwars) starwars # A tibble: 87 x 14 name height mass hair_color skin_color eye_color birth_year sex gender &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Luke S… 172 77 blond fair blue 19 male mascu… 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… 4 Darth … 202 136 none white yellow 41.9 male mascu… 5 Leia O… 150 49 brown light brown 19 fema… femin… 6 Owen L… 178 120 brown, grey light blue 52 male mascu… 7 Beru W… 165 75 brown light blue 47 fema… femin… 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu… 9 Biggs … 183 84 black light brown 24 male mascu… 10 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; In the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by SAS. You can access the csv data here. Note that I exported the data using ‘.’ as the NA/missing character so that it will be easy to read into SAS. library(readr) ## write_csv(starwars[,1:11], &quot;data/starwars.csv&quot;, na = &#39;.&#39;) Let’s set that data up first: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 filename swdat &quot;data/starwars.csv&quot;; 8 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT DATAFILE = swdat OUT = classdat.starwars 10 DBMS = CSV 11 REPLACE; 12 GETNAMES = YES; 13 RUN; 14 15 PROC PRINT DATA=classdat.starwars (obs=10); 16 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. If you want to directly load the SAS datafile, you can find it here Once the data is set up, using filter is actually very simple. Demonstration of filter() in R # Get only the people filter(starwars, species == &quot;Human&quot;) # A tibble: 35 x 14 name height mass hair_color skin_color eye_color birth_year sex gender &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Luke S… 172 77 blond fair blue 19 male mascu… 2 Darth … 202 136 none white yellow 41.9 male mascu… 3 Leia O… 150 49 brown light brown 19 fema… femin… 4 Owen L… 178 120 brown, grey light blue 52 male mascu… 5 Beru W… 165 75 brown light blue 47 fema… femin… 6 Biggs … 183 84 black light brown 24 male mascu… 7 Obi-Wa… 182 77 auburn, wh… fair blue-gray 57 male mascu… 8 Anakin… 188 84 blond fair blue 41.9 male mascu… 9 Wilhuf… 180 NA auburn, gr… fair blue 64 male mascu… 10 Han So… 180 80 brown fair brown 29 male mascu… # … with 25 more rows, and 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, # films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; # Get only the people who come from Tatooine filter(starwars, species == &quot;Human&quot;, homeworld == &quot;Tatooine&quot;) # A tibble: 8 x 14 name height mass hair_color skin_color eye_color birth_year sex gender &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Luke Sk… 172 77 blond fair blue 19 male mascu… 2 Darth V… 202 136 none white yellow 41.9 male mascu… 3 Owen La… 178 120 brown, grey light blue 52 male mascu… 4 Beru Wh… 165 75 brown light blue 47 fema… femin… 5 Biggs D… 183 84 black light brown 24 male mascu… 6 Anakin … 188 84 blond fair blue 41.9 male mascu… 7 Shmi Sk… 163 NA black fair brown 72 fema… femin… 8 Cliegg … 183 NA brown fair blue 82 male mascu… # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, # vehicles &lt;list&gt;, starships &lt;list&gt; In SAS, as in SQL, the filter() operation is accomplished using a where clause. Multiple clauses can be connected using and, and compound statements can be grouped with parentheses. Demonstration of where in SAS Rather than output the whole data table (which would take up a lot of space), I’ve linked the log file from each chunk below the chunk. If you are running this code in SAS, you should NOT copy the proc printto line. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 /* SAS limits dataset names to 8 characters, which is super 8 ! annoying. */ 9 /* Sorry the names aren&#39;t descriptive... */ 10 11 DATA tmp1; /* this is the out dataset */ 12 /* By not having a library attached, SAS places this in WORK */ 13 /* It&#39;s a temporary dataset */ 14 set classdat.starwars; 15 where (species = &#39;Human&#39;); 16 run; NOTE: The data set WORK.TMP1 has 0 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. See the log file here 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA tmp2; 9 set classdat.starwars; 10 where (species = &#39;Human&#39;) and (homeworld = &#39;Tatooine&#39;); 11 run; NOTE: The data set WORK.TMP2 has 0 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. See the log file here At this point, you’ve seen the traditional SAS Data step options, but there is another SAS PROC that may be more useful (and more similar to dplyr). dplyr was developed to provide SQL-like syntax while enabling the use of more advanced computations than are supported in SQL. While SAS doesn’t have anything quite the same as dplyr, it does have PROC SQL. SAS PROC SQL In SQL, as in the SAS DATA step, filter() operations are performed using the keyword WHERE. To limit the output I’m going to cheat a bit and use SELECT statements before I officially teach them to you - this is mostly so you don’t get a table with all 49 variables in it. Similarly, I’m limiting the dataset to the first 5 observations that meet the condition so that we don’t have to see all the water type pokemon. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 SELECT pokedex_number, name, type_1, type_number FROM 9 ! classdat.poke (obs=5) 10 WHERE type_1 = &quot;Water&quot;; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13. If we want to store the output of our query to a new table, we can do that by starting our query with CREATE TABLE &lt;table name&gt; AS - this creates a table with our results. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE aquapoke AS 10 SELECT pokedex_number, name, type_1, type_2, type_number FROM 10 ! classdat.poke 11 WHERE (type_1 = &quot;Water&quot; OR type_2 = &quot;Water&quot;); NOTE: Statement not executed due to NOEXEC option. 12 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 PROC PRINT DATA=aquapoke (obs=10); ERROR: File WORK.AQUAPOKE.DATA does not exist. 14 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,14. 5.2.1 Common Filter() Tasks In dplyr, there are a few helper functions which may be useful when constructing filter statements. row_number() - this is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table. filter(poke, (row_number() %% 2 == 0)) Error in filter(poke, (row_number()%%2 == 0)): object &#39;poke&#39; not found # There are several pokemon who have multiple entries in the table, # so the pokedex_number doesn&#39;t line up with the row number. arrange() - sort rows in the table by one or more variables arrange(poke, desc(total_points)) Error in arrange(poke, desc(total_points)): object &#39;poke&#39; not found slice_max() - this will keep the top values of a specified variable. It’s like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a lot more code. slice_max(poke, order_by = total_points, n = 5) %&gt;% arrange(desc(total_points)) # Sort decreasing Error in slice_max(poke, order_by = total_points, n = 5): object &#39;poke&#39; not found By default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows. slice_max(poke, order_by = total_points, n = 5, with_ties = F) %&gt;% arrange(desc(total_points)) # Sort decreasing Error in slice_max(poke, order_by = total_points, n = 5, with_ties = F): object &#39;poke&#39; not found In SAS, these same tasks can sometimes require a bit more code. Keeping only certain rows in SAS In SAS, to use a variable, you have to define it in one data step, then make another data step in order to use that variable. But, like dplyr, SAS has a row number counter that we can use for this purpose. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA tmp; 9 SET classdat.poke; 10 rownum=_n_; /* SAS shorthand for row number */ 11 RUN; NOTE: The data set WORK.TMP has 0 observations and 50 variables. WARNING: Data set WORK.TMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 DATA evenrow; 14 SET WORK.tmp; 15 WHERE MOD(rownum, 2) = 0; ERROR: Variable rownum is not on file WORK.TMP. 16 DROP rownum; /* ditch temp variable */ 17 RUN; WARNING: The variable rownum in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.EVENROW may be incomplete. When this step was stopped there were 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,15. Top N values We’re going to want to use PROC SORT to get the data arranged before we take the top N values. According to this, we can’t use _n_ in a where statement, and the proposed solution isn’t reliable. So we’ll do it the long way. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SORT DATA = classdat.poke 9 OUT = pokesort; 10 BY descending total_points; 11 RUN; NOTE: The data set WORK.POKESORT has 0 observations and 0 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 DATA poken; 14 SET WORK.pokesort; 15 rownum = _n_; 16 RUN; NOTE: The data set WORK.POKEN has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 DATA poken; 19 SET WORK.poken; 20 WHERE rownum &lt;= 5; 21 DROP rownum; 22 RUN; NOTE: The data set WORK.POKEN has 0 observations and 0 variables. WARNING: Data set WORK.POKEN was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 23 24 PROC PRINT DATA = poken; 25 VAR pokedex_number name status species type_1 total_points; ERROR: Variable POKEDEX_NUMBER not found. ERROR: Variable NAME not found. ERROR: Variable STATUS not found. ERROR: Variable SPECIES not found. ERROR: Variable TYPE_1 not found. ERROR: Variable TOTAL_POINTS not found. 26 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,15. In both cases, the SAS statements required to perform the task require a WHERE clause, but also a few other statements to get things working. The equivalent base R code would be about the same (though tricky in different spots). The thing that makes the tidyverse philosophy so addictive is that it makes these common, everyday tasks both easy and concise (that is, few lines of code are required). PROC SQL filter statements SQL doesn’t have an intrinsic notion of ordered rows, so in order to select even rows, we need to create a temporary dataset with _n_ copied into a variable (just like last time). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA poke; 9 SET classdat.poke; 10 rownum=_n_; 11 RUN; NOTE: The data set WORK.POKE has 0 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 14 SELECT * FROM poke(obs=5) 15 WHERE mod(rownum, 2) = 0; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,15,16. SELECT * says to select all variables. We’ll talk about SELECT in the next section, but with SQL it’s not reqlly possible to avoid using SELECT. If we want the 5 pokemon with the highest total points, we can use ORDER BY to sort the table, and then specify that we only want 5 rows. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 SELECT pokedex_number, name, status, species, type_1, 9 ! total_points 10 FROM classdat.poke(obs=5) 11 ORDER BY total_points DESC; NOTE: Data file CLASSDAT.POKE.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12. As a reminder, if we want to store this new data into a new dataset, we have to start our statement with CREATE TABLE AS, and then follow the statement with our query. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE poketmp AS 10 SELECT pokedex_number, name, status, species, type_1, 10 ! total_points 11 FROM classdat.poke(obs=5) 12 ORDER BY total_points DESC; NOTE: Statement not executed due to NOEXEC option. 13 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 PROC PRINT DATA=poketmp; ERROR: File WORK.POKETMP.DATA does not exist. 15 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12. Try it out Using the pokemon data, can you create a new data set or data frame (SAS and R, respectively) that has only water type pokemon? Can you write a filter statement that looks for any pokemon which has water type for either type1 or type2? R poke &lt;- read_csv(&quot;data/pokemon_ascii.csv&quot;) Warning: 1 parsing failure. row col expected actual file 1028 weight_kg a double . &#39;data/pokemon_ascii.csv&#39; filter(poke, type_1 == &quot;Water&quot;) # A tibble: 134 x 49 pokedex_number name german_name generation status species type_number type_1 &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 7 Squi… Schiggy 1 Normal Tiny T… 1 Water 2 8 Wart… Schillok 1 Normal Turtle… 1 Water 3 9 Blas… Turtok 1 Normal Shellf… 1 Water 4 9 Mega… Turtok 1 Normal Shellf… 1 Water 5 54 Psyd… Enton 1 Normal Duck P… 1 Water 6 55 Gold… Entoron 1 Normal Duck P… 1 Water 7 60 Poli… Quapsel 1 Normal Tadpol… 1 Water 8 61 Poli… Quaputzi 1 Normal Tadpol… 1 Water 9 62 Poli… Quappo 1 Normal Tadpol… 2 Water 10 72 Tent… Tentacha 1 Normal Jellyf… 2 Water # … with 124 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; filter(poke, type_1 == &quot;Water&quot; | type_2 == &quot;Water&quot;) # A tibble: 153 x 49 pokedex_number name german_name generation status species type_number type_1 &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 7 Squi… Schiggy 1 Normal Tiny T… 1 Water 2 8 Wart… Schillok 1 Normal Turtle… 1 Water 3 9 Blas… Turtok 1 Normal Shellf… 1 Water 4 9 Mega… Turtok 1 Normal Shellf… 1 Water 5 54 Psyd… Enton 1 Normal Duck P… 1 Water 6 55 Gold… Entoron 1 Normal Duck P… 1 Water 7 60 Poli… Quapsel 1 Normal Tadpol… 1 Water 8 61 Poli… Quaputzi 1 Normal Tadpol… 1 Water 9 62 Poli… Quappo 1 Normal Tadpol… 2 Water 10 72 Tent… Tentacha 1 Normal Jellyf… 2 Water # … with 143 more rows, and 41 more variables: type_2 &lt;chr&gt;, height_m &lt;dbl&gt;, # weight_kg &lt;dbl&gt;, abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, # ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;, # defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;, # catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;, # growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;, # egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, # against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, against_water &lt;dbl&gt;, # against_electric &lt;dbl&gt;, against_grass &lt;dbl&gt;, against_ice &lt;dbl&gt;, # against_fight &lt;dbl&gt;, against_poison &lt;dbl&gt;, against_ground &lt;dbl&gt;, # against_flying &lt;dbl&gt;, against_psychic &lt;dbl&gt;, against_bug &lt;dbl&gt;, # against_rock &lt;dbl&gt;, against_ghost &lt;dbl&gt;, against_dragon &lt;dbl&gt;, # against_dark &lt;dbl&gt;, against_steel &lt;dbl&gt;, against_fairy &lt;dbl&gt; # The conditions have to be separated by |, which means &quot;or&quot; SAS DATA Step 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA water1; 9 SET classdat.poke; 10 WHERE type_1 = &quot;Water&quot;; 11 RUN; NOTE: The data set WORK.WATER1 has 0 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 DATA water2; 14 SET classdat.poke; 15 WHERE (type_1 = &quot;Water&quot; OR type_2 = &quot;Water&quot;); 16 RUN; NOTE: The data set WORK.WATER2 has 0 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. In the interests of only showing the parts of the log that are useful, I’ve just pasted them into this chunk. Not reproducible, but faster to read. NOTE: There were 134 observations read from the data set CLASSDAT.POKE. WHERE type_1=&#39;Water&#39;; NOTE: The data set WORK.WATER1 has 134 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: There were 153 observations read from the data set CLASSDAT.POKE. WHERE (type_1=&#39;Water&#39;) or (type_2=&#39;Water&#39;); NOTE: The data set WORK.WATER2 has 153 observations and 49 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 5.3 Select: Pick columns Sometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)], or VAR statements, but that can get tedious). In dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple. select(.data, …) So as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions.” So what can go in there? An exhaustive list of ways to select variables in dplyr First, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).21 Most dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions. var3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5 !(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses (&lt;set of vars 1&gt;) &amp; (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2. c() combines sets of variables. dplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them). everything() matches all variables last_col() matches the last variable. last_col(offset = n) selects the n-th to last variable. starts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well. contains(\"xyz\") will match any columns with names containing the literal string “xyz.” Note, contains does not work with regular expressions (you don’t need to know what that means right now). matches(regex) takes a regular expression as an argument and returns all columns matching that expression. num_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range. There are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables. all_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error. any_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set. There’s one final selector - where() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative (but I’ve never actually needed to use it). Let’s try these selector functions out and see what we can accomplish! if (!&quot;nycflights13&quot; %in% installed.packages()) install.packages(&quot;nycflights13&quot;) library(nycflights13) data(flights) str(flights) tibble[,19] [336,776 × 19] (S3: tbl_df/tbl/data.frame) $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ... $ month : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... $ day : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ... $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ... $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ... $ dep_delay : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ... $ arr_time : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ... $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ... $ arr_delay : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ... $ carrier : chr [1:336776] &quot;UA&quot; &quot;UA&quot; &quot;AA&quot; &quot;B6&quot; ... $ flight : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ... $ tailnum : chr [1:336776] &quot;N14228&quot; &quot;N24211&quot; &quot;N619AA&quot; &quot;N804JB&quot; ... $ origin : chr [1:336776] &quot;EWR&quot; &quot;LGA&quot; &quot;JFK&quot; &quot;JFK&quot; ... $ dest : chr [1:336776] &quot;IAH&quot; &quot;IAH&quot; &quot;MIA&quot; &quot;BQN&quot; ... $ air_time : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ... $ distance : num [1:336776] 1400 1416 1089 1576 762 ... $ hour : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ... $ minute : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ... $ time_hour : POSIXct[1:336776], format: &quot;2013-01-01 05:00:00&quot; &quot;2013-01-01 05:00:00&quot; ... We’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands. You might want to try out your EDA skills from the previous module to see what you can find out about the dataset, before seeing how select() works. We could get a data frame of departure information for each flight: select(flights, flight, year:day, tailnum, origin, matches(&quot;dep&quot;)) # A tibble: 336,776 x 9 flight year month day tailnum origin dep_time sched_dep_time dep_delay &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1545 2013 1 1 N14228 EWR 517 515 2 2 1714 2013 1 1 N24211 LGA 533 529 4 3 1141 2013 1 1 N619AA JFK 542 540 2 4 725 2013 1 1 N804JB JFK 544 545 -1 5 461 2013 1 1 N668DN LGA 554 600 -6 6 1696 2013 1 1 N39463 EWR 554 558 -4 7 507 2013 1 1 N516JB EWR 555 600 -5 8 5708 2013 1 1 N829AS LGA 557 600 -3 9 79 2013 1 1 N593JB JFK 557 600 -3 10 301 2013 1 1 N3ALAA LGA 558 600 -2 # … with 336,766 more rows Perhaps we want the plane and flight ID information to be the first columns: flights %&gt;% select(carrier:dest, everything()) # A tibble: 336,776 x 19 carrier flight tailnum origin dest year month day dep_time sched_dep_time &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 UA 1545 N14228 EWR IAH 2013 1 1 517 515 2 UA 1714 N24211 LGA IAH 2013 1 1 533 529 3 AA 1141 N619AA JFK MIA 2013 1 1 542 540 4 B6 725 N804JB JFK BQN 2013 1 1 544 545 5 DL 461 N668DN LGA ATL 2013 1 1 554 600 6 UA 1696 N39463 EWR ORD 2013 1 1 554 558 7 B6 507 N516JB EWR FLL 2013 1 1 555 600 8 EV 5708 N829AS LGA IAD 2013 1 1 557 600 9 B6 79 N593JB JFK MCO 2013 1 1 557 600 10 AA 301 N3ALAA LGA ORD 2013 1 1 558 600 # … with 336,766 more rows, and 9 more variables: dep_delay &lt;dbl&gt;, # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Note that everything() won’t duplicate columns you’ve already added. Exploring the difference between bare name selection and all_of()/any_of() flights %&gt;% select(carrier, flight, tailnum, matches(&quot;time&quot;)) # A tibble: 336,776 x 9 carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 UA 1545 N14228 517 515 830 819 2 UA 1714 N24211 533 529 850 830 3 AA 1141 N619AA 542 540 923 850 4 B6 725 N804JB 544 545 1004 1022 5 DL 461 N668DN 554 600 812 837 6 UA 1696 N39463 554 558 740 728 7 B6 507 N516JB 555 600 913 854 8 EV 5708 N829AS 557 600 709 723 9 B6 79 N593JB 557 600 838 846 10 AA 301 N3ALAA 558 600 753 745 # … with 336,766 more rows, and 2 more variables: air_time &lt;dbl&gt;, # time_hour &lt;dttm&gt; varlist &lt;- c(&quot;carrier&quot;, &quot;flight&quot;, &quot;tailnum&quot;, &quot;dep_time&quot;, &quot;sched_dep_time&quot;, &quot;arr_time&quot;, &quot;sched_arr_time&quot;, &quot;air_time&quot;) flights %&gt;% select(all_of(varlist)) # A tibble: 336,776 x 8 carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 UA 1545 N14228 517 515 830 819 2 UA 1714 N24211 533 529 850 830 3 AA 1141 N619AA 542 540 923 850 4 B6 725 N804JB 544 545 1004 1022 5 DL 461 N668DN 554 600 812 837 6 UA 1696 N39463 554 558 740 728 7 B6 507 N516JB 555 600 913 854 8 EV 5708 N829AS 557 600 709 723 9 B6 79 N593JB 557 600 838 846 10 AA 301 N3ALAA 558 600 753 745 # … with 336,766 more rows, and 1 more variable: air_time &lt;dbl&gt; varlist &lt;- c(varlist, &quot;whoops&quot;) flights %&gt;% select(all_of(varlist)) # this errors out b/c whoops doesn&#39;t exist Error: Can&#39;t subset columns that don&#39;t exist. x Column `whoops` doesn&#39;t exist. flights %&gt;% select(any_of(varlist)) # this runs just fine # A tibble: 336,776 x 8 carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_time &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 UA 1545 N14228 517 515 830 819 2 UA 1714 N24211 533 529 850 830 3 AA 1141 N619AA 542 540 923 850 4 B6 725 N804JB 544 545 1004 1022 5 DL 461 N668DN 554 600 812 837 6 UA 1696 N39463 554 558 740 728 7 B6 507 N516JB 555 600 913 854 8 EV 5708 N829AS 557 600 709 723 9 B6 79 N593JB 557 600 838 846 10 AA 301 N3ALAA 558 600 753 745 # … with 336,766 more rows, and 1 more variable: air_time &lt;dbl&gt; So for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select). Unfortunately, SAS doesn’t make column selection quite as easy. It’s still not hard, but it can be tedious. In SAS, there are two primary methods to select variables: KEEP selects variables, DROP removes variables. # Export flights data for SAS flights %&gt;% sample_frac(size = .25) %&gt;% # Keep file from being too big write_csv(&quot;data/flights.csv&quot;, na = &quot;.&quot;) 6 /* Read in data */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 filename fileloc 8 ! &#39;~/Projects/Class/unl-stat850/2020-stat850/data/flights.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.flights 10 DBMS = csv; /* comma delimited file */ 11 GETNAMES = YES; 12 RUN; ERROR: Errors printed on pages 5,7,12,13. In SAS, a partial variable name either preceded or followed by : serves as a wildcard. Ranges of variables can be specified with two dashes, e.g. var3 -- var5. SAS KEEP statement Unfortunately, the wildcard doesn’t work on both ends, so to get the equivalent of matches(\"dep\"), we have to use two different options in our KEEP statement (plus the extra variables that don’t have dep in them). 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA tmpfly; 9 KEEP flight year--day tailnum origin dep: sched_dep:; 10 SET classdat.flights; 11 RUN; NOTE: The data set WORK.TMPFLY has 0 observations and 9 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 PROC PRINT DATA = tmpfly (obs=10); 14 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. Note also that SAS doesn’t reorder the columns for us like select() does. If we’d prefer to carve out columns (rather than assembling a new dataset with the columns we want to keep), we can use a DROP statement, which works exactly the same way. Let’s see what columns we removed implicitly last time by dropping everything we’d previously kept: SAS DROP statement 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA tmpfly; 9 DROP flight year--day tailnum origin dep: sched_dep:; 10 SET classdat.flights; 11 RUN; NOTE: The data set WORK.TMPFLY has 0 observations and 10 variables. WARNING: Data set WORK.TMPFLY was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 PROC PRINT DATA = tmpfly (obs=10); 14 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. As with the filter statements, we can also use PROC SQL instead of a SAS DATA step. There are even ways to (sort-of) use elements of both. SAS PROC SQL SELECT statement 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE tmpfly 10 AS 11 SELECT flight, year, month, day, tailnum, origin 12 FROM classdat.flights; NOTE: Statement not executed due to NOEXEC option. 13 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 PROC PRINT DATA = tmpfly(obs=10); 15 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. Note that PROC SQL doesn’t have a RUN statement - it is executed immediately. But, using the PROC SQL syntax, we still have to list out all of the variables, and that’s a drag. Luckily, PROC SQL will also let us use some of the DATA step options, if we’re careful about it: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE tmpfly 10 AS 11 SELECT * 12 FROM classdat.flights(drop=year--day flight tailnum origin dep: 12 ! sched_dep:); NOTE: Statement not executed due to NOEXEC option. 13 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 PROC PRINT DATA = tmpfly(obs=10); 15 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. Note the difference - we’re selecting everything (in SQL) but dropping columns when we tell SQL where to look for the data. For the most part, that is what you need to functionally replicate select() syntax. It may be a bit more work because there aren’t the same convenience functions, but it’ll do and you don’t have to remember as many keywords, so that’s a plus. 5.4 Mutate: Add and transform variables Up to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate. However, it’s probably best to start this section out with a very short demonstration of how this process worked in R before the tidyverse came around. Pre-tidyverse base R “mutating” a data frame Lets use the police violence data to demonstrate. Remember the issues you identified with the data during EDA in Module 4 (in SAS)(in R)? The gsub function is basically R’s version of “find and replace.” library(readxl) police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) # There are two categories for &quot;unknown race&quot; table(police_violence$`Victim&#39;s race`, useNA = &#39;ifany&#39;) Asian Black Hispanic Native American 118 1944 1335 112 Pacific Islander Unknown race Unknown Race White 42 670 64 3378 # This line substitutes &quot;race&quot; for &quot;Race&quot; so that there&#39;s consistent capitalization police_violence$race &lt;- gsub(&quot;Race&quot;, &quot;race&quot;, police_violence$`Victim&#39;s race`) # Fixed! table(police_violence$race) Asian Black Hispanic Native American 118 1944 1335 112 Pacific Islander Unknown race White 42 734 3378 You could do a simple operation like that in a single line, but you had to use the name of the data multiple times, and it very quickly becomes a complicated operation. The process in SAS is very similar. It’s recommended that you use one data step to read in your data, and then a separate data step to clean the data, so that you are separating the two operations. SAS DATA STEP - create a new variable We can create our variable a couple of different ways in SAS: - Use the TRANWRD function for find and replace. - Use an if statement and define the replacement ourselves Both are demonstrated below: 2 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 3 4 DATA pvtmp; 5 SET classdat.police; 6 race = tranwrd(victim_s_race, &quot;Race&quot;, &quot;race&quot;); 7 race2 = victim_s_race; /* initialize it to current value */ 8 IF victim_s_race=&#39;Unknown Race&#39; THEN race2 = &#39;Unknown race&#39;; 9 RUN; NOTE: The data set WORK.PVTMP has 0 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 10 11 PROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */ 12 TABLES victim_s_race * race victim_s_race * race2 / 13 NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10; 14 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. The SAS System 1 09:05 Thursday, April 22, 2021 --------- List = ick--------- Item 1 olives Item 2 sausage Item 3 anchovies --------- List = crust--------- Item 1 flour Item 2 yeast Item 3 salt Item 4 water --------- List = yummy_toppings--------- Item 1 asiago Item 2 fontina The SAS System 2 09:05 Thursday, April 22, 2021 Item 3 blue cheese Item 4 bacon Item 5 scallions Average math test scores under the influence of LSD 3 09:05 Thursday, April 22, 2021 The DATASETS Procedure Data Set Name WORK.MATHLSD Observations 7 Member Type DATA Variables 2 Engine V9 Indexes 0 Created 04/22/2021 09:05:47 Observation Length 16 Last Modified 04/22/2021 09:05:47 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 1 First Data Page 1 Max Obs per Page 4061 Obs in First Data Page 7 Number of Data Set Repairs 0 Filename /tmp/SAS_workDC6C0004C1D0_ silicon/mathlsd.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 11278124 Access Permission rw-rw-r-- Owner Name susan File Size 128KB File Size (bytes) 131072 Alphabetic List of Variables and Attributes # Variable Type Len 1 Drugs Num 8 2 Score Num 8 Average math test scores under the influence of LSD 4 09:05 Thursday, April 22, 2021 Obs Drugs Score 1 1.17 78.93 2 2.97 58.20 3 3.26 67.47 4 4.69 37.47 5 5.83 45.65 6 6.00 32.92 7 6.41 29.97 Average math test scores under the influence of LSD 5 09:05 Thursday, April 22, 2021 x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 y 11 12 16 17 Average math test scores under the influence of LSD 6 09:05 Thursday, April 22, 2021 Obs _TYPE_ _FREQ_ x 1 0 466 233168 Average math test scores under the influence of LSD 7 09:05 Thursday, April 22, 2021 Obs name status present 1 Edison nice toy 2 Alex naughty coal 3 Susan toy 4 Ryan neutral toy In both cases we can see that the recode worked the way we wanted and we’ve now gotten rid of the extra “unknown” category\". We can also use PROC SQL to create new variables using relatively complex logic if necessary. SAS PROC SQL - create a new variable In SQL, you define new variables using AS. In SELECT statements, this definition has the computation on the left and the variable on the right22. CASE WHEN is the if-else statement in SQL. When (victim_s_race = ‘Unknown Race’), our variable value will be “Unknown race,” otherwise it will be what ever value is in victim_s_race. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE WORK.pvtmp AS 10 SELECT * , 11 CASE WHEN victim_s_race=&#39;Unknown Race&#39; THEN &#39;Unknown race&#39; ELSE 11 ! victim_s_race END AS race 12 FROM classdat.police; NOTE: Statement not executed due to NOEXEC option. 13 14 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 PROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */ 16 TABLES victim_s_race * race / 17 NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10; 18 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. The choice of which method to use (DATA step or PROC SQL) involves weighing these competing factors: computational time code readability programmer time Personally, I find PROC SQL easier to work with, but I think the code is ugly. There are similar sql-syntax packages in R, but I don’t feel the need to use them, because (for me) dplyr code is much easier to read (and thus, easier to maintain). dplyr code is not as efficient as SQL (or other packages, like data.table) on big datasets, but there are variants such as dbplyr to handle some of those cases, and I find that they don’t come up very often in my work or research. If I were working at Google or Amazon, my opinion might be very different The fundamentals of mutate are very similar to the approaches above; the power of the dplyr approach is only really evident when you are doing multiple operations in the same step. Once you’re working at that level, the dplyr approach produces much more readable code. Mutate (by Allison Horst) mutate() a new variable # The data was read in above... library(dplyr) police_violence %&gt;% mutate(race = gsub(&quot;Race&quot;, &quot;race&quot;, `Victim&#39;s race`)) %&gt;% select(`Victim&#39;s race`, race) %&gt;% table() race Victim&#39;s race Asian Black Hispanic Native American Pacific Islander Asian 118 0 0 0 0 Black 0 1944 0 0 0 Hispanic 0 0 1335 0 0 Native American 0 0 0 112 0 Pacific Islander 0 0 0 0 42 Unknown race 0 0 0 0 0 Unknown Race 0 0 0 0 0 White 0 0 0 0 0 race Victim&#39;s race Unknown race White Asian 0 0 Black 0 0 Hispanic 0 0 Native American 0 0 Pacific Islander 0 0 Unknown race 670 0 Unknown Race 64 0 White 0 3378 The last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab like PROC FREQ gave us in SAS. The learning curve here isn’t actually knowing how to use mutate (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables. I’m not going to be able to teach you how to handle every task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.). It’s actually something of a common meme… In this class, my goal is to expose you to solutions to common problems; unfortunately, there are too many common problems for us to work through line-by-line. Part of the goal of this class is for you to learn how to read through a package description and evaluate whether the package will do what you want; we’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own. Figure 5.1: Apologies for the noninclusive language, but the sentiment is real. Here is a quick list of packages in R which will solve some of the more common problems. Between that and the R cheatsheet, you should be set. In SAS, there are fewer options, so it’s less bewildering to google solutions (but I’ll link you to relevant pieces for the common SAS stuff too). Dates and times: lubridate package in R (esp. ymd_hms() and variants, decimal_date(), and other convenience functions). SAS Dates and Times. String manipulation: stringr package in R (str_replace(), str_remove(), str_detect(), str_split()) Regular Expression Cheatsheet (R) Common String operations in SAS Regular Expressions in SAS We’ll talk more about strings in the next module… 5.5 Summarize The next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and in SAS, PROC MEANS is essentially the same thing. Here (in a trivial example), I compute the overall average age of a victim of police violence, and then also compute the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes. police_violence &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) police_violence %&gt;% mutate(age = as.numeric(`Victim&#39;s age`), name_length = nchar(`Victim&#39;s name`)) %&gt;% summarize(age = mean(age, na.rm = T), name_length = mean(name_length)) Warning in mask$eval_all_mutate(quo): NAs introduced by coercion # A tibble: 1 x 2 age name_length &lt;dbl&gt; &lt;dbl&gt; 1 36.8 16.6 In SAS, we can do something similar: 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA pv; 9 SET classdat.police; 10 age = INPUT(victim_s_age, 3.); 11 name_len = LENGTH(victim_s_name); 12 RUN; NOTE: The data set WORK.PV has 0 observations and 27 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 PROC MEANS DATA=pv; 15 VAR age name_len; 16 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. By default, with SAS, we get a bit more than we bargained for; we can turn the extra output off with options. Another option is to use PROC SQL in SAS, which will have a logical flow similar to dplyr. 6 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA pv; 10 SET classdat.police; 11 age = INPUT(victim_s_age, 3.); 12 name_len = LENGTH(victim_s_name); 13 RUN; NOTE: The data set WORK.PV has 0 observations and 27 variables. WARNING: Data set WORK.PV was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 15 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 16 SELECT AVG(age) as age, AVG(name_len) as name_len 17 FROM pv; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13. The real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail. 5.6 Group By + (?) = Power! Frequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to split my dataset into smaller datasets - one for each day compute summary values for each smaller dataset put my summarized data back together into a single dataset group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well. Replicating frequency tables using dplyr Let’s start with a trivial example: Suppose we want to count up every occurrence of a variable in a dataset. We can already do this with e.g. table(), but work with me for a moment. pv &lt;- read_xlsx(&quot;data/police_violence.xlsx&quot;, guess_max = 7000) %&gt;% mutate(race = gsub(&quot;Race&quot;, &quot;race&quot;, `Victim&#39;s race`), age = as.numeric(`Victim&#39;s age`)) %&gt;% select(name = `Victim&#39;s name`, age, gender = `Victim&#39;s gender`, race) Warning in mask$eval_all_mutate(quo): NAs introduced by coercion # You can rename variables with a select statement # I&#39;m doing this b/c I don&#39;t like to use backticks if I can help it. # Lazy coding = best coding. grouped_pv &lt;- pv %&gt;% group_by(race) grouped_pv # A tibble: 7,663 x 4 # Groups: race [7] name age gender race &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Eric M. Tellez 28 Male White 2 Name withheld by police NA Male Unknown race 3 Terry Hudson 57 Male Black 4 Malik Williams 23 Male Black 5 Frederick Perkins 37 Male Black 6 Michael Vincent Davis 49 Male White 7 Brian Elkins 47 Male Unknown race 8 Debra D. Arbuckle 51 Female White 9 Name withheld by police NA Male Unknown race 10 Cody McCaulou 27 Male White # … with 7,653 more rows So we can see that the object has been somehow grouped by the categorical variable race, in that the grouping is attached to the stored object (strictly speaking, group_by adds an attribute to the table). What matters for our purposes, though, is that each sub-table is treated as a separate entity for calculation purposes. pv_race_sum &lt;- grouped_pv %&gt;% summarize(n = n()) # This counts the number of rows in each group pv_race_sum # A tibble: 7 x 2 race n &lt;chr&gt; &lt;int&gt; 1 Asian 118 2 Black 1944 3 Hispanic 1335 4 Native American 112 5 Pacific Islander 42 6 Unknown race 734 7 White 3378 When we run summarize, we get back a data frame that is not grouped, with one line for each of the previously existing groups. summarize removes one “layer” of grouping with each run. One layer of grouping? What does that mean? tmp &lt;- pv %&gt;% group_by(gender, race) tmp # A tibble: 7,663 x 4 # Groups: gender, race [26] name age gender race &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 Eric M. Tellez 28 Male White 2 Name withheld by police NA Male Unknown race 3 Terry Hudson 57 Male Black 4 Malik Williams 23 Male Black 5 Frederick Perkins 37 Male Black 6 Michael Vincent Davis 49 Male White 7 Brian Elkins 47 Male Unknown race 8 Debra D. Arbuckle 51 Female White 9 Name withheld by police NA Male Unknown race 10 Cody McCaulou 27 Male White # … with 7,653 more rows tmp %&gt;% summarize(min_age = min(age, na.rm = T), max_age = max(age, na.rm = T)) Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in min(age, na.rm = T): no non-missing arguments to min; returning Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf Warning in max(age, na.rm = T): no non-missing arguments to max; returning -Inf `summarise()` has grouped output by &#39;gender&#39;. You can override using the `.groups` argument. # A tibble: 26 x 4 # Groups: gender [5] gender race min_age max_age &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Female Asian 27 49 2 Female Black 20 93 3 Female Hispanic 16 57 4 Female Native American 23 52 5 Female Pacific Islander 26 50 6 Female Unknown race 23 74 7 Female White 5 89 8 Male Asian 16 76 9 Male Black 1 107 10 Male Hispanic 1 80 # … with 16 more rows Apart from some warnings about how it’s hard to take the minimum or maximum of a bunch of missing data (which is fair), we can see a message: summarise() regrouping output by ‘gender’ (override with ‘.groups’ argument) What this message is saying is that it is essentially dropping one layer of grouping (race) and grouping only by gender – but it’s also nice enough to tell you that you can override the default option if you want to do so by using the .groups argument. ?summarize23 gives you several options for how to handle the grouping of the result. We grouped pv by gender and race, then ran summarize, which created one row for each combination of gender and race and “glued” them together. The resulting data frame is still grouped by gender, but because there’s only one row for each race (for each level/group of gender), there’s no reason to have that level of grouping anymore. So it’s dropped by default. Replicating PROC FREQ using PROC SQL 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 8 DATA pv; 9 SET classdat.police; 10 age = INPUT(victim_s_age, 3.); 11 IF victim_s_gender=&#39; &#39; THEN victim_s_gender=&#39;Unknown&#39;; 12 race = victim_s_race; /* initialize it to current value */ 13 IF victim_s_race=&#39;Unknown Race&#39; THEN race = &#39;Unknown race&#39;; 14 RUN; NOTE: The data set WORK.PV has 0 observations and 27 variables. WARNING: Data set WORK.PV was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 16 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 17 SELECT victim_s_gender AS gender, race, count(*) AS n 18 FROM pv 19 GROUP BY race; ERROR: The following columns were not found in the contributing tables: race. 20 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 22 SELECT victim_s_gender AS gender, race, count(*) AS n, 22 ! min(age) AS min_age, max(age) AS max_age 23 FROM pv 24 GROUP BY gender, race; ERROR: The following columns were not found in the contributing tables: race. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16. Let’s try a non-trivial example, using the storms dataset that is part of the dplyr package: Reading in the data (R and SAS) library(dplyr) library(lubridate) # for the make_datetime() function Attaching package: &#39;lubridate&#39; The following objects are masked from &#39;package:base&#39;: date, intersect, setdiff, union data(storms) storms # A tibble: 10,010 x 13 name year month day hour lat long status category wind pressure &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; 1 Amy 1975 6 27 0 27.5 -79 tropical d… -1 25 1013 2 Amy 1975 6 27 6 28.5 -79 tropical d… -1 25 1013 3 Amy 1975 6 27 12 29.5 -79 tropical d… -1 25 1013 4 Amy 1975 6 27 18 30.5 -79 tropical d… -1 25 1013 5 Amy 1975 6 28 0 31.5 -78.8 tropical d… -1 25 1012 6 Amy 1975 6 28 6 32.4 -78.7 tropical d… -1 25 1012 7 Amy 1975 6 28 12 33.3 -78 tropical d… -1 25 1011 8 Amy 1975 6 28 18 34 -77 tropical d… -1 30 1006 9 Amy 1975 6 29 0 34.4 -75.8 tropical s… 0 35 1004 10 Amy 1975 6 29 6 34 -74.8 tropical s… 0 40 1002 # … with 10,000 more rows, and 2 more variables: ts_diameter &lt;dbl&gt;, # hu_diameter &lt;dbl&gt; storms &lt;- storms %&gt;% # Construct a time variable that behaves like a number but is formatted as a date mutate(time = make_datetime(year, month, day, hour)) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 filename fileloc &#39;data/storms.csv&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.00 seconds NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 8 PROC IMPORT datafile = fileloc out=classdat.storms REPLACE 9 DBMS = csv; /* comma delimited file */ 10 GUESSINGROWS=500; 11 GETNAMES = YES; 12 RUN; 13 14 DATA classdat.storms; 15 SET classdat.storms; 16 date = MDY(month, day, year); 17 time = DHMS(date, hour, 0, 0); 18 FORMAT time DATETIME.; 19 RUN; NOTE: The data set CLASSDAT.STORMS has 0 observations and 15 variables. WARNING: Data set CLASSDAT.STORMS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17. We have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes). One thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way: The points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause). If there’s a tie, we might want to know when the maximum wind speed occurred. If that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred. group_by + filter + summary in R max_power_storm &lt;- storms %&gt;% # Storm names can be reused, so we need to have year to be sure it&#39;s the same instance group_by(name, year) %&gt;% filter(pressure == min(pressure, na.rm = T)) %&gt;% filter(wind == max(wind, na.rm = T)) %&gt;% summarize(pressure = mean(pressure), wind = mean(wind), category = unique(category), status = unique(status), time = mean(time)) %&gt;% arrange(time) %&gt;% ungroup() `summarise()` has grouped output by &#39;name&#39;. You can override using the `.groups` argument. max_power_storm # A tibble: 426 x 7 name year pressure wind category status time &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;dttm&gt; 1 Amy 1975 981 60 0 tropical storm 1975-07-02 12:00:00 2 Caroline 1975 963 100 3 hurricane 1975-08-31 06:00:00 3 Doris 1975 965 95 2 hurricane 1975-09-02 21:00:00 4 Belle 1976 957 105 3 hurricane 1976-08-09 00:00:00 5 Gloria 1976 970 80 1 hurricane 1976-09-30 00:00:00 6 Anita 1977 926 150 5 hurricane 1977-09-02 06:00:00 7 Clara 1977 993 65 1 hurricane 1977-09-08 12:00:00 8 Evelyn 1977 994 65 1 hurricane 1977-10-15 00:00:00 9 Amelia 1978 1005 45 0 tropical storm 1978-07-31 00:00:00 10 Bess 1978 1005 40 0 tropical storm 1978-08-07 12:00:00 # … with 416 more rows If we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm. library(ggplot2) ggplot(max_power_storm, aes(x = pressure)) + geom_histogram() `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We could also look to see whether there has been any change over time in pressure. ggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point() It seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic (see this coverage map from 1995). Proc SQL in SAS In SAS, this is going to require some work. Specifically, while dplyr commands are stated in recipe order (do this, then this), SQL statements… aren’t. WHERE comes after SELECT xxx FROM yyy, and GROUP BY comes after that again. There are a couple of ways to handle that: sub-queries, and creating temporary tables. I think the temporary tables approach will be easier to demonstrate, read, and understand, so lets go with that. Another challenge will be the fact that SAS PROC SQL doesn’t handle missing data quite as easily as dplyr does (na.rm is a very nice function, all things considered). We can think through the steps we need to take: 1. Create a table where wind and pressure observations aren’t missing. We’ll call that tmp1. 2. Filter tmp1, keeping only rows with minimum pressure and maximum wind for each storm/year combination (HAVING is like WHERE, but after the GROUP BY clause has been applied). We’ll call that tmp2. We can also select the variables we care about in this step. 3. Summarize tmp2, keeping columns name, year, pressure, wind, category, status, and time, where time is the mean of all maximum-power observations. The other variables should have only one value each. We can accomplish this task using the combination of SELECT and DISTINCT. DISTINCT says “keep only rows with new combinations of these values.” (Note also that we can format values inline in proc SQL Select statements. That forces SAS to treat time as a date-time variable, which will force it to format correctly in e.g. plots.) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 9 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 10 CREATE TABLE tmp1 AS 11 SELECT * 12 FROM classdat.storms 13 WHERE (NOT missing(pressure)) AND (NOT missing(wind)); NOTE: Statement not executed due to NOEXEC option. 14 15 CREATE TABLE tmp2 AS 16 SELECT name, year, pressure, wind, category, status, time, 17 min(pressure) AS minpressure, max(wind) AS maxwind 18 FROM tmp1 19 GROUP BY year, name 20 HAVING pressure = minpressure AND wind = maxwind; NOTE: Statement not executed due to NOEXEC option. 21 22 CREATE TABLE maxpwr AS 23 SELECT DISTINCT name, year, pressure, wind, category, status, 24 mean(time) AS time format=DATETIME. 25 FROM tmp2 26 GROUP BY year, name; NOTE: Statement not executed due to NOEXEC option. 27 28 QUIT; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 PROC PRINT DATA=maxpwr (obs=5); ERROR: File WORK.MAXPWR.DATA does not exist. 31 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 32 33 ODS GRAPHICS ON; 34 ODS TRACE ON; /* this allows us to select only the plot and not 34 ! tables */ 35 ODS SELECT HISTOGRAM; 36 PROC UNIVARIATE DATA=maxpwr; ERROR: File WORK.MAXPWR.DATA does not exist. 37 VAR pressure; ERROR: No data set open to look up variables. 38 HISTOGRAM; 39 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE UNIVARIATE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds WARNING: Output &#39;HISTOGRAM&#39; was not created. Make sure that the output object name, label, or path is spelled correctly. Also, verify that the appropriate procedure options are used to produce the requested output object. For example, verify that the NOPRINT option is not used. 40 ODS TRACE OFF; 41 42 PROC SGPLOT DATA=maxpwr; ERROR: File WORK.MAXPWR.DATA does not exist. 43 scatter X = time Y = pressure; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 44 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 45 46 ODS GRAPHICS OFF; 47 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17. Another interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time? storm_strength_duration &lt;- storms %&gt;% group_by(name, year) %&gt;% summarize(duration = difftime(max(time), min(time), units = &quot;days&quot;), max_strength = max(category)) %&gt;% ungroup() %&gt;% arrange(desc(max_strength)) `summarise()` has grouped output by &#39;name&#39;. You can override using the `.groups` argument. storm_strength_duration %&gt;% ggplot(aes(x = max_strength, y = duration)) + geom_boxplot() Don&#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous. You don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with. In SAS, we have to know that datetimes are stored in seconds. So if we subtract two date time values, and we want our answer in days, then we need to divide by the number of seconds in a day: 24*60*60 = 86400. R has helper functions to do this for us, but it’s not that much harder to just do the computuation ourselves. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE stormlencat AS 9 SELECT name, year, (max(time) - min(time))/86400 AS duration, 9 ! max(category) AS max_strength 10 FROM classdat.storms 11 GROUP BY year, name 12 ORDER BY max_strength; NOTE: Statement not executed due to NOEXEC option. 13 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 PROC BOXPLOT DATA=stormlencat; ERROR: File WORK.STORMLENCAT.DATA does not exist. 15 PLOT duration * max_strength; ERROR: No data set open to look up variables. 16 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE BOXPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18. We could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate) Diameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables). Note the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorial) strength and convert that into something numeric that can be plotted. storm_evolution &lt;- storms %&gt;% filter(!is.na(hu_diameter)) %&gt;% group_by(name, year) %&gt;% mutate(time_since_start = difftime(time, min(time), units = &quot;days&quot;)) %&gt;% ungroup() ggplot(storm_evolution, aes(x = time_since_start, y = hu_diameter, group = name)) + geom_line(alpha = .2) + facet_wrap(~year, scales = &quot;free_y&quot;) Don&#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous. For this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms. We can do something similar in SAS; this time, I decided to get rid of any storm which never had hurricane-force winds - that will get rid of a lot of lines that never leave the x-axis. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE stormevo AS 9 SELECT name, year, (time - min(time))/86400 AS time_since_start, 9 ! category, status, hu_diameter, ts_diameter, max(hu_diameter) AS 9 ! max_hu_diameter 10 FROM classdat.storms 11 WHERE NOT MISSING(hu_diameter) 12 GROUP BY year, name 13 HAVING max_hu_diameter &gt; 0 14 ORDER BY year, name, time_since_start; NOTE: Statement not executed due to NOEXEC option. 15 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 PROC SGPANEL DATA=stormevo; ERROR: File WORK.STORMEVO.DATA does not exist. 17 PANELBY year / COLUMNS = 4 ROWS = 3; ERROR: No data set open to look up variables. 18 SERIES X = time_since_start Y = hu_diameter / GROUP = name; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 19 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPANEL used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 20 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18. PROC SGPANEL in SAS does essentially the same thing as facet_wrap() in R - it allows you to select one or more variables to create sub-plots for. We do have to manually specify how many rows and columns (or SAS will give us 3 separate plots with 4 panels each). The essential components of the graph specification are the same - instead of specifying the use of a line, we specify “series” (which means plot a line). We specify the same x, y, and group variables, though the syntax differs a bit. It seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately. storm_evolution %&gt;% filter(year == 2008) %&gt;% arrange(desc(time_since_start)) # A tibble: 327 x 15 name year month day hour lat long status category wind pressure &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; 1 Ike 2008 9 14 6 35.5 -93.7 tropical s… 0 35 985 2 Ike 2008 9 14 0 33.5 -94.9 tropical s… 0 35 980 3 Ike 2008 9 13 18 31.7 -95.3 tropical s… 0 50 974 4 Ike 2008 9 13 12 30.3 -95.2 hurricane 2 85 959 5 Ike 2008 9 13 7 29.3 -94.7 hurricane 2 95 950 6 Ike 2008 9 13 6 29.1 -94.6 hurricane 2 95 951 7 Ike 2008 9 13 0 28.3 -94 hurricane 2 95 952 8 Fay 2008 8 27 0 35 -85.8 tropical d… -1 15 1005 9 Ike 2008 9 12 18 27.5 -93.2 hurricane 2 95 954 10 Fay 2008 8 26 18 34.6 -86.5 tropical d… -1 20 1004 # … with 317 more rows, and 4 more variables: ts_diameter &lt;dbl&gt;, # hu_diameter &lt;dbl&gt;, time &lt;dttm&gt;, time_since_start &lt;drtn&gt; storm_evolution %&gt;% filter(name == &quot;Ike&quot;) %&gt;% ggplot(aes(x = time, y = hu_diameter, color = category)) + geom_point() The SAS code for this is fairly similar (though I’ll admit to not having the finesse with SAS to get a truly nice looking plot). At this point, we’re going for quick-and-dirty graphics that show us what we want to know - we can figure out how to customize things later. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE ike AS 10 SELECT * FROM classdat.storms WHERE name = &quot;Ike&quot; 11 ORDER BY time; NOTE: Statement not executed due to NOEXEC option. 12 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 PROC SGPLOT DATA=ike; ERROR: File WORK.IKE.DATA does not exist. 14 SCATTER X = time Y = hu_diameter / ERROR: No data set open to look up variables. 15 COLORRESPONSE=category /* color by another variable */ ERROR: No data set open to look up variables. 16 MARKERATTRS=(symbol=CircleFilled) /* use circles for points */ ERROR: No data set open to look up variables. 17 DATALABEL=category; /* label the circles with the value */ ERROR: No data set open to look up variables. 18 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. While I’m tempted to plot out the diameter and location on a map, it’s a bit excessive for this particular problem. Luckily, Wikipedia has us covered: It looks like Ike went long-ways across Cuba, which weakened it. When hurricanes weaken, often their wind fields expand (as they no longer have the angular momentum to maintain a tight structure). Ike crossed into the Gulf of Mexico, restrengthened, and then hit Houston just about dead-on. (I was living just northwest of Houston when it hit (in College Station), and I can verify that it was not a fun time). 5.7 Other dplyr functions: across, relocate The dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time. dplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst) Suppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead. library(lubridate) # for the make_datetime() function data(storms) storms &lt;- storms %&gt;% # Construct a time variable that behaves like a number but is formatted as a date mutate(time = make_datetime(year, month, day, hour)) # Use across to get average of all numeric variables avg_hurricane_intensity &lt;- storms %&gt;% filter(status == &quot;hurricane&quot;) %&gt;% group_by(name) %&gt;% summarize(across(where(is.numeric), mean, na.rm = T), .groups = &quot;drop&quot;) avg_hurricane_intensity %&gt;% select(name, year, month, wind, pressure, ts_diameter, hu_diameter) %&gt;% arrange(desc(wind)) %&gt;% # get top 10 filter(row_number() &lt;= 10) %&gt;% knitr::kable() # Make into a pretty table name year month wind pressure ts_diameter hu_diameter Andrew 1992.000 8.000000 118.2609 946.6522 NaN NaN Mitch 1998.000 10.000000 115.9091 945.3182 NaN NaN Rita 2005.000 9.000000 114.7368 931.6316 305.2952 111.6934 Isabel 2003.000 9.000000 112.1875 946.5417 NaN NaN Gilbert 1988.000 9.000000 110.8929 945.4286 NaN NaN Wilma 2005.000 10.000000 110.3030 939.4242 402.5812 136.1756 Hugo 1989.000 9.000000 106.5789 950.9211 NaN NaN David 1979.000 8.457143 105.1429 956.1429 NaN NaN Luis 1995.000 8.893617 104.1489 951.8298 NaN NaN Ivan 1996.652 9.269663 103.5393 953.9438 315.9642 115.2031 Another handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here if only because it also comes with this handy cartoon illustration. relocate lets you rearrange columns (by Allison Horst) avg_hurricane_intensity %&gt;% relocate(c(wind, pressure), .after = month) # A tibble: 121 x 11 name year month wind pressure day hour lat long ts_diameter &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 AL121991 1991 11 65 980. 1.5 9 38.8 -66.1 NaN 2 Alberto 1999. 7.92 78.3 978. 13.4 8.82 30.8 -49.0 NaN 3 Alex 2006. 7.38 83.1 966. 10 8.24 33 -77.1 304. 4 Alicia 1983 8 84.4 974. 17.5 7.62 28.4 -94.5 NaN 5 Allison 1995 6 65 988. 4.33 10 26.2 -86.2 NaN 6 Andrew 1992 8 118. 947. 24.0 9.09 26.4 -80.4 NaN 7 Anita 1977 8.62 93.1 968. 12.8 9.69 25.3 -94.2 NaN 8 Arthur 2014 7 77.3 978. 3.73 7.55 34.6 -76.1 209. 9 Barry 1983 8 68.3 988. 28 15.7 25.4 -97.1 NaN 10 Belle 1976 8 91 966. 8.5 9 31.2 -74.8 NaN # … with 111 more rows, and 1 more variable: hu_diameter &lt;dbl&gt; # move numeric variables to the front avg_hurricane_intensity %&gt;% relocate(where(is.numeric)) # A tibble: 121 x 11 year month day hour lat long wind pressure ts_diameter hu_diameter &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1991 11 1.5 9 38.8 -66.1 65 980. NaN NaN 2 1999. 7.92 13.4 8.82 30.8 -49.0 78.3 978. NaN NaN 3 2006. 7.38 10 8.24 33 -77.1 83.1 966. 304. 63.6 4 1983 8 17.5 7.62 28.4 -94.5 84.4 974. NaN NaN 5 1995 6 4.33 10 26.2 -86.2 65 988. NaN NaN 6 1992 8 24.0 9.09 26.4 -80.4 118. 947. NaN NaN 7 1977 8.62 12.8 9.69 25.3 -94.2 93.1 968. NaN NaN 8 2014 7 3.73 7.55 34.6 -76.1 77.3 978. 209. 58.1 9 1983 8 28 15.7 25.4 -97.1 68.3 988. NaN NaN 10 1976 8 8.5 9 31.2 -74.8 91 966. NaN NaN # … with 111 more rows, and 1 more variable: name &lt;chr&gt; Try it out Data Setup if (!&quot;gapminder&quot; %in% installed.packages()) install.packages(&quot;gapminder&quot;) library(gapminder) gapminder_unfiltered # A tibble: 3,313 x 6 country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. 7 Afghanistan Asia 1982 39.9 12881816 978. 8 Afghanistan Asia 1987 40.8 13867957 852. 9 Afghanistan Asia 1992 41.7 16317921 649. 10 Afghanistan Asia 1997 41.8 22227415 635. # … with 3,303 more rows readr::write_csv(gapminder_unfiltered, &quot;data/gapminder.csv&quot;, na = &#39;.&#39;) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 filename fileloc &#39;data/gapminder.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.gapminder REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. You can read about the gapminder project here. The gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. I’ve exported the gapminder_unfiltered table to CSV for import into SAS as well - try to do these tasks in both languages. Task 1: How bad is it? Using your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use WHERE or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information. R gapminder_unfiltered %&gt;% group_by(country) %&gt;% summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), missingpop = sum(is.na(pop)), missingGDP = sum(is.na(gdpPercap))) %&gt;% filter(n != length(seq(1952, 2007, by = 5))) # A tibble: 83 x 5 country n missinglifeExp missingpop missingGDP &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 Armenia 4 0 0 0 2 Aruba 8 0 0 0 3 Australia 56 0 0 0 4 Austria 57 0 0 0 5 Azerbaijan 4 0 0 0 6 Bahamas 10 0 0 0 7 Barbados 10 0 0 0 8 Belarus 18 0 0 0 9 Belgium 57 0 0 0 10 Belize 11 0 0 0 # … with 73 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE gapsummary AS 10 SELECT DISTINCT country, COUNT(*) AS n, 11 SUM(MISSING(lifeExp)) AS missinglifeExp, 12 SUM(MISSING(pop)) AS missingpop, 13 SUM(MISSING(gdpPercap)) AS missingGDP 14 FROM classdat.gapminder 15 GROUP BY country; NOTE: Statement not executed due to NOEXEC option. 16 17 /* Print the problem countries only */ NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 PROC PRINT DATA = gapsummary; ERROR: File WORK.GAPSUMMARY.DATA does not exist. 19 WHERE n ^= 12; WARNING: No data sets qualify for WHERE processing. 20 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. In order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data. Task 2: Exclude any data which isn’t at 5-year increments, starting in 1952 (so 1952, 1957, 1962, …, 2007). R gapminder_unfiltered %&gt;% filter(year %in% seq(1952, 2007, by = 5)) # A tibble: 2,013 x 6 country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. 7 Afghanistan Asia 1982 39.9 12881816 978. 8 Afghanistan Asia 1987 40.8 13867957 852. 9 Afghanistan Asia 1992 41.7 16317921 649. 10 Afghanistan Asia 1997 41.8 22227415 635. # … with 2,003 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE gap5 AS 10 SELECT * 11 FROM classdat.gapminder 12 WHERE MOD(year, 5) = 2; NOTE: Statement not executed due to NOEXEC option. 13 14 /* Aus had too much data, so use it to see if the command worked 14 ! */ NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 PROC PRINT DATA = gap5; ERROR: File WORK.GAP5.DATA does not exist. 16 WHERE country = &quot;Australia&quot;; WARNING: No data sets qualify for WHERE processing. 17 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. Task 3: Exclude any countries that don’t have a full set of observations from 1952 - 2007 in 5-year increments. R gapminder_unfiltered %&gt;% filter(year %in% seq(1952, 2007, by = 5)) %&gt;% group_by(country) %&gt;% mutate(nobs = n()) %&gt;% # Use mutate instead of summarize so that all rows stay filter(nobs == 12) %&gt;% select(-nobs) # A tibble: 1,704 x 6 # Groups: country [142] country continent year lifeExp pop gdpPercap &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; 1 Afghanistan Asia 1952 28.8 8425333 779. 2 Afghanistan Asia 1957 30.3 9240934 821. 3 Afghanistan Asia 1962 32.0 10267083 853. 4 Afghanistan Asia 1967 34.0 11537966 836. 5 Afghanistan Asia 1972 36.1 13079460 740. 6 Afghanistan Asia 1977 38.4 14880372 786. 7 Afghanistan Asia 1982 39.9 12881816 978. 8 Afghanistan Asia 1987 40.8 13867957 852. 9 Afghanistan Asia 1992 41.7 16317921 649. 10 Afghanistan Asia 1997 41.8 22227415 635. # … with 1,694 more rows SAS 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE gap5 AS 10 SELECT * 11 FROM classdat.gapminder 12 WHERE MOD(year, 5) = 2; NOTE: Statement not executed due to NOEXEC option. 13 14 CREATE TABLE gap_clean AS 15 SELECT *, COUNT(*) as n 16 FROM gap5 17 GROUP BY country 18 HAVING n = 12; NOTE: Statement not executed due to NOEXEC option. 19 20 /* Clean up extra column */ 21 ALTER TABLE gap_clean 22 DROP n; NOTE: Statement not executed due to NOEXEC option. 23 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 PROC PRINT DATA = gap_clean; ERROR: File WORK.GAP_CLEAN.DATA does not exist. 25 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. References Introduction to dplyr and Single Table dplyr functions Using WHERE with SAS Procedures PROC SQL documentation R for Data Science: Data Transformations Additional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State) Base R data manipulation SAS data manipulation Videos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well. See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎ The philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎ It accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎ This is equivalent to using right assignment in R with -&gt;, which you shouldn’t do unless you have a really good reason, because it’s hard to read.↩︎ or ?summarise if you like UK English – the developer of this package is from NZ↩︎ "],["transforming-data.html", "Module 6 Transforming Data Transforming Data: Module Objectives 6.1 Identifying the problem: Messy data 6.2 String operations: Creating new variables and separating multi-variable columns 6.3 Pivot operations 6.4 Relational Data and Joining Tables 6.5 Example: Gas Prices Data References", " Module 6 Transforming Data Happy families are all alike; every unhappy family is unhappy in its own way. - Leo Tolstoy Tidy datasets are all alike, but every messy dataset is messy in its own way. - Hadley Wickham Transforming Data: Module Objectives Reshape data Transform variables to support analysis and visualization of data Join tables together in order to create a single coherent dataset Most of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data viewing, rather than for statistical analysis - they may be laid out so that there are multiple observations in a single row (e.g., commonly a year’s worth of data, with monthly observations in each column). Unfortunately, this type of data structure is not usually useful to us when we analyze or visualize the data. This section is going to seem like it drags on forever. It covers a lot of material, and a few different concepts. I highly recommend separating it out into 3-4 different “sessions” - Tidy data, Strings, Pivot operations, and Table Joins. For now, you need to know this material well enough to 1) identify what operation needs to happen, and 2) know where to find the sample code for that operation. It will get easier to remember the specific syntax with practice. Load initial packages library(dplyr) # Data wrangling library(tidyr) # Data rearranging library(tibble) # data table 6.1 Identifying the problem: Messy data These datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout. Table 6.1: Table 1 country year cases population Afghanistan 1999 745 19987071 Afghanistan 2000 2666 20595360 Brazil 1999 37737 172006362 Brazil 2000 80488 174504898 China 1999 212258 1272915272 China 2000 213766 1280428583 Here, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column). Table 6.2: Table 2 country year type count Afghanistan 1999 cases 745 Afghanistan 1999 population 19987071 Afghanistan 2000 cases 2666 Afghanistan 2000 population 20595360 Brazil 1999 cases 37737 Brazil 1999 population 172006362 Brazil 2000 cases 80488 Brazil 2000 population 174504898 China 1999 cases 212258 China 1999 population 1272915272 China 2000 cases 213766 China 2000 population 1280428583 Here, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1. Table 6.3: Table 3 country year rate Afghanistan 1999 745/19987071 Afghanistan 2000 2666/20595360 Brazil 1999 37737/172006362 Brazil 2000 80488/174504898 China 1999 212258/1272915272 China 2000 213766/1280428583 This form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being. Table 6.4: Table 4a country 1999 2000 Afghanistan 745 2666 Brazil 37737 80488 China 212258 213766 Table 6.4: Table 4b country 1999 2000 Afghanistan 19987071 20595360 Brazil 172006362 174504898 China 1272915272 1280428583 In this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year. Table 6.5: Table 5 country century year rate Afghanistan 19 99 745/19987071 Afghanistan 20 00 2666/20595360 Brazil 19 99 37737/172006362 Brazil 20 00 80488/174504898 China 19 99 212258/1272915272 China 20 00 213766/1280428583 Table 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would. These variations highlight the principles which can be said to define a tidy dataset: 1. Each variable must have its own column 2. Each observation must have its own row 3. Each value must have its own cell Try it out Go back through the 5 tables and determine whether each table is tidy, and if it is not, which rule or rules it violates. Figure out what you would have to do in order to compute a standardized TB infection rate per 100,000 people. Solution table1 - this is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000. table2 - each variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1. table3 - each value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1. table4a and table 4b - there are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1. table 5 - each variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first). It is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be: Each dataset goes into its own table (or tibble, if you are using R) Each variable gets its own column By the end of this module, you should have the skills to “tidy” each of these tables. 6.2 String operations: Creating new variables and separating multi-variable columns Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated). Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski Alternately, the xkcd version of the above quote The tidyverse package to deal with strings is stringr. The functions in stringr take the form of str_XXX where XXX is a verb. So str_split(), str_replace(), str_remove(), str_to_lower() all should make some sense. For this example, we’ll use a subset of the US Department of Education College Scorecard data. Documentation, Data. I’ve selected a few columns from the institution-level data available on the College Scorecard site. Let’s take a look (Read in the data) college &lt;- read_csv(&quot;data/College_Data_Abbrev.csv&quot;, guess_max = 5000, na = &#39;.&#39;) ── Column specification ──────────────────────────────────────────────────────── cols( UNITID = col_double(), INSTNM = col_character(), CITY = col_character(), STABBR = col_character(), ZIP = col_character(), ACCREDAGENCY = col_character(), INSTURL = col_character(), PREDDEG = col_character(), MAIN = col_character(), NUMBRANCH = col_double(), HIGHDEG = col_character(), CONTROL = col_character(), ST_FIPS = col_double(), LOCALE = col_double(), LATITUDE = col_double(), LONGITUDE = col_double(), State = col_character() ) str(college) spec_tbl_df[,17] [6,806 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame) $ UNITID : num [1:6806] 100654 100663 100690 100706 100724 ... $ INSTNM : chr [1:6806] &quot;Alabama A &amp; M University&quot; &quot;University of Alabama at Birmingham&quot; &quot;Amridge University&quot; &quot;University of Alabama in Huntsville&quot; ... $ CITY : chr [1:6806] &quot;Normal&quot; &quot;Birmingham&quot; &quot;Montgomery&quot; &quot;Huntsville&quot; ... $ STABBR : chr [1:6806] &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; &quot;AL&quot; ... $ ZIP : chr [1:6806] &quot;35762&quot; &quot;35294-0110&quot; &quot;36117-3553&quot; &quot;35899&quot; ... $ ACCREDAGENCY: chr [1:6806] &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; &quot;Southern Association of Colleges and Schools Commission on Colleges&quot; ... $ INSTURL : chr [1:6806] &quot;www.aamu.edu/&quot; &quot;https://www.uab.edu&quot; &quot;www.amridgeuniversity.edu&quot; &quot;www.uah.edu&quot; ... $ PREDDEG : chr [1:6806] &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; &quot;Predominantly bachelor&#39;s-degree granting&quot; ... $ MAIN : chr [1:6806] &quot;main campus&quot; &quot;main campus&quot; &quot;main campus&quot; &quot;main campus&quot; ... $ NUMBRANCH : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ... $ HIGHDEG : chr [1:6806] &quot;Graduate&quot; &quot;Graduate&quot; &quot;Graduate&quot; &quot;Graduate&quot; ... $ CONTROL : chr [1:6806] &quot;Public&quot; &quot;Public&quot; &quot;Private Non Profit&quot; &quot;Public&quot; ... $ ST_FIPS : num [1:6806] 1 1 1 1 1 1 1 1 1 1 ... $ LOCALE : num [1:6806] 12 12 12 12 12 12 32 31 12 13 ... $ LATITUDE : num [1:6806] 34.8 33.5 32.4 34.7 32.4 ... $ LONGITUDE : num [1:6806] -86.6 -86.8 -86.2 -86.6 -86.3 ... $ State : chr [1:6806] &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; &quot;Alabama&quot; ... - attr(*, &quot;spec&quot;)= .. cols( .. UNITID = col_double(), .. INSTNM = col_character(), .. CITY = col_character(), .. STABBR = col_character(), .. ZIP = col_character(), .. ACCREDAGENCY = col_character(), .. INSTURL = col_character(), .. PREDDEG = col_character(), .. MAIN = col_character(), .. NUMBRANCH = col_double(), .. HIGHDEG = col_character(), .. CONTROL = col_character(), .. ST_FIPS = col_double(), .. LOCALE = col_double(), .. LATITUDE = col_double(), .. LONGITUDE = col_double(), .. State = col_character() .. ) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 filename fileloc &#39;data/College_Data_Abbrev.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.college REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 15 PROC PRINT DATA = classdat.college (obs = 5); 16 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. 6.2.1 Basic String Operations What proportion of the schools operating in each state have the state’s name in the school name? We’ll use str_detect() to look for the state name in the college name. library(stringr) # string processing # Outside the pipe str_detect(college$INSTNM, pattern = college$State) [1] TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [13] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [37] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE [61] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE [73] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [109] TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE FALSE TRUE [121] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE [133] FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [145] FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE [157] FALSE TRUE TRUE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE TRUE [169] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [193] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE [205] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [217] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [229] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [277] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [337] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [361] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [469] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE [481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [493] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [505] FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE [517] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [529] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [541] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE [553] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [565] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [589] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [601] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE [613] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE [625] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [637] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [649] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE [661] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [673] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE [697] FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [721] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE [733] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE [745] FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [781] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE [793] FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [829] FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE [841] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE [853] TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE [865] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [925] TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE FALSE [937] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [985] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [1009] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE [1021] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1045] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1057] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [1069] TRUE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE [1081] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1093] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1105] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1117] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1129] TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [1141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [1153] FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [1165] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE [1177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [1189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1201] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE TRUE TRUE [1213] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE [1225] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1237] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [1249] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1261] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [1273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE [1285] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1297] TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [1309] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [1321] FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE [1333] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1345] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE [1357] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE [1369] TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE [1381] TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1393] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1405] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE [1417] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [1429] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [1441] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1453] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1465] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1477] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1489] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1501] FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE [1513] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1525] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1537] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1549] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1573] FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [1585] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [1597] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1609] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1621] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [1633] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1645] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE [1657] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [1669] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1681] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [1693] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE [1705] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE [1717] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1729] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE [1741] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1753] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1765] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [1777] TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1789] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE [1801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1813] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [1825] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1837] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE [1849] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1861] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [1873] FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE [1885] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE [1897] FALSE FALSE TRUE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE [1909] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [1921] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE [1933] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [1945] TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE TRUE FALSE TRUE FALSE [1957] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [1969] FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [1981] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [1993] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2005] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2017] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [2029] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2041] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [2053] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2065] TRUE FALSE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE [2077] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [2089] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE [2101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [2113] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [2125] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2149] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE [2161] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2173] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [2185] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2197] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2209] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2233] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2245] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2257] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [2269] TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [2281] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2293] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2305] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [2317] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2329] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2341] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2353] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2365] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2377] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2389] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2401] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2413] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2425] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2437] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2449] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [2461] TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [2473] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2485] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2497] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2509] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2521] TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2533] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2545] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2557] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2569] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2581] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2593] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2605] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2617] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [2629] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [2641] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2653] TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [2665] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE [2677] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2689] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2701] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2713] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [2725] FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [2737] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [2749] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE [2761] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [2773] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2785] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2797] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE TRUE [2809] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE [2821] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [2833] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2845] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [2857] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE [2869] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE [2881] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2893] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2905] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [2917] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2929] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2941] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [2953] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [2965] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [2977] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [2989] TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3001] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3013] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3025] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [3037] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3049] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [3061] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [3073] FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE FALSE FALSE [3085] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3097] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [3121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [3133] TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [3169] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE [3181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3193] TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE [3205] FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [3217] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE TRUE [3229] FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE [3241] TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE [3253] TRUE TRUE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [3265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [3301] TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [3313] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3349] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3373] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [3385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3397] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE TRUE FALSE [3409] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE [3421] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [3433] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE TRUE [3445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE [3457] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3469] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE [3481] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [3493] FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE [3505] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE [3517] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [3553] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3565] FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE TRUE [3577] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE [3589] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3601] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3613] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [3625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE [3649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3661] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3673] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE [3685] TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE TRUE TRUE FALSE FALSE [3697] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [3709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [3721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [3733] FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [3745] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [3757] FALSE TRUE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE [3769] TRUE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [3781] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE [3793] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE [3805] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [3817] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [3829] TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE TRUE [3841] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE FALSE [3853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3877] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3889] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3901] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3913] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3925] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3937] FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [3949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [3961] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3973] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [3985] FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE [3997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [4009] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4033] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [4045] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [4057] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4069] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4081] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4093] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE NA [4105] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE [4117] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4141] FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4153] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [4165] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [4201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4213] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4225] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [4237] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [4249] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4273] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE [4285] TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [4297] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4309] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE [4321] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [4333] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4345] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4357] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [4369] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [4381] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4393] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [4405] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [4417] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [4429] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE [4441] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [4453] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [4465] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [4477] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4489] FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [4501] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4513] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE [4525] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE [4537] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [4549] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [4561] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4573] FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [4585] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4597] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE [4609] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4621] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4633] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4645] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE [4657] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE [4669] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE [4681] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4693] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE [4705] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [4717] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4729] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [4741] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE [4753] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [4765] TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE [4777] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4789] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4801] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4813] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4825] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [4837] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4849] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [4861] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4873] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4885] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4897] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4909] FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4921] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE [4933] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE [4945] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE [4957] FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE [4969] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [4981] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [4993] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5005] TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5017] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE [5029] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5041] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5053] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5065] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5077] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [5089] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5101] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE TRUE [5113] FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5125] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5137] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5149] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5161] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [5173] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5185] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [5197] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE [5209] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5221] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5233] FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [5245] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [5257] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5269] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [5281] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [5293] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5305] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [5317] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5329] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE [5341] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [5353] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5365] FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE [5377] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5389] FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [5401] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [5413] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5425] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [5437] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [5449] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5461] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5473] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [5485] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [5497] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [5509] TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [5521] FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [5533] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5545] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5557] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5569] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [5581] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [5593] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5605] FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE [5617] FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE [5629] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5641] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [5653] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5665] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [5677] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5689] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5701] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [5713] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [5725] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5737] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [5749] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5761] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5773] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [5785] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [5797] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [5809] TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE TRUE [5821] TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5833] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5845] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5857] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5869] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [5881] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5893] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE TRUE [5905] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE [5917] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [5929] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5941] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE [5953] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5965] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5977] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [5989] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE [6001] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6013] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE [6025] FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE FALSE [6037] TRUE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [6049] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE [6061] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [6073] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6085] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6097] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6109] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE TRUE [6121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [6133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6145] FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [6169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6181] FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [6193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6229] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6241] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [6253] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6301] FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE [6313] FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE [6325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6337] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [6349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE [6361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE [6385] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [6397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6409] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [6421] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [6445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE [6457] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6493] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE [6505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE [6541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6565] FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [6577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [6589] FALSE FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [6601] FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE [6613] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [6625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [6637] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE [6649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6661] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE [6673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6685] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6697] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6721] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [6733] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6745] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6757] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [6769] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE [6781] TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE [6793] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE [6805] FALSE FALSE # Using the pipe and mutate: college &lt;- college %&gt;% mutate(uses_st_name = str_detect(INSTNM, State)) library(ggplot2) # graphs and charts # By state - percentage of institution names college %&gt;% group_by(State) %&gt;% summarize(pct_uses_st_name = mean(uses_st_name), n = n()) %&gt;% filter(n &gt; 5) %&gt;% # only states/territories with at least 5 schools # Reorder state factor level by percentage that uses state name mutate(State = reorder(State, -pct_uses_st_name)) %&gt;% ggplot(data = ., aes(x = State, y = pct_uses_st_name)) + geom_col() + coord_flip() + geom_text(aes(y = 1, label = paste(&quot;Total Schools:&quot;, n)), hjust = 1) In SAS, we use find(x, pattern, 't') to find the location of the pattern, which is 0 if the pattern is not found. To get something equivalent to str_detect, we just test whether this quantity is greater than 0. (The R equivalent of find is str_locate()). Note that SAS pads character fields with spaces so that they are all the same length. So if we want to test for “Alabama” we could omit the ‘t’ option in the command, but since we usually don’t want that, we need to tell SAS to trim the fields before searching for the pattern. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA collegetmp; 9 set classdat.college; 10 uses_st_name = find(INSTNM, State, &#39;t&#39;) GT 0; 11 RUN; NOTE: The data set WORK.COLLEGETMP has 0 observations and 18 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 14 PROC PRINT DATA = collegetmp (obs = 5); 15 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. What are some common substrings in a set of text? For this, we’ll start with working with the single column INSTNM. head(college$INSTNM) %&gt;% str_split(., &quot; &quot;) # Split on every space [[1]] [1] &quot;Alabama&quot; &quot;A&quot; &quot;&amp;&quot; &quot;M&quot; &quot;University&quot; [[2]] [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;at&quot; &quot;Birmingham&quot; [[3]] [1] &quot;Amridge&quot; &quot;University&quot; [[4]] [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;in&quot; &quot;Huntsville&quot; [[5]] [1] &quot;Alabama&quot; &quot;State&quot; &quot;University&quot; [[6]] [1] &quot;The&quot; &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; # We may need to fix certain things that should stay together # But doing too much of that gets tedious... str_replace(college$INSTNM, &quot;A &amp; M&quot;, &quot;A&amp;M&quot;) %&gt;% head() %&gt;% str_split(., &quot;[ -]&quot;) # This pattern says &quot;either &#39; &#39; or &#39;-&#39;&quot; [[1]] [1] &quot;Alabama&quot; &quot;A&amp;M&quot; &quot;University&quot; [[2]] [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;at&quot; &quot;Birmingham&quot; [[3]] [1] &quot;Amridge&quot; &quot;University&quot; [[4]] [1] &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; &quot;in&quot; &quot;Huntsville&quot; [[5]] [1] &quot;Alabama&quot; &quot;State&quot; &quot;University&quot; [[6]] [1] &quot;The&quot; &quot;University&quot; &quot;of&quot; &quot;Alabama&quot; # (but the - has to be at the start or the end) So we could take the time to clean up everything, making sure that e.g. San Diego is treated as a single word, but that’s a pain in the rear. Instead, let’s just see what happens if we brute-force it. tmp &lt;- college %&gt;% select(INSTNM, State) %&gt;% mutate(name_words = str_split(INSTNM, &#39;[ -]&#39;)) # This is a list-column tmp # A tibble: 6,806 x 3 INSTNM State name_words &lt;chr&gt; &lt;chr&gt; &lt;list&gt; 1 Alabama A &amp; M University Alabama &lt;chr [5]&gt; 2 University of Alabama at Birmingham Alabama &lt;chr [5]&gt; 3 Amridge University Alabama &lt;chr [2]&gt; 4 University of Alabama in Huntsville Alabama &lt;chr [5]&gt; 5 Alabama State University Alabama &lt;chr [3]&gt; 6 The University of Alabama Alabama &lt;chr [4]&gt; 7 Central Alabama Community College Alabama &lt;chr [4]&gt; 8 Athens State University Alabama &lt;chr [3]&gt; 9 Auburn University at Montgomery Alabama &lt;chr [4]&gt; 10 Auburn University Alabama &lt;chr [2]&gt; # … with 6,796 more rows unnest(tmp) # Unnest duplicates rows so that the expanded data frame has the Warning: `cols` is now required when using unnest(). Please use `cols = c(name_words)` # A tibble: 28,510 x 3 INSTNM State name_words &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Alabama A &amp; M University Alabama Alabama 2 Alabama A &amp; M University Alabama A 3 Alabama A &amp; M University Alabama &amp; 4 Alabama A &amp; M University Alabama M 5 Alabama A &amp; M University Alabama University 6 University of Alabama at Birmingham Alabama University 7 University of Alabama at Birmingham Alabama of 8 University of Alabama at Birmingham Alabama Alabama 9 University of Alabama at Birmingham Alabama at 10 University of Alabama at Birmingham Alabama Birmingham # … with 28,500 more rows # same structure as the original data List columns are one way to maintain tidy data. They allow you to have several “sub-observations” for each observation and are useful for precisely cases like this, where there are uneven numbers of words in each university’s name. We’re not going to focus on list columns, but if you’re interested, check out purrr and this excellent tutorial. library(purrr) # List columns unnest(tmp) %&gt;% pull(name_words) %&gt;% # this pulls out a single column table() %&gt;% sort(decreasing = T) %&gt;% head(50) Warning: `cols` is now required when using unnest(). Please use `cols = c(name_words)` . College of University School Institute 2716 1732 1723 821 690 Community Beauty Academy State 554 443 440 401 389 Technical and Center Technology Career 366 293 264 230 221 Cosmetology Campus &amp; the New 216 207 186 160 143 The County Medical Hair Nursing 143 136 133 132 129 San Seminary Health Design Education 126 123 118 116 114 American Paul at Valley Mitchell 109 106 103 102 97 Inc City Empire West Barber 95 94 92 89 88 South California International North Central 86 84 84 83 79 Florida Strayer Texas Theological Training 79 76 76 76 75 In SAS, this is a bit more tricky. Most people I know that use both SAS and R will do the data cleaning in R once things get complicated, and then read the clean data in to SAS. That’s a valid approach, but it’s worth seeing what has to be done in SAS this once. As we get further into this class, I’ll probably be more willing to say “we’re just going to use R for this” for two reasons - 1. I know R better, and 2. R is generally better at handling the weird stuff; SAS is built to quickly handle things that are already formatted in a reasonable way. SAS seems to be highly preferred for e.g. fitting mixed/linear models, but it isn’t the easiest tool to use for data cleaning. But, in this particular case, there is documentation about how to break a sentence into words in SAS. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 DATA collegename; 8 SET classdat.college; 9 numWords = countw(INSTNM, &quot; &quot;); 10 DO i = 1 TO numWords; 11 word = scan(INSTNM, i, &quot; &quot;); 12 OUTPUT; 13 END; 14 KEEP word numWords; 15 ; 16 NOTE: The data set WORK.COLLEGENAME has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 PROC PRINT DATA=collegename (obs = 30); 18 run; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 20 PROC FREQ DATA=collegename ORDER=FREQ; 21 TABLES word / MAXLEVELS=30; 22 RUN; NOTE: PROCEDURE FREQ used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. 6.2.2 Regular Expressions Matching exact strings is easy - it’s just like using find and replace. human_talk &lt;- &quot;blah, blah, blah. Do you want to go for a walk?&quot; dog_hears &lt;- str_extract(human_talk, &quot;walk&quot;) dog_hears [1] &quot;walk&quot; But, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler. You may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to both SAS and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to. Short Regular Expression Primer (with R examples) [] enclose sets of characters Ex: [abc] will match any single character a, b, c - specifies a range of characters (A-z matches all upper and lower case letters) to match - exactly, precede with a backslash (outside of []) or put the - last (inside []) . matches any character (except a newline) To match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\\\. will match a literal ., \\\\$ will match a literal $. num_string &lt;- &quot;phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789&quot; ssn &lt;- str_extract(num_string, &quot;[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]&quot;) ssn [1] &quot;123-45-6789&quot; Listing out all of those numbers can get repetitive, though. How do we specify repetition? * means repeat between 0 and inf times + means 1 or more times ? means 0 or 1 times – most useful when you’re looking for something optional {a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times” str_extract(&quot;banana&quot;, &quot;[a-z]{1,}&quot;) # match any sequence of lowercase characters [1] &quot;banana&quot; str_extract(&quot;banana&quot;, &quot;[ab]{1,}&quot;) # Match any sequence of a and b characters [1] &quot;ba&quot; str_extract_all(&quot;banana&quot;, &quot;(..)&quot;) # Match any two characters [[1]] [1] &quot;ba&quot; &quot;na&quot; &quot;na&quot; str_extract(&quot;banana&quot;, &quot;(..)\\\\1&quot;) # Match a repeated thing [1] &quot;anan&quot; num_string &lt;- &quot;phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23&quot; ssn &lt;- str_extract(num_string, &quot;[0-9]{3}-[0-9]{2}-[0-9]{4}&quot;) ssn [1] &quot;123-45-6789&quot; phone &lt;- str_extract(num_string, &quot;[0-9]{3}.[0-9]{3}.[0-9]{4}&quot;) phone [1] &quot;123-456-7890&quot; nuid &lt;- str_extract(num_string, &quot;[0-9]{8}&quot;) nuid [1] &quot;12345678&quot; bank_balance &lt;- str_extract(num_string, &quot;\\\\$[0-9,]+\\\\.[0-9]{2}&quot;) bank_balance [1] &quot;$50,000,000.23&quot; There are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end) ^ has multiple meanings: if it’s the first character in a pattern, ^ matches the beginning of a string if it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c.” $ means the end of a string Combined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses address &lt;- &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot; house_num &lt;- str_extract(address, &quot;^[0-9]{1,}&quot;) # Match everything alphanumeric up to the comma street &lt;- str_extract(address, &quot;[A-z0-9 ]{1,}&quot;) street &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number city &lt;- str_extract(address, &quot;,.*,&quot;) %&gt;% str_remove_all(&quot;,&quot;) %&gt;% str_trim() zip &lt;- str_extract(address, &quot;[0-9-]{5,10}$&quot;) # match 5 and 9 digit zip codes () are used to capture information. So ([0-9]{4}) captures any 4-digit number a|b will select a or b. If you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, though, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on. phone_num_variants &lt;- c(&quot;(123) 456-7980&quot;, &quot;123.456.7890&quot;, &quot;+1 123-456-7890&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})&quot; # \\\\( and \\\\) match literal parentheses if they exist # ([0-9]{3})? captures the area code, if it exists # .? matches any character # ([0-9]{3}) captures the exchange code # ([0-9]{4}) captures the 4-digit individual code str_extract(phone_num_variants, phone_regex) [1] &quot;(123) 456-7980&quot; &quot;123.456.7890&quot; &quot;123-456-7890&quot; str_replace(phone_num_variants, phone_regex, &quot;\\\\1\\\\2\\\\3&quot;) [1] &quot;1234567980&quot; &quot;1234567890&quot; &quot;+1 1234567890&quot; # We didn&#39;t capture the country code, so it remained in the string human_talk &lt;- &quot;blah, blah, blah. Do you want to go for a walk? I think I&#39;m going to treat myself to some ice cream for working so hard. &quot; dog_hears &lt;- str_extract_all(human_talk, &quot;walk|treat&quot;) dog_hears [[1]] [1] &quot;walk&quot; &quot;treat&quot; In SAS, much the same information is true, though you do not have to double-escape special characters. SAS uses PERL-compatible regular expressions (PCRE for short) (these can also be enabled in base-R string functions). In PCRE regular expressions, ‘/’ are used as delimiters. SAS assigns each sequential regular expression a number (so that you can reference them if necessary). PRXMATCH returns the first position of a string where a match is found (0 otherwise) In this example, however, lets just test to see whether PRXMATCH finds a match or not 6 DATA strings; 7 INFILE DATALINES DSD; /* This allows quoted strings */ 8 INPUT string ~ $150.; /* ~ says deal with quoted strings */ 9 DATALINES; NOTE: The data set WORK.STRINGS has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 RUN; 17 18 DATA info; 19 set strings; 20 IF PRXMATCH(&quot;/\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})/&quot;, 20 ! string) GT 0 THEN phone = 1; 21 ELSE phone = 0; 22 IF PRXMATCH(&quot;/(walk|treat)/&quot;, string) GT 0 THEN dog = 1; 23 ELSE dog = 0; 24 IF PRXMATCH(&quot;/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), 24 ! ([0-9]{5})/&quot;, string) GT 0 THEN addr = 1; 25 ELSE addr = 0; /* Changed to require at least 3 characters 25 ! in street and city names */ 26 IF PRXMATCH(&quot;/(..)\\1/&quot;, string) GT 0 THEN abab = 1; 27 ELSE abab = 0; 28 ; 29 NOTE: The data set WORK.INFO has 0 observations and 5 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 30 PROC PRINT DATA=info; 31 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. Note that the equivalent syntax in R would be: strings &lt;- c(&quot;abcdefghijklmnopqrstuvwxyzABAB&quot;, &quot;banana orange strawberry apple&quot;, &quot;ana went to montana to eat a banana&quot;, &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot;, &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot;, &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})&quot; dog_regex &lt;- &quot;(walk|treat)&quot; addr_regex &lt;- &quot;([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})&quot; abab_regex &lt;- &quot;(..)\\\\1&quot; tibble( text = strings, phone = str_detect(strings, phone_regex), dog = str_detect(strings, dog_regex), addr = str_detect(strings, addr_regex), abab = str_detect(strings, abab_regex)) # A tibble: 6 x 5 text phone dog addr abab &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; 1 abcdefghijklmnopqrstuvwxyzABAB FALSE FALSE FALSE TRUE 2 banana orange strawberry apple FALSE FALSE FALSE TRUE 3 ana went to montana to eat a banana FALSE FALSE FALSE TRUE 4 call me at 432-394-2873. Do you want to go for a walk… TRUE TRUE FALSE FALSE 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE FALSE FALSE FALSE 6 1600 Pennsylvania Ave NW, Washington D.C., 20500 FALSE FALSE TRUE FALSE When doing various operations with regular expressions, it can be useful to save the regular expression for later use. PRXPARSE saves a regex for use later PRXSUBSTR saves the starting location and length of a string match SUBSTR extracts the string given the starting location and length PRXPARSE, PRXSUBSTR, SUBSTR 6 DATA strings; 7 INFILE DATALINES DSD; /* This allows quoted strings */ 8 INPUT string ~ $150.; /* ~ says deal with quoted strings */ 9 DATALINES; NOTE: The data set WORK.STRINGS has 0 observations and 1 variables. WARNING: Data set WORK.STRINGS was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 RUN; 17 18 DATA info; 19 SET strings; 20 /* This says use these variables for all rows */ 21 RETAIN REphone REdog REaddr REabab; 22 23 /* This block defined our variables */ 24 IF _N_ = 1 THEN DO; 25 REphone = 25 ! PRXPARSE(&quot;/\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})/&quot;); 26 REdog = PRXPARSE(&quot;/(walk|treat)/&quot;); 27 REaddr = PRXPARSE(&quot;/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. 27 ! ]{3,}), ([0-9]{5})/&quot;); 28 REabab = PRXPARSE(&quot;/(..)\\1/&quot;); 29 END; 30 31 /* This block identifies string start and length for matches 31 ! */ 32 /* Note that phonestart, phonelength, dogstart, doglength, ... 33 are all defined implicitly in this block */ 34 CALL PRXSUBSTR(REphone, string, phonestart, phonelength); 35 CALL PRXSUBSTR(REdog, string, dogstart, doglength); 36 CALL PRXSUBSTR(REaddr, string, addrstart, addrlength); 37 CALL PRXSUBSTR(REabab, string, ababstart, abablength); 38 39 /* This block extracts all of the matches */ 40 IF phonestart GT 0 THEN DO; 41 phonenumber = SUBSTR(string, phonestart, phonelength); 42 END; 43 IF dogstart GT 0 THEN DO; 44 dogword = SUBSTR(string, dogstart, doglength); 45 END; 46 IF addrstart GT 0 THEN DO; 47 addr = SUBSTR(string, addrstart, addrlength); 48 END; 49 IF ababstart GT 0 THEN DO; 50 abab = SUBSTR(string, ababstart, abablength); 51 END; 52 53 54 /* This block keeps only rows with a phone number, dog 54 ! keyword, or address */ 55 IF (phonestart GT 0) OR (dogstart GT 0) OR (addrstart GT 0) OR 55 ! (ababstart GT 0) THEN DO; 56 OUTPUT; 57 END; 58 59 /* This keeps only the variables we care about */ 60 KEEP string phonenumber dogword addr abab; 61 ; 62 NOTE: The data set WORK.INFO has 0 observations and 5 variables. WARNING: Data set WORK.INFO was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 63 PROC PRINT DATA=info; 64 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. Note that the equivalent syntax in R would be: strings &lt;- c(&quot;abcdefghijklmnopqrstuvwxyzABAB&quot;, &quot;banana orange strawberry apple&quot;, &quot;ana went to montana to eat a banana&quot;, &quot;call me at 432-394-2873. Do you want to go for a walk? I&#39;m going to treat myself to some ice cream for working so hard.&quot;, &quot;phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23&quot;, &quot;1600 Pennsylvania Ave NW, Washington D.C., 20500&quot;) phone_regex &lt;- &quot;\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})&quot; dog_regex &lt;- &quot;(walk|treat)&quot; addr_regex &lt;- &quot;([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})&quot; abab_regex &lt;- &quot;(..)\\\\1&quot; tibble( text = strings, phone = str_extract(strings, phone_regex), dog = str_extract(strings, dog_regex), addr = str_extract(strings, addr_regex), abab = str_extract(strings, abab_regex)) # A tibble: 6 x 5 text phone dog addr abab &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 abcdefghijklmnopqrstuvwxyzABAB &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ABAB 2 banana orange strawberry apple &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; anan 3 ana went to montana to eat a bana… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; anan 4 call me at 432-394-2873. Do you w… 432-394-… walk &lt;NA&gt; &lt;NA&gt; 5 phone: (123) 456-7890, nuid: 1234… (123) 45… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 1600 Pennsylvania Ave NW, Washing… &lt;NA&gt; &lt;NA&gt; 1600 Pennsylvania Av… &lt;NA&gt; Find and Replace with PRXCHANGE The next major task is find and replace, where we get to see another feature of perl-style regular expressions. ‘s/xxx/yyy/’ is the general form of a find-and-replace regular expression. Think “substitue yyy for xxx.” 6 DATA ducks; 7 INFILE DATALINES DSD; 8 INPUT line ~ $50.; 9 DATALINES; NOTE: The data set WORK.DUCKS has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 RUN; 15 16 DATA cats; 17 SET ducks; 18 /* define lengths of output strings */ 19 LENGTH new_text $ 50 new_text2 $ 50; 20 21 IF _N_ = 1 THEN DO; 22 REanimal = PRXPARSE(&quot;s/duck/cat/&quot;); 23 REnoise = PRXPARSE(&quot;s/quack/meow/&quot;); 24 END; 25 RETAIN REanimal REnoise; 26 27 /* First, replace duck with cat */ 28 CALL PRXCHANGE(REanimal, -1, line, new_text, r_length, trunc, 28 ! n_of_changes); 29 30 /* Then, fix noises */ 31 CALL PRXCHANGE(REnoise, -1, new_text, new_text2, r_length2, 31 ! trunc2, n_of_changes2); 32 33 /* Warnings if anything was truncated */ 34 IF trunc THEN PUT &quot;Note: new_text was truncated&quot;; 35 IF trunc2 THEN PUT &quot;Note: new_text2 was truncated&quot;; 36 RUN; NOTE: The data set WORK.CATS has 0 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 PROC PRINT DATA=cats; 39 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. The equivalent R code: line &lt;- c( &quot;Five little ducks went out one day,&quot;, &quot;Over the hills and far away.&quot;, &quot;Mother duck said, quack quack quack quack,&quot;, &quot;But only four little ducks came back.&quot; ) str_replace_all(line, &quot;duck&quot;, &quot;cat&quot;) %&gt;% str_replace_all(&quot;quack&quot;, &quot;meow&quot;) [1] &quot;Five little cats went out one day,&quot; [2] &quot;Over the hills and far away.&quot; [3] &quot;Mother cat said, meow meow meow meow,&quot; [4] &quot;But only four little cats came back.&quot; # Or, in one line... str_replace_all(line, c(&quot;duck&quot; = &quot;cat&quot;, &quot;quack&quot; = &quot;meow&quot;)) [1] &quot;Five little cats went out one day,&quot; [2] &quot;Over the hills and far away.&quot; [3] &quot;Mother cat said, meow meow meow meow,&quot; [4] &quot;But only four little cats came back.&quot; In PCRE, a backreference in the same expression would be \\1, \\2, etc., but if you are in the replace block of the regex, you would use $1, $2, …. Don’t expect too much out of yourself as far as regular expressions go. I used them for almost a decade before I (mostly) quit googling “regex for …” to find somewhere to start. Another thing to realize - regular expressions are 100% a language you write, but don’t ever expect to read. So leave yourself lots of comments. Try it out - Squirrel Census The Squirrel Census (https://www.thesquirrelcensus.com/) is a multimedia science, design, and storytelling project focusing on the Eastern gray (Sciurus carolinensis) in NYC’s Central Park. They count squirrels and present their findings to the public. This table contains squirrel data for each of the 3,023 sightings, including location coordinates, age, primary and secondary fur color, elevation, activities, communications, and interactions between squirrels and with humans. Task 1: Fix the date! In both SAS and R, read in the data (link) and format the date correctly. You can do this by carefully specifying how the date is read in (?read_csv in R, informat in SAS) R solution library(readr) library(lubridate) squirrels &lt;- read_csv(&quot;data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv&quot;) %&gt;% mutate(Date = as.character(Date) %&gt;% mdy()) ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_character(), X = col_double(), Y = col_double(), Date = col_double(), `Hectare Squirrel Number` = col_double(), Running = col_logical(), Chasing = col_logical(), Climbing = col_logical(), Eating = col_logical(), Foraging = col_logical(), Kuks = col_logical(), Quaas = col_logical(), Moans = col_logical(), `Tail flags` = col_logical(), `Tail twitches` = col_logical(), Approaches = col_logical(), Indifferent = col_logical(), `Runs from` = col_logical(), `Zip Codes` = col_double(), `Community Districts` = col_double(), `Borough Boundaries` = col_double() # ... with 2 more columns ) ℹ Use `spec()` for the full column specifications. SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 filename fileloc 8 ! &#39;data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.squirrel REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 15 PROC CONTENTS DATA=classdat.squirrel; 16 RUN; NOTE: PROCEDURE CONTENTS used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 17 18 DATA classdat.cleanSquirrel; 19 SET classdat.squirrel; 20 month = FLOOR(date/1000000); 21 day = MOD(FLOOR(date/10000), 100); 22 year = MOD(date, 10000); 23 date = MDY(month, day, year); 24 format date MMDDYY10.; 25 RUN; NOTE: The data set CLASSDAT.CLEANSQUIRREL has 0 observations and 39 variables. WARNING: Data set CLASSDAT.CLEANSQUIRREL was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. Data Set Name CLASSDAT.SQUIRREL Observations 3023 Member Type DATA Variables 36 Engine V9 Indexes 0 Created 04/21/2021 11:42:03 Observation Length 656 Last Modified 04/21/2021 11:42:03 Deleted Observations 0 Protection Compressed NO Data Set Type Sorted NO Label Data Representation SOLARIS_X86_64, LINUX_X86_64, ALPHA_TRU64, LINUX_IA64 Encoding latin1 Western (ISO) Engine/Host Dependent Information Data Set Page Size 65536 Number of Data Set Pages 31 First Data Page 1 Max Obs per Page 99 Obs in First Data Page 89 Number of Data Set Repairs 0 Filename /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas/squirrel.sas7bdat Release Created 9.0401M6 Host Created Linux Inode Number 39064960 Access Permission rw-rw-r– Owner Name susan File Size 2MB File Size (bytes) 2097152 Alphabetic List of Variables and Attributes # Variable Type Len Format Informat 14 Above_Ground_Sighter_Measurement Char 5 $5. $5. 8 Age Char 8 $8. $8. 27 Approaches Char 5 $5. $5. 34 Borough_Boundaries Num 8 BEST12. BEST32. 17 Chasing Char 5 $5. $5. 35 City_Council_Districts Num 8 BEST12. BEST32. 18 Climbing Char 5 $5. $5. 12 Color_notes Char 110 $110. $110. 11 Combination_of_Primary_and_Highl Char 29 $29. $29. 33 Community_Districts Num 8 BEST12. BEST32. 6 Date Num 8 BEST12. BEST32. 19 Eating Char 5 $5. $5. 20 Foraging Char 5 $5. $5. 4 Hectare Char 3 $3. $3. 7 Hectare_Squirrel_Number Num 8 BEST12. BEST32. 10 Highlight_Fur_Color Char 24 $24. $24. 28 Indifferent Char 5 $5. $5. 22 Kuks Char 5 $5. $5. 31 Lat_Long Char 45 $45. $45. 13 Location Char 12 $12. $12. 24 Moans Char 5 $5. $5. 21 Other_Activities Char 134 $134. $134. 30 Other_Interactions Char 70 $70. $70. 36 Police_Precincts Num 8 BEST12. BEST32. 9 Primary_Fur_Color Char 8 $8. $8. 23 Quaas Char 5 $5. $5. 16 Running Char 5 $5. $5. 29 Runs_from Char 5 $5. $5. 5 Shift Char 2 $2. $2. 15 Specific_Location Char 58 $58. $58. 25 Tail_flags Char 5 $5. $5. 26 Tail_twitches Char 5 $5. $5. 3 Unique_Squirrel_ID Char 14 $14. $14. 1 X Num 8 BEST12. BEST32. 2 Y Num 8 BEST12. BEST32. 32 Zip_Codes Char 1 $1. $1. Koch Snowflake Task 2: Clean up the Combination of primary and highlight fur color column A. Get rid of leading and trailing + characters B. Where two highlight colors exist, add the primary color to both of them (so Gray+Cinnamon, White becomes Gray+Cinnamon, Gray+White) You can do this by working with the original values or the combination values; whatever is easiest. R solution squirrels_colorfix &lt;- squirrels %&gt;% # Make it easier to join things back together... mutate(id = 1:n()) %&gt;% # keep the stuff we need for this select(id, primary = `Primary Fur Color`, highlight = `Highlight Fur Color`, combo = `Combination of Primary and Highlight Color`) %&gt;% # Remove all single character strings. # ^ means &quot;front of string&quot;, $ means &quot;end of string&quot;, and . is a wildcard mutate(combo = str_remove(combo, &quot;^.$&quot;)) %&gt;% # Remove trailing + signs mutate(combo = str_remove(combo, &quot;\\\\+$&quot;)) %&gt;% # This allows only two highlight colors mutate(combo = str_replace( combo, &quot;^(Black|Cinnamon|Gray)\\\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$&quot;, &quot;\\\\1+\\\\2, \\\\1+\\\\3&quot;)) %&gt;% # This allows three highlight colors mutate(combo = str_replace( combo, &quot;^(Black|Cinnamon|Gray)\\\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$&quot;, &quot;\\\\1+\\\\2, \\\\1+\\\\3, \\\\1+\\\\4&quot;)) table(squirrels_colorfix$combo) Black 55 74 Black+Cinnamon Black+Cinnamon, Black+White 15 3 Black+Gray Black+Gray, Black+White 8 1 Black+White Cinnamon 2 62 Cinnamon+Black Cinnamon+Black, Cinnamon+White 10 3 Cinnamon+Gray Cinnamon+Gray, Cinnamon+Black 162 3 Cinnamon+Gray, Cinnamon+White Cinnamon+White 58 94 Gray Gray+Black 895 24 Gray+Black, Gray+Cinnamon Gray+Black, Gray+Cinnamon, Gray+White 9 32 Gray+Black, Gray+White Gray+Cinnamon 7 752 Gray+Cinnamon, Gray+White Gray+White 265 489 SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 DATA squirrelcolor; 9 SET classdat.cleanSquirrel; 10 11 LENGTH orig $50 new_text1 $ 50 new_text2 $ 50 new_text3 $ 50; 12 orig = Combination_of_Primary_and_Highl; 13 14 IF _N_ = 1 THEN DO; 15 REthree = 15 ! PRXPARSE(&quot;s/^(Gray|Cinnamon|Black)\\+(Black|Cinnamon|Gray|White), 15 ! (Black|Cinnamon|Gray|White), 15 ! (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3, $1+$4/&quot;); 16 REtwo = 16 ! PRXPARSE(&quot;s/^(Gray|Cinnamon|Black)\\+(Black|Cinnamon|Gray|White), 16 ! (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3/&quot;); 17 REplus = PRXPARSE(&quot;s/[^A-z]$//&quot;); 18 END; 19 RETAIN REplus REtwo REthree; 20 21 CALL PRXCHANGE(REplus, -1, trim(orig), new_text1); 22 CALL PRXCHANGE(REthree, -1, trim(new_text1), new_text2); 23 CALL PRXCHANGE(REtwo, -1, trim(new_text2), new_text3); 24 25 keep orig new_text1 new_text2 new_text3; 26 RUN; NOTE: The data set WORK.SQUIRRELCOLOR has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 27 28 /* print all combinations that occur w/ new value */ 29 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 30 SELECT DISTINCT orig, new_text3 FROM squirrelcolor; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. 6.2.3 Joining and Splitting Variables There’s another string-related task that is fairly commonly encountered: separating variables into two different columns (as in Table 3 above). Figure 6.1: A visual representation of what separating variables means for data set operations. Separating Variables We can use str_extract() if we want, but it’s actually faster to use separate(), which is part of the tidyr package. There is also extract(), which is another tidyr function that uses regular expressions and capture groups to split variables up. table3 %&gt;% separate(col = rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;, remove = F) # A tibble: 6 x 5 country year rate cases population &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghanistan 1999 745/19987071 745 19987071 2 Afghanistan 2000 2666/20595360 2666 20595360 3 Brazil 1999 37737/172006362 37737 172006362 4 Brazil 2000 80488/174504898 80488 174504898 5 China 1999 212258/1272915272 212258 1272915272 6 China 2000 213766/1280428583 213766 1280428583 I’ve left the rate column in the original data frame just to make it easy to compare and verify that yes, it worked. separate() will also take a full on regular expression if you want to capture only parts of a string to put into new columns. The scan() function in SAS can be used similarly, though it doesn’t have quite the simplicity and convenience of separate. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 6 data table3; 7 length country $12 rate $20; 8 input country $ year rate $; 9 datalines; NOTE: The data set WORK.TABLE3 has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 16 ; 17 data table3split; 18 set table3; 19 length var1-var2 $10.; 20 array var(2) $; NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 21 do i = 1 to dim(var); 22 var[i]=scan(rate, i, &#39;/&#39;, &#39;M&#39;); 23 end; 24 count = var1; 25 population = var2; 26 run; NOTE: The data set WORK.TABLE3SPLIT has 0 observations and 8 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. And, of course, there is a complementary operation, which is when it’s necessary to join two columns to get a useable data value. Figure 6.2: A visual representation of what uniting variables means for data set operations. Joining Variables separate() has a complement, unite(), which is useful for handling situations like in table5: table5 %&gt;% unite(col = &quot;year&quot;, century:year, sep = &#39;&#39;) %&gt;% separate(col = rate, into = c(&quot;cases&quot;, &quot;population&quot;), sep = &quot;/&quot;) # A tibble: 6 x 4 country year cases population &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghanistan 1999 745 19987071 2 Afghanistan 2000 2666 20595360 3 Brazil 1999 37737 172006362 4 Brazil 2000 80488 174504898 5 China 1999 212258 1272915272 6 China 2000 213766 1280428583 Note that separate and unite both work with character variables - it’s not necessarily true that you’ll always be working with character formats when you need to do these operations. For instance, it’s relatively common to need to separate dates into year, month, and day as separate columns (or to join them together). Of course, it’s much easier just to do a similar two-step operation (we have to convert to numeric variables to do math) table5 %&gt;% mutate(year = as.numeric(century)*100 + as.numeric(year)) %&gt;% select(-century) # A tibble: 6 x 3 country year rate &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; 1 Afghanistan 1999 745/19987071 2 Afghanistan 2000 2666/20595360 3 Brazil 1999 37737/172006362 4 Brazil 2000 80488/174504898 5 China 1999 212258/1272915272 6 China 2000 213766/1280428583 (Handy shortcut functions in dplyr don’t completely remove the need to think). Similarly, it is possible to do this operation in SAS as well (by string concatenation or using the numeric approach), as shown below: 6 /* read in the data */ 7 DATA table5; 8 LENGTH country $12 century year rate $20; 9 INPUT country $ century year rate $; 10 DATALINES; NOTE: The data set WORK.TABLE5 has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 ; 18 19 /* Format the data */ 20 DATA table5split; 21 SET table5; 22 LENGTH v1-v2 $10. yyc $3. centc $3. yyyyc $4.; 23 ARRAY v(2) $; 24 DO i = 1 TO dim(v); 25 v[i]=scan(rate, i, &#39;/&#39;, &#39;M&#39;); 26 END; 27 count = v1; 28 population = v2; 29 /* Numeric version */ 30 year = century*100 + year; 31 /* Character version */ 32 yyc = PUT(year, 2.); /* convert to character */ 33 centc = PUT(century, 2.); /* convert to character */ 34 yyyyc = CATT(&#39;&#39;, centc, yearc); /* catt is truncate, then 34 ! concatenate */ 35 RUN; NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 30:10 30:24 NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 30:22 NOTE: The data set WORK.TABLE5SPLIT has 0 observations and 13 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 36 37 /* Print the data */ 38 PROC PRINT DATA=table5split; 39 VAR country year yyyyc count population rate centc century yyc 39 ! ; 40 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. 6.3 Pivot operations It’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis. 6.3.1 Longer In many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable. Tables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively. table4a # A tibble: 3 x 3 country `1999` `2000` * &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 Afghanistan 745 2666 2 Brazil 37737 80488 3 China 212258 213766 table4b # A tibble: 3 x 3 country `1999` `2000` * &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 Afghanistan 19987071 20595360 2 Brazil 172006362 174504898 3 China 1272915272 1280428583 The solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below). Figure 6.3: A visual representation of what the pivot_longer operation looks like in practice. Once our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data. In R, wide-to-long conversions are performed using pivot_longer() tba &lt;- table4a %&gt;% pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;cases&quot;) tbb &lt;- table4b %&gt;% pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;population&quot;) # To get the tidy data, we join the two together (see Table joins below) left_join(tba, tbb, by = c(&quot;country&quot;, &quot;year&quot;)) %&gt;% # make year numeric b/c it&#39;s dumb not to mutate(year = as.numeric(year)) # A tibble: 6 x 4 country year cases population &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; 1 Afghanistan 1999 745 19987071 2 Afghanistan 2000 2666 20595360 3 Brazil 1999 37737 172006362 4 Brazil 2000 80488 174504898 5 China 1999 212258 1272915272 6 China 2000 213766 1280428583 The columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember). We identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways: select only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\") select variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation) Which option is easier depends how many things you’re pivoting (and how the columns are structured). If we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form. # Create ID columns table4a.x &lt;- table4a %&gt;% mutate(id = &quot;cases&quot;) table4b.x &lt;- table4b %&gt;% mutate(id = &quot;population&quot;) # Create one table table4 &lt;- bind_rows(table4a.x, table4b.x) table4_long &lt;- table4 %&gt;% # rearrange columns select(country, id, `1999`, `2000`) %&gt;% # Don&#39;t pivot country or id pivot_longer(-c(country:id), names_to = &quot;year&quot;, values_to = &quot;count&quot;) # Intermediate fully-long form table4_long # A tibble: 12 x 4 country id year count &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 Afghanistan cases 1999 745 2 Afghanistan cases 2000 2666 3 Brazil cases 1999 37737 4 Brazil cases 2000 80488 5 China cases 1999 212258 6 China cases 2000 213766 7 Afghanistan population 1999 19987071 8 Afghanistan population 2000 20595360 9 Brazil population 1999 172006362 10 Brazil population 2000 174504898 11 China population 1999 1272915272 12 China population 2000 1280428583 # make wider, with case and population columns table4_tidy &lt;- table4_long %&gt;% pivot_wider(names_from = id, values_from = count) table4_tidy # A tibble: 6 x 4 country year cases population &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; 1 Afghanistan 1999 745 19987071 2 Afghanistan 2000 2666 20595360 3 Brazil 1999 37737 172006362 4 Brazil 2000 80488 174504898 5 China 1999 212258 1272915272 6 China 2000 213766 1280428583 SAS will let you do a single transpose operation, where tidyr requires two separate pivots – this is because tidyr is trying to make the steps readable, even though it means writing more code. In SAS, we use PROC TRANSPOSE to perform wide-to-long pivot operations Friendly guide to PROC TRANSPOSE 6 DATA table4a; 7 input country $12. _1999 _2000; 8 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ; 13 14 PROC TRANSPOSE DATA=table4a out=table4atmp 15 (rename=(COL1 = cases)) /* rename output variable 15 ! (&#39;values_to&#39;) */ 16 NAME = year /* where column names go (&#39;names_to&#39;) */ 17 ; 18 BY country; /* The combination of BY variables defines a row */ 19 VAR _1999 _2000; /* Specify variables to pivot */ 20 RUN; WARNING: The variable COL1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set WORK.TABLE4A. NOTE: The data set WORK.TABLE4ATMP has 2 observations and 2 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 22 DATA table4b; 23 input country $12. _1999 _2000; 24 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 ; 29 30 PROC TRANSPOSE DATA=table4b out=table4btmp 31 (rename=(COL1 = population)) /* rename output variable 31 ! (&#39;values_to&#39;) */ 32 NAME = year /* where the column names go (&#39;names_to&#39;) */ 33 ; 34 BY country; /* The combination of BY variables defines a row */ 35 VAR _1999 _2000; /* Specify variables to pivot */ 36 RUN; WARNING: The variable COL1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set WORK.TABLE4B. NOTE: The data set WORK.TABLE4BTMP has 2 observations and 2 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 DATA table4clean; 39 /* merge the two tables together */ 40 /* (country and year selected automatically as merge vars) */ 41 MERGE table4atmp table4btmp; 42 /* Remove the first character of year, which is _ */ 43 /* Then convert to a numeric variable */ 44 year = input(SUBSTR(year, 2, 5), 4.); 45 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 44:8 NOTE: The data set WORK.TABLE4CLEAN has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 46 47 PROC PRINT DATA=table4clean; 48 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19. In the above code, we let SAS name our output variable COL1, and then renamed it at the end by modifying the output statement. Another option would be to create an ID variable when inputting each data set, and use the ID statement to indicate that variable. 6 DATA table4a; 7 input country $12. _1999 _2000; 8 id = &quot;cases&quot;; 9 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ; 14 15 PROC TRANSPOSE DATA=table4a out=table4atmp 16 NAME = year /* where column names go (&#39;names_to&#39;) */ 17 ; 18 BY country; /* The combination of BY variables defines a row */ 19 VAR _1999 _2000; /* Specify variables to pivot */ 20 ID id; /* This variable holds the output variable name 20 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 21 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4ATMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.TABLE4ATMP was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 23 DATA table4b; 24 input country $12. _1999 _2000; 25 id = &quot;population&quot;; 26 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4B was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 30 ; 31 32 PROC TRANSPOSE DATA=table4b out=table4btmp 33 NAME = year /* where the column names go (&#39;names_to&#39;) */ 34 ; 35 BY country; /* The combination of BY variables defines a row */ 36 VAR _1999 _2000; /* Specify variables to pivot */ 37 ID id; /* This variable holds the output variable name 37 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 38 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4BTMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.TABLE4BTMP was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 39 40 DATA table4clean; 41 /* merge the two tables together */ 42 /* (country and year selected automatically as merge vars) */ 43 MERGE table4atmp table4btmp; 44 /* Remove the first character of year, which is _ */ 45 /* Then convert to a numeric variable */ 46 year = input(SUBSTR(year, 2, 5), 4.); 47 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 46:8 NOTE: The data set WORK.TABLE4CLEAN has 0 observations and 2 variables. WARNING: Data set WORK.TABLE4CLEAN was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 48 49 PROC PRINT DATA=table4clean; 50 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24. This seems a bit odd, as we’re defining constant variables, but it works – and provides some insight into how the reverse operation (long to wide) might work (e.g. nonconstant variables). Imagine instead of transposing each dataset and then merging them, we just stack the two wide-format datasets on top of each other. Then we can do the same transpose operation, but we’ll end up with two columns: cases, and population. 6 DATA table4a; 7 input country $12. _1999 _2000; 8 id = &quot;cases&quot;; 9 datalines; NOTE: The data set WORK.TABLE4A has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4A was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 ; 14 15 DATA table4b; 16 input country $12. _1999 _2000; 17 id = &quot;population&quot;; 18 datalines; NOTE: The data set WORK.TABLE4B has 0 observations and 4 variables. WARNING: Data set WORK.TABLE4B was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 ; 23 24 DATA table4; 25 /* stack the two datasets on top of each other */ 26 SET table4b table4a; 27 /* sort by country */ 28 BY country; 29 RUN; NOTE: The data set WORK.TABLE4 has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 30 31 PROC TRANSPOSE DATA=table4 out=table4tmp NAME = year; 32 BY country; 33 VAR _1999 _2000; 34 ID id; /* This variable holds the output variable names 34 ! (&#39;values_to&#39;) */ ERROR: Variable ID not found. 35 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.TABLE4TMP may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 36 37 DATA table4tmp; 38 SET table4tmp; 39 /* Remove the first character of year, which is _ */ 40 /* Then convert to a numeric variable */ 41 year = input(SUBSTR(year, 2, 5), 4.); 42 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 41:21 NOTE: The data set WORK.TABLE4TMP has 0 observations and 1 variables. WARNING: Data set WORK.TABLE4TMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 43 44 PROC PRINT DATA=table4tmp; 45 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25. It’s not too complicated – and it definitely beats doing that operation by hand, even for short, simple tables. You can imagine how messy the cut/copy/paste job would be in Excel. It takes some getting used to, but once you get a feel for how to do these transpose operations, you’ll be able to handle messy data reproducibly - instead of describing how you did XYZ operations in Excel, you can provide a script that will take the original data as input and spit out clean, analysis-ready data as output. Because wide-to-long transformations end up combining values from several columns into a single column, you can run into issues with type conversions that happen implicitly. If you try to pivot_longer() using a character column mixed in with numeric columns, your “value” column will be converted to a character automatically. Now, let’s look at a “real data” example using HIV case data from the World Health Organization. (download page here). WHO HIV data set up url &lt;- &quot;https://apps.who.int/gho/athena/data/xmart.csv?target=GHO/HIV_0000000026,SDGHIV&amp;profile=crosstable&amp;filter=COUNTRY:*;REGION:*;AGEGROUP:-&amp;x-sideaxis=COUNTRY&amp;x-topaxis=GHO;YEAR;SEX&amp;x-collapse=true&quot; # create colnames in shorter form hiv &lt;- read_csv(url, na = &quot;No data&quot;, ) %&gt;% select(-2) # get rid of only column that has raw totals ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_character() ) ℹ Use `spec()` for the full column specifications. # work with the names to make them shorter and more readable # otherwise, they&#39;re too long for SAS newnames &lt;- names(hiv) %&gt;% str_remove(&quot;New HIV infections \\\\(per 1000 uninfected population\\\\); &quot;) %&gt;% str_replace_all(&quot;(\\\\d{4}); (Male|Female|Both)( sexes)?&quot;, &quot;Rate_\\\\1_\\\\2&quot;) hiv &lt;- set_names(hiv, newnames) %&gt;% # transliterate - get rid of non-ascii characters, replace w/ closest equiv mutate(Country = iconv(Country, to=&quot;ASCII//TRANSLIT&quot;)) write_csv(hiv, path = &quot;data/who_hiv.csv&quot;, na = &#39;.&#39;) # make it easy for SAS Warning: The `path` argument of `write_csv()` is deprecated as of readr 1.4.0. Please use the `file` argument instead. Since I’ve cheated a bit to make this easier to read in using SAS… hopefully that will be uneventful. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 filename fileloc &#39;data/who_hiv.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.hiv REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26. Original Data Structure (WHO HIV data) hiv %&gt;% # Only look at 1st 6 cols, because there are too many select(1:6) %&gt;% head() # A tibble: 6 x 6 Country `Number of new H… `Number of new H… `Number of new … `Number of new … &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghani… 1400 [&amp;lt;500–51… 1300 [&amp;lt;500–48… 1200 [&amp;lt;500–4… 1100 [&amp;lt;500–3… 2 Albania &amp;lt;100 [&amp;lt;100… &amp;lt;100 [&amp;lt;100… &amp;lt;200 [&amp;lt;10… &amp;lt;200 [&amp;lt;10… 3 Algeria 2000 [&amp;lt;500–36… 1900 [510–3300] 1900 [610–3000] 1800 [780–2800] 4 Angola 27 000 [20 000–3… 27 000 [21 000–3… 26 000 [20 000–… 26 000 [20 000–… 5 Argenti… 5700 [4100–8100] 6100 [4400–8200] 5900 [4400–8200] 6100 [4700–8300] 6 Armenia &amp;lt;200 [&amp;lt;200… &amp;lt;200 [&amp;lt;200… &amp;lt;200 [&amp;lt;20… &amp;lt;200 [&amp;lt;20… # … with 1 more variable: Number of new HIV infections; 2014; &lt;chr&gt; Here, the column names (except for the first column) contain information about both group (Male, Female, total) and year. If we want to plot values over time, we’re not going to have much fun. Thinking through the logical steps before writing the code can be helpful - even sketching out what you expect the data to (roughly) look like at each stage. Current data observations: Our column names contain the year and the group (Both, Male, Female) Our values contain estimates with a confidence interval - so est (LB, UB) Some intervals have &amp;lt;, which is HTML code for &lt;. We’ll need to get rid of those. Our final dataset should look like this: Country group year est lb ub narnia both 2020 0.3 0.2 0.4 (give or take column order, capitalization, and/or reality) From this, our steps are: Transpose the data - all columns except Country (our BY variable) Separate the (what was column names) variable into group and year variables convert year to numeric Separate the (what was column values) variable into EST, LB, and UB columns Remove &amp;lt; from the variable so that it’s readable as numeric Remove [, ] and - from the variable so that the values are separated by spaces Read each value into a separate column that’s numeric Rename the columns Clean up any extra variables hanging around. Now that we have a plan, lets execute that plan Wide-to-long transformation in R (WHO HIV data) hiv_tidier &lt;- hiv %&gt;% pivot_longer(-Country, names_to = &quot;key&quot;, values_to = &quot;rate&quot;) hiv_tidier # A tibble: 18,530 x 3 Country key rate &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghanistan Number of new HIV infections; 2018; 1400 [&amp;lt;500–5100] 2 Afghanistan Number of new HIV infections; 2017; 1300 [&amp;lt;500–4800] 3 Afghanistan Number of new HIV infections; 2016; 1200 [&amp;lt;500–4200] 4 Afghanistan Number of new HIV infections; 2015; 1100 [&amp;lt;500–3700] 5 Afghanistan Number of new HIV infections; 2014; 1000 [&amp;lt;500–3200] 6 Afghanistan Number of new HIV infections; 2013; 920 [&amp;lt;500–2700] 7 Afghanistan Number of new HIV infections; 2012; 840 [&amp;lt;500–2300] 8 Afghanistan Number of new HIV infections; 2011; 760 [&amp;lt;500–2000] 9 Afghanistan Number of new HIV infections; 2010; 700 [&amp;lt;500–1800] 10 Afghanistan Number of new HIV infections; 2009; 640 [&amp;lt;500–1600] # … with 18,520 more rows From this point, it’s pretty easy to use things we’ve used in the past (regular expressions, separate, extract) hiv_tidy &lt;- hiv_tidier %&gt;% # Split the key into Rate (don&#39;t keep), year, and group (F, M, Both) separate(key, into = c(NA, &quot;year&quot;, &quot;group&quot;), sep = &quot;_&quot;, convert = T) %&gt;% # Fix the HTML sign for less than - we could remove it as well mutate(rate = str_replace_all(rate, &quot;&amp;lt;&quot;, &quot;&lt;&quot;)) %&gt;% # Split the rate into estimate, lower bound, and upper bound extract(rate, into = c(&quot;est&quot;, &quot;lb&quot;, &quot;ub&quot;), regex = &quot;([\\\\d\\\\.]{1,}) .([&lt;\\\\d\\\\.]{1,}) - ([&lt;\\\\d\\\\.]{1,}).&quot;, remove = T) Warning: Expected 3 pieces. Missing pieces filled with `NA` in 3230 rows [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 110, ...]. hiv_tidy # A tibble: 18,530 x 6 Country year group est lb ub &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 7 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 8 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 9 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; # … with 18,520 more rows Wide-to-long transformation in SAS (WHO HIV data) I’ve thoroughly commented the code below to hopefully make the logical steps clear. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC TRANSPOSE DATA=classdat.hiv OUT = classdat.hivtidy 9 (rename=(col1=rate)) /* &quot;values_to&quot; in tidyr speak */ 10 NAME = key; /* &quot;names_to&quot; in tidyr speak */ 11 /* specify notsorted unless you know your data are sorted */ 12 BY Country NOTSORTED; 13 /* variables to transpose - just list start and end, with -- in 13 ! between */ 14 VAR Rate_2018_Both--Rate_1990_Female; 15 RUN; WARNING: The variable col1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set CLASSDAT.HIV. NOTE: The data set CLASSDAT.HIVTIDY has 87 observations and 2 variables. WARNING: Data set CLASSDAT.HIVTIDY was not replaced because of NOREPLACE option. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 17 title &#39;Intermediate result&#39;; 18 PROC PRINT DATA=classdat.hivtidy(obs=5); RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 20 /* Data step to clean up */ 21 DATA classdat.hivtidy; 22 SET classdat.hivtidy ; 23 /* Create group and year variables from key */ 24 group = scan(key,3,&quot;_&quot;); 25 year = input(scan(key, 2, &quot;_&quot;), 4.); 26 27 /* just get rid of the less than sign */ 28 rate = PRXCHANGE(&quot;s/&amp;lt;//&quot;, -1, rate); WARNING: Apparent symbolic reference LT not resolved. 29 rate = PRXCHANGE(&quot;s/[\\[\\]-]//&quot;, -1, rate); 30 31 /* Create 3 columns for the 3 values - est, lb, ub */ 32 length v1-v3 4.2; /* define format */ 33 array v(3) $; /* create a 3 column array to store values in */ 34 do i = 1 to dim(v); 35 v[i]=scan(rate, i, &#39; &#39;); /* get values from each row of rate 35 ! */ 36 end; 37 38 rename v1 = est v2 = lb v3 = ub; /* rename things to be pretty 38 ! */ 39 40 /* get rid of extra vars */ 41 drop _name_ i key rate; 42 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 24:16 25:21 28:36 29:39 35:15 NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 28:10 29:10 35:5 WARNING: Variable v1 cannot be renamed to est because est already exists. WARNING: Variable v2 cannot be renamed to lb because lb already exists. WARNING: Variable v3 cannot be renamed to ub because ub already exists. WARNING: The variable _name_ in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The data set CLASSDAT.HIVTIDY has 0 observations and 9 variables. WARNING: Data set CLASSDAT.HIVTIDY was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 43 44 title &#39;Final result&#39;; 45 PROC PRINT DATA=classdat.hivtidy(obs=5); RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 46 ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26. Try it out In the next section, we’ll be using the WHO surveillance of disease incidence data (link - 3.1, Excel link) It will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest :) library(readxl) library(purrr) # This uses the map() function as a replacement for for loops. # It&#39;s pretty sweet sheets &lt;- excel_sheets(&quot;data/incidence_series.xls&quot;) sheets &lt;- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name # This command says &quot;for each sheet, read in the excel file with that sheet name&quot; # map_df means paste them all together into a single data frame disease_incidence &lt;- map_df(sheets, ~read_xls(path =&quot;data/incidence_series.xls&quot;, sheet = .)) # Alternately, we could write a loop: disease_incidence2 &lt;- tibble() # Blank data frame for(i in 1:length(sheets)) { disease_incidence2 &lt;- bind_rows( disease_incidence2, read_xls(path = &quot;data/incidence_series.xls&quot;, sheet = sheets[i]) ) } # export for SAS (and R, if you want) write_csv(disease_incidence, path = &quot;data/who_disease_incidence.csv&quot;, na = &quot;.&quot;) Download the exported data here and import it into SAS and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that). Can you make a line plot of cases of measles in Bangladesh over time? R solution who_disease &lt;- read_csv(&quot;data/who_disease_incidence.csv&quot;, na = &quot;.&quot;) ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_double(), WHO_REGION = col_character(), ISO_code = col_character(), Cname = col_character(), Disease = col_character() ) ℹ Use `spec()` for the full column specifications. who_disease_long &lt;- who_disease %&gt;% pivot_longer(matches(&quot;\\\\d{4}&quot;), names_to = &quot;year&quot;, values_to = &quot;cases&quot;) %&gt;% rename(Country = Cname) %&gt;% mutate(Disease = str_replace(Disease, &quot;CRS&quot;, &quot;Congenital Rubella&quot;), year = as.numeric(year)) filter(who_disease_long, Country == &quot;Bangladesh&quot;, Disease == &quot;measles&quot;) %&gt;% ggplot(aes(x = year, y = cases)) + geom_line() SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 filename fileloc &#39;data/who_disease_incidence.csv&#39;; NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 9 PROC IMPORT datafile = fileloc out=classdat.who_disease REPLACE 10 DBMS = csv; /* comma delimited file */ 11 GUESSINGROWS=500; 12 GETNAMES = YES; 13 RUN; 14 15 /* Sort your data by the variables you want as the ID variables 15 ! */ 16 PROC SORT DATA=classdat.who_disease OUT=who_dis_tmp; 17 BY Disease Cname; 18 RUN; NOTE: The data set WORK.WHO_DIS_TMP has 0 observations and 0 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 19 20 PROC TRANSPOSE DATA = who_dis_tmp OUT = classdat.disease_long; 21 BY Disease Cname; /* Same variable order as used to sort */ ERROR: Variable DISEASE not found. ERROR: Variable CNAME not found. 22 VAR _2018--_1980; /* variables to transpose */ ERROR: Variable _2018 not found. 23 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_LONG may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set CLASSDAT.DISEASE_LONG was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 title &#39;Intermediate result 1&#39;; 26 PROC PRINT DATA=classdat.disease_long(obs=5); RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 27 28 /* Data step to clean up */ 29 DATA classdat.disease_long; 30 SET classdat.disease_long (rename=col1=cases); ERROR: Variable col1 is not on file CLASSDAT.DISEASE_LONG. ERROR: Invalid DROP, KEEP, or RENAME option on file CLASSDAT.DISEASE_LONG. 31 year = input(scan(_name_,1,&quot;_&quot;), 4.); 32 drop _name_; 33 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 31:19 NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_LONG may be incomplete. When this step was stopped there were 0 observations and 1 variables. WARNING: Data set CLASSDAT.DISEASE_LONG was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 34 35 title &#39;Final result&#39;; 36 PROC PRINT DATA=classdat.disease_long(obs=5); RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 39 CREATE TABLE bangladesh AS 40 SELECT * FROM classdat.disease_long WHERE (Cname = &quot;Bangladesh&quot;) 40 ! &amp; (Disease = &quot;measles&quot;); NOTE: Statement not executed due to NOEXEC option. 41 42 title &#39;Measles in Bangladesh&#39;; 43 ODS GRAPHICS ON; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 44 PROC SGPLOT DATA=bangladesh; ERROR: File WORK.BANGLADESH.DATA does not exist. 45 SERIES X = year Y = cases; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 46 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 47 ODS GRAPHICS OFF; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27. 6.3.2 Wider While it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again. Table 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows. Figure 6.4: A visual representation of what the pivot_wider operation looks like in practice. In R, long-to-wide conversion operations are performed using pivot_wider() table2 %&gt;% pivot_wider(names_from = type, values_from = count) # A tibble: 6 x 4 country year cases population &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 Afghanistan 1999 745 19987071 2 Afghanistan 2000 2666 20595360 3 Brazil 1999 37737 172006362 4 Brazil 2000 80488 174504898 5 China 1999 212258 1272915272 6 China 2000 213766 1280428583 In SAS, we use PROC TRANSPOSE again 6 DATA table2; 7 input country $12. year type$12. count 12.; 8 datalines; NOTE: The data set WORK.TABLE2 has 0 observations and 4 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 ; 22 23 PROC TRANSPOSE DATA=table2 out=table2tmp; 24 ID type; /* Equivalent to names_from */ 25 BY country year; /* The combination of BY variables defines a 25 ! row */ 26 VAR count; /* Equivalent to values_from */ 27 RUN; NOTE: There were 0 observations read from the data set WORK.TABLE2. NOTE: The data set WORK.TABLE2TMP has 1 observations and 3 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 28 29 PROC PRINT DATA=table2tmp; 30 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28. If you don’t sort your data properly before PROC TRANSPOSE in SAS, you may get a result that has an unexpected shape. SAS works rowwise (compared to R’s column-wise operations) so the row order actually matters in SAS (it generally doesn’t matter much in R). Returning to our WHO HIV example, we might want our data to look like this: Country year measurement Both Male Female Afghanistan 2018 est .02 .03 .01 2018 lb .01 .02 .01 2018 ub .04 .06 .03 As a reminder, the data currently looks like this: hiv_tidy # A tibble: 18,530 x 6 Country year group est lb ub &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 7 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 8 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 9 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 10 Afghanistan NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; # … with 18,520 more rows Think through the steps you’d need to take to get the data into this form. Sketch out any intermediate state your data will need to go through. WHO HIV Data: long-to-wide steps We need est, lb, and ub in long form first, so we need to use a wide-to-long (pivot_longer) transpose operation. names_to = “measure” values_to = “value” BY variables: Country, year, group pivot variables (VAR in SAS): est, lb, ub We need group to be 3 columns: Both, Male, and Female. So we need to use a long-to-wide (pivot_wider) transpose operation. By variables: Country, year, measure (so we’ll need to sort in SAS) ID variable: group VAR variable: value The intermediate form of the data will look like this: Country year group measure value Afghanistan 2018 Both est 0.02 Afghanistan 2018 Both lb 0.01 Afghanistan 2018 Both ub 0.04 Afghanistan 2018 Male est 0.03 WHO HIV Data: long-to-wide in R hiv_tidy %&gt;% pivot_longer(est:ub, names_to = &quot;measure&quot;, values_to = &quot;value&quot;) %&gt;% # Take the opportunity to transform everything to numeric at once... mutate(value = parse_number(value)) %&gt;% pivot_wider(names_from = group, values_from = value) Warning: Values are not uniquely identified; output will contain list-cols. * Use `values_fn = list` to suppress this warning. * Use `values_fn = length` to identify where the duplicates arise * Use `values_fn = {summary_fun}` to summarise duplicates # A tibble: 15,810 x 7 Country year measure `NA` Both Male Female &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 Afghanistan NA est &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; 2 Afghanistan NA lb &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; 3 Afghanistan NA ub &lt;dbl [19]&gt; &lt;NULL&gt; &lt;NULL&gt; &lt;NULL&gt; 4 Afghanistan 2019 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 5 Afghanistan 2019 lb &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 6 Afghanistan 2019 ub &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 7 Afghanistan 2018 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 8 Afghanistan 2018 lb &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 9 Afghanistan 2018 ub &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; 10 Afghanistan 2017 est &lt;NULL&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; &lt;dbl [1]&gt; # … with 15,800 more rows WHO HIV Data: long-to-wide in SAS In SAS, we can do the pivot operations in one step, but we have to sort everything first. 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SORT DATA = classdat.hivtidy; 9 BY Country year group; 10 RUN; NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 11 12 PROC TRANSPOSE DATA = classdat.hivtidy OUT = hivtmp name = 12 ! measure; 13 BY Country year; 14 VAR est lb ub; 15 ID group; 16 RUN; NOTE: There were 0 observations read from the data set CLASSDAT.HIVTIDY. NOTE: The data set WORK.HIVTMP has 3 observations and 3 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 PROC PRINT data=hivtmp(obs=5); 19 RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28. Try it out Use the long-format data you have from the previous Try It Out section (WHO Disease Incidence). Reshape this data into a “wide” format such that each disease is shown in a separate column. Before you start: - Which variable(s) will uniquely identify a row in your output data? - Which variable(s) will be used to create column names? Can you create a plot of polio cases over time for your 3 favorite countries? R solution who_disease_wide &lt;- who_disease_long %&gt;% pivot_wider(id_cols = c(Country, year), names_from = Disease, values_from = cases) who_disease_wide %&gt;% filter(Country %in% c(&quot;Guatemala&quot;, &quot;Central African Republic (the)&quot;, &quot;Pakistan&quot;)) %&gt;% select(Country, year, polio) %&gt;% ggplot(aes(x = year, y = polio, color = Country)) + geom_line() SAS solution 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 8 PROC SORT DATA = classdat.disease_long OUT = dis_long; 9 BY Cname year; /* Variables we want to define rows of data */ 10 RUN; NOTE: The data set WORK.DIS_LONG has 0 observations and 0 variables. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 11 12 PROC TRANSPOSE DATA = dis_long OUT = classdat.disease_wide; 13 ID Disease; /* Variable we want to name columns of data */ ERROR: Variable DISEASE not found. 14 VAR cases; /* Variable we want to be the values in each column 14 ! */ ERROR: Variable CASES not found. 15 BY Cname year; /* These define a single row */ ERROR: Variable CNAME not found. ERROR: Variable YEAR not found. 16 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set CLASSDAT.DISEASE_WIDE may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set CLASSDAT.DISEASE_WIDE was not replaced because this step was stopped. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 title &#39;Polio incidence&#39;; 19 PROC SGPLOT DATA=classdat.disease_wide 20 /* We can use a where clause in the DATA statement, if we want 20 ! */ 21 (WHERE=(Cname in (&quot;Mexico&quot;, &quot;Guatemala&quot;, &quot;Pakistan&quot;))); 22 /* Specify the colors to use for lines */ 23 styleattrs datacontrastcolors= (green orange purple); 24 /* Map variables to axes */ 25 SERIES X = year Y = polio / 26 /* Color lines by Country */ 27 GROUP = Cname 28 /* Give the color mapping a name so you can modify its legend 28 ! */ 29 name = &quot;a&quot; 30 /* Make lines thicker so they are visible */ 31 lineattrs=(thickness = 3); 32 /* Change the legend title and what it is showing (line color) 32 ! */ 33 KEYLEGEND &quot;a&quot; / title = &quot;Country&quot; type = linecolor; 34 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,29. Congratulations! You now know how to reshape your data into all sorts of different formats. Use this knowledge wisely. 6.4 Relational Data and Joining Tables We now know how to work extensively on one table at a time, but data doesn’t always come organized in one table at a time. Instead, some data may be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between individuals that have to be documented. Example: Primary School Organization Each individual has certain characteristics: - full_name - gender - birth date - ID number Each student has specific characteristics: - ID number - parent name - parent phone number - medical information - Class ID Teachers may also have additional information: - ID number - Class ID - employment start date - education level - compensation level There are also fields like grades, which occur for each student in each class, but multiple times a year. - ID number - Student ID - Class ID - year - term number - subject - grade - comment And for teachers, there are employment records on a yearly basis - ID number - Employee ID - year - rating - comment But each class also has characteristics that describe the whole class as a unit: - location ID - class ID - meeting time - grade level Each location might also have some logistical information attached: - location ID - room number - building - number of seats - AV equipment We could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment It would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well. But, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps: get the Class ID get any teachers that are assigned that Class ID - specifically, get their ID number get any students that are assigned that Class ID - specifically, get their ID number append the results from teachers and students so that there is a list of all individuals in the class look through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days. Table joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information. There are 3 main types of table joins: Filtering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change) Ex: finding all teachers or students who have class ClassID Set operations, which treat observations as set elements (e.g. union, intersection, etc.) Ex: taking the union of all student and teacher IDs to get a list of individual IDs Mutating joins, which add columns from one table to matching rows in another table Ex: adding birthday to the table of all individuals in a class keys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table. We’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins. Feel free to read through the other types of joins here 6.4.1 Demonstration dataset setup We’ll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you’ll recall, I had to use a small sample the last time we played with this data…). So before we can work with this data, we have to load the tables into SAS, which means saving them out from R. Instructions We’ll use a function in the dbplyr package to do that.24 if (!&quot;nycflights13&quot; %in% installed.packages()) install.packages(&quot;nycflights13&quot;) if (!&quot;dbplyr&quot; %in% installed.packages()) install.packages(&quot;dbplyr&quot;) library(nycflights13) library(dbplyr) Attaching package: &#39;dbplyr&#39; The following objects are masked from &#39;package:dplyr&#39;: ident, sql nycflights13_sqlite(path = &quot;data/&quot;) Caching nycflights db at data//nycflights13.sqlite &lt;SQLiteConnection&gt; Path: /home/susan/Projects/Class/unl-stat850/stat850-textbook/data/nycflights13.sqlite Extensions: TRUE Then, you’ll have to figure out where on your system database locations (DSNs) are stored. On Unix systems, it’s /etc/odbc.ini (for system-wide access) and ~/.odbc.ini. On windows, you’ll need to use the ODBC Data Source Administrator to set this up. [nycflight] Description = NYC flights database Driver = SQLite3 Database = data/nycflights13.sqlite Try it out Sketch a diagram of which fields in each table match fields in other tables. You can find the solution here (scroll down a bit). 6.4.2 Mutating joins A mutating join combines variables in two tables. There are excellent visual representations of the types of mutating joins here. Every join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side. Joins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept. Left Side Right Side Join Type Rows Cols Rows Cols inner matching all matching all left all all matching all right matching all all all outer all all all all Mutating joins in R t1 &lt;- tibble(x = c(&quot;A&quot;, &quot;B&quot;, &quot;D&quot;), y = c(1, 2, 3)) t2 &lt;- tibble(x = c(&quot;B&quot;, &quot;C&quot;, &quot;D&quot;), z = c(2, 4, 5)) An inner join keeps only rows that exist on both sides, but keeps all columns. inner_join(t1, t2) Joining, by = &quot;x&quot; # A tibble: 2 x 3 x y z &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 B 2 2 2 D 3 5 A left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs. left_join(t1, t2) Joining, by = &quot;x&quot; # A tibble: 3 x 3 x y z &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 A 1 NA 2 B 2 2 3 D 3 5 left_join(t2, t1) Joining, by = &quot;x&quot; # A tibble: 3 x 3 x z y &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 B 2 2 2 C 4 NA 3 D 5 3 There is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there right_join(t1, t2) Joining, by = &quot;x&quot; # A tibble: 3 x 3 x y z &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 B 2 2 2 D 3 5 3 C NA 4 right_join(t2, t1) Joining, by = &quot;x&quot; # A tibble: 3 x 3 x z y &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 B 2 2 2 D 5 3 3 A NA 1 An outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join. full_join(t1, t2) Joining, by = &quot;x&quot; # A tibble: 4 x 3 x y z &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 A 1 NA 2 B 2 2 3 D 3 5 4 C NA 4 In SAS, you can do joins using a data step or PROC SQL. To do a join with data steps, you have to have your data sorted by columns that overlap. PROC SQL has no such requirement. This guide shows the syntax for PROC SQL and DATA step joins side-by-side. PROC SQL Mutating joins 6 data t1; 7 input x $ y; 8 datalines; NOTE: The data set WORK.T1 has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ; 13 14 data t2; 15 input x $ z; 16 datalines; NOTE: The data set WORK.T2 has 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ; 21 22 title &#39;Inner join&#39;; 23 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 24 SELECT * FROM t1 as p1 25 INNER JOIN t2 as p2 26 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 27 28 title &#39;Left join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 30 SELECT * FROM t1 as p1 31 LEFT JOIN t2 as p2 32 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 33 34 title &#39;Right join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 35 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 36 SELECT * FROM t1 as p1 37 RIGHT JOIN t2 as p2 38 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 39 40 title &#39;Full Outer join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 41 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 42 SELECT * FROM t1 as p1 43 FULL OUTER JOIN t2 as p2 44 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 45 46 /* Use coalesce to prevent column duplication */ 47 48 title &#39;Inner join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 49 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 50 SELECT COALESCE(p1.x, p2.x) AS x, y, z 51 FROM t1 as p1 INNER JOIN t2 as p2 52 on p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 53 54 title &#39;Left join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 55 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 56 SELECT COALESCE(p1.x, p2.x) AS x, y, z 57 FROM t1 as p1 LEFT JOIN t2 as p2 58 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 59 60 title &#39;Right join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 61 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 62 SELECT COALESCE(p1.x, p2.x) AS x, y, z 63 FROM t1 as p1 RIGHT JOIN t2 as p2 64 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 65 66 title &#39;Full Outer join&#39;; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 67 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 68 SELECT COALESCE(p1.x, p2.x) AS x, y, z 69 FROM t1 as p1 FULL OUTER JOIN t2 as p2 70 ON p1.x = p2.x; NOTE: Statement not executed due to NOEXEC option. 71 72 title; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31. Data Step Mutating joins NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 6 data t1; 7 input x $ y; 8 datalines; NOTE: The data set WORK.T1 has 0 observations and 2 variables. WARNING: Data set WORK.T1 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 ; 13 14 data t2; 15 input x $ z; 16 datalines; NOTE: The data set WORK.T2 has 0 observations and 2 variables. WARNING: Data set WORK.T2 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 20 ; 21 22 DATA inner; 23 MERGE t1 (IN = p1) t2 (IN = p2); 24 BY x; 25 IF p1 AND p2; 26 RUN; NOTE: The data set WORK.INNER has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 27 28 PROC PRINT DATA=inner;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 29 30 DATA left; 31 MERGE t1 (IN = p1) t2 (IN = p2); 32 BY x; 33 IF p1; 34 RUN; NOTE: The data set WORK.LEFT has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 35 36 PROC PRINT DATA=left;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 37 38 DATA right; 39 MERGE t1 (IN = p1) t2 (IN = p2); 40 BY x; 41 IF p2; 42 RUN; NOTE: The data set WORK.RIGHT has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 43 44 PROC PRINT DATA=right;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 45 46 DATA outer; 47 MERGE t1 (IN = p1) t2 (IN = p2); 48 BY x; 49 RUN; NOTE: The data set WORK.OUTER has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 50 51 PROC PRINT DATA=outer;RUN; NOTE: PROCEDURE PRINT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31. As before, these functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let’s determine whether there’s a relationship between the age of a plane and its delays. In R library(nycflights13) plane_age &lt;- planes %&gt;% mutate(age = 2013 - year) %&gt;% # This gets us away from having to deal with 2 different year columns select(tailnum, age, manufacturer) delays_by_plane &lt;- flights %&gt;% select(dep_delay, arr_delay, carrier, flight, tailnum) # We only need to keep delays that have a plane age, so use inner join res &lt;- inner_join(delays_by_plane, plane_age, by = &quot;tailnum&quot;) ggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + geom_boxplot() + ylab(&quot;Departure Delay (min)&quot;) + xlab(&quot;Plane age&quot;) + coord_cartesian(ylim = c(-20, 50)) Warning: Removed 5306 rows containing missing values (stat_boxplot). Warning: Removed 4068 rows containing non-finite values (stat_boxplot). ggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + geom_boxplot() + ylab(&quot;Arrival Delay (min)&quot;) + xlab(&quot;Plane age&quot;) + coord_cartesian(ylim = c(-30, 60)) Warning: Removed 5306 rows containing missing values (stat_boxplot). Warning: Removed 5011 rows containing non-finite values (stat_boxplot). It doesn’t look like there’s much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren’t enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old). In SAS 6 libname nycair odbc complete = 6 ! XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX; ERROR: CLI error trying to establish connection: [unixODBC][Driver Manager]Can&#39;t open lib &#39;libsqlite3odbc.so&#39; : file not found ERROR: Error in the LIBNAME statement. 7 8 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 9 CREATE TABLE tmp1 AS 10 SELECT 2013 - year AS age, tailnum FROM nycair.planes 11 WHERE ^missing(year); NOTE: Statement not executed due to NOEXEC option. 12 13 CREATE TABLE tmp2 AS 14 SELECT tailnum, dep_delay, arr_delay, air_time, distance, hour, 14 ! carrier, origin 15 FROM nycair.flights; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34. Now that the prep work is done, we can get on with answering the question. 6 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE agedelay AS 9 SELECT COALESCE(p1.tailnum, p2.tailnum) AS tailnum, 10 age, dep_delay, arr_delay, air_time, distance, hour, 10 ! carrier, origin 11 FROM tmp1 as p1 INNER JOIN tmp2 as p2 12 ON p1.tailnum = p2.tailnum 13 WHERE age &lt; 25 /* only work with areas where there&#39;s enough data 13 ! */ 14 ORDER BY age; NOTE: Statement not executed due to NOEXEC option. 15 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 16 PROC MEANS DATA = agedelay NOPRINT; ERROR: File WORK.AGEDELAY.DATA does not exist. 17 BY age; ERROR: No data set open to look up variables. 18 VAR dep_delay arr_delay; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 19 OUTPUT OUT = agemeans; 20 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.AGEMEANS may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 21 22 PROC TRANSPOSE DATA = agemeans(where = (_STAT_=&quot;MEAN&quot;)) 23 OUT = agemeanst(drop= _Label_) ERROR: Variable _STAT_ is not on file WORK.AGEMEANS. 24 name=VAR; 25 BY age _FREQ_; /* Just to keep _FREQ_ around */ ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 26 VAR dep_delay arr_delay; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 27 ID _STAT_; ERROR: No data set open to look up variables. 28 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.AGEMEANST may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34. In theory, we could use some sort of linear model, but lets start with a simple plot of age * average delay and see where that takes us. 6 PROC SGPANEL DATA = agemeanst; 7 PANELBY VAR; ERROR: Variable VAR not found. 8 SCATTER X = age Y = MEAN; ERROR: Variable AGE not found. ERROR: Variable MEAN not found. 9 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPANEL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35. I started wondering about that pattern - to me, it doesn’t look like there’s any particular trend so much as there’s just low - high - low clusters. So I decided to plot frequency as well, and, lo and behold, the frequency count is similarly distributed. So my current working hypothesis is that there are a lot more observations in the middle group of planes that are 5-15 years old. 6 7 PROC SGPLOT DATA = agemeanst; 8 SCATTER X = age Y = _FREQ_; ERROR: Variable AGE not found. ERROR: Variable _FREQ_ not found. 9 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35. It’s also entirely possible that planes between 5-15 years of age have more mechanical issues, but I suspect most delays are due to airport, weather, etc. rather than mechanical malfunctions. 6.5 Example: Gas Prices Data The US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table (or you can look at the screenshot below, as I don’t really trust that the site design will stay the same…) Gas prices at US EIA site The data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable. But looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph. 6.5.1 Option 1: Manual formatting in Excel An excel spreadsheet of the data as downloaded in Sept 2020 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format? What steps are involved? Copy the year-month column, creating one vertical copy for every set of columns Move each block of two columns down to the corresponding vertical copy Delete empty rows Format dates Delete empty columns Here is a video of me doing most of these steps. I skipped out on the data cleaning stage because Excel is miserable for working with dates. 6.5.2 Option 2: R This section shows you how to clean the data up without any sort of database merges, but with 2 pivot operations. The SAS section shows how to clean the data up with database merges and a single pivot operation. You can, of course, use either approach in either language. If you want to try this for yourself first, there is a skeleton file here that you can fill in. Use the intermediate datasets shown to guide you through the steps, then click on the expandable sections to see the code that was used. Reading in the data from the web library(rvest) # scrape data from the web library(xml2) # parse xml data url &lt;- &quot;https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&amp;s=emm_epm0u_pte_nus_dpg&amp;f=w&quot; htmldoc &lt;- read_html(url) gas_prices_raw &lt;- html_table(htmldoc, fill = T, trim = T) [[5]] head(gas_prices_raw) # A tibble: 6 x 13 `Year-Month` `Week 1` `Week 1` `Week 2` `Week 2` `Week 3` `Week 3` `Week 4` &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 &quot;Year-Month&quot; &quot;End Dat… &quot;Value&quot; &quot;End Dat… &quot;Value&quot; &quot;End Dat… &quot;Value&quot; &quot;End Da… 2 &quot;1994-Nov&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;11/28&quot; 3 &quot;1994-Dec&quot; &quot;12/05&quot; &quot;1.143&quot; &quot;12/12&quot; &quot;1.118&quot; &quot;12/19&quot; &quot;1.099&quot; &quot;12/26&quot; 4 &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; 5 &quot;1995-Jan&quot; &quot;01/02&quot; &quot;1.104&quot; &quot;01/09&quot; &quot;1.111&quot; &quot;01/16&quot; &quot;1.102&quot; &quot;01/23&quot; 6 &quot;1995-Feb&quot; &quot;02/06&quot; &quot;1.103&quot; &quot;02/13&quot; &quot;1.099&quot; &quot;02/20&quot; &quot;1.093&quot; &quot;02/27&quot; # … with 5 more variables: Week 4 &lt;chr&gt;, Week 5 &lt;chr&gt;, Week 5.1 &lt;chr&gt;, &lt;lgl&gt;, # .1 &lt;lgl&gt; Initial data cleaning - fix up column names, get rid of empty rows library(tidyverse) ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ✓ forcats 0.5.1 ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── x lubridate::as.difftime() masks base::as.difftime() x gdata::combine() masks dplyr::combine() x lubridate::date() masks base::date() x tidyr::extract() masks magrittr::extract() x dplyr::filter() masks stats::filter() x gdata::first() masks dplyr::first() x readr::guess_encoding() masks rvest::guess_encoding() x dbplyr::ident() masks dplyr::ident() x lubridate::intersect() masks base::intersect() x gdata::keep() masks purrr::keep() x dplyr::lag() masks stats::lag() x gdata::last() masks dplyr::last() x purrr::set_names() masks magrittr::set_names() x lubridate::setdiff() masks base::setdiff() x dbplyr::sql() masks dplyr::sql() x lubridate::union() masks base::union() library(magrittr) # pipe friendly operations # Function to clean up column names # Written as an extra function because it makes the code a lot cleaner fix_gas_names &lt;- function(x) { # Add extra header row information paste(x, c(&quot;&quot;, rep(c(&quot;Date&quot;, &quot;Value&quot;), times = 5))) %&gt;% # trim leading/trailing spaces str_trim() %&gt;% # replace characters in names that aren&#39;t ok for variables in R make.names() } # Clean up the table a bit gas_prices_raw &lt;- gas_prices_raw %&gt;% set_names(fix_gas_names(names(.))) %&gt;% # remove first row that is really an extra header row filter(Year.Month != &quot;Year-Month&quot;) %&gt;% # get rid of empty rows filter(Year.Month != &quot;&quot;) head(gas_prices_raw) # A tibble: 6 x 13 Year.Month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1994-Nov &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; 2 1994-Dec &quot;12/05&quot; &quot;1.143&quot; &quot;12/12&quot; &quot;1.118&quot; &quot;12/19&quot; 3 1995-Jan &quot;01/02&quot; &quot;1.104&quot; &quot;01/09&quot; &quot;1.111&quot; &quot;01/16&quot; 4 1995-Feb &quot;02/06&quot; &quot;1.103&quot; &quot;02/13&quot; &quot;1.099&quot; &quot;02/20&quot; 5 1995-Mar &quot;03/06&quot; &quot;1.103&quot; &quot;03/13&quot; &quot;1.096&quot; &quot;03/20&quot; 6 1995-Apr &quot;04/03&quot; &quot;1.116&quot; &quot;04/10&quot; &quot;1.134&quot; &quot;04/17&quot; # … with 7 more variables: Week.3.Value &lt;chr&gt;, Week.4.Date &lt;chr&gt;, # Week.4.Value &lt;chr&gt;, Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;, X &lt;lgl&gt;, # Date &lt;lgl&gt; Separate year and month into different columns gas_prices_raw &lt;- gas_prices_raw %&gt;% separate(Year.Month, into = c(&quot;year&quot;, &quot;month&quot;), sep = &quot;-&quot;) Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [319]. Warning: Expected 2 pieces. Missing pieces filled with `NA` in 2 rows [320, 321]. head(gas_prices_raw) # A tibble: 6 x 14 year month Week.1.Date Week.1.Value Week.2.Date Week.2.Value Week.3.Date &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1994 Nov &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; 2 1994 Dec &quot;12/05&quot; &quot;1.143&quot; &quot;12/12&quot; &quot;1.118&quot; &quot;12/19&quot; 3 1995 Jan &quot;01/02&quot; &quot;1.104&quot; &quot;01/09&quot; &quot;1.111&quot; &quot;01/16&quot; 4 1995 Feb &quot;02/06&quot; &quot;1.103&quot; &quot;02/13&quot; &quot;1.099&quot; &quot;02/20&quot; 5 1995 Mar &quot;03/06&quot; &quot;1.103&quot; &quot;03/13&quot; &quot;1.096&quot; &quot;03/20&quot; 6 1995 Apr &quot;04/03&quot; &quot;1.116&quot; &quot;04/10&quot; &quot;1.134&quot; &quot;04/17&quot; # … with 7 more variables: Week.3.Value &lt;chr&gt;, Week.4.Date &lt;chr&gt;, # Week.4.Value &lt;chr&gt;, Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;, X &lt;lgl&gt;, # Date &lt;lgl&gt; Move from wide to long format (part 1: extra-long format) gas_prices_long &lt;- pivot_longer(gas_prices_raw, -c(year, month), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) head(gas_prices_long) # A tibble: 6 x 4 year month variable value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1994 Nov Week.1.Date &quot;&quot; 2 1994 Nov Week.1.Value &quot;&quot; 3 1994 Nov Week.2.Date &quot;&quot; 4 1994 Nov Week.2.Value &quot;&quot; 5 1994 Nov Week.3.Date &quot;&quot; 6 1994 Nov Week.3.Value &quot;&quot; Move from wide to long format (part 2: data cleaning and pivot long-to-wide) We need to get our variables into two columns: one for what the value contains, and one indicating which week the value is from. gas_prices_long &lt;- gas_prices_long %&gt;% # First, take &quot;Week.&quot; off of the front mutate(variable = str_remove(variable, &quot;Week\\\\.&quot;)) %&gt;% # Then separate the two values separate(variable, into = c(&quot;week&quot;, &quot;variable&quot;), sep = &quot;\\\\.&quot;) Warning: Expected 2 pieces. Missing pieces filled with `NA` in 642 rows [11, 12, 23, 24, 35, 36, 47, 48, 59, 60, 71, 72, 83, 84, 95, 96, 107, 108, 119, 120, ...]. head(gas_prices_long) # A tibble: 6 x 5 year month week variable value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1994 Nov 1 Date &quot;&quot; 2 1994 Nov 1 Value &quot;&quot; 3 1994 Nov 2 Date &quot;&quot; 4 1994 Nov 2 Value &quot;&quot; 5 1994 Nov 3 Date &quot;&quot; 6 1994 Nov 3 Value &quot;&quot; Now we’re ready to move back into wide-er form # gas_prices &lt;- gas_prices_long %&gt;% # filter out empty values filter(value != &quot;&quot;) %&gt;% pivot_wider( names_from = variable, values_from = value ) head(gas_prices) # A tibble: 6 x 5 year month week Date Value &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 1994 Nov 4 11/28 1.175 2 1994 Dec 1 12/05 1.143 3 1994 Dec 2 12/12 1.118 4 1994 Dec 3 12/19 1.099 5 1994 Dec 4 12/26 1.088 6 1995 Jan 1 01/02 1.104 Clean up dates and format variables properly We can read the date in as MDY format if we just add the year to the end of the month/day column. library(lubridate) # dates and times gas_prices &lt;- gas_prices %&gt;% mutate(Date = paste(Date, year, sep = &quot;/&quot;)) %&gt;% mutate(Date = mdy(Date)) # And now we can get rid of redundant columns gas_prices &lt;- gas_prices %&gt;% select(Date, Value) # Finally, our value variable is a character variable, so lets fix that quick gas_prices &lt;- gas_prices %&gt;% mutate(Value = as.numeric(Value)) head(gas_prices) # A tibble: 6 x 2 Date Value &lt;date&gt; &lt;dbl&gt; 1 1994-11-28 1.18 2 1994-12-05 1.14 3 1994-12-12 1.12 4 1994-12-19 1.10 5 1994-12-26 1.09 6 1995-01-02 1.10 # Lets look at our data: ggplot(gas_prices, aes(x = Date, y = Value)) + geom_line() The full code file for this analysis is here. 6.5.3 Option 3: SAS If you want to try this for yourself first, there is a skeleton file here that you can fill in. Use the intermediate datasets shown to guide you through the steps, then click on the expandable sections to see the code that was used. I spent quite a while trying to get collectcode = T to run so that I could put the SAS code in in smaller bites, but couldn’t get it to work. Please forgive the screenshots, but ugh, I’m tired of fooling with SAS at the moment. Read in the data 6 /*********************************************************/ 7 /* Step 1: Read in the data and drop missing rows/cols */ 8 /*********************************************************/ 9 options missing=&#39; &#39;; 10 data gas_raw; 11 infile &#39;data/gas_prices_raw.csv&#39; firstobs=3 delimiter = &#39;,&#39; 11 ! MISSOVER DSD; 12 informat ym $10. ; 13 informat date1 $8. ; informat value1 4.3 ; 14 informat date2 $8. ; informat value2 4.3 ; 15 informat date3 $8. ; informat value3 4.3 ; 16 informat date4 $8. ; informat value4 4.3 ; 17 informat date5 $8. ; informat value5 4.3 ; 18 informat blank $2. ; informat blank $2. ; 19 format ym $10. ; 20 format date1 $8. ; format value1 4.3 ; 21 format date2 $8. ; format value2 4.3 ; 22 format date3 $8. ; format value3 4.3 ; 23 format date4 $8. ; format value4 4.3 ; 24 format date5 $8. ; format value5 4.3 ; 25 format blank $2. ; format blank $2. ; 26 input ym $ date1 $ value1 date2 $ value2 date3 $ value3 date4 $ 26 ! value4 date5 $ value5 blank $ blank $; 27 /* drop extra columns */ 28 drop blank; 29 30 /* delete any rows where ym is not there */ 31 if missing(ym) then delete; 32 run; NOTE: The data set WORK.GAS_RAW has 0 observations and 11 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 33 ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35. Dates are formated as 8-character strings, values as numbers with 3 decimal places. The options missing piece allows us to test for whether information is missing or not. SAS output - gas_raw Split up year and month, format month properly 6 7 proc format; 8 value $ mon &#39;Jan&#39; = 1 &#39;Feb&#39; = 2 &#39;Mar&#39; = 3 &#39;Apr&#39; = 4 &#39;May&#39; = 5 _ 22 76 8 ! &#39;Jun&#39; = 6 &#39;Jul&#39; = 7 &#39;Aug&#39; = 8 &#39;Sep&#39; = 9 &#39;Oct&#39; = 10 &#39;Nov&#39; = 11 8 ! &#39;Dec&#39; = 12; ERROR 22-322: Syntax error, expecting one of the following: a quoted string, a format name. ERROR 76-322: Syntax error, statement will be ignored. 9 run; WARNING: RUN statement ignored due to previous errors. Submit QUIT; to terminate the procedure. NOTE: PROCEDURE FORMAT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 10 11 data gas_raw; 12 set gas_raw; 13 length var1-var2 $4.; 14 array var(2) $; NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 15 do i = 1 to dim(var); 16 var[i]=scan(ym,i,&#39;-&#39;); 17 end; 18 rename var1 = year var2 = month; 19 drop i ym; 20 if date1 = &quot;NA&quot; then delete; /* get rid of NA stuff */ 21 run; NOTE: The data set WORK.GAS_RAW has 0 observations and 12 variables. WARNING: Data set WORK.GAS_RAW was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 22 23 /* format month as a number */ 24 data gas_raw; 25 set gas_raw; 26 format month $ mon.; ____ 48 ERROR 48-59: The format $MON was not found or could not be loaded. 27 run; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.GAS_RAW may be incomplete. When this step was stopped there were 0 observations and 12 variables. WARNING: Data set WORK.GAS_RAW was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35. SAS output - gas_raw Get long data with just the dates 6 7 PROC TRANSPOSE DATA = gas_raw OUT = split_long1 8 (rename=(col1=date)) NAME = week; 9 BY year month NOTSORTED; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. 10 VAR date1 date2 date3 date4 date5; 11 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG1 may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 data split_long1; 14 set split_long1; 15 IF _N_ = 1 THEN DO; 16 REGEXname = PRXPARSE(&quot;s/date//&quot;); 17 END; 18 RETAIN REGEXname; 19 20 CALL PRXCHANGE(REGEXname, -1, week, week); _________ 716 WARNING 716-185: Argument #4 is a numeric variable, while a character variable must be passed to the PRXCHANGE subroutine call in order for the variable to be updated. 21 DROP REGEXname; 22 IF missing(date) THEN delete; 23 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 20:31 20:37 NOTE: The data set WORK.SPLIT_LONG1 has 0 observations and 2 variables. WARNING: Data set WORK.SPLIT_LONG1 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC SORT data = split_long1 out = split_long1; 26 BY year month week; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. ERROR: Variable WEEK not found. 27 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG1 may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.SPLIT_LONG1 was not replaced because this step was stopped. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36. SAS output - split_long1 Get long data with just the prices 6 7 PROC TRANSPOSE DATA=gas_split OUT =split_long2 ERROR: File WORK.GAS_SPLIT.DATA does not exist. 8 (rename=(col1=price)) NAME = week; 9 BY year month NOTSORTED; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 10 VAR value1 value2 value3 value4 value5; ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. ERROR: No data set open to look up variables. 11 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG2 may be incomplete. When this step was stopped there were 0 observations and 0 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 13 data split_long2; 14 set split_long2; 15 IF _N_ = 1 THEN DO; 16 REGEXname = PRXPARSE(&quot;s/value//&quot;); 17 END; 18 RETAIN REGEXname; 19 20 CALL PRXCHANGE(REGEXname, -1, week, week); _________ 716 WARNING 716-185: Argument #4 is a numeric variable, while a character variable must be passed to the PRXCHANGE subroutine call in order for the variable to be updated. 21 DROP REGEXname; 22 IF missing(price) THEN delete; 23 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 20:31 20:37 NOTE: The data set WORK.SPLIT_LONG2 has 0 observations and 2 variables. WARNING: Data set WORK.SPLIT_LONG2 was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC SORT data = split_long2 out = split_long2; 26 BY year month week; ERROR: Variable YEAR not found. ERROR: Variable MONTH not found. ERROR: Variable WEEK not found. 27 RUN; NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT_LONG2 may be incomplete. When this step was stopped there were 0 observations and 0 variables. WARNING: Data set WORK.SPLIT_LONG2 was not replaced because this step was stopped. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37. SAS output - split_long2 Merge the two long datasets together 6 7 PROC SQL; NOTE: PROC SQL set option NOEXEC and will continue to check the syntax of statements. 8 CREATE TABLE gas_prices AS 9 SELECT COALESCE(p1.year, p2.year) AS year, 10 COALESCE(p1.month, p2.month) AS month, 11 COALESCE(p1.week, p2.week) AS week, 12 date, price 13 FROM split_long1 as p1 14 RIGHT JOIN split_long2 as p2 15 ON p1.year = p2.year AND p1.month = p2.month AND p1.week = 15 ! p2.week; NOTE: Statement not executed due to NOEXEC option. NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38. SAS output - gas prices Format dates 6 NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SQL used (Total process time): real time 0.01 seconds cpu time 0.01 seconds 7 DATA split2; 8 SET gas_prices; ERROR: File WORK.GAS_PRICES.DATA does not exist. 9 length var1-var2 3; 10 array var(2); NOTE: The array var has the same name as a SAS-supplied or user-defined function. Parentheses following this name are treated as array references and not function references. 11 do i = 1 to dim(var); 12 var[i]=scan(date,i,&#39;/&#39;); 13 end; 14 RENAME var1 = monthnum var2 = day; 15 DROP i date; /* month is also in numeric form */ 16 RUN; NOTE: Numeric values have been converted to character values at the places given by: (Line):(Column). 12:19 NOTE: Character values have been converted to numeric values at the places given by: (Line):(Column). 12:7 NOTE: The SAS System stopped processing this step because of errors. WARNING: The data set WORK.SPLIT2 may be incomplete. When this step was stopped there were 0 observations and 2 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 18 DATA gas_prices; 19 SET split2; 20 date = MDY(monthnum, day, year); 21 FORMAT date yymmdd10.; 22 KEEP date price; 23 RUN; WARNING: The variable price in the DROP, KEEP, or RENAME list has never been referenced. NOTE: The data set WORK.GAS_PRICES has 0 observations and 1 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 24 25 PROC SORT DATA=gas_prices OUT = gas_prices; 26 BY date; 27 RUN; NOTE: The data set WORK.GAS_PRICES has 0 observations and 0 variables. WARNING: Data set WORK.GAS_PRICES was not replaced because new file is incomplete. NOTE: PROCEDURE SORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38. SAS output - gas prices, final version 6 7 PROC SGPLOT DATA = gas_prices; 8 SERIES X = date Y = price; ERROR: Variable PRICE not found. 9 TITLE &#39;Gas Prices&#39;; 10 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. You can see the full script here. References String manipulation R4DS chapter - strings SAS and Perl regular expressions PCRE tester R regex tester - has a short timeout period and will disconnect you if you’re idle too long. But you can also clone the repo here and run it locally. SAS scan statement Tidy data tutorials/references Tidy Data - Data Skills for Reproducible Science Five Ways to Flip-Flop Your Data PROC TRANSPOSE reference tidyr reference Relational Data &amp; Joins: R4DS chapter - Relational data Merge statement in SAS 5 little known, but highly valuable and widely useful PROC SQL Programming Techniques Other references SAS rename statement SAS graph customization SAS SGPLOT procedure Data Visualization with ggplot R graph gallery Videos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of tidyr/dplyr as well. Way back in Module 2, I briefly mentioned list-columns in tibbles. At the time, you didn’t have enough R knowledge to use that information, but now you do!! You can see a couple of examples here (but they assume that you know things that you’ll only learn in a few modules). Incidentally, dbplyr is basically dplyr for databases, and is worth checking out, even if we aren’t covering it in this class.↩︎ "],["debugging.html", "Module 7 Principles of Debugging Debugging: Module Objectives 7.1 Step 1: Check your spelling. 7.2 Defensive Programming 7.3 General Debugging Strategies 7.4 Debugging Tools in R 7.5 Debugging Tools in SAS 7.6 Minimal Working Examples References", " Module 7 Principles of Debugging Debugging: Module Objectives Break down a complex procedure into simpler steps, mapping each step to a separate function which performs a single task. Simplify a problem to the minimal components necessary to reproduce the error, and use that information to ask for help appropriately. Use built-in debugging tools to trace an error to its source Use online forums and mailing lists to research error messages Note: The skills in this chapter take a lifetime to truly master. The real goal here is that you know how to ask for help appropriately (and in a way that people will respond positively to) and that you know how to do the research to get help yourself. The faces of debugging (by Allison Horst) 7.1 Step 1: Check your spelling. I’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation. 7.2 Defensive Programming One of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively. By that, I mean, code in a way that you will make debugging things easier later. Modularize your code. Each function should do only one task, ideally in the least-complex way possible. Make your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly. Comment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you. Don’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex. Reduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse is notorious for this. It’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constitutent packages are some of the most installed R packages on CRAN.) Add safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a.”) Don’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression. (This is easier if you’re writing modular code to begin with) Collect your often-reused code in packages (R) or scripts (SAS) that you can easily load and make available to “future you” Wikipedia’s article on defensive programming is much more general than the applications to statistical programming, but may be worth scanning. 7.3 General Debugging Strategies Debugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once While defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in Advanced R by H. Wickham25; I’ve copied it here because it’s such a succinct distillation of the process, but I’ve adapted some of the explanations to this class rather than the original c ontext of package development. Realize that you have a bug Google! In R you can automate this with the errorist and searcher packages. In SAS, if the error message isn’t that clear you’ll find a SAS forum page where someone else has made the same mistake - I can almost guarantee it. Make the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help. Use binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.) Generate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. Note which inputs don’t generate the bug – this negative “data” is helpful when asking for help. Figure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements. Fix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked. There are several other general strategies for debugging: Retype (from scratch) your code This works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative. Visualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application. Tracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x,” “returning from function y,” and so on. Rubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, nevermind?” Rubber ducking outsources the problem to a nonjudgemental entity, such as a rubber duck26. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com. Figure 7.1: You may find it helpful to procure a rubber duck expert for each language you work in. I use color-your-own rubber ducks to endow my ducks with certain language expertise. Other people use plain rubber ducks and give them capes. Do not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own t-shirt, in addition to an xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea. 7.4 Debugging Tools in R Now that we’ve discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging in R. 7.4.1 Low tech debugging with print() and other tools Sometimes called “tracing” techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. When the code is executed, you get a window into what the variables look like during execution. Simple example Imagine we start with this: a &lt;- function(x) { b &lt;- function(y) { c &lt;- function(z) { z + y } c(3) } x + b(4) } a(5) [1] 12 and the goal is to understand what’s happening in the code. We might add some lines: a &lt;- function(x) { print(paste(&quot;Entering a(). x = &quot;, x)) b &lt;- function(y) { print(paste(&quot;Entering b(). x = &quot;, x, &quot;y = &quot;, y)) c &lt;- function(z) { print(paste(&quot;Entering c(). x = &quot;, x, &quot;y = &quot;, y, &quot;z = &quot;, z)) cres &lt;- z + y print(paste(&quot;Returning&quot;, cres, &quot;from c()&quot;)) cres } bres &lt;- c(3) print(paste(&quot;Returning&quot;, bres, &quot;from b()&quot;)) bres } ares &lt;- x + b(4) print(paste(&quot;Returning&quot;,ares, &quot;from a()&quot;)) ares } a(5) [1] &quot;Entering a(). x = 5&quot; [1] &quot;Entering b(). x = 5 y = 4&quot; [1] &quot;Entering c(). x = 5 y = 4 z = 3&quot; [1] &quot;Returning 7 from c()&quot; [1] &quot;Returning 7 from b()&quot; [1] &quot;Returning 12 from a()&quot; [1] 12 For more complex data structures, it can be useful to add str(), head(), or summary() functions. Real world example I was recently writing a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I was assuming that the order of the parents would be “html,” “body,” “div,” “table,” “tbody,” “tr” - descending from outer to inner (if you know anything about HTML/XML structure). library(xml2) # read html search_for_parent &lt;- function(node, tag) { # Get all of the parent nodes parents &lt;- xml2::xml_parents(node) # Get the tags of every parent node tags &lt;- purrr::map_chr(parents, rvest::html_name) print(tags) # Find matching taggs matches &lt;- which(tags == tag) print(matches) # Take the minimum matching tag min_match &lt;- min(matches) if (length(matches) == 1) return(parents[min_match]) else return(NULL) } page &lt;- read_html(&quot;data/realclearpolitics_frag.html&quot;) node &lt;- xml_find_all(page, &quot;//td[@class=&#39;lp-results&#39;]&quot;) # find all poll results in any table search_for_parent(node[1], &quot;table&quot;) # find the table that contains it [1] &quot;tr&quot; &quot;tbody&quot; &quot;table&quot; &quot;div&quot; &quot;body&quot; &quot;html&quot; [1] 3 {xml_nodeset (1)} [1] &lt;table cellpadding=&quot;2&quot; cellspacing=&quot;0&quot; class=&quot;sortable&quot;&gt;\\n&lt;thead&gt;&lt;tr clas ... By printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node. Try it out Not all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully. library(ggplot2) library(dplyr) library(magrittr) library(maps) library(ggthemes) worldmap &lt;- map_data(&quot;world&quot;) # Load the data data(storms, package = &quot;dplyr&quot;) The code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code. # Make base map to be used for each iteration basemap &lt;- ggplot() + # Country shapes geom_polygon(aes(x = long, y = lat, group = group), data = worldmap, fill = &quot;white&quot;, color = &quot;black&quot;) + # Zoom in coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + # Don&#39;t need scales b/c maps provide their own geographic context... theme_map() for (i in 1:5) { # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(status == i) # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) print(plot) } Solution First, lets split the setup from the loop. # Make base map to be used for each iteration basemap &lt;- ggplot() + # Country shapes geom_polygon(aes(x = long, y = lat, group = group), data = worldmap, fill = &quot;white&quot;, color = &quot;black&quot;) + # Zoom in coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + # Don&#39;t need scales b/c maps provide their own geographic context... theme_map() print(basemap) # make sure the basemap is fine # Load the data data(storms, package = &quot;dplyr&quot;) str(storms) # make sure the data exists and is formatted as expected tibble[,13] [10,010 × 13] (S3: tbl_df/tbl/data.frame) $ name : chr [1:10010] &quot;Amy&quot; &quot;Amy&quot; &quot;Amy&quot; &quot;Amy&quot; ... $ year : num [1:10010] 1975 1975 1975 1975 1975 ... $ month : num [1:10010] 6 6 6 6 6 6 6 6 6 6 ... $ day : int [1:10010] 27 27 27 27 28 28 28 28 29 29 ... $ hour : num [1:10010] 0 6 12 18 0 6 12 18 0 6 ... $ lat : num [1:10010] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ... $ long : num [1:10010] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ... $ status : chr [1:10010] &quot;tropical depression&quot; &quot;tropical depression&quot; &quot;tropical depression&quot; &quot;tropical depression&quot; ... $ category : Ord.factor w/ 7 levels &quot;-1&quot;&lt;&quot;0&quot;&lt;&quot;1&quot;&lt;&quot;2&quot;&lt;..: 1 1 1 1 1 1 1 1 2 2 ... $ wind : int [1:10010] 25 25 25 25 25 25 25 30 35 40 ... $ pressure : int [1:10010] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ... $ ts_diameter: num [1:10010] NA NA NA NA NA NA NA NA NA NA ... $ hu_diameter: num [1:10010] NA NA NA NA NA NA NA NA NA NA ... Everything looks ok in the setup chunk… for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(status == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) # print(plot) # Don&#39;t print plots - clutters up output at the moment } [1] &quot;Category 1 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 2 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 3 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 4 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 5 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; Ok, so from this we can see that something is going wrong with our filter statement - we have no rows of data. head(storms) # A tibble: 6 x 13 name year month day hour lat long status category wind pressure &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; &lt;int&gt; &lt;int&gt; 1 Amy 1975 6 27 0 27.5 -79 tropical de… -1 25 1013 2 Amy 1975 6 27 6 28.5 -79 tropical de… -1 25 1013 3 Amy 1975 6 27 12 29.5 -79 tropical de… -1 25 1013 4 Amy 1975 6 27 18 30.5 -79 tropical de… -1 25 1013 5 Amy 1975 6 28 0 31.5 -78.8 tropical de… -1 25 1012 6 Amy 1975 6 28 6 32.4 -78.7 tropical de… -1 25 1012 # … with 2 more variables: ts_diameter &lt;dbl&gt;, hu_diameter &lt;dbl&gt; Whoops. I meant “category” when I typed “status.” for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(category == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) # print(plot) # Don&#39;t print plots - clutters up output at the moment } [1] &quot;Category 1 storms&quot; [1] &quot;subdata dims: nrow 13 ncol 13&quot; [1] &quot;Category 2 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 3 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 4 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; [1] &quot;Category 5 storms&quot; [1] &quot;subdata dims: nrow 0 ncol 13&quot; Ok, that’s something, at least. We now have some data for category 1 storms… filter(storms, year == 2013) %&gt;% # Get max category for each named storm group_by(name) %&gt;% filter(category == max(category)) %&gt;% ungroup() %&gt;% # See what categories exist select(name, category) %&gt;% unique() # A tibble: 14 x 2 name category &lt;chr&gt; &lt;ord&gt; 1 Andrea 0 2 Barry 0 3 Chantal 0 4 Dorian 0 5 Erin 0 6 Fernand 0 7 Gabrielle 0 8 Eight -1 9 Humberto 1 10 Ingrid 1 11 Jerry 0 12 Karen 0 13 Lorenzo 0 14 Melissa 0 It looks like 2013 was just an incredibly quiet year for tropical activity. 2004, however, was not. So let’s just make sure our code works by checking out 2004. for (i in 1:5) { print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2004) %&gt;% filter(category == i) print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # str(subdata) works too, but produces more clutter. I started # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) print(plot) # Don&#39;t print plots - clutters up output at the moment } [1] &quot;Category 1 storms&quot; [1] &quot;subdata dims: nrow 45 ncol 13&quot; [1] &quot;Category 2 storms&quot; [1] &quot;subdata dims: nrow 39 ncol 13&quot; [1] &quot;Category 3 storms&quot; [1] &quot;subdata dims: nrow 29 ncol 13&quot; [1] &quot;Category 4 storms&quot; [1] &quot;subdata dims: nrow 32 ncol 13&quot; [1] &quot;Category 5 storms&quot; [1] &quot;subdata dims: nrow 12 ncol 13&quot; If we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code). for (i in 1:5) { # print(paste0(&quot;Category &quot;, i, &quot; storms&quot;)) # Subset the data subdata &lt;- storms %&gt;% filter(year == 2013) %&gt;% filter(category == i) # print(paste0(&quot;subdata dims: nrow &quot;, nrow(subdata), &quot; ncol &quot;, ncol(subdata))) # # str(subdata) works too, but produces more clutter. I started # # with str() and moved to dim() when I saw the problem # Plot the data - path + points to show the observations plot &lt;- basemap + geom_path(aes(x = long, y = lat, color = name), data = subdata) + geom_point(aes(x = long, y = lat, color = name), data = subdata) + ggtitle(paste0(&quot;Category &quot;, i, &quot; storms in 2013&quot;)) if (nrow(subdata) &gt; 0) print(plot) } 7.4.2 After an error has occurred - traceback() traceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This can help, but it can also be pretty arcane. traceback() example a &lt;- function(x) { b &lt;- function(y) { c &lt;- function(z) { stop(&#39;there was a problem&#39;) # This generates an error } c() } b() } a() Error in c(): there was a problem For more information, you could run traceback traceback() Which will provide the following output: 4: stop(&quot;there was a problem&quot;) at #4 3: c() at #6 2: b() at #8 1: a() Reading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code. If you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error. Figure 7.2: Both Show Traceback and Rerun with Debug are useful tools If you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. 7.4.3 browser() - debugging your own code, interactively The browser() function is useful for debugging your own code. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on. Example of using browser() Suppose that I want to write a function that will plot an xkcd comic in R. I start with library(png) library(xml2) # get the most current xkcd get_xkcd &lt;- function(id = NULL) { url &lt;- &quot;http://xkcd.com&quot; page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) readPNG(source = image) } Watch this live-coding video to see how I use browser() to figure out what’s going on in the function and how to fix it. Vidgrid link. Alternate YouTube Link Here’s the final function library(png) library(xml2) # get the most current xkcd get_xkcd &lt;- function(id = NULL) { url &lt;- &quot;http://xkcd.com&quot; page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) # Fix image address so that we can access the image image &lt;- substr(image, 3, nchar(image)) # Download the file to a temp file and read from there file_location &lt;- tempfile(fileext = &quot;.png&quot;) download.file(image, destfile = file_location, quiet = T) readPNG(source = file_location) } get_xkcd() %&gt;% as.raster() %&gt;% plot() Try it out Each xkcd has a corresponding ID number (ordered sequentially from 1 to 2328 at the time this was written). Modify the XKCD function above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic. Use browser() to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL. What things might you add to make this function “defensive programming” compatible? Solution # get the most current xkcd or the specified number get_xkcd &lt;- function(id = NULL) { if (is.null(id)) { # Have to get the location of the image ourselves url &lt;- &quot;http://xkcd.com&quot; } else if (is.numeric(id)) { url &lt;- paste0(&quot;http://xkcd.com/&quot;, id, &quot;/&quot;) } else { # only allow numeric or null input stop(&quot;To get current xkcd, pass in NULL, otherwise, pass in a valid comic number&quot;) } page &lt;- read_html(url) # Find the comic image &lt;- xml_find_first(page, &quot;//div[@id=&#39;comic&#39;]/img&quot;) %&gt;% # pull the address out of the tag xml_attr(&quot;src&quot;) # Fix image address so that we can access the image image &lt;- substr(image, 3, nchar(image)) # cut the first 2 characters off # make temp file location &lt;- tempfile(fileext = &quot;png&quot;) download.file(image, destfile = location, quiet = T) # This checks to make sure we saved the file correctly if (file.exists(location)) { readPNG(source = location) } else { # Give a good informative error message stop(paste(&quot;Something went wrong saving the image at &quot;, image, &quot; to &quot;, location)) } } get_xkcd(2259) %&gt;% as.raster() %&gt;% plot() 7.4.4 debug() - the general debugging workhorse In the traceback() Rstudio output, the other option is “rerun with debug.” In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow. debug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code. Example of using debug data(iris) tmp &lt;- lm(Species ~ ., data = iris) Warning in model.response(mf, &quot;numeric&quot;): using type = &quot;numeric&quot; with a factor response will be ignored Warning in Ops.factor(y, z$residuals): &#39;-&#39; not meaningful for factors summary(tmp) Warning in Ops.factor(r, 2): &#39;^&#39; not meaningful for factors Call: lm(formula = Species ~ ., data = iris) Residuals: Error in quantile.default(resid): factors are not allowed We get this weird warning, and then an error about factors when we use summary() to look at the coefficients. debug(lm) # turn debugging on tmp &lt;- lm(Species ~ ., data = iris) summary(tmp) undebug(lm) # turn debugging off) The first thing I see when I run lm after turning on debug (screenshot) The variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function After pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function. We can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong. We probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened. I can hit “Stop” or type “Q” to exit the debug environment. But, until I run undebug(lm), every call to lm will take me into the debug window. undebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run. Try it out larger(x, y) is supposed to return the elementwise maximum of two vectors. larger &lt;- function(x, y) { y.is.bigger &lt;- y &gt; x x[y.is.bigger] &lt;- y[y.is.bigger] x } larger(c(1, 5, 10), c(2, 4, 11)) [1] 2 5 11 larger(c(1, 5, 10), 6) [1] 6 NA 10 Why is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it. Solution I’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined x &lt;- c(1, 5, 10) y &lt;- 6 # Inside of larger() with x = c(1, 5, 10), y = 6 (y.is.bigger &lt;- y &gt; x ) # putting something in () prints it out [1] TRUE TRUE FALSE y[y.is.bigger] # This isn&#39;t quite what we were going for, but it&#39;s what&#39;s causing the issue [1] 6 NA x[y.is.bigger] # What gets replaced [1] 1 5 # Better option larger &lt;- function(x, y) { y.is.bigger &lt;- y &gt; x ifelse(y.is.bigger, y, x) } 7.5 Debugging Tools in SAS In SAS, there are two stages that occur after you submit lines to the console. The Compilation Phase: code is parsed. In this step, SAS will catch the logic errors, misspellings, missing key words, etc. The Execution Phase: program is run. In this step, SAS will catch any wrong assignments, loop issues, etc. Sas recognizes four types of errors: Syntax errors - violations of the language structure Semantic errors - structure of the statement is incorrect, but the syntax is correct. e.g. trying to reference an index that doesn’t exist. Execution-time errors - errors that occur when the compiled function is run on data values – e.g. division by zero Data errors - errors that occur when statements are correct but data is invalid (taking the log of a negative number, etc.) SAS is built around enterprise users, as opposed to R’s open-source philosophy. SAS code also is more formulaic than R code, which means it is usually easier to figure out what is going wrong with the code. As a result, you may find that errors in your SAS code are much easier to diagnose than errors in your R code. Generally, it will tell you exactly where you are missing a semicolon, or exactly what word it thinks you’ve misspelled (and usually, it tries to correct that for you, but it doesn’t always succeed). In my experience with SAS (which is very limited and mostly contained in this book), SAS error messages are much easier to google and find solutions, right up until you’re working in Linux or some other not-well-supported system and the error is related to how the underlying OS handles some task. As a downside, though, trying to do a task SAS doesn’t think you need to do can be much more difficult than necessary. See the references section for a couple of good guides to SAS error statements and warnings. These guides are likely sufficient for most of your SAS debugging needs. There are certainly other errors which can occur in SAS – logic errors are not something SAS can protect you from . These errors can have dramatic consequences, as demonstrated in this twitter thread about a JAMA retraction due to a coding error. To debug these types of errors, you can use the same print() techniques demonstrated in R. For these types of errors, there’s nothing special about what language you’re using (outside of the usual quirks of every language) - the error is in the logic, not the encoding of that logic. 7.6 Minimal Working Examples If all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and SAS browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors. So, with that said, there are numerous resources for writing what’s called a “minimal working example,” “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example. The goal is to reproduce the error message with information that is minimal - as little code as possible to still reproduce the problem complete - everything necessary to reproduce the issue is contained in the description/question reproducible - test the code you provide to reproduce the problem. You should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working. 7.6.1 Example: MWEs You haven’t gotten to module 8 yet, but at one point I had issues with SAS graphs rendering in black and white most of the time. I started debugging the issue with the following code chunk: {r sas-cat-aes-map-07, engine=&quot;sashtml&quot;, engine.path=&quot;sas&quot;, fig.path = &quot;image/&quot;} libname classdat &quot;sas/&quot;; PROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled) group = Abb; /* maps to point color by default */ RUN; QUIT; PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */ SCATTER x = Population y = Assault / markercharattrs=(size=8) markerchar = Abb /* specify marker character variable */ group = Abb ; RUN; QUIT; (I moved the chunk header to the next line so that you can see the whole chunk) After running the code separately in SAS and getting a figure that looked like what I’d expected, I set out to construct a reproducible example so that I could post to the SASmarkdown github issues page and ask for help. The first thing I did was strip out all of the extra stuff that didn’t need to be in the chunk - this chunk generates 2 pictures; I only need one. This chunk requires the fbiwide data; I replaced it with a dataset in the sashelp library. When I was done, the chunk looked like this: PROC SGPLOT data=sashelp.snacks; SCATTER x = date y = QtySold / markerattrs=(size=8pt symbol=circlefilled) group = product; /* maps to point color by default */ RUN; QUIT; Then, I started constructing my reproducible example. I ran ?sas_enginesetup to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file). I copied the example from that page: indoc &lt;- &#39; --- title: &quot;Basic SASmarkdown Doc&quot; author: &quot;Doug Hemken&quot; output: html_document --- # I&#39;ve deleted the intermediate chunks because they screw # everything up when I print this chunk out &#39; knitr::knit(text=indoc, output=&quot;test.md&quot;) rmarkdown::render(&quot;test.md&quot;) Then, I created several chunks which would do the following: 1. Write the minimal example SAS code above to a file 2. Call that file in a SASmarkdown chunk using the %include macro, which dumps the listed file into the SAS program 3. Call the file using SAS batch mode Finally, I included the image generated from the batch mode call manually. You can see the resulting code here. I pasted my example into the issues page, and then included some additional information: A screenshot of the rendered page The image files themselves A description of what happened My suspicions (some obvious option I’m missing?) An additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks. This process took me about 45 minutes, but that was still much shorter than the time I’d spent rerunning code trying to get it to work with no success. In less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. I had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do). Then, I had to tinker with the book a bit to figure out if there were easier ways to get the same result. Hopefully, at this point, all of the SAS graphs are now in full color, as modern graphics intended. 7.6.2 Try It Out Use this list of StackOverflow posts to try out your new debugging techniques. Can you figure out what’s wrong? What information would you need from the poster in order to come up with a solution? References Stalking the elusive computer bug - the etymology and historical use of the term “debugging,” from Thomas Edison to Grace Hopper. Debugging (lecture materials from software construction class at MIT) - written with java, but mostly comprehensible for any language. Debugging with RStudio An Introduction to the Interactive Debugging Tools in R Stackoverflow: General Suggestions for Debugging in R WTF R - What They Forgot to Teach You about R Debugging chapter Debugging 101 in SAS Debugging SAS Programs - Ch. 1: The Basics of Debugging A webinar by Jenny Bryan/RStudio on Reproducible Examples the 0th step is from the 1st edition, the remaining steps are from the 2nd.↩︎ Some people use cats, but I find that they don’t meet the nonjudgemental criteria. Of course, they’re equally judgemental whether your code works or not, so maybe that works if you’re a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you’re upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.↩︎ "],["data-vis-intro.html", "Module 8 Introduction to Data Visualization Visualization: Module Objectives 8.1 Why do we create graphics? 8.2 General approaches to creating graphics 8.3 The Grammar of Graphics 8.4 Good charts 8.5 Other things worth exploring (and future expansions for this chapter) References", " Module 8 Introduction to Data Visualization Visualization: Module Objectives Create statistical charts in SAS and R Use the grammar of graphics to describe different types of charts and graphs Use the grammar of graphics to create layered graphics and highlight different features of a data set Modify or construct alternate charts showing the same material with better readability and accessibility There are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. This is one of the less serious schemes I’ve seen But, in my opinion, Randall missed the opportunity to put a pie chart as Neutral Evil. Hopefully by the end of this, you will be able to at least make the charts which are most commonly used to show data and statistical concepts. This is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. But, this is also the chapter where we really start to focus on R instead of SAS. Even the hardcore SAS users I know (in this department and others) go into R when they want to make a publication-quality chart. Visualization and statistical graphics are also my research area, so I’m probably going to be a bit more passionate about this chapter, which means there’s probably going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested. R package setup library(readr) library(dplyr) library(tidyr) library(ggplot2) 8.1 Why do we create graphics? The greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John Tukey, Data Based Graphics: Visual Display in the Decades to Come Charts are easier to understand than raw data. (more details inside) When you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around.” Consider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options: Print out the data set Create some summary statistics of each variable and perhaps the covariance between the two variables Draw a scatter plot of the two variables Which one would you rather use? Why? Our brains are very good at processing large amounts of visual information quickly. Evolutionarily, it’s important to be able to e.g. survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel). It’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out. 8.2 General approaches to creating graphics There are two general approaches to generating statistical graphics computationally: Manually specify the plot that you want, doing the preprocessing and summarizing before you create the plot. Describe the relationship between the plot and the data, using sensible defaults that can be customized for common operations. In the introduction to The Grammar of Graphics, Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects.” His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics.” Base R graphics and the original SAS graphics engine fall firmly into the first camp. ggplot2 was designed using the philosophy of the grammar of graphics, and is still the primary place that people learn about the grammar of graphics in statistics. The SAS ODS Graphics Engine27 falls somewhere in between the two approaches - it provides some sensible defaults, but its design isn’t rooted in the philosophy of the grammar of graphics, so if (as some SAS manuals have claimed) there is a similarity between the two, it’s functional and not philosophical or because of a similarity in the design strategy. You’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while option 2 does have a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it. There are other graphics systems in R (namely, lattice, plus some web-based rendering engines) that you could explore, but it’s far more important that you know how to functionally create plots in R and/or SAS. I don’t recommend you try to become proficient in all of them. Pick one (two at most) and get to know that, then google for the rest. Before we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years, so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible at the end of this module. R base graphics example # Setup the data poke &lt;- read_csv(&quot;data/pokemon_ascii.csv&quot;, na = &#39;.&#39;) %&gt;% mutate(generation = factor(generation)) Let’s start with what we want: for each generation, we want the total number of pokemon. To get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation. # Create summary of pokemon by type tmp &lt;- poke %&gt;% group_by(generation) %&gt;% count() pie(tmp$n, labels = tmp$generation) # with(., &lt;base command&gt;) let you use the pipe easily with a base R command # that is otherwise not pipe friendly, e.g. # with(tmp, pie(n, labels = generation)) We could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion. # Create summary of pokemon by type tmp &lt;- poke %&gt;% group_by(generation) %&gt;% count() # Matrix is necessary for a stacked bar chart matrix(tmp$n, nrow = 8, ncol = 1, dimnames = list(tmp$generation)) %&gt;% barplot(beside = F, legend.text = T, main = &quot;Generations of Pokemon&quot;) R ggplot2 graphics In ggplot2, we start by specifying which variables we want to be mapped to which features of the data. In a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack.” So it’s easiest to specify our x variable as a constant, \"\". We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable. Then, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”). ggplot(aes(x = &quot;&quot;, fill = generation), data = poke) + geom_bar(position = &quot;stack&quot;) If we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\). ggplot(aes(x = &quot;&quot;, fill = generation), data = poke) + geom_bar(position = &quot;stack&quot;) + coord_polar(&quot;y&quot;, start = 0) Notice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and SAS, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation. SAS Examples Original SAS graphics engine Note: This code runs, but it causes every other sas chunk in this document to go haywire… so you’ll have to copy the code and run it on your own to see what it looks like. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 /* This step creates a constant variable in the data frame, 10 so that all generations can be stacked in one bar */ 11 DATA poketmp; 12 SET classdat.poke; 13 i = 1; 14 RUN; NOTE: The data set WORK.POKETMP has 0 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 16 PROC GCHART data=poketmp; 17 pie generation / other = 0; 18 RUN; WARNING: RUN statement ignored due to previous errors. Submit QUIT; to terminate the procedure. 19 QUIT; NOTE: PROCEDURE GCHART used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 20 21 PROC GCHART data=poketmp; 22 VBAR i / SUBGROUP = generation; 23 RUN; WARNING: RUN statement ignored due to previous errors. Submit QUIT; to terminate the procedure. 24 QUIT; NOTE: PROCEDURE GCHART used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. ODS Graphics Note: This is a terrible example in SAS because there isn’t an easy way to create a pie chart28. We have to resort to using SAS Graph Template Language. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA poketmp; 10 SET classdat.poke; 11 i = 1; 12 RUN; NOTE: The data set WORK.POKETMP has 0 observations and 50 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 /* Define a pie chart template */ 15 PROC TEMPLATE; 16 DEFINE STATGRAPH WORK.simplepie; 17 BEGINGRAPH; 18 LAYOUT REGION; 19 PIECHART category=generation; 20 ENDLAYOUT; 21 ENDGRAPH; 22 END; ERROR: Template &#39;Work.Simplepie&#39; was unable to write to template store! 23 RUN; NOTE: PROCEDURE TEMPLATE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds WARNING: Errors were produced. NOTE: The SAS System stopped processing this step because of errors. 24 25 /* Make the pie chart */ 26 PROC SGRENDER data=classdat.poke template=WORK.simplepie; 27 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGRENDER used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 QUIT; 29 30 /* Use SGPLOT to make a stacked bar chart */ 31 PROC SGPLOT data=poketmp; 32 VBAR i / GROUP = generation; 33 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 34 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. As in base R, the syntax between the two types of charts is different, even though the underlying operations required to make the plots are very similar. This is one example of why I don’t agree with the assertion that ODS graphics is like ggplot2 syntax - the functionality may be similar, but the structure is not. If only because it provides us with a place to start (because otherwise, we would just work through a graph gallery or two, and that’s boring), we’ll talk first about the general idea behind the grammar of graphics. For each concept, I’ll provide you first with the ggplot grammar of graphics code, and then, where it is possible to replicate the chart easily in base R or SAS ODS graphics, I will provide code for that as well - so that you can compare the approaches, but also so that you get a sense for what is easy and what is possible in each plotting system. 8.3 The Grammar of Graphics In the grammar of graphics, a plot consists of several mostly independent specifications: aesthetics - links between data variables and graphical features (position, color, shape, size) layers - geometric elements (points, lines, rectangles, text, …) transformations - transformations specify a functional link between the data and the displayed information (identity, count, bins, density, regression). Transformations act on the variables. scales - scales map values in data space to values in the aesthetic space. Scales change the coordinate space of an aesthetic, but don’t change the underlying value (so the change is at the visual level, not the mathematical level). coordinate system - e.g. polar or Cartesian faceting - facets allow you to split plots by other variables to produce many sub-plots. theme - formatting items, such as background color, fonts, margins… We can contrast this with other plotting systems (e.g. Base R), where transformations and scales must be handled manually, there may be separate plotting systems for different coordinate systems, etc. Functionally, the biggest difference between the two systems is that in the grammar of graphics system (as implemented in ggplot2), we work with a full tabular data set. So like the rest of the tidyverse, ggplot2 will allow you to reference bare column names as if they were variables, so long as you’ve passed in the data set to the data = argument.29 Building a masterpiece, by Allison Horst Let’s get the data set up in both SAS and R if (!&quot;classdata&quot; %in% installed.packages()) devtools::install_github(&quot;heike/classdata&quot;) # A package of data sets which are useful for class demonstrations library(classdata) library(tidyr) Attaching package: &#39;tidyr&#39; The following object is masked from &#39;package:magrittr&#39;: extract library(dplyr) data(happy) # you&#39;ll use this for try it out sections data(fbi) fbiwide &lt;- fbi %&gt;% select(-Violent.crime) %&gt;% pivot_wider(names_from = Type, values_from = Count) %&gt;% # Rename variables rename(Murder = Murder.and.nonnegligent.Manslaughter, Assault = Aggravated.assault, Larceny = Larceny.theft, Auto.theft = Motor.vehicle.theft) %&gt;% mutate(Rape = as.numeric(Rape)) # Write a csv out to a file to read into sas if(!file.exists(&quot;data/happy.csv&quot;)) write_csv(happy, &quot;data/happy.csv&quot;, na = &quot;.&quot;) if(!file.exists(&quot;data/fbi.csv&quot;)) write_csv(fbi, &quot;data/fbi.csv&quot;, na = &quot;.&quot;) if(!file.exists(&quot;data/fbiwide.csv&quot;)) write_csv(fbiwide, &quot;data/fbiwide.csv&quot;, na = &quot;.&quot;) 6 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 7 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 8 PROC IMPORT datafile = &quot;data/happy.csv&quot; out=classdat.happy 9 DBMS = CSV 10 REPLACE; 11 GETNAMES = YES; 12 GUESSINGROWS = 5000; 13 RUN; 14 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 15 PROC IMPORT datafile = &quot;data/fbi.csv&quot; out=classdat.fbi 16 DBMS = CSV 17 REPLACE; 18 GETNAMES = YES; 19 GUESSINGROWS = 5000; 20 RUN; 21 NOTE: PROCEDURE IMPORT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. 22 PROC IMPORT datafile = &quot;data/fbiwide.csv&quot; out=classdat.fbiwide 23 DBMS = CSV 24 REPLACE; 25 GETNAMES = YES; 26 GUESSINGROWS = 2000; 27 RUN; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. 8.3.1 Demonstration: Aesthetic Mappings and Basic Plots 8.3.1.1 Basic Scatter plots A basic scatter plot is a combination of an aesthetic mapping for x and y (position), combined with the specification that the geometric object to be displayed is a point. Without any of the components (x, y, geom_point) you don’t have a scatter plot. ggplot(fbiwide, aes(x = Burglary, y = Murder)) + geom_point() Base R plot(x = Burglary, y = Murder, data = fbiwide) # this doesn&#39;t work Error in plot(x = Burglary, y = Murder, data = fbiwide): object &#39;Burglary&#39; not found plot(x = fbiwide$Burglary, y = fbiwide$Murder) # you can use numeric vector arguments # another option is to use formula notation, that is, y ~ x. # If you use formula notation, you can pass in a data frame using the # data argument and R will interpret things correctly plot(Murder ~ Assault, data = fbiwide) SAS ODS Graphics The workhorse of the ODS graphics engine is SGPLOT. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.fbiwide; 10 SCATTER x = Burglary y = Murder; 11 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 12 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. 8.3.1.2 Adding labels and titles We can add a title to the plot ggplot(fbiwide, aes(x = Murder, y = Assault)) + geom_point() + ggtitle(&quot;Murders and Assaults&quot;) You can also add labels using xlab() and ylab(). We’ll talk more about how to make fine-grained modifications of axis scales later, but the shorthand commands for changing the title and labels are convenient. Base R plot(Murder ~ Assault, data = fbiwide) title(&quot;Murders and Assaults&quot;) SAS ODS Graphics 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.fbiwide; 10 TITLE &quot;Murders and Assaults&quot;; 11 SCATTER x = Assault y = Murder; 12 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39. 8.3.1.3 Changing characteristics of plotted objects We can also modify the appearance of the plotted objects ggplot(fbiwide, aes(x = Population, y = Assault)) + geom_point(color = &quot;blue&quot;, alpha = .5) Alpha blending, or transparency, allows us to see the structure in over-plotted (crowded) charts. Here, I’ve specified that the points should have 50% opacity. Another point to note: inside the aes() function, variables are mapped to geometric object characteristics, but outside, those same parameters can be mapped to constant values. If something is inside aes(), it should be a variable. Base R Base R doesn’t support alpha blending by default, so we have to load the scales package in order to get that functionality. library(scales) Attaching package: &#39;scales&#39; The following object is masked from &#39;package:readr&#39;: col_factor The following object is masked from &#39;package:purrr&#39;: discard # Using constant alpha: plot(Assault ~ Population, col = alpha(&quot;blue&quot;, .5), pch = 16, data = fbiwide) SAS ODS Graphics 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.fbiwide; 10 SCATTER x = Population y = Assault / 11 markerattrs=(size=8pt symbol=circlefilled color=&quot;blue&quot;) 12 transparency=0.5; 13 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39,40. 8.3.1.4 Mapping categorical variables to aesthetics like color If we want to get fancy, we can map other variables to aesthetics like color, size, etc. We could explore the relationship between Population and Assault, but we’d expect that over time, observations from each state would be closely related. We could explore that by coloring each point by state (note, this doesn’t allow us to really see which states belong to which points so much as it visually associates connected points.) ggplot(fbiwide, aes(x = Population, y = Assault, color = Abb)) + geom_point() Base R variable mapping In base R, you can also create scatter plots that have different plot aesthetics, but we have to get a bit more hands-on than we did with ggplot2. # This doesn&#39;t work because the values in Abb aren&#39;t actually colors # and plot() doesn&#39;t handle the mapping between color and abbreviation for us plot(Assault ~ Population, col = Abb, data = fbiwide) Error in plot.xy(xy, type, ...): invalid color name &#39;AL&#39; We have to specify the values of color manually for each point that we want to plot - essentially, where ggplot2 handled the scale for us, now we have to manually specify it ourselves. We could specify a legend, but in this particular case it’s unlikely to be that useful to us because the colors aren’t allowing us to identify the lines, they’re just differentiating the lines from other lines. # We need a vector of colors equal to the number of abbreviations we have state_colors &lt;- rainbow(length(unique(fbiwide$Abb))) # then we have to figure out which color goes with which abbreviation fbiwide$Abb_factor &lt;- factor(fbiwide$Abb) fbiwide$state_color &lt;- state_colors[fbiwide$Abb_factor] plot(Assault ~ Population, col = state_color, data = fbiwide) You can use ?points to get information on all of the graphical parameters for points. SAS ODS Graphics variable mapping 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.fbiwide; 10 SCATTER x = Population y = Assault / 11 markerattrs=(size=8pt symbol=circlefilled) 12 group = Abb; /* maps to point color by default */ 13 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 14 QUIT; 15 16 PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate 16 ! a legend */ 17 SCATTER x = Population y = Assault / 18 markercharattrs=(size=8) 19 markerchar = Abb /* specify marker character variable */ 20 group = Abb 21 ; 22 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 23 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39,40. 8.3.1.5 Line plots A better way to graphically examine this hypothesis (the patterns in the data are clusters of each state’s points) might be to change the geometric object - we could use lines instead of points to show this data. In that case, we’d want to show separate lines (groups) for each state. ggplot(fbiwide, aes(x = Population, y = Assault, group = Abb)) + geom_line() Or, we could be lazy and leave color specified; which will have the same effect. Color and most other aesthetics implicitly group the data. ggplot(fbiwide, aes(x = Population, y = Assault, color = Abb)) + geom_line() One of the best parts of ggplot is that you can get away with only changing one or two things and end up with a totally different plot. Base R line plots If we want to make a line plot, we can do it one of two ways: we can use a for loop to plot each state’s line separately, subsetting the data each time, or, we can transform the data to wide format and plot that using the matplot function # For loop method plot(Assault ~ Population, pch = NA, data = fbiwide) # generate a blank plot for (i in unique(fbiwide$Abb)) { subdata &lt;- filter(fbiwide, Abb == i) lines(Assault ~ Population, data = subdata) # add the line to the plot } # ggplot2 code # ggplot(fbiwide, aes(x = Population, y = Assault, group = Abb)) + geom_line() # matplot method subdata &lt;- fbiwide %&gt;% select(Assault, Population, Year, Abb) # get cols we need # Matrix of x values - Population popdat &lt;- pivot_wider(subdata, id_cols = Year, names_from = Abb, values_from = Population) %&gt;% arrange(Year) %&gt;% select(-Year) # Matrix of y values - Assaults assaultdat &lt;- pivot_wider(subdata, id_cols = Year, names_from = Abb, values_from = Assault) %&gt;% arrange(Year) %&gt;% select(-Year) matplot(popdat, assaultdat, type = &quot;l&quot;) # by default, matplot uses 5 different colors and linetypes # so that you can differentiate the lines in different columns If we want to add color in and have it mean something specific, it’s relatively straightforward because we already defined a new column in fbiwide that has state colors. plot(Assault ~ Population, pch = NA, data = fbiwide) # generate a blank plot for (i in unique(fbiwide$Abb)) { subdata &lt;- filter(fbiwide, Abb == i) lines(Assault ~ Population, col = state_color, data = subdata) # add the line to the plot } SAS ODS line plots 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; 10 SERIES x = Population y = Assault / 11 lineattrs=(pattern=solid) 12 group = Abb; /* maps to color by default */ 13 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 QUIT; 15 ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39,40. SAS Line attributes and patterns options 8.3.1.6 Mapping numeric variables to aesthetics We can color our points (or lines) by a categorical variable, but that’s not all! We can also map numeric variables to aesthetics! There are perhaps too many data points in the FBI data to effectively show size variation, so let’s temporarily switch to pokemon and examine the relationship between attack and special attack points. Let’s also map alpha to weight, so that lighter points correspond to lighter pokemon. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, alpha = weight_kg)) + geom_point() See more numeric scales We could get fancier still and map the Pokemon’s height to size. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, size = height_m, alpha = weight_kg)) + geom_point() Note that ggplot2 will create separate legends for size and height by default. If we map size and alpha to the same variable, however, it will automatically combine the legends. poke %&gt;% ggplot(aes(x = attack, y = sp_attack, size = weight_kg, alpha = weight_kg)) + geom_point() Warning: Removed 1 rows containing missing values (geom_point). Base R legends, numeric variables, and aesthetics If we use ?points and find out that the size parameter is cex in base R, we might think something like this would work: plot(sp_attack ~ attack, cex = weight_kg, data = poke) # cex controls the point size argument # note the lack of a legend # Also, we have points that are *way* too big. But, remember, in base graphics, we have to do the transformations ourselves. After some tinkering, I came up with this. plot(sp_attack ~ attack, cex = 0.5 + weight_kg/1000*3, data = poke) # we have to do the scale transformation ourselves # but there&#39;s no legend... We also have to make the legend ourselves. Sigh. plot(sp_attack ~ attack, cex = 0.5 + weight_kg/1000*3, data = poke) legend(&quot;topleft&quot;, legend = c(250, 500, 750, 1000), # labels pch = 1, # must specify point shape (pch = point character) pt.cex = 0.5 + c(250, 500, 750, 1000)/1000 * 3) # must specify cex value # for points specifically If we want to map a value to alpha, we have to make our own scale again… but it may be easier to just make a transformation function that we can pass stuff into. That way we can create the legend and the data transformation using the same function (and if we want to change the transformation, it will be much easier to do so if we only have to change things in one place). library(scales) # Define the transformation wt_trans_alpha &lt;- function(x) { # rescale function is from the scales package trans_x &lt;- rescale( x = x, to = c(.2, 1), # don&#39;t go to 0 because then we can&#39;t see the points from = range(poke$weight_kg, na.rm = T) # input domain ) # alpha values have to be between 0 and 1, so we need to make sure to # truncate the scale trans_x %&gt;% pmin(., 1) %&gt;% # don&#39;t go any higher than 1 pmax(., 0) # don&#39;t go any lower than 0 } range(poke$weight_kg, na.rm = T) # get the range [1] 0.1 999.9 # Make the legend variables leg_label &lt;- seq(0, 1000, length.out = 5) # cut weight range into 5 leg_value &lt;- alpha(&quot;black&quot;, alpha = wt_trans_alpha(leg_label)) select(poke, attack, sp_attack, weight_kg) %&gt;% mutate(alpha = wt_trans_alpha(weight_kg), color = alpha(&quot;black&quot;, alpha = alpha)) %&gt;% plot(sp_attack ~ attack, col = color, pch = 16, data = .) legend(&quot;topleft&quot;, title = &quot;Weight (kg)&quot;, legend = leg_label, # labels pch = 16, # must specify point shape (pch = point character) col = leg_value) # must specify point color using alpha() function If we want to do color and size simultaneously, we have to define another transformation, but we can re-use the same legend. wt_trans_size &lt;- function(x) { trans_x &lt;- rescale( x = x, to = c(0.5, 3), from = range(poke$weight_kg, na.rm = T) # input domain ) # cex values have to be &gt;0, so we need to make sure to truncate the scale trans_x %&gt;% pmax(., 0) # don&#39;t go any lower than 0 } range(poke$weight_kg, na.rm = T) # get the range [1] 0.1 999.9 # Make the legend variables leg_label &lt;- seq(0, 1000, length.out = 5) # cut weight range into 5 leg_color &lt;- alpha(&quot;black&quot;, alpha = wt_trans_alpha(leg_label)) leg_size &lt;- wt_trans_size(leg_label) select(poke, attack, sp_attack, weight_kg) %&gt;% mutate(alpha = wt_trans_alpha(weight_kg), color = alpha(&quot;black&quot;, alpha = alpha), size = wt_trans_size(weight_kg)) %&gt;% plot(sp_attack ~ attack, col = color, cex = size, pch = 16, data = .) legend(&quot;topleft&quot;, title = &quot;Weight (kg)&quot;, legend = leg_label, # labels pch = 16, # must specify point shape (pch = point character) col = leg_color, pt.cex = leg_size) SAS ODS graphics and numeric variable transformations In order to change point size to show a different variable value, we have to use BUBBLE instead of SCATTER. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPLOT data=classdat.poke; 10 BUBBLE x = attack y = sp_attack size = weight_kg / 11 BRADIUSMAX=10 BRADIUSMIN =2 12 transparency=0.5; 13 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 15 QUIT; ERROR: Errors printed on pages 5,7,12,13,16,17,18,19,24,25,26,27,28,30,31, 34,35,36,37,38,39,40. As best as I can tell, there’s no similar plot option that would let us map a variable to transparency. There are attribute maps, but the options are discrete (which weight is not) or range (which would let us define ranges to map to a single value), and they don’t appear to support transparency. In theory, I think we could go to SAS Template graph language, but … ugh. To be honest, at this point, I’d move the data into R and graph things there. 8.3.1.7 Try it out Explore the happy data set, which is a selection of variables from the general social survey (?happy) related to happiness. What associations can you find between the variables? Can you use what you know about the graphics help resources to figure out how to create new plot types? 8.3.2 Syntax and General Structure: Layers One of the main advantages of ggplot2 is that the syntax is basically consistent across very different types of plots. In base R and SAS, this is not the case - you have to look up the available options for each different plot type. In ggplot2, I might have to look up what the aesthetic names are for a specific geom, but I can guess most of the time. So let’s look a bit more into what ggplot2’s approach to graph specification is and what it allows us to do. You’re fairly used to the syntax of the pipe by now; but ggplot works on a slightly different (but similar) concept that we’ve used implicitly up until this point. There is the initial plot statement, ggplot(), and successive layers are added using +. You can specify a data set and aesthetic variables in the ggplot() statement (which is what we’ll usually do), but you can also have a completely blank ggplot() statement and specify your aesthetic mappings and data sets for each layer separately. This approach is more useful when you start creating complex plots, because you may need to plot summary information and raw data, or e.g. separate tables with city information, geographic boundaries, and rivers, all of which need to be represented in the same map. In this extended example, we’ll examine the different features we need to make a map and how to add new layers to a map. We’ll also look at some new geoms: geom_polygon and geom_path. pkgs &lt;- installed.packages() # Install any of the packages that you don&#39;t have install.packages(setdiff(c(&quot;ggthemes&quot;, &quot;mapdata&quot;, &quot;maps&quot;), pkgs)) 8.3.2.1 Initial data set up and exploration of the data sets Initial data set up and exploration of the data sets library(ggplot2) library(maps) library(mapdata) library(ggthemes) # theme_map # Create a data frame of the outline of the US us_map &lt;- map_data(&quot;usa&quot;) states &lt;- map_data(&quot;state&quot;) # Read in some data about places of worship in the US (compiled from IRS filings) worship &lt;- read_csv(&quot;data/Places_of_Worship.csv&quot;, guess_max = 5000) ── Column specification ──────────────────────────────────────────────────────── cols( EIN = col_double(), NAME = col_character(), STREET = col_character(), CITY = col_character(), STATE = col_character(), ZIP = col_double(), AFFILIATION = col_double(), FOUNDATION = col_double(), long = col_double(), lat = col_double() ) Let’s look at the map data first: head(us_map) long lat group order region subregion 1 -101.4078 29.74224 1 1 main &lt;NA&gt; 2 -101.3906 29.74224 1 2 main &lt;NA&gt; 3 -101.3620 29.65056 1 3 main &lt;NA&gt; 4 -101.3505 29.63911 1 4 main &lt;NA&gt; 5 -101.3219 29.63338 1 5 main &lt;NA&gt; 6 -101.3047 29.64484 1 6 main &lt;NA&gt; It looks like we have longitude and latitude, group, and point order. What does group mean? select(us_map, group, region) %&gt;% unique() group region 1 1 main 6888 2 martha&#39;s vineyard 6925 3 nantucket island 6956 4 manhattan 6973 5 staten island 6984 6 long island 7153 7 san juan island 7171 8 lopez island 7189 9 orcas island 7209 10 whidbey island The regions are all islands, and group is a number that corresponds to each region. This is important because if we plot polygons, we have to worry about what to do when we jump from one “island” to another (sometimes, regions might be contiguous areas, such as states or zip codes). As long as we pass in a group argument (so group = group) we should avoid most of the complications of plotting spatial data. The state data is similarly structured: head(states) long lat group order region subregion 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; Now, let’s look at our worship place data. The full data set was more extensive (and much larger), so I’ve reduced the number of columns so that I’m able to put it on GitHub. head(worship) # A tibble: 6 x 10 EIN NAME STREET CITY STATE ZIP AFFILIATION FOUNDATION long lat &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2.92e4 ST GEORG… 523 E … SOUT… MA 2127 9 10 -71.0 42.3 2 6.36e5 MINISTER… 454 ES… LAWR… MA 1840 3 10 -71.2 42.7 3 2.03e6 CHURCH O… 569 BR… NEWA… NJ 7104 3 10 -74.2 40.8 4 2.05e6 GENERAL … 3210 S… ORCH… NY 14127 9 10 -78.7 42.8 5 1.02e7 CHILD EV… 431 CA… LIVE… ME 4254 9 15 -70.1 44.4 6 1.02e7 BIBLE SO… 105 HA… PORT… ME 4103 3 16 -70.3 43.7 We have tax numbers, entity names, mailing address, affiliation, foundation, and latitude/longitude. A codebook for the affiliation and foundation information is available from the IRS. While I’m tempted to play around with the names of the organizations a bit (for instance, how many different “First XXX Church” exist in the country?) it’s probably for the best that we keep moving. Basic map geoms - geom_path and geom_polygon Let’s try plotting this out. Maybe we can start with just plotting the outline using geom_line? ggplot() + geom_line(aes(x = long, y = lat, group = group), data = us_map) A line geom is plotted sequentially with respect to the magnitude of x. That doesn’t really work for us - we want x and y to be plotted in order of the observations. To do that, we need geom_path. ggplot() + geom_path(aes(x = long, y = lat, group = group), data = us_map) Not too bad. What if we want to fill in the country, so that it looks different from the background? To do that, we’ll need geom_polygon ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map) # Let&#39;s make the fill white. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map, fill = &quot;white&quot;, color = &quot;black&quot;) # What happens if we get rid of group? ggplot() + geom_polygon(aes(x = long, y = lat), data = us_map, fill = &quot;white&quot;, color = &quot;black&quot;) If we get rid of the group argument, then all of the little islands that we have in the dataset are connected to each other and to the mainland by lines that don’t really belong on our map. Generally, your base map layer will be plotted with geom_polygon, and you will use geom_path and/or geom_polygon to plot additional layers on top of the base map. You may also add points or other geoms on top of those layers, depending on your application. Let’s add more layers! Let’s plot points for the location of every place of worship (according to the IRS) in the US. First, we can plot the points by themselves: ggplot() + geom_point(aes(x = long, y = lat), data = worship) Wow. One problem we’re going to have is Alaska – it isn’t on our base map of the contiguous states. Let’s go ahead and remove those points for now - we could make the map more complicated, but it won’t add much to the overall value of the example. worship_full &lt;- worship worship &lt;- filter(worship, STATE != &quot;AK&quot;) ggplot() + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) At this point it’s worth remembering the population density issue. With that said, we have quite a lot of churches here, and while some of the darker areas are dense populations, not all of the church clusters are in areas I’d call “dense.” Suppose we want to add our church data to our national map. We can plot our US map first, then add the points on top of it. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = us_map, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) The order of the geom statements determines the plotting order. It might help to add some state information in too. We could plot states on top of the full US map, but (hopefully) the states tile the national map, so it’s better just to plot the states. ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) It might be helpful to figure out where there are major cities - some of them are obvious, but I suspect there are certain areas where there are more churches per capita30. The us.cities database has all US cities over 40k people, plus state capitals of any size. Let’s add major US cities as open circles (so that we don’t obscure the church data too much). large_cities &lt;- us.cities %&gt;% filter(country.etc != &quot;AK&quot;) %&gt;% filter(country.etc != &quot;HI&quot;) %&gt;% filter(pop &gt; 100000 | capital == 2) %&gt;% mutate(type = ifelse(capital == 2, &quot;State Capital&quot;, &quot;City&quot;)) layer_map &lt;- ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = worship, size = .1, alpha = .1) + geom_point(aes(x = long, y = lat, color = type, size = pop), data = large_cities, shape = 1) + # put legend below the map theme(legend.position = &quot;bottom&quot;) layer_map Just to examine the South in a little more depth, because there’s an extremely dark area northeast of Atlanta, GA and southwest of Charlotte, NC that doesn’t correspond to any major cities. layer_map + # coord_fixed maintains the proper aspect ratio coord_fixed(xlim = c(-86.5, -78.5), ylim = c(31, 36)) It looks like most of the dark area I noticed corresponds to sprawling Atlanta suburbs, which may not be big enough individually to meet our criteria of 100k people or more. There are also several smaller clusters of churches along what I suspect is the interstate between Atlanta and Charlotte, corresponding to the Greenville-Spartanburg metro (the metro is over 100k people, but individual cities don’t meet the threshold). We could add a layer corresponding to the interstates to provide more geographic context, but I think the point here is pretty clear - you can do some very cool things with maps. Try it out Pick a few denominations or religious organizations (or even just a more generic pattern, like names starting with ST, or schools) and do some rough string matching. What spatial patterns do you detect? (This won’t be totally accurate, and doesn’t need to be, in order to paint a picture of the country’s religious affiliations and congregations) One solution First, let’s do a bit of string processing. common_words &lt;- worship %&gt;% pull(NAME) %&gt;% str_split(&quot;[- &amp;/%]&quot;, simplify = T) %&gt;% as.character() %&gt;% table() %&gt;% sort(decreasing = TRUE) common_words[1:40] . CHURCH OF INC BAPTIST 3427562 119645 79566 59809 25940 MINISTRIES CHRISTIAN THE GOD LUTHERAN 24719 21974 21039 17550 13004 CENTER CHRIST FELLOWSHIP COMMUNITY NEW 12385 12172 11256 10922 10915 FIRST ASSEMBLY INTERNATIONAL ST LIFE 10106 9367 9150 8992 8923 FAITH DE IN BIBLE UNITED 7780 6513 6101 5915 5682 IGLESIA METHODIST MINISTRY GRACE MISSIONARY 5610 5440 5191 5167 5141 AND TEMPLE PRESBYTERIAN GOSPEL HOPE 4873 4635 4155 4144 3915 CHAPEL OUTREACH LIVING FOR EPISCOPAL 3909 3634 3607 3491 3464 Let’s work with the following denominations: BAPTIST, LUTHERAN, METHODIST, PRESBYTERIAN, EPISCOPAL - I’ve left out CHRISTIAN, ASSEMBLY (because it could be ASSEMBLY OF GOD or a more generic term for a congregation), and TEMPLE because it could refer to any number of religions - Jewish, Hindu, etc. This will give us a very rough categorization (e.g. there are Methodist Episcopal churches which will be counted as one and not the other, and UMC is a common abbreviation for United Methodist Church but won’t be counted as one this way). worship &lt;- worship %&gt;% mutate(type = str_extract(NAME, &quot;(BAPTIST|LUTHERAN|METHODIST|PRESBYTERIAN|EPISCOPAL)&quot;)) denom_churches = worship %&gt;% filter(!is.na(type)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat, color = type), data = denom_churches, size = .1, alpha = .1) + # We want to see the legend, so we have to tell ggplot to override the aesthetics guides(color = guide_legend(override.aes = list(size = 2, alpha = 1))) + theme(legend.position = &quot;bottom&quot;) + theme_map() We could also look for other keywords: bahai &lt;- worship %&gt;% filter(str_detect(NAME, &quot;BAHAI&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = bahai, alpha = .1) + theme_map() + ggtitle(&quot;Baha&#39;i Organizations&quot;) zen &lt;- worship %&gt;% filter(str_detect(NAME, &quot;ZEN&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = zen, alpha = .1) + theme_map() + ggtitle(&quot;Zen Organizations&quot;) friends &lt;- worship %&gt;% filter(str_detect(NAME, &quot;(MEETING OF FRIENDS)|(FRIENDS MEETING)&quot;)) ggplot() + geom_polygon(aes(x = long, y = lat, group = group), data = states, fill = &quot;white&quot;, color = &quot;grey&quot;) + geom_point(aes(x = long, y = lat), data = friends, alpha = .1) + theme_map() + ggtitle(&quot;Quaker Religious Organizations&quot;) Try it out – Middle Earth edition Dedicated fans have re-created middle earth in digital format using ArcGIS files. These map file formats, called shape files, can be read into R and plotted. You may need to install a few spatial packages first (Mac and Windows, Linux) The sf package in R contains a special geom, geom_sf, which will plot map objects with an appropriate geom, whether they are points, lines, or polygons. In complicated maps with many layers, this is a really awesome feature. I’ve provided some code to get you started, but there are many other shapefiles in the dataset. Pick some layers which you think are interesting, and plot them with appropriate geoms to make a map of Middle Earth. Unfortunately, in this map there is not an underlying polygon (the coastline is a series of shorter segments). To resolve this, I have provided a theme statement that will have a white background, so that you can add useful layers without the grey grid background. library(ggplot2) library(ggthemes) library(sf) Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1 if (!file.exists(&quot;data/MiddleEarthMap.zip&quot;)) { download.file(&quot;https://github.com/jvangeld/ME-GIS/archive/master.zip&quot;, &quot;data/MiddleEarthMap.zip&quot;, mode = &quot;wb&quot;) } if (!dir.exists(&quot;data/ME-GIS-master/&quot;)) { unzip(&quot;data/MiddleEarthMap.zip&quot;, exdir = &quot;data/&quot;) } coastline &lt;- read_sf(&quot;data/ME-GIS-master/Coastline2.shp&quot;) cities &lt;- read_sf(&quot;data/ME-GIS-master/Cities.shp&quot;) forests &lt;- read_sf(&quot;data/ME-GIS-master/Forests.shp&quot;) lakes &lt;- read_sf(&quot;data/ME-GIS-master/Lakes.shp&quot;) rivers &lt;- read_sf(&quot;data/ME-GIS-master/Rivers.shp&quot;) roads &lt;- read_sf(&quot;data/ME-GIS-master/Roads.shp&quot;) ggplot() + geom_sf(data = coastline) + geom_sf(data = forests, color = NA, fill = &quot;darkgreen&quot;, alpha = .2) + geom_sf(data = rivers, color = &quot;blue&quot;, alpha = .1) + geom_sf(data = lakes, fill = &quot;blue&quot;, color = NA, alpha = .2) + theme_map() 8.3.3 Statistics and Different Types of Plots At this point, we’ve primarily looked at charts which have two continuous variables - scatter plots, and line plots. There are a number of situations where these types of charts are inadequate. For one thing, we might want to only look at the distribution of a single variable. Or, we might want to look at how a continuous response variable changes when the level of a categorical variable changes. In this section, we’ll hit the most common types of plots, but there are almost infinite variations. Sites like the Data Viz Catalogue can be useful if you’re trying to accomplish a specific task and want to know what type of plot to use. In all of the plots which we discuss in this section, there is an implicit statistical function applied to the data before plotting. So while you may specify e.g. x = var1, what is plotted is f(var1), where f() might be the mean/median/quartiles, a binned count, or a computed kernel density. In ggplot2, you can formally specify a statistic by using stat_xxx functions, but many geoms implicitly call these same functions. Box Plots A box plot can show some summary information about the distribution of a single continuous variable, and usually is used to show differences in the level of a response variable at different levels of a categorical variable. Let’s look at the relative frequency of different types of crimes, putting all states on an equal scale of 100k residents so that the numbers are comparable and also meaningful. Box plots in ggplot2 fbi %&gt;% filter(Year == max(Year)) %&gt;% mutate(per_100k = Count/Population*100000) %&gt;% # make nicer names mutate(crime = str_replace_all(Type, c(&quot;Murder.*&quot; = &quot;Murder&quot;, &quot;.*[Rr]ape&quot; = &quot;Rape&quot;, &quot;.*vehicle.theft&quot; = &quot;Vehicle Theft&quot;, &quot;Larceny.theft&quot; = &quot;Theft&quot;, &quot;.*assault&quot; = &quot;Assault&quot;))) %&gt;% ggplot(aes(x = crime, y = per_100k)) + geom_boxplot() + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) Warning: Removed 52 rows containing non-finite values (stat_boxplot). We might want to sort crimes by incidence level, because it makes the plot prettier and also gives us an ordered list of crimes in descending frequency. tmp &lt;- fbi %&gt;% filter(Year == max(Year)) %&gt;% mutate(per_100k = Count/Population*100000) %&gt;% # make nicer names mutate(crime = str_replace_all(as.character(Type), c(&quot;Murder.*&quot; = &quot;Murder&quot;, &quot;.*[Rr]ape&quot; = &quot;Rape&quot;, &quot;.*vehicle.theft&quot; = &quot;GTA&quot;, &quot;Larceny.theft&quot; = &quot;Theft&quot;, &quot;.*assault&quot; = &quot;Assault&quot;))) tmpsum &lt;- tmp %&gt;% group_by(crime) %&gt;% summarize(ref = mean(per_100k, na.rm = T)) %&gt;% ungroup() %&gt;% arrange(desc(ref)) Error in summarize(., ref = mean(per_100k, na.rm = T)): argument &quot;by&quot; is missing, with no default tmp &lt;- tmp %&gt;% mutate(crime = factor(crime, levels = tmpsum$crime)) Error: Problem with `mutate()` input `crime`. x object &#39;tmpsum&#39; not found ℹ Input `crime` is `factor(crime, levels = tmpsum$crime)`. tmp %&gt;% ggplot(aes(x = crime, y = per_100k, color = Violent.crime)) + geom_boxplot() + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) Warning: Removed 52 rows containing non-finite values (stat_boxplot). Additional ggplot2 details - stat_fivenumber(), geoms, and statistics Box plots implicitly call stat_fivenumber(), which computes the five-number summary used to construct the box plot lines and box. A statistic is an aggregation function that exists between the data and the geom. Statistics output values which are then plotted directly by the geom. You could, in theory, create a box plot by calling something like this (I stopped at the boxes, but you get the idea) tibble(x = rep(1:2, each = 50), y = rnorm(100)) %&gt;% ggplot(aes(x = x, y = y, group = x)) + # the box - actually make 2 half-boxes geom_rect(aes(xmin = x - after_stat(width)/2, xmax = x + after_stat(width)/2, ymin = after_stat(lower), ymax = after_stat(middle)), stat = &#39;fivenumber&#39;, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_rect(aes(xmin = x - after_stat(width)/2, xmax = x + after_stat(width)/2, ymin = after_stat(middle), ymax = after_stat(upper)), stat = &#39;fivenumber&#39;, fill = &quot;white&quot;, color = &quot;black&quot;) Box plots in base R boxplot(per_100k~crime, data = tmp) Box plots in SAS 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA WORK.fbitmp; 10 SET classdat.fbi; NOTE: Data file CLASSDAT.FBI.DATA is in a format that is native to another host, or the file encoding does not match the session encoding. Cross Environment Data Access will be used, which might require additional CPU resources and might reduce performance. 11 per_100k = Count/Population*100000; 12 RUN; NOTE: The data set WORK.FBITMP has 0 observations and 8 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 PROC SGPLOT data=fbitmp; 15 hbox per_100k / group = Type; 16 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,17,18,19,20,25,26,27,29,30, 32,33,36,37,38,39,40,41,43. There are other variants of box plots (and similar concepts). Here is a violin plot, which attempts to show some more information about the distribution of the continuous variable (instead of hiding it all in the summary statistics). tmp %&gt;% ggplot(aes(x = crime, y = per_100k, color = Violent.crime)) + geom_violin(scale = &quot;width&quot;, # This ensures all plots have the same max width # Otherwise, theft is too skinny to really see draw_quantiles = c(.25, .5, .75)) + ggtitle(paste(&quot;Crimes per 100k people,&quot;, max(fbi$Year))) Warning: Removed 52 rows containing non-finite values (stat_ydensity). Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm): collapsing to unique &#39;x&#39; values Histograms and Density Plots Box plots aren’t the only way to show distributions, though. If we want to, we can show distributions using histograms or density plots. A histogram is created by binning the variable, then counting the number of observations that fall within each specified range. Usually, these ranges have constant width (but not always). Histograms in ggplot2 poke %&gt;% ggplot(aes(x = hp)) + geom_histogram(color = &quot;black&quot;, fill = &quot;grey&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. poke %&gt;% ggplot(aes(x = hp)) + geom_histogram(color = &quot;black&quot;, fill = &quot;grey&quot;, binwidth = 20) You should always look at a histogram under multiple bin widths, because that parameter can change the visual appearance of the distribution quite a bit. Histograms and stat_bin in ggplot2 In a histogram, the statistic that is computed is stat_bin - we break the x range up into intervals and then count how many points lie in each interval. tibble(x = rnorm(100)) %&gt;% ggplot(aes(x = x)) + stat_bin(geom = &quot;bar&quot;) # this is equivalent to geom_histogram `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. In base R, you can get histograms with the hist() function. Histograms in SAS 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA WORK.fbitmp; 10 SET classdat.fbi; 11 per_100k = Count/Population*100000; 12 RUN; NOTE: The data set WORK.FBITMP has 0 observations and 8 variables. WARNING: Data set WORK.FBITMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); 15 HISTOGRAM per_100k; 16 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41. A kernel density is an empirical method of estimating the probability density function of a variable. In a density plot, instead of count, the y axis is labeled “density” and the continuous curve will be scaled so that the distribution integrates to 1. Density plots can be quite useful for continuous variables, but are also dependent on parameter selection - instead of a bin width, you may need to adjust a kernel width (though there are algorithms that will select this by default, so usually you only have to tweak it a bit). ggplot2 density plot poke %&gt;% ggplot(aes(x = hp)) + geom_density(color = &quot;black&quot;, fill = &quot;grey&quot;) If you try to create a density plot for a variable which has only a limited number of values, you’ll often get an extremely spiky distribution. If your variable is e.g. measured in imprecise intervals, even though it’s continuous, you can adjust the bandwidth, but you may be better off using a histogram and choosing your bin width to be a multiple of your measurement precision. A spiky density curve can alert you to other problems in your data - for instance, the presence of a large number of 0s might tell you a mixture distribution would be a more appropriate distribution when you’re modeling the data. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_density(color = &quot;black&quot;, fill = &quot;grey&quot;) Warning: Removed 104 rows containing non-finite values (stat_density). In base R, you can get a density plot of a single variable with plot(density(...)). Density plots in SAS 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA WORK.fbitmp; 10 SET classdat.fbi; 11 per_100k = Count/Population*100000; 12 RUN; NOTE: The data set WORK.FBITMP has 0 observations and 8 variables. WARNING: Data set WORK.FBITMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); 15 density per_100k / type = kernel; 16 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 17 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41. Try It Out Read the help files for stat_density and stat_histogram and attempt to create your own histogram/density combination, before looking at the solution below If you’re careful to specify your mappings properly, you can plot a density curve on top of a histogram. Since both geoms compute statistics, you have to read the help files to see that they actually are computing multiple statistics. geom_histogram lists computed variables count, density, ncount, and ndensity. It will be easiest just to use the computed variable density, which we access by putting y = ..density.. in the aesthetic statement for the histogram geom. This moves the histogram to the same scale as the density - both will be scaled to integrate/sum to 1. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_histogram(aes(y = ..density..)) + geom_density(color = &quot;black&quot;, fill = &quot;white&quot;, alpha = .25) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Warning: Removed 104 rows containing non-finite values (stat_bin). Warning: Removed 104 rows containing non-finite values (stat_density). We could similarly tell the density function we want a plot on the count scale, but this is harder to read and compare. Personally, I prefer the first version. poke %&gt;% ggplot(aes(x = catch_rate)) + geom_histogram() + geom_density(aes(y = ..count..), color = &quot;black&quot;, fill = &quot;white&quot;, alpha = .25) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Warning: Removed 104 rows containing non-finite values (stat_bin). Warning: Removed 104 rows containing non-finite values (stat_density). In SAS, this is actually easier than it is in ggplot2: As long as you put the histogram first in the SGPLOT statement, you’ll get the right scale. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 DATA WORK.fbitmp; 10 SET classdat.fbi; 11 per_100k = Count/Population*100000; 12 RUN; NOTE: The data set WORK.FBITMP has 0 observations and 8 variables. WARNING: Data set WORK.FBITMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 14 PROC SGPLOT data=fbitmp(WHERE=(Type = &quot;Robbery&quot;)); 15 histogram per_100k; 16 density per_100k / type = kernel; 17 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,41. Higher Dimensional Histograms and Density Plots You can use density plots and histograms in two dimensions to show the bivariate relationship between two variables, but at that point we have to represent the height of the density or number of points in the bin using another aesthetic (as x and y are taken by the two variables you’re computing a density for). One common aesthetic choice is to map density (or counts) to the fill color, leaving you to imagine a “height” associated with the color. poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_bin2d(aes(fill = ..count..), bins = 18, drop = F) + scale_x_log10() + scale_y_log10() Warning: Removed 1 rows containing non-finite values (stat_bin2d). poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_hex(aes(fill = ..count..), drop = F) + scale_x_log10() + scale_y_log10() Warning: Ignoring unknown parameters: drop Warning: Removed 1 rows containing non-finite values (stat_binhex). poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_density2d_filled() + scale_x_log10() + scale_y_log10() Warning: Removed 1 rows containing non-finite values (stat_density2d_filled). Alternately, it may be preferable to plot only the contour outlines. poke %&gt;% ggplot(aes(x = height_m, y = weight_kg)) + geom_density2d(bins = 15) + scale_x_log10() + scale_y_log10() Warning: Removed 1 rows containing non-finite values (stat_density2d). I found a blog post describing how to compute two-dimensional bins and make a heatmap bin plot in SAS. Of course, joint distributions can also be shown using a scatterplot. Another useful geom when examining distributions is geom_rug, which shows univariate distributions in the margins of the plot. Bar Charts A bar chart is a plot with a categorical variable on one axis and a summary statistic on the other (usually, this is a count). Note that a bar chart is NOT the same as a histogram (a histogram looks very similar, but has a binned numeric variable on one axis and counts on the other). Geometrically, bar charts are rectangles; typically each rectangle will have equal width and variable height. Bar Charts in ggplot2 ggplot(poke, aes(x = generation)) + geom_bar() As with other types of charts, we can add more information by adding aesthetic mappings: poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% ggplot(aes(x = generation, fill = status)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) We can get different visual information from the same data, if we switch which variable is on the axis and which is the fill variable. poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% ggplot(aes(x = status, fill = generation)) + geom_bar(position = &quot;dodge&quot;) We can also get useful information by changing the statistic: what if we use the proportion of responses in each category instead of the raw count? To do this, as far as I know, we have to do at least a little bit of pre-processing, because we need a nested order of grouping factors, and ggplot doesn’t do that very well. poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% group_by(status, generation) %&gt;% count() %&gt;% group_by(status) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes(x = status, fill = generation, weight = prop)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) poke %&gt;% mutate(status = factor( status, levels = c(&quot;Normal&quot;, &quot;Mythical&quot;, &quot;Sub Legendary&quot;, &quot;Legendary&quot;) %&gt;% rev() # ggplot orders things backwards of how you&#39;d want them )) %&gt;% group_by(generation, status) %&gt;% count() %&gt;% group_by(generation) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() %&gt;% ggplot(aes(x = generation, fill = status, weight = prop)) + geom_bar() + # The default colors are awful for colorblind people (like me), so lets fix it scale_fill_brewer(palette = &quot;Paired&quot;) Bar Charts in SAS 6 7 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 7 ! */ 8 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 9 10 PROC SGPLOT data=classdat.poke; 11 VBAR status; 12 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 QUIT; 14 15 PROC SGPLOT data=classdat.poke; 16 HBAR type_1; 17 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPLOT used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,42. Other useful geoms There are a few other geoms which may be useful from time to time, especially when adding additional information to a plot. geom_hline draws horizontal lines geom_vline draws vertical lines geom_abline draws a line in slope-intercept form geom_function draws a function over the domain of the plot geom_smooth draws a smooth line over e.g. a scatterplot. You can use this to fit an implicit linear regression, but by default it uses loess or generalized additive models to produce a more flexible fit. 8.3.4 Small Multiples Sometimes, you want to show separate plots for each level of a factor (or a combination of factors). Often, this happens when you expect the gross shape of the plotted information to change - something relatively obvious. Other times, though, you might just have too much data and need to break it down somehow. These sub-plots within a plot are called small multiples, or facets, or panel plots, depending on the software or discipline. In ggplot2 fbi %&gt;% filter(Abb == &quot;NE&quot;) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_wrap(~Type) Warning: Removed 54 rows containing missing values (position_stack). The options to make panel plots in ggplot2 are facet_wrap, which wraps the specified panels into an overall plot with a reasonable aspect ratio, and facet_grid, which creates a grid of panels. We can control the scales on the panels - in some cases, it is advantageous to have the same scale for all panels, while at other times, such as in the example above, varying the scales at the panel level may allow you to see more detail. fbi %&gt;% filter(Abb == &quot;NE&quot;) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_wrap(~Type, scales = &quot;free_y&quot;) Warning: Removed 54 rows containing missing values (position_stack). Using a grid-style facet, we can make it easy to compare the relative trends in sub-populations. fbi %&gt;% filter(Abb %in% c(&quot;D.C.&quot;, &quot;NY&quot;), Type %in% c(&quot;Burglary&quot;, &quot;Robbery&quot;)) %&gt;% ggplot(aes(x = Year, y = Count/Population)) + geom_col() + facet_grid(Abb~Type) Panel plots in SAS Basically the same syntax as SGPLOT, but using SGPANEL and a PANELBY statement. This is very similar to ggplot2’s facet_ addition. 6 ODS HTML style= HTMLBlue; /* needed for color graphs in bookdown 6 ! */ 7 libname classdat &quot;sas/&quot;; NOTE: Libref CLASSDAT was successfully assigned as follows: Engine: V9 Physical Name: /home/susan/Projects/Class/unl-stat850/stat850-textbook/sas 8 9 PROC SGPANEL data=classdat.poke; 10 PANELBY status; 11 HBAR type_1; 12 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPANEL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 13 QUIT; ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,42. Try it out The classdata package contains the box dataset, which has weekly box office numbers. Conduct an exploratory data analysis of the dataset, using different types of plots appropriately. What is the most interesting aspect of the data you found in your exploration? One possible solution library(classdata) library(lubridate) # Work with dates and times data(box) head(box) Rank Rank.Last.Week Movie Distributor Gross Change 1 1 1 Joker Warner Bros. 55861403 -42 2 2 NA The Addams Family United Artists 30300007 NA 3 3 NA Gemini Man Paramount Pictures 20552372 NA 4 4 2 Abominable Universal 6072235 -49 5 5 3 Downton Abbey Focus Features 4881075 -39 6 6 4 Hustlers STX Entertainment 3887018 -39 Thtrs. Per.Thtr. Total.Gross Week Date 1 4374 12771 193590190 2 2019-10-11 2 4007 7562 30300007 1 2019-10-11 3 3642 5643 20552372 1 2019-10-11 4 3496 1737 47873585 3 2019-10-11 5 3019 1617 82668665 4 2019-10-11 6 2357 1649 98052357 5 2019-10-11 ggplot(box, aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .1) # Ok, that&#39;s weird. Some movie was in theaters for 100,000 weeks? Or re-released? filter(box, Week &gt; 100) %&gt;% group_by(Movie) %&gt;% summarize(max_week = max(Week)) %&gt;% arrange(desc(max_week)) # A tibble: 120 x 2 Movie max_week &lt;chr&gt; &lt;dbl&gt; 1 Ghanchakkar 105065 2 The Captive 5186 3 The Wizard of Oz 3869 4 Detour (1945) (Re-Release) 3812 5 Le Corbeau (1943) (Re-Release) 3681 6 The Fallen Idol 3497 7 Monsieur Verdoux 3456 8 The Third Man 3434 9 Olivia 3415 10 Little Fugitive 3138 # … with 110 more rows box %&gt;% filter(Week &lt; 100) %&gt;% ggplot(aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .01) + scale_y_log10() Warning in self$trans$transform(x): NaNs produced Warning: Transformation introduced infinite values in continuous y-axis Warning: Removed 2 row(s) containing missing values (geom_path). To me, this looks like there are two main groups of movies - the less common movies that are hugely grossing initially, but decline relatively quickly in weekly gross, and the more common movies that show a gradual decline until they are removed from theaters. Most movies don’t seem to go beyond 25 weeks. # Still kind of a mess. Let&#39;s facet by year of initial release movie_summary &lt;- box %&gt;% group_by(Movie) %&gt;% summarize(release_year = min(year(Date)), weeks = max(Week), total.gross = Total.Gross[Week == max(Week)], .groups = &quot;drop_last&quot;) box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Week &lt; 25) %&gt;% ggplot(aes(x = Week, y = Gross, group = Movie)) + geom_line(alpha = .05) + facet_wrap(~release_year) + scale_y_log10() Joining, by = &quot;Movie&quot; Warning in self$trans$transform(x): NaNs produced Warning: Transformation introduced infinite values in continuous y-axis Warning: Removed 2 row(s) containing missing values (geom_path). This pattern is relatively consistent year after year, but some years have many more high-grossing movies. Let’s look at the relationship between last week’s rank and this week’s rank. box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Week &lt; 25, Week &gt; 1) %&gt;% ggplot(aes(x = Rank.Last.Week, y = Rank, group = Movie)) + geom_point(alpha = .05) + scale_y_reverse() + facet_wrap(~Week) Joining, by = &quot;Movie&quot; Warning: Removed 1979 rows containing missing values (geom_point). It’s very strong, as expected, but this is extremely overplotted, so we might be missing some details. Let’s look at the 2d density to see if we get anything interesting. box %&gt;% filter(Week &lt; 15, Week &gt; 1) %&gt;% ggplot(aes(x = Rank.Last.Week, y = Rank)) + geom_density2d_filled() + scale_y_reverse() + facet_wrap(~Week) Warning: Removed 1586 rows containing non-finite values (stat_density2d_filled). As we go week by week, we see the mode of the distribution shift down in rank. By week 9, it’s extremely uncommon for a movie to be in the top 10. Next, lets look at the relationship between the number of theaters showing a movie and the weekly gross receipts. box %&gt;% left_join(select(movie_summary, Movie, release_year)) %&gt;% filter(Rank &lt;= 25) %&gt;% ggplot(aes(x = Thtrs., y = Gross)) + geom_density2d_filled() + scale_y_log10() Joining, by = &quot;Movie&quot; It would be interesting to see if this relationship is the same for all distributors. distributors &lt;- box %&gt;% group_by(Distributor, Movie) %&gt;% filter(Week == max(Week)) %&gt;% group_by(Distributor) %&gt;% summarize(n = n(), median.gross = median(Total.Gross)) %&gt;% arrange(desc(median.gross)) head(distributors, 10) # A tibble: 10 x 3 Distributor n median.gross &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 Walt Disney 76 185808558. 2 Columbia 1 169077585 3 Universal 118 66197380 4 20th Century Fox 105 65007045 5 Warner Bros. 150 53930748. 6 Paramount Pictures 81 52822418 7 Sony Pictures 130 37709109 8 MGM 4 34693428 9 Focus / Gramercy 1 26583369 10 IMAX Films 4 26349032 distributors %&gt;% filter(n &gt;= 25) %&gt;% # want distributors with at least 25 movies slice(1:10) %&gt;% # keep top 10 rows left_join(box) %&gt;% mutate(Distributor = factor(Distributor) %&gt;% reorder(-median.gross)) %&gt;% filter(Week &lt;= 10) %&gt;% # Only look at the first 10 weeks of a movie ggplot(aes(x = Thtrs., y = Gross)) + geom_density2d_filled(contour_var = &quot;ndensity&quot;) + scale_y_log10() + facet_wrap(~Distributor) Joining, by = &quot;Distributor&quot; It’s rather obvious that Disney is the real outlier here - it has movies that are extremely widely released and very high grossing, and a limited-release movie is a relatively rare thing. Compare this to e.g. Lionsgate, where most movies are limited to a few theaters and it’s relatively rare to have a widely released movie. 8.4 Good charts Earlier, I mentioned that we’ve known pie charts suck for 100 years. But what makes a chart good? And how do we tell? This is my area of research, so I’m going to try to keep this to a broad overview, but I may not succeed. Sorry in advance. A chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate. Unfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms. 8.4.1 Perceptual and Cognitive Factors 8.4.1.1 Color Our eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum. Sensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons) Not everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances. You can take a test designed to screen for colorblindness here Your monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here. In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown. It is possible to simulate the effect of color blindness and color deficiency on an image. In addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context. Figure 8.1: The color constancy illusion. The squares marked A and B are actually the same color Figure 8.2: The color constancy illusion. The squares marked A and B are actually the same color Our brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade. Implications and Guidelines Do not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance. Avoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people. To “colorblind-proof” a graphic, you can use a couple of strategies: double encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out If you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it! If you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use. If you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly. Be conscious of what certain colors “mean” Leveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -&gt; blue gradients for showing rainfall totals) Some colors can can provoke emotional responses that may not be desirable.31 It is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women32. There are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes. 8.4.1.2 Short Term Memory We have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time. Without rehearsing information, short term memory lasts a few seconds. Try it out Click here, read the information, and then click to hide it. 1 4 2 2 3 9 8 0 7 8 Wait a few seconds, then expand this section What was the third number? Without rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information. In charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart. Where possible, try to keep your legends to 6 or 7 characteristics. Implications and Guidelines Limit the number of categories in your legends to minimize the short term memory demands on your reader. When using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude. Use colors and symbols which have implicit meaning to minimize the need to refer to the legend. Add annotations on the plot, where possible, to reduce the need to re-read captions. 8.4.1.3 Grouping and Sense-making Imposing order on visual chaos. What does the figure below look like to you? When faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options. Consider this image - what do you see? Did you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that?” In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience. Now, look at the logo for the Pittsburgh Zoo. Do you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background. The ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs. The catchphrase of Gestalt psychology is The whole is greater than the sum of the parts That is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene. The Gestalt Heuristics help us to impose order on ambiguous visual stimuli You can read about the gestalt rules here, but they are also demonstrated in the figure above. In graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders. Example: Suppose I want to emphasize the change in the murder rate between 1980 and 2010. I could use a bar chart fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) + geom_col(position = &quot;dodge&quot;) + coord_flip() + ylab(&quot;Murders per 100,000 residents&quot;) Or, I could use a line chart fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) + geom_line() + ylab(&quot;Murders per 100,000 residents&quot;) Or, I could use a box plot fbiwide %&gt;% filter(Year %in% c(1980, 2010)) %&gt;% ggplot(aes(x = factor(Year), y = Murder/Population*100000)) + geom_boxplot() + ylab(&quot;Murders per 100,000 residents&quot;) Which one best demonstrates that in every state and region, the murder rate decreeased? The line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships. The aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.33 8.4.2 General guidelines for accuracy There are certain tasks which are easier for us relative to other, similar tasks. Figure 8.3: Which of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels. When making judgements corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.34 Position (common scale) Position (non-aligned scale) Length, Direction, Angle, Slope Area Volume, Density, Curvature Shading, Color Saturation, Color Hue If we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts. When creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc. There is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area. . Ted ED: How to spot a misleading graph - Lea Gaslowitz Business Insider: The Worst Graphs Ever Extra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration). 8.4.3 How do we know? We do experiments on people (evil cackle)! No, seriously, we do. See this paper for a review of graphical testing in statistics, and this paper for one example of how we test competing designs to figure out that polar coordinates make everything harder. If you’re still curious after reading those, set up an appointment and let’s talk!. 8.5 Other things worth exploring (and future expansions for this chapter) The patchwork package lets you arrange ggplots (image by Allison Horst) (so do the gridExtra and cowplot packages) The rayshader package lets you create 3d maps and graphs (image by Allison Horst) People have made some very cool visualizations using rayrender: bending space time volcano topology (with map overlay) A super suspenseful animation whose only purpose is demonstrating solid object rendering 3D maps (which you can create STL files from and 3D print…) Rendering a table, on a laptop, on a table References 8.5.1 Motivation If you think this all sounds complicated, read this blog post about saving graphics from Excel. 8.5.2 R graphics ggplot2 cheat sheet ggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet ggplot2 reference guide R graph cookbook Data Visualization in R (@ramnathv) Base R plots - Notes by Dr. Bilder Combine multiple plots with the cowplot and gridExtra packages 8.5.3 SAS graphics SGPLOT cheat sheet Replicating ggplot2 in SAS SGPLOT ODS Graphics in SAS Periodic Table of SAS ODS graphics (code and actual graphic) Holiday Graphics in SAS, and, on that same theme, Captain America in SAS 8.5.4 Types of Charts and Chart Styling The Data Vis Project Data Visualization Catalogue The pros and cons of chart taxonomies Data Visualization Style Guidelines 8.5.5 Other Graphics packages plotnine in python (ggplot2 clone) matplotlib in python Tableau - student licenses are free d3 - javascript graphics I’ll fully admit my bias here - I think ODS graphics are better than the default SAS graphics, but I still prefer the syntax and logic behind ggplot2. But, if you prefer SAS Graphics, you do you. Better you than me, is all I’m saying.↩︎ It’s not often you’ll find me approving of SAS graphics, but making it hard to make pie charts is definitely a point in SAS’s favor↩︎ This can sometimes be a pain, though, depending on the set up, because you may end up with labels that are repeated many, many times. As with any system, you just have to make sure you’re formatting your data consistent with the underlying philosophy.↩︎ I’ve driven through enough small towns in the South, and I have relatives in NW Iowa. There are a LOT of churches in those places↩︎ When the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information.↩︎ Lisa Charlotte Rost. What to consider when choosing colors for data visualization.↩︎ See this paper for more details. This is the last chapter of my dissertation, for what it’s worth. It was a lot of fun. (no sarcasm, seriously, it was fun!)↩︎ See this paper for the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.↩︎ "],["simulation.html", "Module 9 Simulation and Reproducibility Simulation: Module Objectives 9.1 Pseudorandom Number Generation 9.2 Built-in simulations from distributions 9.3 Simulation to test model assumptions 9.4 Monte Carlo methods 9.5 References", " Module 9 Simulation and Reproducibility Simulation: Module Objectives Understand how pseudorandom number generation works and necessary conditions for reproducibility Be able to implement a simulation for a specific task, process, or model Simulation is an extremely important part of computational statistics. Bayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models. In this module, we’re going to touch on a few foundational pieces of simulation in computing, and you will get more exposure to simulation-based methods in other courses down the line. 9.1 Pseudorandom Number Generation Computers are almost entirely deterministic, which makes it very difficult to come up with “random” numbers. In addition to the deterministic nature of computing, it’s also somewhat important to be able to run the same code and get the same results every time, which isn’t possible if you rely on truly random numbers. Historically, pseudorandom numbers were generated using linear congruential generators (LCGs). These algorithms aren’t typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem “random” but are actually deterministic. LCGs use modular arithmetic: \\(X_{n+1} = (aX_n + c) \\mod m\\) where \\(X_0\\) is the start value (the seed), \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. When using a LCG, the user generally specifies only the seed. LCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia) The important thing to note here is that if you specify the same generator values (a, c, m, and \\(X_0\\)), you will always get the same series of numbers. Since a, c, m are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time. It is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.35 Once you set your seed, the remaining results will only be reproducible if you generate the same set of random numbers every time. set.seed(342512) # Get 10 numbers after the seed is set sample(1:100, 10) [1] 65 51 64 21 45 53 3 6 43 8 # Compute something else that depends on randomness mean(rnorm(50)) [1] -0.1095366 # Get 10 more numbers sample(1:100, 10) [1] 4 57 69 10 76 15 67 1 3 91 Compare the results above to these results: set.seed(342512) # Get 10 numbers after the seed is set sample(1:100, 10) [1] 65 51 64 21 45 53 3 6 43 8 # Compute something else that depends on randomness mean(rnorm(30)) [1] -0.1936645 # Get 10 more numbers sample(1:100, 10) [1] 49 37 6 34 9 3 100 43 7 29 Notice how the results have changed? To make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I’ve already set the seed earlier. This introduces certain “fixed points” where results won’t change immediately after I’ve re-set the seed. This is particularly important when I’m generating bootstrap estimates, fitting models, or simulating data for graphics experiments. Pick your seed in any way you want. I tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people’s birthdays, the current time in hhmmss format… basically, you can use anything. 9.2 Built-in simulations from distributions Often, we can get away with just simulating data from a known distribution. As both R and SAS are meant for statistical computing, this is extremely easy by design. In R You can see the various distribution options using ?Distributions. In general, dxxx is the PDF/PMF, pxxx is the CDF, qxxx is the quantile function, and rxxx gives you random nubmers generated from the distribution. (xxx, obviously, is whatever distribution you’re looking to use.) library(tibble) library(dplyr) library(tidyr) library(ggplot2) set.seed(109025879) tibble( norm = rnorm(500), gamma = rgamma(500, shape = 3, scale = 1), exp = rexp(500, rate = 1), # R uses a exp(-ax) t = rt(500, df = 5), chisq = rchisq(500, 5) ) %&gt;% pivot_longer(1:5, names_to = &quot;dist&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = &quot;free&quot;, nrow = 1) In SAS You can see the various distribution options in the RAND documentation. 6 %let N=500; /* size of sample */ 7 8 DATA sample; 9 call streaminit(12532); 10 DO i = 1 to &amp;N; /* &amp;N is the value of the macro variable defined 10 ! above */ 11 id = i; 12 norm = rand(&quot;Normal&quot;, 0, 1); 13 gamma = rand(&quot;Gamma&quot;, 3, 1); 14 exp = rand(&quot;Exponential&quot;, 1); /* SAS uses 1/a exp(-x/a) */ 15 t = rand(&quot;T&quot;, 5); 16 chisq = rand(&quot;Chisq&quot;, 5); 17 OUTPUT; 18 END; 19 RUN; NOTE: The data set WORK.SAMPLE has 0 observations and 7 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.01 seconds 20 21 PROC TRANSPOSE data=sample out=longsample 22 (rename=(COL1 = value)) /* rename output variable 22 ! (&#39;values_to&#39;) */ 23 NAME = dist /* where the column names go (&#39;names_to&#39;) */ 24 ; 25 BY id; 26 VAR norm gamma exp t chisq; 27 RUN; WARNING: The variable COL1 in the DROP, KEEP, or RENAME list has never been referenced. NOTE: There were 0 observations read from the data set WORK.SAMPLE. NOTE: The data set WORK.LONGSAMPLE has 5 observations and 2 variables. NOTE: PROCEDURE TRANSPOSE used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 28 29 PROC SGPANEL data=longsample; 30 PANELBY dist / COLUMNS = 5 UNISCALE = ROW NOVARNAME; 31 DENSITY value / TYPE = KERNEL; ERROR: Variable VALUE not found. 32 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE SGPANEL used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,42. Try it out Generate variables x and y, where x is a sequence from -10 to 10 and y is equal to \\(x + \\epsilon\\), \\(\\epsilon \\sim N(0, 1)\\). Fit a linear regression to your simulated data (in R, lm, in SAS, PROC REG). In R set.seed(20572983) data &lt;- tibble(x = seq(-10, 10, .1), y = x + rnorm(length(x))) regression &lt;- lm(y ~ x, data = data) summary(regression) Call: lm(formula = y ~ x, data = data) Residuals: Min 1Q Median 3Q Max -3.14575 -0.70986 0.03186 0.65429 2.40305 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.01876 0.06869 -0.273 0.785 x 0.99230 0.01184 83.823 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9738 on 199 degrees of freedom Multiple R-squared: 0.9725, Adjusted R-squared: 0.9723 F-statistic: 7026 on 1 and 199 DF, p-value: &lt; 2.2e-16 In SAS 6 DATA tmp; 7 call streaminit(20572983); 8 DO i = -10 to 10 by .1; 9 x = i; 10 y = x + rand(&quot;Normal&quot;); 11 OUTPUT; 12 END; 13 RUN; NOTE: The data set WORK.TMP has 0 observations and 3 variables. WARNING: Data set WORK.TMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 15 PROC REG data = tmp; 16 MODEL y = x; ERROR: Variable Y not found. 17 RUN; WARNING: RUN statement ignored due to previous errors. Submit QUIT; to terminate the procedure. NOTE: PROCEDURE REG used (Total process time): real time 0.00 seconds cpu time 0.00 seconds NOTE: The SAS System stopped processing this step because of errors. ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,43. 9.3 Simulation to test model assumptions One of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model. Suppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors. How bad will your results be? Where can you trust the results, and where should you be cautious? Example: Confidence Interval coverage rates Suppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our data. set.seed(40295023) sim &lt;- tibble( id = rep(1:100, each = 25), # generate 100 samples of 25 points each ln_x = rnorm(25*100), # generate the normal deviates x = exp(ln_x), # transform into lognormal deviates ) %&gt;% # this creates a 100-row data frame, with one row for each id. # the columns x, ln_x are stored in the data list-column as a tibble. nest(data = c(x, ln_x)) head(sim) # A tibble: 6 x 2 id data &lt;int&gt; &lt;list&gt; 1 1 &lt;tibble[,2] [25 × 2]&gt; 2 2 &lt;tibble[,2] [25 × 2]&gt; 3 3 &lt;tibble[,2] [25 × 2]&gt; 4 4 &lt;tibble[,2] [25 × 2]&gt; 5 5 &lt;tibble[,2] [25 × 2]&gt; 6 6 &lt;tibble[,2] [25 × 2]&gt; sim$data[[1]] # A tibble: 25 x 2 x ln_x &lt;dbl&gt; &lt;dbl&gt; 1 0.310 -1.17 2 0.622 -0.475 3 0.303 -1.19 4 1.05 0.0525 5 0.529 -0.636 6 1.09 0.0891 7 1.97 0.676 8 8.94 2.19 9 0.598 -0.514 10 0.183 -1.70 # … with 15 more rows You want to assess the coverage probability of a confidence interval computed under two different modeling scenarios: Working with the log-transformed values, ln(x), and then transform the computed interval back Working with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal. Under scenario 1, our theoretical interval should be exp((-1.96/5, 1.96/5)) (because \\(\\mu\\) is 0, and \\(\\sigma\\) is 1, so \\(SE(\\overline x) = 1/\\sqrt{25} = 1/5\\)). \\((0.6757041,1.4799377)\\) Under scenario 2, the expected value of the lognormal distribution is \\(\\exp(1/2) = 1.6487213\\), the variance is \\((\\exp(1) - 1)(\\exp(1)) = 4.6707743\\) and our theoretical interval should be \\((0.8015319, 2.4959107)\\). This interval contains 0, which is implausible for lognormally distributed data. Our expected values are different under scenario 1 and scenario 2: in scenario 1 we are computing an interval for \\(\\mu\\), in scenario 2, we are computing an interval for the population mean, which is \\(\\exp(\\mu + .5\\sigma^2)\\). Both are valid quantities we might be interested in, but they do not mean the same thing. The purrr::map notation specifies that we’re using the map function from the purrr package. When functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function. purrr::map takes an argument and for each “group” calls the compute_interval function, storing the results in res. So each row in res is a 1x2 tibble with columns lb and ub. This pattern is very useful in all sorts of applications. I wish we had time to cover purrr explicitly, but I at least want to expose you to how clean it makes your code. compute_interval &lt;- function(x) { s1 &lt;- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x))) s2 &lt;- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x)) tibble(scenario = c(&quot;scenario_1&quot;, &quot;scenario_2&quot;), mean = c(1, exp(1/2)), lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]), in_interval = (lb &lt; mean) &amp; (ub &gt; mean)) } sim_long &lt;- sim %&gt;% # This line takes each data entry and computes an interval for x. # .$x is code for take the argument you passed in to map and get the x column mutate(res = purrr::map(data, ~compute_interval(.$x))) %&gt;% # this &quot;frees&quot; res and we end up with two columns: lb and ub, for each scenario unnest(res) ci_df &lt;- tibble(scenario = c(&quot;scenario_1&quot;, &quot;scenario_2&quot;), mu = c(1, exp(1/2)), lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5), ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5)) ggplot() + geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), data = ci_df, fill = &quot;grey&quot;, alpha = .5, color = NA) + geom_vline(aes(xintercept = mu), data = ci_df) + geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval), data = sim_long) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) + theme_bw() + facet_wrap(~scenario) From this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain. Example: Regression and high-leverage points What happens if we have one high-leverage point (e.g. a point which is an outlier in both x and y)? How pathological do our regression coefficient estimates get? The challenging part here is to design a data generating mechanism. gen_data &lt;- function(n = 30, o = 1, error_sd = 2) { # generate the main part of the regression data data &lt;- tibble(x = rnorm(n = n - o, mean = seq(-10, 10, length.out = n - o), sd = .1), y = x + rnorm(length(x), mean = 0, sd = error_sd)) # generate the outlier - make it at ~(-10, 5) outdata &lt;- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd)) bind_rows(data, outdata) } sim_data &lt;- tibble( id = 1:300, o = rep(0:2, each = 100), # call gen_data for each row in sim_data, but don&#39;t really use id as a parameter. data = purrr::map(o, ~gen_data(o = .)) ) head(sim_data) # A tibble: 6 x 3 id o data &lt;int&gt; &lt;int&gt; &lt;list&gt; 1 1 0 &lt;tibble[,2] [30 × 2]&gt; 2 2 0 &lt;tibble[,2] [30 × 2]&gt; 3 3 0 &lt;tibble[,2] [30 × 2]&gt; 4 4 0 &lt;tibble[,2] [30 × 2]&gt; 5 5 0 &lt;tibble[,2] [30 × 2]&gt; 6 6 0 &lt;tibble[,2] [30 × 2]&gt; # plot a few datasets just to check they look like we expect: sim_data %&gt;% filter(id %% 100 &lt; 3) %&gt;% unnest(data) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + facet_grid(id %% 100 ~ o ) library(broom) # the broom package cleans up model objects to tidy form sim_data &lt;- sim_data %&gt;% # fit linear regression mutate(model = purrr::map(data, ~lm(y ~ x, data = .))) %&gt;% mutate(tidy_model = purrr::map(model, tidy)) # Get the coefficients out tidy_coefs &lt;- select(sim_data, id, o, tidy_model) %&gt;% unnest(tidy_model) %&gt;% mutate(group = case_when(o == 0 ~ &quot;No HLPs&quot;, o == 1 ~ &quot;1 HLP&quot;, o == 2 ~ &quot;2 HLPs&quot;) %&gt;% factor(levels = c(&quot;No HLPs&quot;, &quot;1 HLP&quot;, &quot;2 HLPs&quot;))) ggplot(tidy_coefs, aes(x = estimate, color = group)) + facet_grid(term ~ .) + geom_density() Obviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0). Here is a similar example worked through in SAS with IML. Note the use of BY-group processing to analyze each group at once - this is very similar to the use of purrr::map() in the R code. Try it out Let’s explore what happens to estimates when certain observations are censored. Suppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102\\(^\\circ F\\); for these temperatures, the thermometer will record a value of 102.0. It is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness. Given that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer’s censoring. Hint If most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution? What if you assume that 101 and 102.5 are the 2SD bounds? Solution If 101 and 102.5 are the anchor points we have, let’s assume that 95% of normal pet temperatures fall in that range. So our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375. We can simulate 1000 observations from \\(N(101.75, 0.375)\\), create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are. 6 DATA dogtemp; 7 call streaminit(20572983); 8 DO i = 1 to 1000; 9 actual = rand(&quot;Normal&quot;, 101.75, 0.375); 10 IF actual &gt; 102 THEN read = 102; 11 IF actual &lt;= 102 THEN read = actual; 12 OUTPUT; 13 END; 14 RUN; NOTE: The data set WORK.DOGTEMP has 0 observations and 3 variables. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 15 16 PROC MEANS DATA = dogtemp; 17 VAR actual read; 18 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,43. set.seed(204209527) dogtemp &lt;- tibble( actual = rnorm(1000, 101.75, 0.375), read = pmin(actual, 102) ) dogtemp %&gt;% summarize_all(mean) # A tibble: 1 x 2 actual read &lt;dbl&gt; &lt;dbl&gt; 1 102. 102. The effect of the thermometer’s censoring in both cases is around 0.06 degrees F. 9.4 Monte Carlo methods Monte carlo methods are methods which rely on repeated random sampling in order to solve numerical problems. Often, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically. In general, a MC problem involves these steps: Define the input domain Generate inputs randomly from an appropriate probability distribution Perform a computation using those inputs Aggregate the results. Let’s try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1. More precisely, if \\(u_i \\sim U(0,1)\\), where _{i=1}^k u_i &gt; 1, what is the expected value of \\(k\\)? In this simulation, our input domain is [0,1]. Our input is \\(u_i \\sim U(0,1)\\) We generate new \\(u_i\\) until \\(\\sum_{i=1}^k &gt; 1\\) and save the value of \\(k\\) We average the result of \\(N\\) such simulations. # It&#39;s easier to think through the code if we write it inefficiently first sim_fcn &lt;- function() { usum &lt;- 0 k &lt;- 0 # prevent infinite loops by monitoring the value of k as well while (usum &lt; 1 &amp; k &lt; 15) { usum &lt;- runif(1) + usum k &lt;- k + 1 } return(k) } set.seed(302497852) res &lt;- tibble(k = replicate(1000, sim_fcn(), simplify = T)) mean(res$k) [1] 2.717 If we want to see whether the result converges to something, we can increase the number of trials we run: set.seed(20417023) sim_res &lt;- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) sim_res &lt;- sim_res %&gt;% mutate(running_avg_est = cummean(samp), N = row_number()) ggplot(aes(x = N, y = running_avg_est), data = sim_res) + geom_hline(yintercept = exp(1), color = &quot;red&quot;) + geom_line() The expected number of uniform RV draws required to sum to 1 is \\(e\\)! Explanation of why this works Monte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods). Suppose you want to integrate \\[\\int_0^1 e^{-x^3}dx\\] You could set up Riemann integration and evaluate the integral using a sum over \\(K\\) points, but that approach only converges for smooth functions (and besides, that’s boring calc 2 stuff, right?). Instead, let’s observe that this is equivalent to \\(\\int_0^1 e^{-x^3}\\cdot 1 dx\\), where \\(p(x) = 1\\) for a uniform random variable. That is, this integral can be written as the expected value of the function over the interval \\([0,1]\\). What if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at that point, and average the result? Implementation set.seed(20491720) fn &lt;- function(x) exp(-x^3) sim_data &lt;- tibble(x = runif(100000), y = fn(x)) mean(sim_data$y) [1] 0.8076082 6 DATA tmp; 7 CALL streaminit(20283492); 8 DO i = 1 to 100000; 9 x = RAND(&quot;Uniform&quot;, 0, 1); 10 y = EXP(-x**3); 11 OUTPUT; 12 END; 13 RUN; NOTE: The data set WORK.TMP has 0 observations and 3 variables. WARNING: Data set WORK.TMP was not replaced because this step was stopped. NOTE: DATA statement used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 14 15 PROC MEANS data=tmp; 16 VAR y; ERROR: Variable Y not found. 17 RUN; NOTE: The SAS System stopped processing this step because of errors. NOTE: PROCEDURE MEANS used (Total process time): real time 0.00 seconds cpu time 0.00 seconds 18 ERROR: Errors printed on pages 4,5,7,11,12,13,16,17,18,19,24,25,27,28,29, 30,33,34,35,36,37,38,43. You can use the law of large numbers to prove that this approach will converge. Example stolen from this set of lecture notes Try it out Buffon’s needle is a mathematical problem which can be boiled down to a simple physical simulation. Read this science friday description of the problem and develop a monte carlo simulation method which estimates \\(\\pi\\) using the Buffon’s needle method. Your method should be a function which allows the user to specify how many sticks are dropped plots the result of the physical simulation prints out a numerical estimate of pi. Solution Let’s start out with horizontal lines at 0 and 1, and set our stick length to 1. We need to randomly generate a position (of one end of the stick) and an angle. The position in \\(x\\) doesn’t actually make much of a difference (since what we care about is the \\(y\\) coordinates), but we can draw a picture if we generate \\(x\\) as well. needle_sim &lt;- function(sticks = 100) { df &lt;- tibble(xstart = runif(sticks, 0, 10), ystart = runif(sticks, 0, 1), angle = runif(sticks, 0, 360), xend = xstart + cos(angle/180*pi), yend = ystart + sin(angle/180*pi) ) %&gt;% # We can see if a stick crosses a line if the floor() function of ystart is # different than floor(yend). Note this only works for integer line values... mutate(crosses_line = floor(ystart) != floor(yend)) gg &lt;- ggplot() + geom_hline(yintercept = c(0, 1)) + geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend, color = crosses_line), data = df) + coord_fixed() return(list(est = 2 * sticks/ sum(df$crosses_line), plot = gg)) } needle_sim(10) $est [1] 2.8571429 $plot needle_sim(100) $est [1] 2.8985507 $plot needle_sim(1000) $est [1] 3.1298905 $plot needle_sim(10000) $est [1] 3.1235358 $plot This blog post contains code for a SAS implementation 9.5 References Simulation (R programming for Data Science chapter) Simulation - R Studio lesson Simulation, focusing on statistical modeling (R) Simulating Data with SAS (Excerpt) Simulating a Drunkard’s Walk in 2D in SAS Simulation from a triangle distribution (SAS) Simulating the Monty Hall problem (SAS) When to use purrr (part of the ‘teaching the tidyverse’ series) - essentially, purrr is a great intro to functional programming, but there are other ways to solve iterative problems in R as well, and some of them are easier than purrr (but purrr is a general approach that is very powerful). I once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds). We got brilliant results, but couldn’t ever reproduce them, because I hadn’t set the seed first and we never could figure out what the original seed was. Learn from my mistakes.↩︎ "],["docs-reports.html", "Module 10 Documents and Reports Documents and Reports: Module Objectives 10.1 Literate Programming, knitr, and rmarkdown 10.2 A Very Brief Introduction to LaTeX 10.3 Slides 10.4 Posters 10.5 Resume/CV 10.6 Using Github Pages 10.7 References", " Module 10 Documents and Reports Documents and Reports: Module Objectives Be able to create presentation slides in LaTeX and rmarkdown posters in LaTeX and rmarkdown A CV in LaTeX and/or rmarkdown This chapter will be shorter in length than many of the rest, but you should not devote less time to it. Instead, you should spend the time playing with the different options presented here and deciding which one of each is your favorite. Rather than detailing all of the customization options in each package, I think you’ll have an easier time looking at examples, trying to customize them yourself to get the effect you want, and figuring out how to do that by reading the documentation, stackoverflow posts, and other help files – those are the skills you’ll need when you try to put this knowledge into action. At the end of this chapter there are a few extras – for instance, how to use GitHub to host your documents, how to create a blog with blogdown, and more. You should feel free to investigate, but as long as you are able to create presentation slides, posters, and a CV, you’re good to go. Reproducibility with Rmarkdown (by Allison Horst) 10.1 Literate Programming, knitr, and rmarkdown Literate programming is a programming method where you explain the code in natural language (e.g. English) in roughly the same space that you write the code (in a programming language). This solves two problems: code isn’t always clear as to what its goals are, and natural language descriptions of algorithms aren’t always clear enough to contain the details of how something is actually implemented. The knitr and Rmarkdown packages are both implementations of literate programming (and the two packages tend to overlap a bit, because both were written by the same author, Yihui Xie). knitr is primarily focused on the creation of Rnw (r no weave) files, which are essentially LaTeX files with R code inside. Rnw files are compiled into pdfs. rmarkdown uses Rmd or Rmarkdown files, which can then be compiled into many different formats: pdf, html, markdown, Microsoft Word. One major advantage of knitr and Rmarkdown from a practical perspective is that it largely removes the need to keep track of graphs and charts when you’re writing a paper, making a presentation, etc. The charts and tables based on your method automatically update when the document is recompiled. If you’re not reading this chapter early, you’ve been using Rmarkdown for the entire semester to submit your homework. Hopefully that’s been fairly easy - you’ve been creating Rmarkdown documents all semester. In this chapter, we’re going to explore some other applications of literate programming: creating slides, posters, and more. 10.2 A Very Brief Introduction to LaTeX LaTeX is a document preparation utility that attempts to take the focus off of layout (so you don’t have to spend 30 minutes trying to get the page break in the right place in e.g. Word) and bibliographic details. I’m not convinced LaTeX succeeds at freeing you from layout concerns, but it’s certainly true that it is much more powerful than Word for layout purposes. The philosophy of LaTeX is that presentation shouldn’t get in the way of content: you should be able to change the presentation formatting systematically, without having to mess with the content. This allows you to switch templates easily, make document-wide changes in a single command, and more. In Rstudio, copy the text in the document below, paste it into a text file in the editor window, and name it test.tex. You should see a Compile PDF button show up at the top of the document. Click that button to compile the document. \\documentclass{article} % this tells LaTeX what type of document to make % Note, comments are prefaced by a % sign. If you need to type the actual symbol % you will have to escape it with \\%. \\begin{document} Hello \\LaTeX! \\end{document} Most commonly, you’ll use the article document class for papers, and beamer for presentations and posters. Other useful classes include moderncv (for CVs) and book, as well as the LaTeX class maintained by the UNL math department for thesis formatting. Note that by changing the extension of any .tex file to .Rnw, you can easily add R code chunks to a LaTeX file. There are several types of latex commands: Declarations: statements like \\documentclass, \\usepackage or \\small, which are stated once and take effect until further notice. Environments: statements with matching \\begin{xxx} and \\end{xxx} clauses that define a block of the document which is treated differently. Common environments include figures and tables. Special characters: another type of command that don’t define formatting or structure, but may print special characters, e.g. \\% to print a literal % character. Both declarations and environments may come with both optional and required arguments. Required arguments are placed in {...} brackets, while optional arguments are placed in [...] brackets. You can, for instance, start your document with \\documentclass[12pt]{article} to specify the base font size. One of the most useful features in LaTeX is math mode, which you can enter by enclosing text in $...$ (for inline statements), $$...$$ (for statements on their own line), or using other environments like \\begin{array}...\\end{array} that come in math-specific packages. Once in math mode, you can use math symbol commands to get characters like \\(\\theta, \\pi, \\sum, \\int, \\infty\\), and more. Try it out With any document creation software, the easiest way to learn how to do it is to find a sample document, tinker with it, see if you can make things the way you want them to be, and then google the errors when you inevitably screw something up. Take the sample document up above and see if you can do the following tasks: (I’ve linked to documentation that may be useful) Add an image Add the quadratic formula and the PDF of a normal distribution to the document In extremely large text, print LaTeX using the \\LaTeX command In extremely small, italic text, print your name Solution \\documentclass{article} % this tells LaTeX what type of document to make % Add the graphicx package so that we can include images \\usepackage{graphicx} \\begin{document} Hello \\LaTeX! % Include a figure \\begin{figure}[h] \\centering \\includegraphics[width=.5\\textwidth]{../image/IllusoryContour.png} \\caption{Illusory contour image} \\end{figure} % Add the quadratic formula and the normal PDF to the document $y = ax^2 + bx + c$ can be solved to get $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$ The PDF of a normal distribution is $$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$ % In extremely large text, print \\LaTeX \\Huge\\LaTeX % In extremely small italic text, print your name \\tiny\\emph{Your name} \\end{document} You can see the compiled pdf here. 10.2.1 Knitr R code chunks are embedded in LaTeX documents using: % start of chunk &lt;&lt;chunk-name, ...options...&gt;&gt;= @ % end of chunk You can embed numerical results inline using \\Sexpr{...} where your R code goes in the .... Unfortunately, knitr does not work with SAS… for that, you’ll need Rmarkdown (or you can use a Jupyter notebook). How this works To compile a Rnw document, knitr first runs all of the R code, generating any figures or tables or text output for each chunk. For each chunk, knitr replaces the chunk code with LaTeX code to include the results; the result of this operation is saved to a tex file. Once the tex file is created, knitr compiles the tex file into a pdf. 10.3 Slides 10.3.1 Beamer (LaTeX) and knitr Beamer is a powerful LaTeX class which allows you to create slides. The only change necessary to turn a beamer slide deck into a knitr slide deck is to add fragile as an option to any slide with verbatim content. You can also create Beamer slides with Rmarkdown. Example presentation. Standard tradeoffs (formatting details vs. document complexity) apply. Try it out Download and compile beamer-demo.Rnw. What happens when you remove the [fragile] from each frame declaration? Can you change the theme of the presentation? Add another slide, and on that slide, show an appropriate style ggplot2 graph of the distribution of board game ratings, reading in the board game ratings using the following code: board_games &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv&quot;) Karl Broman has a set of slides that show how to use beamer + knitr to make reproducible slides with notes. You can also create Beamer slides using Rmarkdown, if you want, but you’ll probably have more control over the fine details if you go straight to the Rnw file without going through Rmd first. It’s a trade-off – the file will probably be simpler in Rmarkdown, but you won’t have nearly as much control. 10.3.2 HTML slides RStudio has a host of other options for html slide presentations. There are some definite advantages to HTML presentations: they’re easy to share (via URL), you can add gifs, emojis, and interactive graphics, and you can set up github to host the presentations as well36. The downside to HTML slides is that there are approximately 100000 different javascript libraries that create HTML slides, and all of them have different capabilities. Many of these libraries have R extensions that will let you create Rmarkdown slides, but they each have slightly different markdown syntax and capabilities. Slide options available by default in RStudio You can get the full details of any fully supported slide class in Rmarkdown by looking at the Rmarkdown book, which is freely available online. These guidelines will give you specifics about how to customize slides, add incremental information, change transitions, print your slides to PDF, and include speaker notes. It should be relatively straightforward to create an ioslides or slidy presentation, given that you’ve been using Rmarkdown all semester. From some reading, it seems as if slidy has more options, but ioslides is simpler to use. However, the library I prefer at the moment is xaringan, which is a package written by Yihui Xie (same guy that wrote rmarkdown/knitr). If you install the xaringan package, you can easily create a xaringan presentation by selecting the “From Template” option in the “New R markdown” window (shown above). Yihui has an excellent blog post describing the features of xaringan that aren’t found in other libraries. Rather than repeat the documentation for each slide package in this document, I think it is probably easier just to link you to the documentation and a sample presentation for each option. ioslides Example presentation slidy Example presentation xaringan Example presentation, Example presentation 2 using UNL CSS theme reveal.js Example presentation If you’re familiar with CSS (or happier tinkering to get the look of something exactly right) then xaringan is an excellent full-featured option. A nice feature of reveal.js presentations (my favorite option before xaringan) is support for 2D slide layouts, so you can have multiple sections in your presentation, and move vertically through each section, or horizontally between sections. That is useful for presentations where you may not plan on covering everything, but where you want to have all of the information available if necessary. I relied heavily on that during my PhD prelim and defense. Try it out Take a few minutes and try each of them out to see what feels right to you. Each one has a slightly different “flavor” of Rmarkdown, so read through the example to get a sense for what is different. 10.4 Posters Posters are another common vehicle for presenting academic project results. Because posters are typically printed on paper or fabric, the standard file format is still PDF. With that said, a number of HTML poster options exist and seem to be relatively well polished37, and some have PDF export capabilities so that you can have the best of both worlds - interactivity online, and static, stable PDF exports as well. 10.4.1 LaTeX Overleaf has a fantastic gallery of posters made in LaTeX. There are several LaTeX options for making scientific posters: baposter, beamerposter, tikzposter are among the most common. We’ll focus on beamerposter here, but you are free to explore the other poster classes at will. As with beamer, you can easily integrate knitr code chunks into a document, so that you are generating your images reproducibly. Basic code for a poster in beamer (along with the necessary style files) that I’ve minimally customized to meet UNL branding requirements can be found here. Try it out Download the beamer template and do the following: Change the 3-column span box to a 2-column span box. Make the “Block Colors” box purple Move the References block up to fill the 4th column. 10.4.2 Posterdown To start, install posterdown with install.packages(\"posterdown\"). Use the RStudio menu to create a posterdown presentation file – with a prefilled template I have provided an example posterdown theme here. You can also find the additional customization options here. As with other markdown items, you can customize things even more using CSS. The nice thing about HTML posters, though, is that you can directly link to them. You can also print a poster to PDF by running the following command: pagedown::chrome_print(\"myfile.Rmd\"). See the pdf version of my customized UNL-themed poster. 10.4.3 Pagedown The pagedown package also has a couple of poster templates, including poster-relaxed and poster-jacobs. There are also templates for letters, business cards, and more in pagedown, if you’re feeling ambitious. Try it out Download the pagedown template and do the following: Change the 3-column layout to 4 columns. Adjust the breaks ({.mybreak}) accordingly to make the poster look good. Make the 2nd-level headers #249ab5 (cerulean) Move the References block to the 4th column. Print your poster to a PDF 10.5 Resume/CV You can also create resumes and CVs in markdown and LaTeX. There is no real substitute for playing around with these classes, but I really like moderncv in LaTeX38 Pagedown also comes with a html resume template (Use the menu -&gt; Rmarkdown -&gt; From Template -&gt; HTML Resume) that can be printed to html and pdf simultaneously. There is also the vitae package, which has even more templates, integration with other packages/sites, and more.39 10.6 Using Github Pages Github will host HTML content for you using Github pages (case in point: this textbook). This means you can version control your content (for instance, presentations or your CV) and have GitHub do the hosting (so you don’t have to find a webserver, buy a domain name, etc). Create a new repository named username.github.io Clone your repository Add an index.html file (this can be anything, e.g. a text file that says “hello world,” so long as it has an extension of html) and push your changes Go to https://username.github.io (YouTube Link) Github will render any README.md file as actual HTML; it will also allow you to host straight HTML pages. By default, the README file is rendered first, but in subsequent directories, a file named index.html will be rendered as the “home page” for the subdirectory, if you have such a file. Otherwise you’ll have to know the file name. I tend to separate things out into separate repositories, but you can host HTML content on other repositories too, by enabling github pages in the repository settings. On my personal page, I have repositories for my CV, Presentations40, etc. Each repository that has pages enabled can be accessed via srvanderplas.github.io/&lt;repository name&gt;/&lt;repository file path&gt;. So, to see my unl-stat850 repository, you’d go to https://srvanderplas.github.io/unl-stat850/ (but you’re already there!). (YouTube Link) This mechanism provides a very convenient way to showcase your work, share information with collaborators, and more - instead of sending files, you can send a URL and no one has to download anything overtly. If you want to track your Rmarkdown code and then render the output to a separate folder, you can use the docs/ folder. Github has this as an option as well – where we selected “master” branch above, we would select “docs/” instead (it’s greyed out b/c there isn’t a docs folder in the repo). That is how this book is hosted - the book compiles to the docs/ folder, and that way the book is rendered in final form and you don’t have to see all of the other crud that is in the repository. 10.7 References There are many other XXXdown packages, including blogdown bookdown (what I’m using to make this book) pkgdown (to easily build documentation websites for R packages) ROpenSci tutorial: How to set up hosting on github liftr - use Docker to make persistently reproducible documents I have a repository for all of the presentations I’ve given, and I use github pages to render the html presentations. Very easy, convenient, and I never have to carry a flash drive around↩︎ See this list of Rmarkdown poster options.↩︎ You can see my highly customized version here, with timelines and numbered publications. It has to be compiled multiple times to get everything right, though.↩︎ At this point, the biggest reason I haven’t switched to HTML is that I really like my timeline CV and I don’t have enough time to fiddle with it more.↩︎ I’ve been putting my presentations on Github since 2014, so it has a pretty good record of every set of slides I’ve created for anything important (and many not-so-important things as well). I highly recommend this strategy - by storing everything online, you make it very easy to share your work with others, very easy to reference later, and more importantly, easy for you to find in 3 years when you need that one specific picture.↩︎ "],["animated-and-interactive-graphics.html", "Module 11 Animated and Interactive Graphics Animated and Interactive Graphics: Module Objectives 11.1 Plotly 11.2 Leaflet maps 11.3 Shiny 11.4 General References", " Module 11 Animated and Interactive Graphics Interactive and animated graphics are one of the major advantages of using the Rmarkdown ecosystem - because you can easily create web pages in markdown (without the pain of HTML), you aren’t limited by paper any more. We’ll cover two different technologies that allow you to create different types of interactive charts, graphs, and interfaces. It is helpful to think about interactivity in a couple of different ways: What does it require? Do you need to be doing statistical calculations in the background, or can you precompute all of the data ahead of time? What type of activity or interactivity do you need? Zoom in/out? Provide additional information in response to user actions (mouseover, click) Provide information over time (animation) Keep track of a data point over multiple plots? (linked plots) Keep track of one or more data points and change their appearance based on user interaction (brushing) Allow the user to change the underlying statistical model or data? (This is not a full list of all of the types of interactivity, just a few of the more common options) In this section, we’ll cover two ways to easily create interactive graphics or applets in R. There are, of course, many others – many javascript libraries have R extensions of one form or another. Animated and Interactive Graphics: Module Objectives Create interactive charts with appropriate tools Use Shiny to create interactive web applets 11.1 Plotly Plotly is a graphing library that uses javascript to add interactivity to graphics. There are several different ways to create plotly graphs in R, but by far the easiest is ggplotly, which converts a ggplot to a plotly plot automatically (so you don’t have to specify most of the details). 11.1.1 ggplotly: ggplot2 to plotly conversions Set up the data if (!&quot;plotly&quot; %in% installed.packages()) install.packages(&quot;plotly&quot;) if (!&quot;tidytuesdayR&quot; %in% installed.packages()) { devtools::install_github(&quot;thebioengineer/tidytuesdayR&quot;) } library(dplyr) library(tidyr) library(ggplot2) library(tibble) library(lubridate) # dates and times library(tidytuesdayR) # get interesting data library(plotly) library(stringr) # Load the data from TidyTuesday on May 12 full_data &lt;- tt_load(&#39;2020-05-12&#39;) volcano &lt;- full_data$volcano eruptions &lt;- full_data$eruptions events &lt;- full_data$events sulfur &lt;- full_data$sulfur trees &lt;- full_data$tree_rings Let’s try out plotly while doing a bit of exploratory data analysis on this dataset. Cleaning up volcano volcano &lt;- volcano %&gt;% filter(tectonic_settings != &quot;Unknown&quot;) %&gt;% separate(tectonic_settings, into = c(&quot;zone&quot;, &quot;crust&quot;), sep = &quot;/&quot;, remove = F) %&gt;% # Remove anything past the first punctuation character - that will catch (xx) and ? mutate(volcano_type = str_remove(primary_volcano_type, &quot;[[:punct:]].*$&quot;)) Let’s start by seeing whether the elevation of a volcano changes based on the type of zone it’s on - we might expect that Rift zone volcanos (where plates are pulling away from each other) might not be as high. p &lt;- volcano %&gt;% ggplot(aes(x = zone, y = elevation)) + geom_boxplot() + coord_flip() ggplotly(p) But it doesn’t really look like there’s much difference. Does volcano type makes a difference? p &lt;- volcano %&gt;% ggplot(aes(x = elevation, color = volcano_type)) + geom_density() + # Rug plots show each observation as a tick just below the x axis geom_rug() ggplotly(p) Here, the interactivity actually helps a bit: we don’t need to use the legend to see what each curve corresponds to. We can see that submarine volcanoes are typically much lower in elevation (ok, duh), but also that subglacial volcanoes are found in a very limited range. If we double-click on a legend entry, we can get rid of all other curves and examine each curve one by one. I added the rug layer after the initial bout because I was curious how much data each of these curves were based on. If we want only curves with n &gt; 10 observations, we can do that: p &lt;- volcano %&gt;% group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;% filter(n &gt; 10) %&gt;% ggplot(aes(x = elevation, color = volcano_type)) + geom_density() + # Rug plots show each observation as a tick just below the x axis geom_rug(aes(text = paste0(volcano_name, &quot;, &quot;, country))) Warning: Ignoring unknown aesthetics: text ggplotly(p) If we want to specify additional information that should show up in the tooltip, we can do that as well by adding the text aesthetic even though geom_rug doesn’t take a text aesthetic. You may notice that ggplot2 complains about the unknown aesthetic I’ve added to geom_rug: That allows us to mouse over each data point in the rug plot and see what volcano it belongs to. So we can tell from the rug plot that the tallest volcano is Ojas de Salvado, in Chile/Argentina (I believe that translates to Eyes of Salvation?). At any rate, there isn’t nearly as much variation as I was expecting in the elevation of different types of volcanoes. ggplotly makes it very easy to generate plots that have a ggplot2 equivalent; you can customize these plots further using plotly functions that we’ll see in the next section. But first, try the interface out on your own. Try it out Conduct an exploratory data analysis of the eruptions dataset. What do you find? My solution head(eruptions) # A tibble: 6 x 15 volcano_number volcano_name eruption_number eruption_catego… area_of_activity &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 266030 Soputan 22354 Confirmed Erupt… &lt;NA&gt; 2 343100 San Miguel 22355 Confirmed Erupt… &lt;NA&gt; 3 233020 Fournaise, P… 22343 Confirmed Erupt… &lt;NA&gt; 4 345020 Rincon de la… 22346 Confirmed Erupt… &lt;NA&gt; 5 353010 Fernandina 22347 Confirmed Erupt… &lt;NA&gt; 6 273070 Taal 22344 Confirmed Erupt… &lt;NA&gt; # … with 10 more variables: vei &lt;dbl&gt;, start_year &lt;dbl&gt;, start_month &lt;dbl&gt;, # start_day &lt;dbl&gt;, evidence_method_dating &lt;chr&gt;, end_year &lt;dbl&gt;, # end_month &lt;dbl&gt;, end_day &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt; summary(eruptions %&gt;% mutate(eruption_category = factor(eruption_category))) volcano_number volcano_name eruption_number Min. :210010 Length:11178 Min. :10001 1st Qu.:263310 Class :character 1st Qu.:12817 Median :290050 Mode :character Median :15650 Mean :300284 Mean :15667 3rd Qu.:343030 3rd Qu.:18464 Max. :600000 Max. :22355 eruption_category area_of_activity vei Confirmed Eruption :9900 Length:11178 Min. :0.0000 Discredited Eruption: 166 Class :character 1st Qu.:1.0000 Uncertain Eruption :1112 Mode :character Median :2.0000 Mean :1.9479 3rd Qu.:2.0000 Max. :7.0000 NA&#39;s :2906 start_year start_month start_day evidence_method_dating Min. :-11345.00 Min. : 0.0000 Min. : 0.000 Length:11178 1st Qu.: 680.00 1st Qu.: 0.0000 1st Qu.: 0.000 Class :character Median : 1847.00 Median : 1.0000 Median : 0.000 Mode :character Mean : 622.85 Mean : 3.4509 Mean : 7.015 3rd Qu.: 1950.00 3rd Qu.: 7.0000 3rd Qu.:15.000 Max. : 2020.00 Max. :12.0000 Max. :31.000 NA&#39;s :1 NA&#39;s :193 NA&#39;s :196 end_year end_month end_day latitude Min. :-475.0 Min. : 0.0000 Min. : 0.000 Min. :-77.530 1st Qu.:1895.0 1st Qu.: 3.0000 1st Qu.: 4.000 1st Qu.: -6.102 Median :1957.0 Median : 6.0000 Median :15.000 Median : 17.600 Mean :1917.3 Mean : 6.2208 Mean :13.316 Mean : 16.866 3rd Qu.:1992.0 3rd Qu.: 9.0000 3rd Qu.:21.000 3rd Qu.: 40.821 Max. :2020.0 Max. :12.0000 Max. :31.000 Max. : 85.608 NA&#39;s :6846 NA&#39;s :6849 NA&#39;s :6852 longitude Min. :-179.970 1st Qu.: -77.656 Median : 55.708 Mean : 31.572 3rd Qu.: 139.394 Max. : 179.580 # Historical (very historical) dates are a bit of a pain to work with, so I # wrote a helper function which takes year, month, and day arguments and formats # them properly fix_date &lt;- function(yyyy, mm, dd) { # First, negative years (BCE) are a bit of a problem. neg &lt;- yyyy &lt; 0 subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later # for now, set to 0 year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0 # sometimes the day or month isn&#39;t known, so just use 1 for both. # recorded value may be NA or 0. day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1)) month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1)) # Need to format things precisely, so use sprintf # %0xd ensures that you have at least x digits, padding the left side with 0s # lubridate doesn&#39;t love having 3-digit years. date_str &lt;- sprintf(&quot;%04d/%02d/%02d&quot;, year_fixed, month_fixed, day_fixed) # Then we can convert the dates and subtract off the years for pre-CE dates date &lt;- ymd(date_str) - years(subtract_years) } erupt &lt;- eruptions %&gt;% # Don&#39;t work with discredited eruptions filter(eruption_category == &quot;Confirmed Eruption&quot;) %&gt;% # Create start and end dates mutate( start_date = fix_date(start_year, start_month, start_day), end_date = fix_date(end_year, end_month, end_day), # To get duration, we have to start with a time interval, # convert to duration, then convert to a numeric value duration = interval(start = start_date, end = end_date) %&gt;% as.duration() %&gt;% as.numeric(&quot;days&quot;)) Warning: 1 failed to parse. Warning: 5895 failed to parse. Let’s start out seeing what month most eruptions occur in… # Note, I&#39;m using the original month, so 0 = unknown p &lt;- ggplot(erupt, aes(x = factor(start_month))) + geom_bar() ggplotly(p) # I could rename some of the factors to make this pretty, but... nah Another numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released. # VEI is volcano explosivity index, p &lt;- ggplot(erupt, aes(x = vei)) + geom_bar() ggplotly(p) Warning: Removed 2270 rows containing non-finite values (stat_count). We can also look at the frequency of eruptions over time. We’ll expect some historical bias - we don’t have exact dates for some of these eruptions, and if no one was around to write the eruption down (or the records were destroyed) there’s not going to be a date listed here. p &lt;- erupt %&gt;% filter(!is.na(end_date)) %&gt;% filter(start_year &gt; 0) %&gt;% ggplot(aes(x = start_date, xend = start_date, y = 0, yend = duration, color = evidence_method_dating)) + geom_segment() + geom_point(size = .5, aes(text = volcano_name)) + xlab(&quot;Eruption Start&quot;) + ylab(&quot;Eruption Duration (days)&quot;) + facet_wrap(~vei, scales = &quot;free_y&quot;) Warning: Ignoring unknown aesthetics: text ggplotly(p) As expected, it’s pretty rare to see many eruptions before ~1800 AD, which is about when we have reliable historical records41 for most of the world (exceptions include e.g. Vestuvius, which we have extensive written information about). p &lt;- erupt %&gt;% filter(!is.na(end_date)) %&gt;% # Account for recency bias (sort of) filter(start_year &gt; 1800) %&gt;% ggplot(aes(x = factor(vei), y = duration)) + geom_violin() + xlab(&quot;VEI&quot;) + ylab(&quot;Eruption Duration (days)&quot;) + scale_y_sqrt() ggplotly(p) Warning in self$trans$transform(x): NaNs produced Warning: Transformation introduced infinite values in continuous y-axis Warning: Removed 3 rows containing non-finite values (stat_ydensity). Warning: Groups with fewer than two data points have been dropped. It seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully). 11.1.2 plot_ly: Like base plotting, but interactive! You can also create plotly charts that aren’t limited by what you can do in ggplot2, using the plot_ly function. Plotly cheat sheet We can start with a scatterplot of volcanoes along the Earth’s surface: plot_ly(type = &quot;scattergeo&quot;, lon = volcano$longitude, lat = volcano$latitude) No scattergeo mode specifed: Setting the mode to markers Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode And then we can start customizing: plot_ly(type = &quot;scattergeo&quot;, lon = volcano$longitude, lat = volcano$latitude, mode = &quot;markers&quot;, # Add information to mouseover text = ~paste(volcano$volcano_name, &quot;\\n&quot;, &quot;Last Erupted: &quot;, volcano$last_eruption_year), # Change the markers because why not? marker = list(color = &quot;#d00000&quot;, opacity = 0.25) ) The plot_ly function is also pipe friendly. Variable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable. # Load RColorBrewer for palettes library(RColorBrewer) volcano %&gt;% group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;% filter(n &gt; 15) %&gt;% plot_ly(type = &quot;scattergeo&quot;, lon = ~longitude, lat = ~latitude, mode = &quot;markers&quot;, # Add information to mouseover text = ~paste(volcano_name, &quot;\\n&quot;, &quot;Last Erupted: &quot;, last_eruption_year), color = ~ volcano_type, # Specify a palette colors = brewer.pal(length(unique(.$volcano_type)), &quot;Paired&quot;), # Change the markers because why not? marker = list(opacity = 0.5) ) Plotly will handle some variable mappings for you, depending on which “trace” type (plot/geom) you’re using. The plotly documentation often uses plyr and reshape2 – which are older versions of dplyr and tidyr. If you load plyr and reshape2, it may seriously mess up your day – a lot of the function names are the same. So, instead, here’s a shortcut: cast is pivot_wider and melt is pivot_longer. That should at least help with understanding what the code is doing. If you do accidentally load plyr or reshape2, that’s fine: just restart your R session so that your loaded packages are cleared and you can start over. Or, if you must, you can reference a plyr function using plyr::function_name without loading the package – that’s a safe way to use the plotly demo code as-is. Let’s explore traces a bit. According to the plotly documentation, A trace is just the name we give a collection of data and the specifications of which we want that data plotted. Notice that a trace will also be an object itself, and these will be named according to how you want the data displayed on the plotting surface In ggplot2 terms, it seems that a trace is somewhat akin to a geom. trace0 &lt;- rnorm(100, mean = 5) trace1 &lt;- rnorm(100, mean = 0) trace2 &lt;- rnorm(100, mean = -5) data &lt;- tibble(x = 1:100, trace0, trace1, trace2) # Let&#39;s see how this goes with one trace plot_ly(data, x = ~x) %&gt;% add_trace(y = ~trace0, name = &#39;trace0&#39;, mode = &#39;lines&#39;) No trace type specified: Based on info supplied, a &#39;scatter&#39; trace seems appropriate. Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter # Adding some more traces plot_ly(data, x = ~x) %>% add_trace(y = ~trace0, name = 'trace0', mode = 'lines') %>% add_trace(y = ~trace1, name = \"trace1\", mode = 'lines+markers') %>% add_trace(y = ~trace2, name = \"trace2\", mode = 'markers') No trace type specified: Based on info supplied, a 'scatter' trace seems appropriate. Read more about this trace type -> https://plotly.com/r/reference/#scatter No trace type specified: Based on info supplied, a 'scatter' trace seems appropriate. Read more about this trace type -> https://plotly.com/r/reference/#scatter No trace type specified: Based on info supplied, a 'scatter' trace seems appropriate. Read more about this trace type -> https://plotly.com/r/reference/#scatter But, if you want all of the variables to be shown with the same trace type, it’s probably easier to get to long form: data %&gt;% pivot_longer(matches(&quot;trace&quot;), names_to = &quot;trace&quot;, names_prefix = &quot;trace&quot;, values_to = &quot;y&quot;) %&gt;% plot_ly(x = ~x, y = ~y, color = ~trace, mode = &quot;lines+markers&quot;) No trace type specified: Based on info supplied, a &#39;scatter&#39; trace seems appropriate. Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter There are many different trace types in plotly, but your best bet is to check the documentation to see what is available. 11.1.3 Animation Plotly can also animate your plots for you. library(classdata) data(fbi) fbi %&gt;% mutate(State = factor(State), Rate_100k = Count/Population*100000) %&gt;% filter(Type == &quot;Aggravated.assault&quot;) %&gt;% arrange(Year, State, Type) %&gt;% plot_ly( x = ~State, y = ~Rate_100k, color = ~Type, frame = ~Year, type = &quot;scatter&quot;, mode = &quot;markers&quot; ) Sometimes the animations get a bit trippy, don’t they? You can even animate by something other than time, if you’re so inclined, though it’s not necessarily going to make sense if there isn’t any context shared between successive observations. So animating over space might make sense, but animating over a factor makes a lot less sense. fbi %&gt;% mutate(State = factor(State), Rate_100k = Count/Population*100000) %&gt;% arrange(Year, State, Type) %&gt;% plot_ly( x = ~Year, y = ~Rate_100k, color = ~Type, frame = ~State, type = &quot;scatter&quot;, mode = &quot;lines&quot; ) There are other types of animations as well, including the ability to change plot formats, trace types, and more. 11.2 Leaflet maps Leaflet is another javascript library that allows for interactive data visualization. We’re only going to briefly talk about it here, but there is extensive documentation that includes details of how to work with different types of geographical data, chloropleth maps, plugins, and more. To explore the leaflet package, we’ll start out playing with a dataset of Bigfoot sightings assembled from the Bigfoot Field Researchers Organization’s Google earth tool ── Column specification ──────────────────────────────────────────────────────── cols( .default = col_double(), observed = col_character(), location_details = col_character(), county = col_character(), state = col_character(), season = col_character(), title = col_character(), date = col_date(format = &quot;&quot;), classification = col_character(), geohash = col_character(), precip_type = col_character(), summary = col_character() ) ℹ Use `spec()` for the full column specifications. if (!&quot;leaflet&quot; %in% installed.packages()) install.packages(&quot;leaflet&quot;) library(leaflet) library(readr) bigfoot_data &lt;- read_csv(&quot;https://query.data.world/s/egnaxxvegdkzzrhfhdh4izb6etmlms&quot;) We can start out by plotting a map with the location of each sighting. I’ve colored the points in a seasonal color scheme, and added the description of each incident as a mouseover label. bigfoot_data %&gt;% filter(classification == &quot;Class A&quot;) %&gt;% mutate(seasoncolor = str_replace_all(season, c(&quot;Fall&quot; = &quot;orange&quot;, &quot;Winter&quot; = &quot;skyblue&quot;, &quot;Spring&quot; = &quot;green&quot;, &quot;Summer&quot; = &quot;yellow&quot;)), # This code just wraps the description to the width of the R terminal # and inserts HTML for a line break into the text at appropriate points desc_wrap = purrr::map(observed, ~strwrap(.) %&gt;% paste(collapse = &quot;&lt;br/&gt;&quot;) %&gt;% htmltools::HTML())) %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(~longitude, ~latitude, color = ~seasoncolor, label = ~desc_wrap) Warning in validateCoords(lng, lat, funcName): Data contains 459 rows with either missing or invalid lat/lon values and will be ignored Of course, because this is an interactive map library, we aren’t limited to any one scale. We can also plot data at the city level: if(!&quot;nycsquirrels18&quot; %in% installed.packages()) { devtools::install_github(&quot;mine-cetinkaya-rundel/nycsquirrels18&quot;) } library(nycsquirrels18) Attaching package: &#39;nycsquirrels18&#39; The following object is masked _by_ &#39;.GlobalEnv&#39;: squirrels data(squirrels) head(squirrels) # A tibble: 6 x 35 long lat unique_squirrel_… hectare shift date hectare_squirrel… age &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; 1 -74.0 40.8 13A-PM-1014-04 13A PM 2018-10-14 4 &lt;NA&gt; 2 -74.0 40.8 15F-PM-1010-06 15F PM 2018-10-10 6 Adult 3 -74.0 40.8 19C-PM-1018-02 19C PM 2018-10-18 2 Adult 4 -74.0 40.8 21B-AM-1019-04 21B AM 2018-10-19 4 &lt;NA&gt; 5 -74.0 40.8 23A-AM-1018-02 23A AM 2018-10-18 2 Juve… 6 -74.0 40.8 38H-PM-1012-01 38H PM 2018-10-12 1 Adult # … with 27 more variables: primary_fur_color &lt;chr&gt;, highlight_fur_color &lt;chr&gt;, # combination_of_primary_and_highlight_color &lt;chr&gt;, color_notes &lt;chr&gt;, # location &lt;chr&gt;, above_ground_sighter_measurement &lt;chr&gt;, # specific_location &lt;chr&gt;, running &lt;lgl&gt;, chasing &lt;lgl&gt;, climbing &lt;lgl&gt;, # eating &lt;lgl&gt;, foraging &lt;lgl&gt;, other_activities &lt;chr&gt;, kuks &lt;lgl&gt;, # quaas &lt;lgl&gt;, moans &lt;lgl&gt;, tail_flags &lt;lgl&gt;, tail_twitches &lt;lgl&gt;, # approaches &lt;lgl&gt;, indifferent &lt;lgl&gt;, runs_from &lt;lgl&gt;, # other_interactions &lt;chr&gt;, zip_codes &lt;dbl&gt;, community_districts &lt;dbl&gt;, # borough_boundaries &lt;dbl&gt;, city_council_districts &lt;dbl&gt;, # police_precincts &lt;dbl&gt; squirrels %&gt;% mutate(color = tolower(primary_fur_color)) %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(~long, ~lat, color = ~color) We can also plot regions, instead of just points. I downloaded a dataset released by the US Forest Service, Bailey’s Ecoregions and Subregions dataset, which categorizes the US into different climate and ecological zones. To map colors to variables, we have to define a color palette and variable mapping ourselves, and pass that function into the leaflet object we’re adding. library(rgdal) Loading required package: sp rgdal: version: 1.5-23, (SVN revision 1121) Geospatial Data Abstraction Library extensions to R successfully loaded Loaded GDAL runtime: GDAL 3.0.4, released 2020/01/28 Path to GDAL shared files: /usr/share/gdal GDAL binary built with GEOS: TRUE Loaded PROJ runtime: Rel. 6.3.1, February 10th, 2020, [PJ_VERSION: 631] Path to PROJ shared files: /usr/share/proj Linking to sp version:1.4-5 To mute warnings of possible GDAL/OSR exportToProj4() degradation, use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading rgdal. ecoregions &lt;- readOGR(&quot;data/Bailey_s_Ecoregions_and_Subregions_Dataset.geojson&quot;) OGR data source with driver: GeoJSON Source: &quot;/home/susan/Projects/Class/unl-stat850/stat850-textbook/data/Bailey_s_Ecoregions_and_Subregions_Dataset.geojson&quot;, layer: &quot;Bailey_s_Ecoregions_and_Subregions_Dataset&quot; with 3072 features It has 12 fields # Define a palette region_pal &lt;- colorFactor(c(&quot;#E67E22&quot;, &quot;#0B5345&quot;, &quot;#229954&quot;, &quot;#B3B6B7&quot;), ecoregions$DOMAIN) ecoregions %&gt;% leaflet() %&gt;% addTiles() %&gt;% addPolygons(stroke = TRUE, fillOpacity = 0.25, fillColor = ~region_pal(DOMAIN), color = ~region_pal(DOMAIN), label = ~SECTION) Try it out Download the Shapefiles for the 116th Congress Congressional Districts. Unzip the file and read it in using the code below (you’ll have to change the file path). Use the MIT Election Data and Science Lab’s US House election results42, and merge this data with the shapefiles to plot the results of the 2018 midterms in a way that you think is useful (you can use any of the available data). Some notes: - FIPS codes are used to identify the state and district, with 00 indicating at-large districts (one district for the state) and 98 indicating non-voting districts. - If you would like to add in the number of citizens of voting age, you can get that information here but you will have to do some cleaning in order to join the table with the others. - Minnesota’s Democratic-farmer-labor party caucuses with the Democrats but maintains its name for historical reasons. You can safely recode this if you want to. library(sf) # Read in the districts congress_districts &lt;- st_read(&quot;data/116_congress/cb_2018_us_cd116_5m.shp&quot;) Reading layer `cb_2018_us_cd116_5m&#39; from data source `/home/susan/Projects/Class/unl-stat850/stat850-textbook/data/116_congress/cb_2018_us_cd116_5m.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 441 features and 8 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: -179.14734 ymin: -14.552549 xmax: 179.77847 ymax: 71.352561 Geodetic CRS: NAD83 # Read in the results election_results &lt;- read_csv(&quot;data/1976-2018-house2.csv&quot;) %&gt;% filter(year == 2018) %&gt;% mutate(state_fips = sprintf(&quot;%02d&quot;, state_fips), district = sprintf(&quot;%02d&quot;, district)) ── Column specification ──────────────────────────────────────────────────────── cols( year = col_double(), state = col_character(), state_po = col_character(), state_fips = col_double(), state_cen = col_double(), state_ic = col_double(), office = col_character(), district = col_double(), stage = col_character(), runoff = col_logical(), special = col_logical(), candidate = col_character(), party = col_character(), writein = col_logical(), mode = col_character(), candidatevotes = col_double(), totalvotes = col_double(), unofficial = col_logical(), version = col_double() ) # Clean up congress districts congress_districts &lt;- congress_districts %&gt;% # Convert factors to characters mutate(across(where(is.factor), as.character)) %&gt;% # Handle at-large districts mutate(district = ifelse(CD116FP == &quot;00&quot;, &quot;01&quot;, CD116FP)) One solution library(sf) library(htmltools) # to mark labels as html code Attaching package: &#39;htmltools&#39; The following object is masked _by_ &#39;.GlobalEnv&#39;: a # Read in the results election_results &lt;- read_csv(&quot;data/1976-2018-house2.csv&quot;) %&gt;% filter(year == 2018) %&gt;% mutate(state_fips = sprintf(&quot;%02d&quot;, state_fips), district = sprintf(&quot;%02d&quot;, district)) %&gt;% group_by(state, state_fips, state_po, district, stage) %&gt;% arrange(candidatevotes) %&gt;% mutate(pct = candidatevotes/totalvotes) %&gt;% # Keep the winner only filter(pct == max(pct)) %&gt;% # Fix Minnesota mutate(party = ifelse(party == &quot;democratic-farmer-labor&quot;, &quot;democrat&quot;, party)) ── Column specification ──────────────────────────────────────────────────────── cols( year = col_double(), state = col_character(), state_po = col_character(), state_fips = col_double(), state_cen = col_double(), state_ic = col_double(), office = col_character(), district = col_double(), stage = col_character(), runoff = col_logical(), special = col_logical(), candidate = col_character(), party = col_character(), writein = col_logical(), mode = col_character(), candidatevotes = col_double(), totalvotes = col_double(), unofficial = col_logical(), version = col_double() ) # Read in the districts congress_districts &lt;- st_read(&quot;data/116_congress/cb_2018_us_cd116_5m.shp&quot;) %&gt;% mutate(geometry = st_transform(geometry, crs = st_crs(&quot;+proj=longlat +datum=WGS84&quot;))) Reading layer `cb_2018_us_cd116_5m&#39; from data source `/home/susan/Projects/Class/unl-stat850/stat850-textbook/data/116_congress/cb_2018_us_cd116_5m.shp&#39; using driver `ESRI Shapefile&#39; Simple feature collection with 441 features and 8 fields Geometry type: MULTIPOLYGON Dimension: XY Bounding box: xmin: -179.14734 ymin: -14.552549 xmax: 179.77847 ymax: 71.352561 Geodetic CRS: NAD83 # Clean up congress districts congress_districts &lt;- congress_districts %&gt;% # Convert factors to characters mutate(across(where(is.factor), as.character)) %&gt;% # Handle at-large districts mutate(district = ifelse(CD116FP == &quot;00&quot;, &quot;01&quot;, CD116FP)) # Merge congress_districts &lt;- congress_districts %&gt;% left_join(election_results, by = c(&quot;STATEFP&quot; = &quot;state_fips&quot;, &quot;CD116FP&quot; = &quot;district&quot;)) %&gt;% mutate(party = factor(party, levels = c(&quot;republican&quot;, &quot;democrat&quot;)), short_party = ifelse(party == &quot;republican&quot;, &quot;R&quot;, &quot;D&quot;), label = paste0(state_po, &quot;-&quot;, district, candidate, &quot; (&quot;, short_party, &quot;)&quot;)) # Define a palette region_pal &lt;- colorFactor(c(&quot;#e9141d&quot;, &quot;#0015bc&quot;), congress_districts$party) congress_districts %&gt;% leaflet() %&gt;% addTiles() %&gt;% addPolygons(stroke = TRUE, fillOpacity = ~pct/2, # still want to see what&#39;s underneath, even in safe districts fillColor = ~region_pal(party), color = ~region_pal(party), label = ~label) 11.3 Shiny https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/readme.md Before we get started on Shiny, take a few minutes and poke around the RStudio Shiny user showcase. It helps to have some motivation, and to get a sense of what is possible before you start learning something. One of the more amusing ones I found was an exploration of lego demographics. Shiny is a framework for building interactive web applications in R. Unlike plotly and other graphics engines, Shiny depends on an R instance on a server to do computations. This means Shiny is much more powerful and has more capabilities, but also that it’s harder to share and deploy - you have to have access to a web server with R installed on it. If you happen to have a server like that, though, Shiny is pretty awesome. RStudio runs a service called shinyapps.io that will provide some limited free hosting, as well as paid plans for apps that have more web traffic, but you can also create Shiny apps for local use - I often do this for model debugging when I’m using neural networks, because they’re so complicated. RStudio has a set of well produced video tutorials to introduce Shiny. I’d recommend you at least listen to the introduction if you’re a visual/audio learner (the whole tutorial is about 2 hours long). There is also a written tutorial if you prefer to learn in written form (7 lessons, each is about 20 minutes long). I generally think it’s better to send you to the source when there are well-produced resources, rather than trying to rehash something to put my own spin on it. One other interesting feature to keep in mind when using Shiny - you can integrate Shiny reactivity into Rmarkdown by adding runtime: shiny to the markdown header. 11.4 General References R graph gallery interactive charts points you in the right direction for which package to use for different tasks. Shiny articles Reactivity in Shiny Leaflet introduction for R 11.4.1 Other interactive tools htmlwidgets - a generic wrapper for any Javascript library (htmlwidgets is used under the hood in both Leaflet and Plotly R integration) dash - Another dashboard program supported by plotly. dash is the python equivalent of shiny, but also has R integration (though I’m not sure how well it’s supported). 11.4.2 Debugging Debugging with Dean - Shiny debugging - YouTube video with debugging in realtime. ShinyJS - Using Shiny and JavaScript together Using Shiny in Production - Joe Cheng There are obviously exceptions - we can figure out the exact date and approximate time that there was an earthquake along the Cascadia subduction zone based on a combination of oral histories of the indigenous people and records of a massive tsunami in Japan Excellent read, if you’re interested, and the Nature paper.↩︎ Alternately, you can find the rehosted file here.↩︎ "],["lists-nested-lists-and-functional-programming.html", "Module 12 Lists, Nested Lists, and Functional Programming 12.1 Review: Lists and Vectors 12.2 Introduction to map 12.3 Creating (and Using) List-columns 12.4 Ways to use map 12.5 Beyond map: Functions with multiple inputs Purrr References", " Module 12 Lists, Nested Lists, and Functional Programming 12.1 Review: Lists and Vectors A vector is a 1-dimensional R data structure that contains items of the same simple (‘atomic’) type (character, logical, integer, factor). (logical_vec &lt;- c(T, F, T, T)) [1] TRUE FALSE TRUE TRUE (numeric_vec &lt;- c(3, 1, 4, 5)) [1] 3 1 4 5 (char_vec &lt;- c(&quot;A&quot;, &quot;AB&quot;, &quot;ABC&quot;, &quot;ABCD&quot;)) [1] &quot;A&quot; &quot;AB&quot; &quot;ABC&quot; &quot;ABCD&quot; You index a vector using brackets: to get the 3rd element of the vector x, you would use x[3]. logical_vec[3] [1] TRUE numeric_vec[3] [1] 4 char_vec[3] [1] &quot;ABC&quot; You can also index a vector using a logical vector: numeric_vec[logical_vec] [1] 3 4 5 char_vec[logical_vec] [1] &quot;A&quot; &quot;ABC&quot; &quot;ABCD&quot; logical_vec[logical_vec] [1] TRUE TRUE TRUE A list is a 1-dimensional R data structure that has no restrictions on what type of content is stored within it. (mylist &lt;- list(logical_vec, numeric_vec, third_thing = char_vec[1:2])) [[1]] [1] TRUE FALSE TRUE TRUE [[2]] [1] 3 1 4 5 $third_thing [1] &quot;A&quot; &quot;AB&quot; A list is a vector, but it is not an atomic vector - that is, it does not necessarily contain things that are all the same type. List components may have names (or not), be homogeneous (or not), have the same length (or not). There are 3 ways to index a list: With single square brackets, just like we index atomic vectors. In this case, the return value is always a list. mylist[1] [[1]] [1] TRUE FALSE TRUE TRUE mylist[2] [[1]] [1] 3 1 4 5 mylist[c(T, F, T)] [[1]] [1] TRUE FALSE TRUE TRUE $third_thing [1] &quot;A&quot; &quot;AB&quot; With double square brackets. In this case, the return value is the thing inside the specified position in the list, but you also can only get one entry in the main list at a time. You can also get things by name. mylist[[1]] [1] TRUE FALSE TRUE TRUE mylist[[&quot;third_thing&quot;]] [1] &quot;A&quot; &quot;AB&quot; Using x$name. This is equivalent to using x[[\"name\"]]. Note that this does not work on unnamed entries in the list. mylist$third_thing [1] &quot;A&quot; &quot;AB&quot; You can get a more thorough review of vectors and lists from Jenny Bryan’s purrr tutorial. Operations in R are vectorized - that is, by default, they operate on vectors. This is primarily a feature that applies to atomic vectors (and we don’t even think about it): (rnorm(10) + rnorm(10, mean = 3)) [1] 4.59585925 2.10638746 1.54544529 4.75582887 1.79105041 0.29428439 [7] 2.92836828 3.35514882 2.58827225 5.08325563 We didn’t have to use a for loop to add these two vectors with 10 entries each together. In python (and SAS, and other languages), this might instead look like: a &lt;- rnorm(10) b &lt;- rnorm(10, mean = 3) result &lt;- rep(0, 10) for(i in 1:10) { result[i] &lt;- a[i] + b[i] } result [1] 3.79110137 0.68687374 3.36213487 0.30480441 4.33392607 5.36777924 [7] 2.04321045 3.38061701 4.41166080 1.68245799 That is, we would apply or map the + function to each entry of a and b. For atomic vectors, it’s easy to do this by default; with a list, however, we need to be a bit more explicit (because everything that’s passed into the function may not be the same type). This logic is the basis behind the purrr package (and similar base functions apply, lapply, sapply, tapply, and mapply - I find the purrr package easier to work with, but you may use the base package versions if you want, and you can find a side-by-side comparison in the purrr tutorial). 12.2 Introduction to map library(tidyverse) library(purrr) # list functions library(repurrrsive) # examples We’ll use one of the datasets in repurrsive, got_chars, to start playing with the map_ series of functions. data(got_chars) length(got_chars) [1] 30 got_chars[[1]] $url [1] &quot;https://www.anapioficeandfire.com/api/characters/1022&quot; $id [1] 1022 $name [1] &quot;Theon Greyjoy&quot; $gender [1] &quot;Male&quot; $culture [1] &quot;Ironborn&quot; $born [1] &quot;In 278 AC or 279 AC, at Pyke&quot; $died [1] &quot;&quot; $alive [1] TRUE $titles [1] &quot;Prince of Winterfell&quot; [2] &quot;Captain of Sea Bitch&quot; [3] &quot;Lord of the Iron Islands (by law of the green lands)&quot; $aliases [1] &quot;Prince of Fools&quot; &quot;Theon Turncloak&quot; &quot;Reek&quot; &quot;Theon Kinslayer&quot; $father [1] &quot;&quot; $mother [1] &quot;&quot; $spouse [1] &quot;&quot; $allegiances [1] &quot;House Greyjoy of Pyke&quot; $books [1] &quot;A Game of Thrones&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; $povBooks [1] &quot;A Clash of Kings&quot; &quot;A Dance with Dragons&quot; $tvSeries [1] &quot;Season 1&quot; &quot;Season 2&quot; &quot;Season 3&quot; &quot;Season 4&quot; &quot;Season 5&quot; &quot;Season 6&quot; $playedBy [1] &quot;Alfie Allen&quot; It appears that each entry in this 30-item list is a character from Game of Thrones, and there are several sub-fields for each character. What characters do we have? We can use purrr::map(x, \"name\") to get a list of all characters’ names. Since they are all the same type, we could also use an extension of map, map_chr, which will coerce the returned list into a character vector (which may be simpler to operate on). There are several packages with map() functions including functions that are meant to actually plot maps; it generally saves time and effort to just type the function name with the package you want; you don’t have to do so, but if you have a lot of other (non tidyverse, in particular) packages loaded, it will save you a lot of grief. purrr::map(got_chars, &quot;name&quot;) [[1]] [1] &quot;Theon Greyjoy&quot; [[2]] [1] &quot;Tyrion Lannister&quot; [[3]] [1] &quot;Victarion Greyjoy&quot; [[4]] [1] &quot;Will&quot; [[5]] [1] &quot;Areo Hotah&quot; [[6]] [1] &quot;Chett&quot; [[7]] [1] &quot;Cressen&quot; [[8]] [1] &quot;Arianne Martell&quot; [[9]] [1] &quot;Daenerys Targaryen&quot; [[10]] [1] &quot;Davos Seaworth&quot; [[11]] [1] &quot;Arya Stark&quot; [[12]] [1] &quot;Arys Oakheart&quot; [[13]] [1] &quot;Asha Greyjoy&quot; [[14]] [1] &quot;Barristan Selmy&quot; [[15]] [1] &quot;Varamyr&quot; [[16]] [1] &quot;Brandon Stark&quot; [[17]] [1] &quot;Brienne of Tarth&quot; [[18]] [1] &quot;Catelyn Stark&quot; [[19]] [1] &quot;Cersei Lannister&quot; [[20]] [1] &quot;Eddard Stark&quot; [[21]] [1] &quot;Jaime Lannister&quot; [[22]] [1] &quot;Jon Connington&quot; [[23]] [1] &quot;Jon Snow&quot; [[24]] [1] &quot;Aeron Greyjoy&quot; [[25]] [1] &quot;Kevan Lannister&quot; [[26]] [1] &quot;Melisandre&quot; [[27]] [1] &quot;Merrett Frey&quot; [[28]] [1] &quot;Quentyn Martell&quot; [[29]] [1] &quot;Samwell Tarly&quot; [[30]] [1] &quot;Sansa Stark&quot; purrr::map_chr(got_chars, &quot;name&quot;) [1] &quot;Theon Greyjoy&quot; &quot;Tyrion Lannister&quot; &quot;Victarion Greyjoy&quot; [4] &quot;Will&quot; &quot;Areo Hotah&quot; &quot;Chett&quot; [7] &quot;Cressen&quot; &quot;Arianne Martell&quot; &quot;Daenerys Targaryen&quot; [10] &quot;Davos Seaworth&quot; &quot;Arya Stark&quot; &quot;Arys Oakheart&quot; [13] &quot;Asha Greyjoy&quot; &quot;Barristan Selmy&quot; &quot;Varamyr&quot; [16] &quot;Brandon Stark&quot; &quot;Brienne of Tarth&quot; &quot;Catelyn Stark&quot; [19] &quot;Cersei Lannister&quot; &quot;Eddard Stark&quot; &quot;Jaime Lannister&quot; [22] &quot;Jon Connington&quot; &quot;Jon Snow&quot; &quot;Aeron Greyjoy&quot; [25] &quot;Kevan Lannister&quot; &quot;Melisandre&quot; &quot;Merrett Frey&quot; [28] &quot;Quentyn Martell&quot; &quot;Samwell Tarly&quot; &quot;Sansa Stark&quot; Similar shortcuts work to get the nth item in each sub list: purrr::map_chr(got_chars, 4) [1] &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; [9] &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; [17] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; [25] &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; Specifying the output type using e.g. map_chr works if each item in the list is an atomic vector of length 1. If the list is more complicated, though, these shortcuts will issue an error: purrr::map(got_chars, &quot;books&quot;) [[1]] [1] &quot;A Game of Thrones&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; [[2]] [1] &quot;A Feast for Crows&quot; &quot;The World of Ice and Fire&quot; [[3]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [[4]] [1] &quot;A Clash of Kings&quot; [[5]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [[6]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; [[7]] [1] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; [[8]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [4] &quot;A Dance with Dragons&quot; [[9]] [1] &quot;A Feast for Crows&quot; [[10]] [1] &quot;A Feast for Crows&quot; [[11]] NULL [[12]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [4] &quot;A Dance with Dragons&quot; [[13]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; [[14]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; [3] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; [5] &quot;The World of Ice and Fire&quot; [[15]] [1] &quot;A Storm of Swords&quot; [[16]] [1] &quot;A Feast for Crows&quot; [[17]] [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; &quot;A Dance with Dragons&quot; [[18]] [1] &quot;A Feast for Crows&quot; &quot;A Dance with Dragons&quot; [[19]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [[20]] [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [3] &quot;A Feast for Crows&quot; &quot;A Dance with Dragons&quot; [5] &quot;The World of Ice and Fire&quot; [[21]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; [[22]] [1] &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; [3] &quot;The World of Ice and Fire&quot; [[23]] [1] &quot;A Feast for Crows&quot; [[24]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [4] &quot;A Dance with Dragons&quot; [[25]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [4] &quot;A Feast for Crows&quot; [[26]] [1] &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; &quot;A Feast for Crows&quot; [[27]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Feast for Crows&quot; [4] &quot;A Dance with Dragons&quot; [[28]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Storm of Swords&quot; [4] &quot;A Feast for Crows&quot; [[29]] [1] &quot;A Game of Thrones&quot; &quot;A Clash of Kings&quot; &quot;A Dance with Dragons&quot; [[30]] [1] &quot;A Dance with Dragons&quot; purrr::map_chr(got_chars, &quot;books&quot;) Error: Result 1 must be a single string, not a character vector of length 3 What if we want to extract several things? This trick works off of the idea that [ is a function: that is, the single brackets we used before are actually a special type of function. In R functions, there is often the argument ..., which is a convention that allows us to pass arguments to other functions that are called within the main function we are using (you’ll see … used in plotting and regression functions frequently as well). Here, we use ... to pass in our list of 3 things we want to pull from each item in the list. purrr::map(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) [[1]] [[1]]$name [1] &quot;Theon Greyjoy&quot; [[1]]$gender [1] &quot;Male&quot; [[1]]$born [1] &quot;In 278 AC or 279 AC, at Pyke&quot; [[2]] [[2]]$name [1] &quot;Tyrion Lannister&quot; [[2]]$gender [1] &quot;Male&quot; [[2]]$born [1] &quot;In 273 AC, at Casterly Rock&quot; [[3]] [[3]]$name [1] &quot;Victarion Greyjoy&quot; [[3]]$gender [1] &quot;Male&quot; [[3]]$born [1] &quot;In 268 AC or before, at Pyke&quot; [[4]] [[4]]$name [1] &quot;Will&quot; [[4]]$gender [1] &quot;Male&quot; [[4]]$born [1] &quot;&quot; [[5]] [[5]]$name [1] &quot;Areo Hotah&quot; [[5]]$gender [1] &quot;Male&quot; [[5]]$born [1] &quot;In 257 AC or before, at Norvos&quot; [[6]] [[6]]$name [1] &quot;Chett&quot; [[6]]$gender [1] &quot;Male&quot; [[6]]$born [1] &quot;At Hag&#39;s Mire&quot; [[7]] [[7]]$name [1] &quot;Cressen&quot; [[7]]$gender [1] &quot;Male&quot; [[7]]$born [1] &quot;In 219 AC or 220 AC&quot; [[8]] [[8]]$name [1] &quot;Arianne Martell&quot; [[8]]$gender [1] &quot;Female&quot; [[8]]$born [1] &quot;In 276 AC, at Sunspear&quot; [[9]] [[9]]$name [1] &quot;Daenerys Targaryen&quot; [[9]]$gender [1] &quot;Female&quot; [[9]]$born [1] &quot;In 284 AC, at Dragonstone&quot; [[10]] [[10]]$name [1] &quot;Davos Seaworth&quot; [[10]]$gender [1] &quot;Male&quot; [[10]]$born [1] &quot;In 260 AC or before, at King&#39;s Landing&quot; [[11]] [[11]]$name [1] &quot;Arya Stark&quot; [[11]]$gender [1] &quot;Female&quot; [[11]]$born [1] &quot;In 289 AC, at Winterfell&quot; [[12]] [[12]]$name [1] &quot;Arys Oakheart&quot; [[12]]$gender [1] &quot;Male&quot; [[12]]$born [1] &quot;At Old Oak&quot; [[13]] [[13]]$name [1] &quot;Asha Greyjoy&quot; [[13]]$gender [1] &quot;Female&quot; [[13]]$born [1] &quot;In 275 AC or 276 AC, at Pyke&quot; [[14]] [[14]]$name [1] &quot;Barristan Selmy&quot; [[14]]$gender [1] &quot;Male&quot; [[14]]$born [1] &quot;In 237 AC&quot; [[15]] [[15]]$name [1] &quot;Varamyr&quot; [[15]]$gender [1] &quot;Male&quot; [[15]]$born [1] &quot;At a village Beyond the Wall&quot; [[16]] [[16]]$name [1] &quot;Brandon Stark&quot; [[16]]$gender [1] &quot;Male&quot; [[16]]$born [1] &quot;In 290 AC, at Winterfell&quot; [[17]] [[17]]$name [1] &quot;Brienne of Tarth&quot; [[17]]$gender [1] &quot;Female&quot; [[17]]$born [1] &quot;In 280 AC&quot; [[18]] [[18]]$name [1] &quot;Catelyn Stark&quot; [[18]]$gender [1] &quot;Female&quot; [[18]]$born [1] &quot;In 264 AC, at Riverrun&quot; [[19]] [[19]]$name [1] &quot;Cersei Lannister&quot; [[19]]$gender [1] &quot;Female&quot; [[19]]$born [1] &quot;In 266 AC, at Casterly Rock&quot; [[20]] [[20]]$name [1] &quot;Eddard Stark&quot; [[20]]$gender [1] &quot;Male&quot; [[20]]$born [1] &quot;In 263 AC, at Winterfell&quot; [[21]] [[21]]$name [1] &quot;Jaime Lannister&quot; [[21]]$gender [1] &quot;Male&quot; [[21]]$born [1] &quot;In 266 AC, at Casterly Rock&quot; [[22]] [[22]]$name [1] &quot;Jon Connington&quot; [[22]]$gender [1] &quot;Male&quot; [[22]]$born [1] &quot;In or between 263 AC and 265 AC&quot; [[23]] [[23]]$name [1] &quot;Jon Snow&quot; [[23]]$gender [1] &quot;Male&quot; [[23]]$born [1] &quot;In 283 AC&quot; [[24]] [[24]]$name [1] &quot;Aeron Greyjoy&quot; [[24]]$gender [1] &quot;Male&quot; [[24]]$born [1] &quot;In or between 269 AC and 273 AC, at Pyke&quot; [[25]] [[25]]$name [1] &quot;Kevan Lannister&quot; [[25]]$gender [1] &quot;Male&quot; [[25]]$born [1] &quot;In 244 AC&quot; [[26]] [[26]]$name [1] &quot;Melisandre&quot; [[26]]$gender [1] &quot;Female&quot; [[26]]$born [1] &quot;At Unknown&quot; [[27]] [[27]]$name [1] &quot;Merrett Frey&quot; [[27]]$gender [1] &quot;Male&quot; [[27]]$born [1] &quot;In 262 AC&quot; [[28]] [[28]]$name [1] &quot;Quentyn Martell&quot; [[28]]$gender [1] &quot;Male&quot; [[28]]$born [1] &quot;In 281 AC, at Sunspear, Dorne&quot; [[29]] [[29]]$name [1] &quot;Samwell Tarly&quot; [[29]]$gender [1] &quot;Male&quot; [[29]]$born [1] &quot;In 283 AC, at Horn Hill&quot; [[30]] [[30]]$name [1] &quot;Sansa Stark&quot; [[30]]$gender [1] &quot;Female&quot; [[30]]$born [1] &quot;In 286 AC, at Winterfell&quot; If this is ugly syntax to you, that’s fine - the magrittr package also includes an extract function that works the same way. purrr::map(got_chars, magrittr::extract, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) [[1]] [[1]]$name [1] &quot;Theon Greyjoy&quot; [[1]]$gender [1] &quot;Male&quot; [[1]]$born [1] &quot;In 278 AC or 279 AC, at Pyke&quot; [[2]] [[2]]$name [1] &quot;Tyrion Lannister&quot; [[2]]$gender [1] &quot;Male&quot; [[2]]$born [1] &quot;In 273 AC, at Casterly Rock&quot; [[3]] [[3]]$name [1] &quot;Victarion Greyjoy&quot; [[3]]$gender [1] &quot;Male&quot; [[3]]$born [1] &quot;In 268 AC or before, at Pyke&quot; [[4]] [[4]]$name [1] &quot;Will&quot; [[4]]$gender [1] &quot;Male&quot; [[4]]$born [1] &quot;&quot; [[5]] [[5]]$name [1] &quot;Areo Hotah&quot; [[5]]$gender [1] &quot;Male&quot; [[5]]$born [1] &quot;In 257 AC or before, at Norvos&quot; [[6]] [[6]]$name [1] &quot;Chett&quot; [[6]]$gender [1] &quot;Male&quot; [[6]]$born [1] &quot;At Hag&#39;s Mire&quot; [[7]] [[7]]$name [1] &quot;Cressen&quot; [[7]]$gender [1] &quot;Male&quot; [[7]]$born [1] &quot;In 219 AC or 220 AC&quot; [[8]] [[8]]$name [1] &quot;Arianne Martell&quot; [[8]]$gender [1] &quot;Female&quot; [[8]]$born [1] &quot;In 276 AC, at Sunspear&quot; [[9]] [[9]]$name [1] &quot;Daenerys Targaryen&quot; [[9]]$gender [1] &quot;Female&quot; [[9]]$born [1] &quot;In 284 AC, at Dragonstone&quot; [[10]] [[10]]$name [1] &quot;Davos Seaworth&quot; [[10]]$gender [1] &quot;Male&quot; [[10]]$born [1] &quot;In 260 AC or before, at King&#39;s Landing&quot; [[11]] [[11]]$name [1] &quot;Arya Stark&quot; [[11]]$gender [1] &quot;Female&quot; [[11]]$born [1] &quot;In 289 AC, at Winterfell&quot; [[12]] [[12]]$name [1] &quot;Arys Oakheart&quot; [[12]]$gender [1] &quot;Male&quot; [[12]]$born [1] &quot;At Old Oak&quot; [[13]] [[13]]$name [1] &quot;Asha Greyjoy&quot; [[13]]$gender [1] &quot;Female&quot; [[13]]$born [1] &quot;In 275 AC or 276 AC, at Pyke&quot; [[14]] [[14]]$name [1] &quot;Barristan Selmy&quot; [[14]]$gender [1] &quot;Male&quot; [[14]]$born [1] &quot;In 237 AC&quot; [[15]] [[15]]$name [1] &quot;Varamyr&quot; [[15]]$gender [1] &quot;Male&quot; [[15]]$born [1] &quot;At a village Beyond the Wall&quot; [[16]] [[16]]$name [1] &quot;Brandon Stark&quot; [[16]]$gender [1] &quot;Male&quot; [[16]]$born [1] &quot;In 290 AC, at Winterfell&quot; [[17]] [[17]]$name [1] &quot;Brienne of Tarth&quot; [[17]]$gender [1] &quot;Female&quot; [[17]]$born [1] &quot;In 280 AC&quot; [[18]] [[18]]$name [1] &quot;Catelyn Stark&quot; [[18]]$gender [1] &quot;Female&quot; [[18]]$born [1] &quot;In 264 AC, at Riverrun&quot; [[19]] [[19]]$name [1] &quot;Cersei Lannister&quot; [[19]]$gender [1] &quot;Female&quot; [[19]]$born [1] &quot;In 266 AC, at Casterly Rock&quot; [[20]] [[20]]$name [1] &quot;Eddard Stark&quot; [[20]]$gender [1] &quot;Male&quot; [[20]]$born [1] &quot;In 263 AC, at Winterfell&quot; [[21]] [[21]]$name [1] &quot;Jaime Lannister&quot; [[21]]$gender [1] &quot;Male&quot; [[21]]$born [1] &quot;In 266 AC, at Casterly Rock&quot; [[22]] [[22]]$name [1] &quot;Jon Connington&quot; [[22]]$gender [1] &quot;Male&quot; [[22]]$born [1] &quot;In or between 263 AC and 265 AC&quot; [[23]] [[23]]$name [1] &quot;Jon Snow&quot; [[23]]$gender [1] &quot;Male&quot; [[23]]$born [1] &quot;In 283 AC&quot; [[24]] [[24]]$name [1] &quot;Aeron Greyjoy&quot; [[24]]$gender [1] &quot;Male&quot; [[24]]$born [1] &quot;In or between 269 AC and 273 AC, at Pyke&quot; [[25]] [[25]]$name [1] &quot;Kevan Lannister&quot; [[25]]$gender [1] &quot;Male&quot; [[25]]$born [1] &quot;In 244 AC&quot; [[26]] [[26]]$name [1] &quot;Melisandre&quot; [[26]]$gender [1] &quot;Female&quot; [[26]]$born [1] &quot;At Unknown&quot; [[27]] [[27]]$name [1] &quot;Merrett Frey&quot; [[27]]$gender [1] &quot;Male&quot; [[27]]$born [1] &quot;In 262 AC&quot; [[28]] [[28]]$name [1] &quot;Quentyn Martell&quot; [[28]]$gender [1] &quot;Male&quot; [[28]]$born [1] &quot;In 281 AC, at Sunspear, Dorne&quot; [[29]] [[29]]$name [1] &quot;Samwell Tarly&quot; [[29]]$gender [1] &quot;Male&quot; [[29]]$born [1] &quot;In 283 AC, at Horn Hill&quot; [[30]] [[30]]$name [1] &quot;Sansa Stark&quot; [[30]]$gender [1] &quot;Female&quot; [[30]]$born [1] &quot;In 286 AC, at Winterfell&quot; What if we want this to be a data frame instead? We can use map_dfr to get a data frame that is formed by row-binding each element in the list. purrr::map_dfr(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) # A tibble: 30 x 3 name gender born &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Theon Greyjoy Male &quot;In 278 AC or 279 AC, at Pyke&quot; 2 Tyrion Lannister Male &quot;In 273 AC, at Casterly Rock&quot; 3 Victarion Greyjoy Male &quot;In 268 AC or before, at Pyke&quot; 4 Will Male &quot;&quot; 5 Areo Hotah Male &quot;In 257 AC or before, at Norvos&quot; 6 Chett Male &quot;At Hag&#39;s Mire&quot; 7 Cressen Male &quot;In 219 AC or 220 AC&quot; 8 Arianne Martell Female &quot;In 276 AC, at Sunspear&quot; 9 Daenerys Targaryen Female &quot;In 284 AC, at Dragonstone&quot; 10 Davos Seaworth Male &quot;In 260 AC or before, at King&#39;s Landing&quot; # … with 20 more rows # Equivalent to purrr::map(got_chars, `[`, c(&quot;name&quot;, &quot;gender&quot;, &quot;born&quot;)) %&gt;% dplyr::bind_rows() # A tibble: 30 x 3 name gender born &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 Theon Greyjoy Male &quot;In 278 AC or 279 AC, at Pyke&quot; 2 Tyrion Lannister Male &quot;In 273 AC, at Casterly Rock&quot; 3 Victarion Greyjoy Male &quot;In 268 AC or before, at Pyke&quot; 4 Will Male &quot;&quot; 5 Areo Hotah Male &quot;In 257 AC or before, at Norvos&quot; 6 Chett Male &quot;At Hag&#39;s Mire&quot; 7 Cressen Male &quot;In 219 AC or 220 AC&quot; 8 Arianne Martell Female &quot;In 276 AC, at Sunspear&quot; 9 Daenerys Targaryen Female &quot;In 284 AC, at Dragonstone&quot; 10 Davos Seaworth Male &quot;In 260 AC or before, at King&#39;s Landing&quot; # … with 20 more rows 12.3 Creating (and Using) List-columns Data structures in R are typically list-based in one way or another. Sometimes, more complicated data structures are actually lists of lists, or tibbles with a list-column, or other variations on “list within a ____.” In combination with purrr, this is an incredibly powerful setup that can make working with simulations and data very easy. Suppose, for instance, I want to simulate some data for modeling purposes, where I can control the number of outliers in the dataset: data_sim &lt;- function(n_outliers = 0) { tmp &lt;- tibble(x = seq(-10, 10, .1), y = rnorm(length(x), mean = x, sd = 1)) outlier_sample &lt;- c(NULL, sample(tmp$x, n_outliers)) # Create outliers tmp %&gt;% mutate( is_outlier = x %in% outlier_sample, y = y + is_outlier * sample(c(-1, 1), n(), replace = T) * runif(n(), 5, 10) ) } data_sim() # A tibble: 201 x 3 x y is_outlier &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; 1 -10 -11.1 FALSE 2 -9.9 -9.19 FALSE 3 -9.8 -9.85 FALSE 4 -9.7 -8.81 FALSE 5 -9.6 -11.3 FALSE 6 -9.5 -7.00 FALSE 7 -9.4 -9.46 FALSE 8 -9.3 -9.58 FALSE 9 -9.2 -7.46 FALSE 10 -9.1 -9.73 FALSE # … with 191 more rows Now, lets suppose that I want 100 replicates of each of 0, 5, 10, and 20 outliers. sim &lt;- crossing(rep = 1:100, n_outliers = c(0, 5, 10, 20)) %&gt;% mutate(sim_data = purrr::map(n_outliers, data_sim)) I could use unnest(sim_data) if I wanted to expand my data a bit to see what I have, but in this case, it’s more useful to leave it in its current, compact form. Instead, suppose I fit a linear regression to each of the simulated data sets, and store the fitted linear regression object in a new list-column? sim &lt;- sim %&gt;% mutate(reg = purrr::map(sim_data, ~lm(data = ., y ~ x))) Here, we use an anonymous function in purrr: by using ~{expression}, we have defined a function that takes the argument . (which is just a placeholder). So in our case, we’re saying “use the data that I pass in to fit a linear regression of y using x as a predictor.” Let’s play around a bit with this: We might want to look at our regression coefficients or standard errors to see how much the additional outliers affect us. We could use a fancy package for tidy modeling, such as broom, but for now, lets do something a bit simpler and apply the purrr name extraction functions we used earlier. It can be helpful to examine one of the objects just to see what you’re dealing with: str(sim$reg[[1]]) List of 12 $ coefficients : Named num [1:2] 0.01 0.959 ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;x&quot; $ residuals : Named num [1:201] -1.253 -1.56 -0.138 0.512 1.277 ... ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ effects : Named num [1:201] -0.1417 78.9134 0.0919 0.7403 1.5041 ... ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;(Intercept)&quot; &quot;x&quot; &quot;&quot; &quot;&quot; ... $ rank : int 2 $ fitted.values: Named num [1:201] -9.58 -9.49 -9.39 -9.3 -9.2 ... ..- attr(*, &quot;names&quot;)= chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ assign : int [1:2] 0 1 $ qr :List of 5 ..$ qr : num [1:201, 1:2] -14.1774 0.0705 0.0705 0.0705 0.0705 ... .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : chr [1:201] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;x&quot; .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ..$ qraux: num [1:2] 1.07 1.11 ..$ pivot: int [1:2] 1 2 ..$ tol : num 1e-07 ..$ rank : int 2 ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; $ df.residual : int 199 $ xlevels : Named list() $ call : language lm(formula = y ~ x, data = .) $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language y ~ x .. ..- attr(*, &quot;variables&quot;)= language list(y, x) .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. .. ..$ : chr [1:2] &quot;y&quot; &quot;x&quot; .. .. .. ..$ : chr &quot;x&quot; .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;x&quot; .. ..- attr(*, &quot;order&quot;)= int 1 .. ..- attr(*, &quot;intercept&quot;)= int 1 .. ..- attr(*, &quot;response&quot;)= int 1 .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x55b2d13128a0&gt; .. ..- attr(*, &quot;predvars&quot;)= language list(y, x) .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;y&quot; &quot;x&quot; $ model :&#39;data.frame&#39;: 201 obs. of 2 variables: ..$ y: num [1:201] -10.84 -11.05 -9.53 -8.78 -7.92 ... ..$ x: num [1:201] -10 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 ... ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language y ~ x .. .. ..- attr(*, &quot;variables&quot;)= language list(y, x) .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. .. .. ..$ : chr [1:2] &quot;y&quot; &quot;x&quot; .. .. .. .. ..$ : chr &quot;x&quot; .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;x&quot; .. .. ..- attr(*, &quot;order&quot;)= int 1 .. .. ..- attr(*, &quot;intercept&quot;)= int 1 .. .. ..- attr(*, &quot;response&quot;)= int 1 .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x55b2d13128a0&gt; .. .. ..- attr(*, &quot;predvars&quot;)= language list(y, x) .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;y&quot; &quot;x&quot; - attr(*, &quot;class&quot;)= chr &quot;lm&quot; If we pull out the coefficients by name we get a vector of length two. So before we unnest, we need to change that so that R formats it as a row of a data frame. sim$reg[[1]]$coefficients %&gt;% as_tibble_row() # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 0.0100 0.959 This will make our formatting a lot easier and prevent any duplication that might occur if we unnest a vector that has length &gt; 1. sim &lt;- sim %&gt;% mutate(coefs = purrr::map(reg, &quot;coefficients&quot;) %&gt;% purrr::map(as_tibble_row)) sim$coefs[1:5] [[1]] # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 0.0100 0.959 [[2]] # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 0.157 1.02 [[3]] # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 -0.139 1.04 [[4]] # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 0.135 0.983 [[5]] # A tibble: 1 x 2 `(Intercept)` x &lt;dbl&gt; &lt;dbl&gt; 1 0.0282 0.999 Then, we can plot our results: sim %&gt;% unnest(coefs) %&gt;% select(rep, n_outliers, `(Intercept)`, x) %&gt;% pivot_longer(-c(rep, n_outliers), names_to = &quot;coef&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(x = value, color = factor(n_outliers))) + geom_density() + facet_wrap(~coef, scales = &quot;free_x&quot;) So as there are more and more outliers, the coefficient estimates get a wider distribution, but remain (relatively) centered on the “true” values of 0 and 1, respectively. Notice that we keep our data in list column form right up until it is time to actually unnest it - which means that we have at the ready the simulated data, the simulated model, and the conditions under which it was simulated, all in the same data structure. It’s a really nice, organized system. 12.4 Ways to use map There are 3 main use cases for map (and its cousins pmap, map2, etc.): Use with an existing function Use with an anonymous function, defined on the fly Use with a formula (which is just a concise way to define an anonymous function) I’ll use a trivial example to show the difference between these options: # An existing function res &lt;- tibble(x = 1:10, y1 = map_dbl(x, log10)) # An anonymous function res &lt;- res %&gt;% mutate(y2 = map_dbl(x, function(z) z^2/10)) # A formula equivalent to function(z) z^5/(z + 10) # the . is the variable you&#39;re manipulating res &lt;- res %&gt;% mutate(y3 = map_dbl(x, ~.^5/(.+10))) It can be a bit tricky to differentiate between options 2 and 3 in practice - the biggest difference is that you’re not using the keyword function and your variable is the default placeholder variable . used in the tidyverse. .reset() library(tidyverse) ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ✓ ggplot2 3.3.3.9000 ✓ purrr 0.3.4 ✓ tibble 3.1.0 ✓ dplyr 1.0.5 ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ✓ readr 1.4.0 ✓ forcats 0.5.1 ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── x tidyr::extract() masks magrittr::extract() x dplyr::filter() masks stats::filter() x dplyr::lag() masks stats::lag() x purrr::set_names() masks magrittr::set_names() library(repurrrsive) Try it out Use each of the 3 options for defining a method in purrr to pull out a single string of all of the books each character was in. To do this, you’ll need to collapse the list of books for each character into a single string, which you can do with the paste function and the collapse argument. letters[1:10] %&gt;% paste(collapse = &quot;|&quot;) [1] &quot;a|b|c|d|e|f|g|h|i|j&quot; Start with this data frame of character names and book list-columns: data(got_chars) got_df &lt;- tibble(name = map_chr(got_chars, &quot;name&quot;), id = map_int(got_chars, &quot;id&quot;), books = map(got_chars, &quot;books&quot;)) Error in map.poly(database, regions, exact, xlim, ylim, boundary, interior, : no recognized region names Solution # Define a function my_collapse &lt;- function(x) paste(x, collapse = &quot; | &quot;) data(got_chars) got_df &lt;- tibble(name = map_chr(got_chars, &quot;name&quot;), id = map_int(got_chars, &quot;id&quot;), books = map(got_chars, &quot;books&quot;)) Error in map.poly(database, regions, exact, xlim, ylim, boundary, interior, : no recognized region names got_df &lt;- got_df %&gt;% mutate( fun_def_res = map_chr(books, my_collapse), # Here, I don&#39;t have to define a function, I just pass my additional # argument in after the fact... fun_base_res = map_chr(books, paste, collapse = &quot; | &quot;), # Here, I can just define a new function without a name and apply it to # each entry fun_anon_res = map_chr(books, function(x) paste(x, collapse = &quot; | &quot;)), # And here, I don&#39;t even bother to specifically say that I&#39;m defining a # function, I just apply a formula to each entry fun_formula_res = map_chr(books, ~paste(., collapse = &quot; | &quot;)) ) Error in mutate(., fun_def_res = map_chr(books, my_collapse), fun_base_res = map_chr(books, : object &#39;got_df&#39; not found head(got_df) Error in head(got_df): object &#39;got_df&#39; not found 12.5 Beyond map: Functions with multiple inputs Sometimes, you might need to map a function over two vectors/lists in parallel. purrr has you covered with the map2 function. As with map, the syntax is map2(thing1, thing2, function, other.args); the big difference is that function takes two arguments. Let’s create a simple times-table: crossing(x = 1:10, y = 1:10) %&gt;% mutate(times = map2_int(x, y, `*`)) %&gt;% pivot_wider(names_from = y, names_prefix = &#39;y=&#39;, values_from = times) # A tibble: 10 x 11 x `y=1` `y=2` `y=3` `y=4` `y=5` `y=6` `y=7` `y=8` `y=9` `y=10` &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 1 2 3 4 5 6 7 8 9 10 2 2 2 4 6 8 10 12 14 16 18 20 3 3 3 6 9 12 15 18 21 24 27 30 4 4 4 8 12 16 20 24 28 32 36 40 5 5 5 10 15 20 25 30 35 40 45 50 6 6 6 12 18 24 30 36 42 48 54 60 7 7 7 14 21 28 35 42 49 56 63 70 8 8 8 16 24 32 40 48 56 64 72 80 9 9 9 18 27 36 45 54 63 72 81 90 10 10 10 20 30 40 50 60 70 80 90 100 # we could use `multiply_by` instead of `*` if we wanted to If you are using formula notation to define functions with map2, you will need to refer to your two arguments as .x and .y. You can determine this from the Usage section when you run map2, which shows you map2(.x, .y, .f, ...) - that is, the first argument is .x, the second is .y, and the third is the function. Try it out Use map2 to determine if each Game of Thrones character has more titles than aliases. Start with this code: library(repurrrsive) library(tidyverse) data(got_chars) got_names &lt;- tibble(name = purrr::map_chr(got_chars, &quot;name&quot;), titles = purrr::map(got_chars, &quot;titles&quot;), aliases = purrr::map(got_chars, &quot;aliases&quot;)) Solution got_names %&gt;% mutate(more_titles = map2_lgl(titles, aliases, ~length(.x) &gt; length(.y))) # A tibble: 30 x 4 name titles aliases more_titles &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;lgl&gt; 1 Theon Greyjoy &lt;chr [3]&gt; &lt;chr [4]&gt; FALSE 2 Tyrion Lannister &lt;chr [2]&gt; &lt;chr [11]&gt; FALSE 3 Victarion Greyjoy &lt;chr [2]&gt; &lt;chr [1]&gt; TRUE 4 Will &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE 5 Areo Hotah &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE 6 Chett &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE 7 Cressen &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE 8 Arianne Martell &lt;chr [1]&gt; &lt;chr [1]&gt; FALSE 9 Daenerys Targaryen &lt;chr [5]&gt; &lt;chr [11]&gt; FALSE 10 Davos Seaworth &lt;chr [4]&gt; &lt;chr [5]&gt; FALSE # … with 20 more rows Like map, you can specify the type of the output response using map2. This makes it very easy to format the output appropriately for your application. You can use functions with many arguments with map by using the pmap variant; here, you pass in a list of functions, which are identified by position (..1, ..2, ..3, etc). Note the .. - you are referencing the list first, and the index within the list argument 2nd. Purrr References The Joy of Functional Programming (for Data Science): Hadley Wickham’s talk on purrr and functional programming. ~1h video and slides. (The Joy of Cooking meets Data Science, with illustrations by Allison Horst) Pirating Web Content Responsibly with R and purrr (a blog post in honor of international talk like a pirate day) Happy R Development with purrr Web mining with purrr Text Wrangling with purrr Setting NAs with purrr (uses the naniar package) Mappers with purrr - handy ways to make your code simpler if you’re reusing functions a lot. Function factories - code optimization with purrr Stats and Machine Learning examples with purrr "],["high-performance-computing.html", "Module 13 High Performance Computing HPC References", " Module 13 High Performance Computing The material for this week will be provided in a lecture given by the Holland Computing Center. Here, I’m going to list off a few of the useful resources at the Holland Computing Center. An introduction to High Performance Computing Creating an account Connecting to the HCC clusters Basic Linux Commands (and a video tutorial) Using X11 Forwarding (how to get minimal graphical application support via SSH) Using R software on the cluster HPC References Working with the Unix Shell Software Carpentry lesson "],["tidy-models.html", "Module 14 Tidy Models", " Module 14 Tidy Models Tidymodels workflow Tidymodels is a set of packages designed to help with the modeling part of the tidy workflow shown above. We’ll be working off of the Tidymodels introduction. There are 5 articles in the sequence; we’ll work through them together in class. Please make sure you have the tidymodels package installed as well as the broom.mixed package. install.packages(&quot;tidymodels&quot;) install.packages(&quot;broom.mixed&quot;) "],["spatial-packages.html", "Module 15 Spatial packages References", " Module 15 Spatial packages Map Projections Matter! We’ll work off of this set of 3 tutorials: Drawing Beautiful maps with R, sf, and ggplot2: Basics Layers Layouts References Geocomputation with R - a whole textbook on spatial work in R, including ecology, marketing, transportation, and connecting to GIS software. Analyzing Honeybee Permits in Minneapolis "],["common-problems-and-general-computing-topics.html", "Module 16 Common Problems and General Computing Topics 16.1 File Paths, Working Directories, and Reproducibility", " Module 16 Common Problems and General Computing Topics 16.1 File Paths, Working Directories, and Reproducibility 16.1.1 File paths A file path is how you tell the computer where to find a file. You might be familiar with the “C:\\Program Files\\” structure - that’s a file path telling Windows to look at the C: drive, in the Program Files folder. There are two types of file paths: absolute and relative. - An absolute file path is a file path that tells the computer the location of the file, regardless of where the computer is currently working from (regardless of the current working directory). An absolute file path is something like an address - no matter where you’re currently located, an address will give you the information necessary to get to the correct house. Examples: - C:\\USER\\DOCS\\LETTER.TXT (Windows) - \\\\SERVER01\\USER\\DOCS\\LETTER.TXT (Windows, for e.g. remote file servers) - /home/user/docs/Letter.txt (UNIX/macOS) A relative file path is a file path that tells the computer how to get to a file from its current location. To continue the analogy, a relative file path is like “Go down the hallway, turn left, take a right at the next hallway, and go to the third door on the right.” - it gives you information on how to get from your current location to the destination, but that information wouldn’t necessarily work for someone who’s in a different location. Examples: ../../Letter.txt\"` (this says go up two directories and look for Letter.txt) ./Letter.txt (this says look for Letter.txt in your current directory) ./data/Letter.txt (this says look for Letter.txt in the data folder in the current directory) If you are using a UNIX-like environment (Linux, macOS), you have an additional shortcut available: ~/ is the shortcut for the user’s home directory. So ~/ is equivalent to /home/ted/ as long as you’re logged in as ted. If you’re logged in as theodora, though, ~/ is equivalent to /home/theodora/. This YouTube video has a good explanation as well. 16.1.2 Working Directory When you start a program on a computer, it starts with a working directory. In many cases, this may be the user’s home directory, but that’s not always the case. For instance, if you are working in an RStudio project, your working directory is the folder containing the .Rproj file. If you are compiling an Rmarkdown document, your working directory is the folder where the document is saved. Your working directory determines what relative file path you should be using. Here is a video showing how to change your working directory in SAS, and in R. 16.1.3 Reproducibility Reproducibility is the idea that I should be able to run your code and get the same results you got. Ideally, to do this, I wouldn’t have to configure my computer in exactly the same way your computer is set up. Instead, ideally, your code will use relative file paths, with a working directory that is appropriate to the project set-up. Since we’re storing everything on GitHub for this class, a natural file setup is to have the project working directory be the same directory the git repository is based in (the directory should contain a .git folder, but you may have to view hidden files to see it). You can store your data in a data/ subfolder, your extra scripts in a code/ folder, etc., but all of the files you need to run the code should be included in the git repository unless they are too large for git. Then, when someone else clones your repository, they will have access to the data they need to run the code, and the code will be written with relative file paths that match the file structure. This also has the advantage of saving you tons of time trying to help your PI figure out how to run your code… you can direct them to GitHub, have them download the folder (or clone the repo, if they’re tech-savvy), and everything should just work. "],["other-resources-articles-and-food-for-thought.html", "Module 17 Other Resources, Articles, and Food for Thought 17.1 Comparing Languages", " Module 17 Other Resources, Articles, and Food for Thought 17.1 Comparing Languages Data Scientist’s Analysis Toolbox: Comparison of Python, R, and SAS Performance "],["references-8.html", "References", " References "]]
