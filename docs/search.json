[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computing Tools for Statisticians",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#textbook-status",
    "href": "index.html#textbook-status",
    "title": "Computing Tools for Statisticians",
    "section": "Textbook Status",
    "text": "Textbook Status\nOnce upon a time (pre 2022), this book covered R and SAS. We are slowly transitioning away from SAS to use R and, eventually, python. This book is in a state of transition; I have removed most of the SAS content and what is left is not interactive (because I don’t have a SAS license anymore), but here and there mentions of SAS remain for comparison purposes. I am in the process of ensuring that the Python content is as detailed and thorough as the R content, but I expect that this will take at least a year to get right."
  },
  {
    "objectID": "index.html#forward",
    "href": "index.html#forward",
    "title": "Computing Tools for Statisticians",
    "section": "Forward",
    "text": "Forward\nThis textbook is intended to be a substitute for hours and hours of lectures consisting of me reading code to you and showing you what it does. That’s awful, and boring, and not particularly effective for teaching a class of people with vastly different computing experience the same topics.\n\n\nIf you want boring, or have insomnia, check out this Microsoft Word tutorial from 1989. Youtube says it’s the most boring video ever made.\nInstead, I hope that you’ll be able to work through this book week by week. I’ve included comics, snark, gifs, YouTube videos, illustrations, LEGO constructions, and more, with the goal of making this a collection of the best information I could find on learning some combination of R, SAS, Python, and general statistical programming. In every chapter, there are “Try it Out” sections to show you what different concepts look like in code - these are meant to provide you with an interactive way to explore the content, and skipping these sections will seriously limit the amount of information you take away from this book.\nI’ve designed this textbook to work with Stat 850 as taught at University of Nebraska-Lincoln. Here is a syllabus of Stat 850 for reference; feel free to contact me if you have any additional questions about the material or what the course covers."
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "Computing Tools for Statisticians",
    "section": "How to Use This Book",
    "text": "How to Use This Book\nI’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\nButtons/Links\nThe book contains a number of features which should help you navigate, use, improve, and respond to the textbook.\n\n\nTextbook features, menus, and interactive options\n\nSpecial Sections\n\n\n\n\n\n\nWarnings\n\n\n\nThese sections contain things you may want to look out for: common errors, mistakes, and unfortunate situations that may arise when programming.\n\n\n\n\n\n\n\n\nExamples\n\n\n\nThese sections contain code and other illustrations of the concepts discussed in the chapter. Don’t skip them, even though they may be long!\n\n\n\n\n\n\n\n\nTry it out\n\n\n\nThese sections contain activities you should do to reinforce the things you’ve just read. You will be much more successful if you read the material, review the example, and then try to write your own code. Most of the time, these sections will have a specific format:\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe problem will be in the first tab for you to start with\n\n\nA solution will be provided in R, potentially with an explanation.\n\n\nA solution will be provided in Python as well.\n\n\n\nIn some cases, the problem will be more open-ended and may not adhere to this format, but most try it out sections in this book will have solutions provided. I highly recommend that you attempt to solve the problem yourself before you look at the solutions - this is the best way to learn. Passively reading code does not result in retention of the information.\n\n\n\n\n\n\n\n\nEssential Reading\n\n\n\nThese sections may direct you to additional reading material that is essential for understanding the topic. For instance, I will sometimes link to other online textbooks rather than try to rehash the content myself when someone else has done it better.\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThese sections will direct you to additional resources that may be helpful to consult as you learn about a topic. You do not have to use these sections unless you are 1) bored, or 2) hopelessly lost. They’re provided to help but are not expected reading (Unlike the essential reading sections in red).\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThese generic sections contain information I may want to call attention to, but that isn’t necessarily urgent or a common error trap.\n\n\nExpandable Sections\nThese are expandable sections, with additional information when you click on the line\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\n\n\n\nAnother type of expandable note\n\n\n\n\n\nAnswers or punchlines may be hidden in this type of expandable section as well."
  },
  {
    "objectID": "tools.html#module-1-objectives",
    "href": "tools.html#module-1-objectives",
    "title": "1  Exploring the Toolbox",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nSet up necessary software for this class on personal machines\nDetect and resolve problems related to file systems, working directories, and system paths when troubleshooting software installation\nUse version control to track changes to a document (git add, commit, push, pull)"
  },
  {
    "objectID": "tools.html#some-computer-basics",
    "href": "tools.html#some-computer-basics",
    "title": "1  Exploring the Toolbox",
    "section": "\n1.1 Some Computer Basics",
    "text": "1.1 Some Computer Basics\nI grew up in the 1990s, and to use computers, you had to know a bit more about how computers worked than you do now. We also weren’t so far away from the days of file cabinets with actual file folders.\nMany people teaching computing in the 2020s have noticed that students are lacking some essential awareness of some of the basics of how a computer works… essentials that help out a lot when it comes time to program a computer. So this section is intended to serve as a quick overview of some of those principles. Scan it, and if you don’t understand what a term means, please do some googling.\n\n1.1.1 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\nLearn More about Computer Hardware\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture\n\n1.1.2 Operating Systems\nOperating systems, such as Windows , MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time.\n\n\n\n\n\n1.1.3 File Systems\n\nFor this class, it will probably be important to distinguish between local file storage (C:/ drive , /user/your-name/ , or /home/your-name/ ) and network/virtual file systems, such as OneDrive and iCloud. Over time, it has become harder to ensure that you are working on a local machine, but working “in the cloud” can cause odd errors when programming and in particular when working with version control systems1.\nYou want to save your files in this class to your physical hard drive. This will save you a lot of troubleshooting time.\n\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders). We will practice this by working in RStudio projects.\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home.\n\n1.1.4 System Paths\nWhen you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don’t need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu, and you find your programs there.\nUnfortunately, that isn’t sufficient when you’re programming, because you may need to know where a program is in order to reference that program – for instance, if you need to pop open a browser window as part of your program, you’re (most likely) going to have to tell your computer where that browser executable file lives.\nTo simplify this process, operating systems have what’s known as a “system path” or “user path” - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available.\n\n\n\n\n\n\nHow To Modify System Paths\n\n\n\nHow to set system paths (general)\nOperating-system specific instructions cobbled together from a variety of different sources:\n\n On Windows\n On Mac\n On Linux\n\n\n\nIf you run across an error that says something along the lines of\n\ncould not locate xxx.exe\nThe system cannot find the path specified\nCommand Not Found\n\nyou might start thinking about whether your system path is set correctly for what you’re trying to do.\nIf you want to locate where an executable is found (in this example, we’ll use git), you can run where git on windows, or which git on OSX/Linux.\nSome programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools &gt; Global Options &gt; Git/SVN, you can set the path to git.\n\n1.1.5 Working Directories\nWhen you launch a program, that program starts up with a specific directory as its “location” - the place where it will look for files. Most of the time, we don’t think too much about this when using graphical programs, but it’s much more important to get this right when programming things ourselves.\nIn RStudio, you can set your working directory in several ways:\n\nWork in an RStudio Project (File &gt; New Project), which will make it easy to access your project folder, and will set your working directory to the right folder automatically. I’ve created a project in /home/susan/Projects/Class/unl-stat850/test-project\nUse setwd() to set your working directory. If I’m not working in the RStudio project described above, I can set my working directory to that folder using: setwd(\"/home/susan/Projects/Class/unl-stat850/test-project\")\nUse the files tab (bottom right) to navigate to your preferred folder. Then, click More &gt; Set As Working Directory.\n\nIn this class, we’re going to focus on reproducibility - making sure your code runs on any other computer with the right project setup. So with that in mind:\n\nWork in (RStudio) projects2\n\nStore the data for the project in your project folder\nUse local paths - if you’re in your project folder and trying to reference “data.xls”, then you would just use “data.xls” as the path. If your data is in a “code” folder, you would use “code/data.xls” as the path.\n\nThis will ensure that your code should work on any other machine, as long as your data is in your git repository and your code uses local file paths."
  },
  {
    "objectID": "tools.html#acquiring-the-tools-setting-up-your-computer",
    "href": "tools.html#acquiring-the-tools-setting-up-your-computer",
    "title": "1  Exploring the Toolbox",
    "section": "\n1.2 Acquiring the Tools: Setting Up Your Computer",
    "text": "1.2 Acquiring the Tools: Setting Up Your Computer\nFirst, lets cover the basics of setting up the software you’re going to be using in this course. You don’t at this point have to understand what the different software DOES, but you should at least get it installed and working – this may be tricky, so we’ll take it slow.\nFirst, we’re going to install all of the software you need for this class. The order here matters.\nIn the next section, I’ll explain what each piece of software is, and what it is used for.\n\nInstall R.\n\nDownload and run the R installer for your operating system from CRAN. Please make sure you are using at least R 4.13\n\n\n Windows: https://cran.rstudio.com/bin/windows/base/\n\n\n Mac: https://cran.rstudio.com/bin/macosx/\n\n\n Linux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\n\nIf you are on Windows, install the Rtools4 package; this will ensure you get fewer warnings later when installing packages.\nMore detailed instructions for Windows are available here\n\n\n\n\n\n\n\n\n\n\nWalkthrough videos and additional help\n\n\n\n\n\n\nVideos (these use very OLD versions of R, but the instructions are the same.)\n\n\n R on Windows installation\n\n\n R on Mac installation\n\n\n R on Linux installation (Text-based tutorial)\nOn Linux, instead of a YouTube video, you get a text-based tutorial. One of the Debian maintainers, Dirk Eddelbuettel, is also on R core, which means that R tends to work extremely well with Debian-based distributions, like Ubuntu and Linux Mint. R does work on RPM based distros, and you can of course also compile it from source for Arch/Gentoo, but I’ve stuck with Deb-based distributions for approximately 7 years because it seems to be a bit less hassle. Additional troubleshooting can be found here.\n\n\n\n\n\n\n\n\nInstall RStudio (version 2022.02.2 or higher)\n\nYou can find RStudio here. You want the open source edition of RStudio Desktop. If you’re feeling brave, you can install the preview release - this may have a few bugs, but tends to be relatively stable and has the latest features.\nIf you’re on Linux, go ahead and import RStudio’s public key so that software validation works. (All of the commands are provided in the linked page)\n\n\nInstall git using the instructions here. Consult the troubleshooting guide if you have issues. If that fails, then seek help in office hours.\n\nInstall Python 3 and Jupyter\n\npython 3\nInstructions to install Jupyter (install with pip unless you know what you’re doing)\n\n\nInstall Quarto. We will be using Quarto with RStudio throughout this book.\n\nInstall LaTeX and rmarkdown:\n\nLaunch R, and type the following commands into the console:\n\n\n\n\ninstall.packages(c(\"tinytex\", \"knitr\", \"rmarkdown\", \"quarto\"))\nlibrary(tinytex)\ninstall_tinytex()\n\n\n\n\n\n\n\nImportant\n\n\n\nTo get things fully set up you will need to read the rest of this chapter, and do all of the Try it Out sections. They are essential for ensuring that you have the correct packages installed!"
  },
  {
    "objectID": "tools.html#explaining-the-tools-what-have-i-done",
    "href": "tools.html#explaining-the-tools-what-have-i-done",
    "title": "1  Exploring the Toolbox",
    "section": "\n1.3 Explaining the Tools: What Have I Done?",
    "text": "1.3 Explaining the Tools: What Have I Done?\n\n1.3.1 RStudio: The IDE\nAn IDE is an integraged development environment - a fancy, souped up text editor that is built to make programming easier. Back in the dark ages, people wrote programs in text editors4 and then used the command line to compile those programs and run them.\nRStudio provides a cheat-sheet for the IDE if you are so inclined.\n\n\n\n\n\n\nNote\n\n\n\nRStudio is not R - it’s just a layer on top of R. So if you have a question about the user interface, you have an RStudio question. If you have a question about the code, you have an R question.\n\n\nThis book is written with the idea that you will use RStudio to write, run, and debug your code (in both R and python).\nNavigating RStudio\n\n\nThe RStudio window will look something like this.\n\n\nIn the top-left pane is the text editor. This is where you’ll do most of your work.\nIn the top right, you’ll find the environment, history, and connections tabs. The environment tab shows you the objects available in R (variables, data files, etc.), the history tab shows you what code you’ve run recently, and the connections tab is useful for setting up database connections.\nOn the bottom left is the console. There are also other tabs to give you a terminal (command line) prompt, and a jobs tab to monitor progress of long-running jobs. In this class we’ll primarily use the console tab.\n\nOn the bottom right, there are a set of tabs:\n\nfiles (to give you an idea of where you are working, and what files are present),\nplots (which will be self-explanatory),\npackages (which extensions to R are installed and loaded),\nthe help window (where documentation will show up), and\nthe viewer window, which is used for interactive graphics or previewing HTML documents.\n\n\n\n\n\n\n\n\n\nTry It Out: RStudio\n\n\n\n\nTo get started, open RStudio and find the console window.\nType 2+2 into the console window and hit enter.\nNow, open the text editor (File -&gt; New -&gt; R script).\nType 2+2 into the text editor and press the run button that is on the pane’s shortcut bar (or, you can hit Ctrl-Enter/CMD-Enter to send a single line to the console).\n\nIf both of those things worked, you’re probably set up correctly!\n\n\nNext, try typing this into the text editor, then run the line. Look in the environment tab and see if you can see what has changed.\n\n\na &lt;- 3 # store 3 in the variable a\n\n\n\nYour environment window should now look something like this (the .Last.value entry may not be there, and that’s ok)\n\nYou can use the environment window to preview your data, check on the status of variables, and more. Note that while R is running, the window doesn’t update (so you can’t check on the status of a loop while the loop is running using the window).\n\n\nNext, try typing this into the text editor, and then run all of the lines at the same time. You should see something change in the Plots tab.\n\n\nx &lt;- seq(0, 2*pi, length.out = 50)\ny &lt;- sin(x)\nplot(x = x, y = y, type = 'l')\ntext(x = pi*1.5, y = 0, labels = \"Welcome to R!\")\n\n\n\nProgramming Languages\nHaving established the generic definitions of the concepts which apply to almost any programming language, we now must examine how specific programming languages implement these concepts.\nR is a statistical programming languages - it is specifically designed to work with data, which means that it makes compromises that other languages do not in order to make it easier to write code where the data (rather than the functions, classes, methods, or objects) are the primary concern. (SAS is also a statistical programming language, but is only briefly covered in this book, with some additional content in an appendix).\nPython is a general purpose programming language. It can be much more difficult to accomplish simple data-driven tasks in Python, but Python’s construction will be much more familiar to anyone who has spent time in C, Java, or other general-purpose languages.\nHere are a few quick facts about each of the programming languages that are mentioned (even peripherally) in this book; at least two of the three are used in most statistics departments, though which 2 tends to vary based on a number of factors including the types of applied statisticians in the department and how modern the department is.\n\n\nR\nPython\nSAS\n\n\n\n\nPredecessor: S\nR is an open-source clone of the S statistical programming language used internally at AT&T Bell Labs. S dates back to 1976; R first appeared in 1991 and was made public in 1993.\nHistory of R\nLanguage type:functional - consists mainly of functions that manipulate objects\n\n\n\n\nPredecessor: ABC programming language\nHistory of Python\nLanguage type:interpreted general purpose high level programming language. Focus on readable code using indentation to indicate grouping.\n\n\n\n\nPredecessor: None\nSAS dates back to the 1960s and has syntax which is unique because it predates C and Fortran.\nHistory of SAS\nLanguage type:procedural - consists of steps/procedures that list the manipulations that will be performed on a set of data\n\nOther Notes:\n\nSAS is actually several languages. Commands that work in the DATA step do not necessarily work in a PROC step. SAS also has its own general purpose programming language called IML (Interactive Matrix programming Language).\nThe way SAS has been included in this course is different than it is usually taught, because I try to introduce SAS concepts at the same time as the corresponding R concept. If you are just needing to use SAS for e.g. your linear modeling classes, please just use the SAS documentation - it will be more than sufficient.\n\n\n\n\n\n\nIn this section, we’ll talk a bit more about R and python specifically, as they are covered extensively in this book. SAS content is relegated to the appendix - I no longer teach SAS at\n\n\n\n\n\n\nIf you’ve used R/SAS/Python before…\n\n\n\nIf you’ve programmed before, the next few chapters are going to be boring. Sorry, there’s no help for that. Some of your classmates haven’t ever so much as written “Hello World”, and we have to get them up to speed.\nIf you’re bored, or feel like you know this material, skim through it anyways just to confirm (and if I’m doing something that’s really out there, or there’s an easier way to do it, tell me!). You can then either find something in the references that you don’t know already ([1] is always a great place to start if you want to be quickly confused), or help your classmates that are less experienced, which will help you learn new things and build relationships with other students.\n\n\nR\nR is a statistical programming language. Unlike more general-purpose languages, R is optimized for working with data and doing statistics. If you want to build an independent, standalone graphical interface, or run a web server, R is probably not the ideal language to use (you might want C/python or PHP/python, respectively). If you want to vacuum up a bunch of data, fit several regression models, and then compare the models, R is a great option and will be faster than working in a more general-purpose language like C or base python.\n\n\nA brief illustrated history of R. R is an open-source clone of S programming language, which was invented at Bell Labs. S was inspired by Scheme and Lisp, but is ultimately compiled in Fortran and C.\n\nR is\n\nvector-based\n1 indexed (start counting 1, 2, 3, …)\na scripting language (R code does not have to be compiled before it is run)\n\nOne of R’s strengths is the package repository, CRAN, that allows anyone (yes, even you!) to write an R package. This means that R generally has the latest statistical methods available, and one of the best ways to ensure someone uses your work is to write an R package to make that work accessible to the general population of statisticians/biologists/geneticists.\nTo install the tibble package in R, we would use the following code:\n\ninstall.packages(\"tibble\")\n\nThen, to use the functions within that package, we need to load the package:\n\nlibrary(tibble)\n\nWhen you load a package, all of the functions in that package are added to your R Namespace (this is a technical term) - basically the list of all of the things R knows about. This may be problematic if you have two packages with the same function name.\nIf you want to use a function from a package without loading the package into your namespace, you can do that by using pkgname::function syntax.\nFor instance, this code creates a sample data frame using the tribble function in the tibble package.\n\ntibble::tribble(~col1, ~col2, 1, 'a', 2, 'b', 3, 'c')\n## # A tibble: 3 × 2\n##    col1 col2 \n##   &lt;dbl&gt; &lt;chr&gt;\n## 1     1 a    \n## 2     2 b    \n## 3     3 c\n\n\n\n\n\n\n\nTry it Out (R package edition)\n\n\n\nTry to run the following code, which will install (most of) the packages you need to run the code in this book.\n\n# Read in a list of all packages that are required\npkgs &lt;- readLines(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/packages\")\n\n# Remove packages only available on github\npkgs &lt;- setdiff(pkgs, c(\"nycsquirrels18\", \"emo\", \"tweetrmd\", \"classdata\"))\n\n# Remove any already installed packages from the list\npkgs &lt;- setdiff(pkgs, installed.packages())\n\n# The following code will not make a lot of sense... yet. Come back to it in a \n# few weeks and see how much you understand (or can decode)\n\n# This ensures that if the installation fails, the code will keep running\ntry_install_pkg &lt;- function(...) try(install.packages(..., dependencies = T))\nlapply(pkgs, try_install_pkg)\npkgs &lt;- setdiff(pkgs, installed.packages())\n\nif (length(pkgs) &gt; 0) {\n  paste(\"The following packages did not install: \\n\", paste(pkgs, collapse = \"\\n\"))\n}\n\n# Try installing github packages\ndevtools::install_github(c(\"mine-cetinkaya-rundel/nycsquirrels18\", \n                           \"hadley/emo\", \n                           \"gadenbuie/tweetrmd\", \n                           \"heike/classdata\"))\n\n\n\nPython\nPython is a general-purpose programming language. Some people like to say that Python is the 2nd best language for everything. Python by itself is a terrible language for data analysis, but when combined with a couple of critical packages, Python becomes an excellent language for data analysis, and in particular, for machine learning tasks. Python’s strength is that it has access to a number of libraries for tasks like image processing, computer vision, and artificial intelligence that go outside of the domain of “traditional statistics” but are highly relevant to modern applied statistical tasks.\n\n\nA brief illustrated history of python. Python was created as an attempt to fix some problems with the ABC programming language, which focused on creating short, readable code. When python is combined with numpy and pandas, two libraries for data analysis, it is a very powerful language for statistical computing.\n\nAs we’ll be using python within RStudio, now is a good time to check and make sure that RStudio can find your python installation.\n\n\n\n\n\n\nTry it Out (Python + RStudio)\n\n\n\nGo to Tools -&gt; Global Options and select Python on the left-hand side of the popup menu. Ensure that there is at least one python installation available for RStudio.\n\n\nScreenshot of global options window and toolbar to find the python menu\n\n\n\nPython Packages\nPython packages can be installed in a couple of different ways, but for now, we’re going to talk about installing python packages using pip.\nPip is the most common python package manager. You should already have pip installed if you followed the directions to install python provided above. If you do not have pip installed, see these instructions.\nTo install packages using pip, you’ll need to use the command line [2].\nOne easy way to do that is to open RStudio and start a terminal within RStudio.\n\nThe terminal window should appear in the bottom left pane (by default).\nIn that terminal, you would type pip install pkgname to install the pkgname package.\nThen, when you want to use a python package, you would use the following code to load numpy:\n\nimport numpy as np\n\nThis says we’re going to refer to the numpy package as np, so any functions from that package will be called using np.functionname. This is different from how R typically handles this (and more similar to the pkgname::functionname syntax), but it ensures that there are not namespace collisions when you load multiple python packages. In general, this is much clearer.\nIf you want to use R-style package loading, you can, but this is generally discouraged because it can cause issues when you are using multiple packages.\n\nfrom numpy import *\n\n\n\n\n\n\n\nTry it Out (Python package edition)\n\n\n\nUse pip to install the numpy and pandas python packages. This will install many other python packages as well - these packages are known as dependencies, and are required to use numpy and pandas.\npip install numpy pandas\nWhen I try to install numpy and pandas, I get this response:\n It is helpful to read the messages in the terminal so that you can see what the computer is trying to tell you. In my case, pip is telling me I need to manually upgrade pip, and it tells me exactly what to run to do that.\n\n\n\n1.3.2 Version Control\n\n\n\n\n\n\nNote\n\n\n\nMost of this section is either heavily inspired by Happy Git and Github for the UseR [3] or directly links to that book.\n\n\n\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is the answer to this file naming problem.\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is … essential. In this class, we’ll be using both git and github, and your homework will be managed with GitHub Classroom.\nGit Basics\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything.\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\n\n\n\n\n\n\nEssential Reading: Git\n\n\n\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, if you haven’t already.\n\n\nNow that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub.\nGitHub: Git on the Web\n\n\n\n\n\n\nSet up a GitHub Account Now\n\n\n\nInstructions for setting up a GitHub account.\nBe sure you remember your signup email, username, and password - you will need them later.\n\n\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\n\n\n\n\n\n\nGit and Github: Slightly crude (but memorable) analogy\n\n\n\n\n\nGit is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories.\n\n\n\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.5\nOptional: Install a git client\nInstructions\nI don’t personally use a git client other than RStudio, but you may prefer to have a client that allows you to use a point-and-click interface. It’s up to you.\n\n1.3.3 Document Creation\nLaTeX\nLaTeX is a typesetting program, which makes it different from most other document creation software, such as MS Word, which is “WYSIWYG” - what you see is what you get. In LaTeX, you’ll type in code to create a document, and LaTeX will compile the document into something pretty. The goal is that you have to think less about formatting and what goes on which page - LaTeX will handle that part for you - so that you can think more about the content.\nIn practice, it doesn’t usually work out like that, so there are programs like markdown which aim to simplify document creation even more to free you from the formatting that LaTeX requires.\nLaTeX[4] is often used for typesetting statistical and mathematical papers because its equation editor is top notch. (It was actually written by Donald Knuth because he got so annoyed trying to write his dissertation that he took some time off to write TeX first, and then used it to write his dissertation).6\n\n\n\n\n\n\nTry it Out: LaTeX\n\n\n\nIf you want to try to make a simple LaTeX document in RStudio, try out this LaTeX tutorial.\n\n\nMarkdown\nWe’ll work with LaTeX later in the semester, but for now, we’ll be primarily working with Quarto or Rmarkdown, which is much simpler. Here’s a quick guide to rmarkdown basics.\nWhen you use markdown within RStudio, you don’t have to worry about where pandoc lives on your computer (e.g. the path). You can just click the button at the top of the file that says “Knit” or “Preview” to see what your file looks like.\n\n\n\n\n\n\nTry it Out: Rmarkdown\n\n\n\nCreate a new Rmarkdown or quarto document by going to File &gt; New File &gt; Rmarkdown (or Quarto Markdown) and testing out the simple document it starts you out with. Hit the compile button to see what the output looks like, then change things.\n\n\nThis cheat sheet is a very summary of Rmarkdown (and quarto markdown)"
  },
  {
    "objectID": "tools.html#using-version-control-with-rstudio",
    "href": "tools.html#using-version-control-with-rstudio",
    "title": "1  Exploring the Toolbox",
    "section": "\n1.4 Using Version Control (with RStudio)",
    "text": "1.4 Using Version Control (with RStudio)\nThe first skill you need to actually practice in this class is using version control. By using version control from the very beginning, you will learn better habits for programming, but you’ll also get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\n\n\n\n\n\n\n\n\n\n\nSo, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is based off of a relatively high-level understanding of git - I do not have any idea how it works under the hood, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion.\n\n1.4.1 Introduce yourself to git and set up SSH authentication\nYou need to tell git what your name and email address are, because every “commit” you make will be signed. This needs to be done once on each computer you’re using.\nFollow the instructions here, or run the lines below:\n\n\n\n\n\n\nNote\n\n\n\nThe lines of code below use interactive prompts. Click the copy button in the upper right corner of the box below, and then paste the whole thing into the R console. You will see a line that says “Your full name:” - type your name into the console. Similarly, the next line will ask you for an email address.)\n\n\n\n\nuser_name &lt;- readline(prompt = \"Your full name: \")\nuser_email &lt;- readline(prompt = \"The address associated w your github account: \")\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = user_name, user.email = user_email, scope = \"user\")\n\n# Tell git to ignore all files that are OS-dependent and don't have useful data.\ngit_vaccinate() \n\n# Create a ssh key if one doesn't already exist\nif (!file.exists(git2r::ssh_path(\"id_rsa.pub\"))) {\n  # Create an ssh key (with no password - less secure, but simpler)\n  system(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ''\") \n  # Find the ssh-agent that will keep track of the password\n  system(\"eval $(ssh-agent -s)\")\n  # Add the key\n  system(\"ssh-add ~/.ssh/id_rsa\")\n} \n\nThen, in RStudio, go to Tools &gt; Global Options &gt; Git/SVN. View your public key, and copy it to the clipboard.\nThen, proceed to github. Make sure you’re signed into GitHub. Click on your profile pic in upper right corner and go Settings, then SSH and GPG keys. Click “New SSH key”. Paste your public key in the “Key” box. Give it an informative title. For example, you might use 2022-laptop to record the year and computer. Click “Add SSH key”.\n\n1.4.2 Create a Repository\nRepositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. If you combine a git repository with an RStudio project, you get a very powerful combination that will make your life much easier, allowing you to focus on writing code instead of figuring out where all of your files are for each different project you start.\nTo create a repository, you can start with your local computer first, or you can start with the online repository first.\n\n\n\n\n\n\nImportant\n\n\n\nBoth methods are relatively simple, but the options you choose depend on which method you’re using, so be careful not to get them confused.\n\n\nLocal repository first\nLet’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it.\nIf you want, you can use the following R code to set this up:\n\ndir &lt;- \"./hello-world-1\"\nif (!dir.exists(dir)) {\n  dir.create(dir)\n}\nfile &lt;- file.path(dir, \"README\")\nif (!file.exists(file)) {\n  writeLines(\"hello world\", con = file)\n}\n\nTo create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in\ngit init\nAlternately, if you prefer a GUI (graphical user interface) approach, that will work too:\n\nOpen Rstudio\nProject (upper right corner) -&gt; New Project -&gt; Existing Directory. Navigate to the directory.\n(In your new project) Tools -&gt; Project options -&gt; Git/SVN -&gt; select git from the dropdown, initialize new repository. RStudio will need to restart.\nNavigate to your new Git tab on the top right.\n\n\n\nThe next step is to add our file to the repository.\nUsing the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\".\nUsing the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button.\n\n\nThe final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you’re logged in. Create a new repository using the “New” button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time).\nYou’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go.\n\n\nGitHub repository first\nIn the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally).\nGUI method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nOpen RStudio -&gt; Project -&gt; New Project -&gt; From version control. Paste your repository URL into the box. Hit enter.\nMake a change to the README file\nClick commit, then push your changes\nCheck that the remote repository (Github) updated\n\n\n\nCommand line method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nNavigate to the location you want your repository to live on your machine.\nClone the repository by using the git shell or terminal: git clone &lt;your repo url here&gt;. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git\n\nMake a change to your README file and save the change\nCommit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking).\nPush your changes to the remote (GitHub) repository and check that the repo has updated: git push\n\n\n\n\n\n1.4.3 Adding files\ngit add tells git that you want it to track a particular file.\n\n\ngit add diagram: add tells git to add the file to the index of files git monitors.\n\nYou don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about”, and what it thinks belongs in each commit.\nIf you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (sort-of, kind-of) equivalent to clicking the check box.\nWhat files should I add to git?\nGit is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB7. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free.\nIn general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository).\nYou should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good.\nI highly recommend that you make a point to only add and commit files which you consciously want to track.\n\n1.4.4 Staging your changes\nIn RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file.\nMore formally, staging is saying “I’d like these changes to be added to the current version, I think”. Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out.\nIndividually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do).\n\n1.4.5 Committing your changes\nA git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache.\n\n\nHere, we commit the red line as a change to our file.\n\nIn general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things:\n\nYou’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing)\nWhen you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred.\n\n1.4.6 Pushing and Pulling\nWhen you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document.\n\nIf you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document.\nFor this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible.\n\nPulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up).\nPushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine.\n\n\n\n\n\ngit push copies the version of the project on your computer to GitHub\n\n\n\n\n\ngit pull copies the version of the project on GitHub to your computer\n\n\n\nFigure 1.1: Git push and git pull are used to sync your computer with the remote repository (usually hosted on GitHub)\n\n\nIn general, your workflow will be\n\nClone the project or create a new repository\nMake some changes\nStage the changes with git add\nCommit the changes with git commit\nPull any changes from the remote repository\nResolve any merge conflicts\nPush the changes (and merged files) with git push\n\nIf you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways.\n\n\n\n\n\n\nTopic Sequencing\n\n\n\nIn several places in this class, you’ll have to use material that you haven’t been formally taught yet. I will do my absolute best to provide thorough instructions, help you along as much as I can, and generally provide enough support that you can muddle through. But it’s going to be hard to teach you everything you need to e.g. analyze some data, before providing you the opportunity to SEE that data using visualization packages. And it’s silly to teach you plotting before you know how to read data in. But to teach you how to read data in, you need to be able to take a look at the data, and plots are the best way to do that. To do any of this stuff, you need to know about functions, but it can be easier to figure out how to run a function than to write a function.\nYou see the problem.\nSo instead, what I’m going to do is to leave you lots of comments as to what a piece of code does when I’m using things you haven’t been formally shown yet. Then, you can copy/paste/modify those pieces of code, and if they break, you can ask why and we’ll dig into it (breaking code is usually a good thing, because it means you’re learning how to program). For each chapter, focus on learning how to write code that accomplishes that chapter’s objectives. If you understand some of the code you’re modifying that covers other topics not in that chapter, so much the better. But it’s not an expectation or a requirement.\nIf you’re confused, please reach out so that those who have seen this material before can help you out.\n\n\n\n\n\n\n\n\nLearn More\n\n\nExtra Resources\n\nA different guide to using R and RStudio can be found in Section 1 of ModernDive’s book [6, Ch. 1]\nR quick start guide [7]\nHappy Git and GitHub for the useR - Guide to using git, R, and RStudio together. [8]\nGit “Hello World” Tutorial on GitHub\nCrash course on git (30 minute YouTube video) [9]\nGit and GitHub for poets YouTube playlist (this is supposed to be the best introduction to Git out there…) [10]\nMore advanced git concepts, in comic form, by Erika Heidi [11]\nA quick guide to the command line (Terminal) [2]\n\n\nReferences\n\n\n\n\n[1] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nJ. Wei, “A Quick Guide to Using Command Line (Terminal),” Towards Data Science. Jul. 2019 [Online]. Available: https://towardsdatascience.com/a-quick-guide-to-using-command-line-terminal-96815b97b955. [Accessed: May 27, 2022]\n\n\n[3] \nJ. Bryan, J. Hester, and The Stat 545 TAs, Happy Git and GitHub for the useR. 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 09, 2022]\n\n\n[4] \nD. E. Knuth, “Literate programming,” Comput. J., vol. 27, no. 2, pp. 97–111, May 1984, doi: 10.1093/comjnl/27.2.97. [Online]. Available: https://doi.org/10.1093/comjnl/27.2.97\n\n\n\n[5] \nY. Xie, Dynamic documents with R and knitr, 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC, 2015 [Online]. Available: http://yihui.org/knitr/\n\n\n\n[6] \nC. Ismay and A. Y. Kim, Statistical Inference via Data Science, Online. Chapman; Hall/CRC, 2019 [Online]. Available: https://moderndive.com/. [Accessed: May 27, 2022]\n\n\n[7] \nRobert Kabacoff, “Quick-R,” Quick R. 2017 [Online]. Available: https://www.statmethods.net/. [Accessed: May 27, 2022]\n\n\n[8] \nJenny Bryan, the STAT 545 TAs, and Jim Hester, Happy Git and GitHub for the useR. bookdown, 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 27, 2022]\n\n\n[9] \nTraversy Media, Git & GitHub Crash Course For Beginners. (Feb. 2017) [Online]. Available: https://www.youtube.com/watch?v=SWYqp7iY_Tc. [Accessed: May 27, 2022]\n\n\n[10] \nThe Coding Train, “Introduction: Git and GitHub for Poets,” vol. 1. Apr. 2016 [Online]. Available: https://www.youtube.com/watch?v=BCQHnlnPusY. [Accessed: May 27, 2022]\n\n\n[11] \nErica Heidi, “Stage. Commit. Push. A Git Story (Comic),” DEV Community. Jan. 2020 [Online]. Available: https://dev.to/erikaheidi/stage-commit-push-a-git-story-comic-a37. [Accessed: May 27, 2022]"
  },
  {
    "objectID": "tools.html#footnotes",
    "href": "tools.html#footnotes",
    "title": "1  Exploring the Toolbox",
    "section": "",
    "text": "To disable onedrive sync for certain windows folders, use this guide↩︎\nNone of the good practices here are dependent on RStudio, but RStudio projects do make it easier for you to e.g. keep track of where files are living and what your current working directory is. You can do the same thing in a purely python project if you want to, but think of RStudio here as training wheels for those in the class who are not familiar with file paths and reproducibility.↩︎\n4.2 is the most recently released version as of May 2022, when I’m writing this.↩︎\nOk, in prehistory they programmed by punching holes in cards, but that was before my time… back when 🦖 roamed the earth.↩︎\nYes, I’m aware that this sounds paranoid. It’s been a very rare occasion that I’ve needed to restore something from another backup. You don’t want to take chances. I knew a guy who had to retype his entire masters thesis from the printed out version the night before it was due because he had stored it on a network drive that was decommissioned. You don’t want to be that person.↩︎\nAmusingly, knitr [5] was written in much the same manner. Yihui Xie had to substitute-teach ISU’s graduate introduction to computing on the day we covered Sweave (a predecessor to knitr). He got so frustrated teaching the class that he went home and started writing knitr. Later, he developed Rmarkdown, bookdown, blogdown, and several other packages aimed at making writing documents + code easier to handle.\nMoral of the story - if you get frustrated with the tools you have, you’re in good company. Use it as fuel to make better tools.↩︎\nYes, I’m seriously pushing it with this book; several of the datasets are ~30 MB↩︎"
  },
  {
    "objectID": "intro-prog.html#module-2-objectives",
    "href": "intro-prog.html#module-2-objectives",
    "title": "2  Programming First Steps",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nUnderstand the difference between reserved words and variables\nCreate a program flow map by breaking a problem down into smaller steps\nExplain the difference between interactive mode and command-line execution of a script\nUnderstand the difference between scripts and notebooks and explain when each is appropriate\n\nMore informally, the goal is to get familiar with the basics of each programming language, and to show you where to find references for how to use each command – because (at least) half of programming is knowing where to look something up.\n\n\n\n\n\n\nCheat Sheets and Reference Guides\n\n\n\nI kept the classic R reference card by my computer for about 5 years, and referenced it at least once or twice a day for that entire period. There will be other cheat sheets and reference cards scattered through this book because if you can’t remember something’s name, you might be able to remember where it is on the reference card (or at least, that’s how I learned R).\n\n\nR\nPython\nSAS\n\n\n\n\n\nR Cheatsheet - this is a simplified cheat sheet offered by RStudio.\nR Cheatsheet (classic)\n\n\n\n\nData Wrangling in Pandas\nNumPy Cheat Sheet\nData Input in Python\n\n\n\n\nSAS Cheatsheet (from another class like this)\nSAS Cheatsheet (by SAS)\nSAS Programming for R Users (free book)"
  },
  {
    "objectID": "intro-prog.html#what-is-programming",
    "href": "intro-prog.html#what-is-programming",
    "title": "2  Programming First Steps",
    "section": "\n2.1 What is programming?",
    "text": "2.1 What is programming?\n\nProgramming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\n\nbreak a problem down into smaller, easily solvable problems\nsolve small problems\ncommunicate the solution to a computer using a programming language\n\nThis book will demonstrate statistical programming using both R and Python. We will be using these languages to solve problems that are related to working with data. At first, we’ll start with smaller, simpler problems that don’t involve data, but by the end of the semester, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical “sentences”. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they’ll last you a lifetime (and be applicable to many other things beyond programming, like research)."
  },
  {
    "objectID": "intro-prog.html#breaking-problems-down",
    "href": "intro-prog.html#breaking-problems-down",
    "title": "2  Programming First Steps",
    "section": "\n2.2 Breaking Problems Down",
    "text": "2.2 Breaking Problems Down\nThe most fundamental part of programming that you will need to learn in this class is how to break down a big problem into smaller (hopefully solvable) problems. This post is a great example of the process of breaking things down for programming, but the same concept applies outside of programming too!\n\n\n\n\n\n\nBreaking Problems Down IRL\n\n\n\n\n\nScenario\nBrainstorming Solutions\nConclusion\n\n\n\nMy spouse and I recently decided to replace our shower curtain with glass doors because the curtain didn’t really prevent water from getting all over the floor.\nWe went to the store and picked out the parts, and the installation instructions broke the steps down like this:\n\nInstall the base of the track\nInstall the top of the track\nHang shower doors\nAdd hardware to shower doors\n\nSo we started in on the instructions, only to find out that when our house was built, our shower wasn’t leveled properly.\n\n\nThe instructions had a solution - we could send off for a $300 custom part that would level our floor, but we’d have to wait at least 4-6 weeks for them to make and ship the part to us. At this point, we’d already taken apart the shower and bought the shower door, so having the bathroom out of commission for 4-6 weeks was not appealing.\nWe’re both programmer-adjacent, so we started thinking through how we could deal with our problem a different way. We considered ignoring the instructions – our shower was about 1/8” off of level, surely that couldn’t be so important, right? My spouse is a bit more … detail oriented … than I am, so he wasn’t good with that suggestion.\nWe considered adding a ton of caulk or plaster to try to level the shower out. But we figured that 1) probably wouldn’t work, and 2) would look awful.\nFinally, I suggested that my spouse 3D print sections of track-leveler using our 3D printer. Now, this isn’t an option for most people, but it is for us - we have a small 3D printer, and my husband knows how to use OpenSCAD to create very accurate, custom dimension 3D printer files. He tested things out a few times, and printed up a series of 12 ~5” sections that, when assembled, were equivalent to the $300 custom part we could have ordered. He added those sections to the shower, caulked them in, and proceeded with the rest of the installation.\n\n\nBecause we had a list of subproblems (steps for installation), we could focus our efforts on debugging the one problem we had (not level shower ledge) and we didn’t have to get bogged down in “it’s impossible to get this job done”.\nWe knew that if we could solve the little problem, we’d be able to get the bigger job done. Programming is just like this - if you can break your problem down into steps (and not steps with code), you can think through how to solve a single step of the problem before you worry about the next step.\n\n\n\n\n\nOne tool that is often used to help break a problem down is a flowchart. In a flowchart, each step in a process is described in sequential order, with decisions represented as forks in the “flow”.\n\n\n\n\n\n\nSketch Tool Recommendation\n\n\n\nA tool that I often use to make flowcharts and other diagrams/sketches while programming is Excalidraw.\n\n\n\n\n\n\n\n\nTry it out\n\n\n\nThe biggest advantage to breaking problems down into smaller steps is that it allows you to focus on solving a small, approachable problem.\nLet’s think through an example:\n\n\nProblem\nManual Solution\nAnalysis\nProgram Flow Map\nFormal Solution\n\n\n\nPrint out a pyramid of stars, 10 lines high.\nYes, I remember, I haven’t taught you how to write any code yet. Don’t worry about code right now - let’s just think about how we might create a pyramid of stars.\nStart by doing the task manually on paper or in a text editor. Observe the steps you go through and think about how you might generalize those steps.\n\n\nFirst, we have to think about what we would need to make a pyramid of stars. So let’s make a miniature one by hand (I’m using - for spaces here to make things visible):\n---*---\n--***--\n-*****-\nTo make my miniature star pyramid, I started out by adding space on the first line, then a star, then more space. When I moved to the next line, I added space, but one less space than I’d added before, and then 3 stars, and then more space.\n\n\nExamining the manual solution, it seems we can break our problem down into two(ish) components:\n\nHow much space? (one side)\nHow many stars?\n(redundant piece) How much space (the other side).\n\nThinking my way through how I created my manual pyramid, I realized that I was adding \\(n\\) spaces (where \\(n\\) is the total number of rows) on the first line, and then \\(n - i\\) spaces on subsequent lines, if we start with i=0.\nBut I am an R programmer, so we start with \\(i=1\\), which means I need to have \\(n - i + 1\\) spaces on each row first.\nThen, for \\(i=1\\) the first row, we have \\(2*i - 1\\) stars - i = 1, stars = 1, then i = 2, stars = 3, then i = 3, stars = 5…. you can do the regression if you want to, but it’s pretty easy to see the relationship.\nFinally, we have to (in theory) add the same amount of space on the other side – strictly speaking, this is optional, but it makes the lines the same length, so it is nice.\n\n\n\n\nProgram flow map for stars\n\n\n\nIf we want a pyramid that is \\(n\\) rows high, we might think of creating it by using the following line-by-line formula, where \\(i\\) is our current line:\n\\(n - i +1\\) spaces, \\(2i - 1\\) stars, \\(n - i + 1\\) spaces\nWorking this out in a small example helped me come up with that formula; now, I can write a “loop”:\n\nline 1: i = 1, n = 10, 10 spaces, 1 star, 10 spaces\nline 2: i = 2, n = 10, 9 spaces, 3 stars, 9 spaces\nline 3: i = 3, n = 10, 8 spaces, 5 stars, 8 spaces\nline 4: i = 4, n = 10, 7 spaces, 7 stars, 7 spaces\n…\nline n: i = n, n = 10, 1 space, 19 stars, 1 space"
  },
  {
    "objectID": "intro-prog.html#programming-concepts",
    "href": "intro-prog.html#programming-concepts",
    "title": "2  Programming First Steps",
    "section": "Programming Concepts",
    "text": "Programming Concepts\nMany programming resources talk about 3, or 5, or 10 core concepts in any programming language. Here, we’re going to discuss the generic concepts, and then how these concepts are implemented in the languages we’re working with.\nInterestingly, the “core concepts” aren’t necessarily the same across lists. Here is a consensus list of concepts which are generic across languages and usually important [5]:\n\nVariables\na symbolic name or reference to some kind of information. In the expression a + b &gt; a, both a and b are variables. Variables may have a specific type (what data can be stored in the variable), scope (where the variable can be accessed), and location (where the information is stored in the computer’s memory). [2] has a nice explanation of the difference between variables in programming and variables in math.\nConditional statements (if statements)\nThese statements allow the program to handle information adaptively - if a statement is true, one set of instructions will be used, and if the statement is false, a different set of instructions will be used.\nLooping and iteration\nAn iteration is any time a sequence of steps is executed. Most languages have several different types of loops or iteration: for loops, which allow for the sequence of steps to be executed a specific number of times, while loops, which allow for the sequence of steps to be executed while a conditional statement is true, recursion, where a block of code calls itself.\nData types and data structures\nthese concepts determine what information a variable can hold. Data types are lower-level, simple objects (floating-point numbers, integers, Boolean T/F, characters, strings). Data structures may include lists (sequences of many objects) and vectors (sequences of many objects of the same type), dictionaries (a list of key-value pairs), objects (data structures which may hold multiple related pieces of information).\nFunctions\nself-contained modules of code that accomplish a particular task.\nSyntax\nThe set of rules that define which combinations of symbols consist of correctly structured and interpretable commands in the language.\nTools\nThe set of external programs which may help with development and writing code. Some common tools are IDEs (Integrated Development Environments), which may correct syntax and typos, organize files for you, allow you to keep track of which variables you have defined, and assist you with code organization and navigation. Other tools include compilers (which take human-written code and translate it into efficient machine code), version control systems (which help you track changes to code over time), debuggers, and documentation generators. Not all of these tools are necessary for all languages - scripting languages such as python and R do not require compilers by default, for instance.\nSequence of commands\nIt’s important to have the right commands in the right order.\n\n\n\nIt’s important to get both the level of specificity and the order of the commands just right when programming."
  },
  {
    "objectID": "intro-prog.html#programming-vocabulary-hello-world",
    "href": "intro-prog.html#programming-vocabulary-hello-world",
    "title": "2  Programming First Steps",
    "section": "\n2.3 Programming Vocabulary: Hello world",
    "text": "2.3 Programming Vocabulary: Hello world\nI particularly like the way that Python for Everybody [6] explains vocabulary:\n\nUnlike human languages, the Python vocabulary is actually pretty small. We call this “vocabulary” the “reserved words”. These are words that have very special meaning to Python. When Python sees these words in a Python program, they have one and only one meaning to Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of Python’s reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like “sit”, “stay”, and “fetch”. When you talk to a dog and don’t use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, “I wish more people would walk to improve their overall health”, what most dogs likely hear is, “blah blah blah walk blah blah blah blah.” That is because “walk” is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while    \n\nThat is it, and unlike a dog, Python is already completely trained. When you say ‘try’, Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the Python equivalent of “speak” (in human-to-dog language). The nice thing about telling Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nprint('Hello world!')\n## Hello world!\n\n\nAnd we have even written our first syntactically correct Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nR has a slightly smaller set of reserved words:\nif          else     repeat      while        \nfor         in       next        break  \nTRUE        FALSE    NULL        Inf         \nNA_integer_ NA_real_ NA_complex_ NA_character_  \nNaN         NA       function    ...\nIn R, the “Hello World” program looks exactly the same as it does in python.\n\nprint('Hello world!')\n## [1] \"Hello world!\"\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history, because it is also similar to Lisp, but both languages are still very similar to C and run C or C++ code in the background."
  },
  {
    "objectID": "intro-prog.html#interactive-mode",
    "href": "intro-prog.html#interactive-mode",
    "title": "2  Programming First Steps",
    "section": "\n2.4 Interactive Mode",
    "text": "2.4 Interactive Mode\nR and python both have an “interactive mode” that you will use most often. In the previous chapter, we talked about scripts and markdown documents, both of which are non-interactive methods for writing R and python code. But for the moment, let’s work with the interactive console in both languages in order to get familiar with how we talk to R and python.\nLet’s start by creating a Qmd file (File -&gt; New File -&gt; Quarto Document) - this will let us work with R and python at the same time.\nAdd an R chunk to your file by typing ```{r} into the first line of the file, and then hit return. RStudio should add a blank line followed by ```.\nAdd a python chunk to your file by typing ```{python} on a blank line below the R chunk you just created, and then hit return. RStudio should add a blank line followed by ```.\nYour file should look like this:\n\n\nScreenshot of qmd file after adding an empty r and python chunk\n\nIf instead your file looks like this:\n\n\nScreenshot of qmd file with visual markdown editing on\n\nyou have visual markdown mode on. To turn it off, click on the source button at the top left of your editor:\n\n\nScreenshot of editor window toolbar, with visual editing mode icon highlighted in green\n\n\n\n\n\n\n\nNote\n\n\n\nRStudio’s interface is constantly changing, so some of the screenshots may not match with your version of RStudio but hopefully they are close.\n\n\nIf we are working in interactive mode, why did I have you start out by creating a markdown document? Good Question! RStudio allows you to switch back and forth between R and python seamlessly, which is good and bad - it’s hard to get a python terminal without telling R which language you’re working in! You can create a python script if you’d prefer to work in a script instead of a markdown document, but that would involve working in 2 separate files, which I personally find rather tedious.\n\n\nThe R Console\nThe Python Console\n\n\n\nIn your R chunk or script, type in 2+2 and hit Ctrl+Enter (or Cmd+Enter on a mac). Look down to the Console (which is usually below the editor window) and see if 4 appears. If you’re like me, output shows up in two places at once:\n\n\n\n\n\n\nLocation\nPicture\n\n\n\nChunk\n\n\n\nScript\n\n\n\nConsole\n\n\n\n\nR will indicate that it is waiting for your command with a &gt; character in the console. If you don’t see that &gt; character, chances are you’ve forgotten to finish a statement - check for parentheses and brackets.\nWhen you are working in an R script, any output is shown only in the console. When you are working in an R code chunk, output is shown both below the chunk and in the console.\nIf you want, you can also just work within the R console. This can be useful for quick, interactive work, or if, like me, you’re too lazy to pull up a calculator on your machine and you just want to use R to calculate something quickly. You just type your R command into the console:\n\n\nR console with commands “Hello”, print(“Hello”), and (unquoted) “I love R”, which causes an error\n\nThe first two statements in the above example work - “Hello” is a string, and is thus a valid statement equivalent to typing “2” into the console and getting “2” back out. The second command, print(\"Hello\"), does the same thing - “Hello” is returned as the result. The third command, I love R, however, results in an error - there is an unexpected symbol (the space) in the statement. R thinks we are telling it to do something with variables I and love (which are not defined), and it doesn’t know what we want it to do to the two objects.\nSuppose we define I and love as variables by putting a value into each object using &lt;-, which is the assignment operator. Then, typing “I love” into the console generates the same error, and R tells us “hey, there’s an unexpected symbol here” - in this case, maybe we meant to add the two variables together.\n\n\nR console with commands “Hello”, print(“Hello”), and (unquoted) “I love R”, which causes an error. Defining variables I and love provides us a context in which R’s error message about unexpected symbols makes sense - R is reminding us that we need a numerical operator in between the two variable names.\n\n\n\nIn your python chunk or script, type in 2+2 and hit Ctrl+Enter (or Cmd+Enter on a mac). Look down to the Console (which is usually below the editor window) and see if 4 appears. If you’re like me, output shows up in two places at once:\n\n\n\n\n\n\nLocation\nPicture\n\n\n\nChunk\n\n\n\nScript\n\n\n\nConsole\n\n\n\n\nNotice that in the console, you get a bit of insight into how RStudio is running the python code: we see reticulate::repl_python(), which is R code telling R to run the line in Python. The python console has &gt;&gt;&gt; instead of &gt; to indicate that python is waiting for instructions.\nNotice also that the only difference between the R and python script file screenshots is that there is a different logo on the documents: . Personally, I think it’s easier to work in a markdown document and keep my notes with specific chunks labeled by language when I’m learning the two languages together, but when you are writing code for a specific project in a single language, it is probably better to use a script file specific to that language.\nIf you want to start the python console by itself (without a script or working in a markdown document), you can simply type reticulate::repl_python() into the R console.\n\n\nScreenshot of how to get the python console\n\nR is nice enough to remind you that to end the conversation with python, you just need to type “exit” or “quit”.\nIf you want to start a python console outside of RStudio, bring up your command prompt (Darwin on mac, Konsole on Linux, CMD on Windows) and type python3 into that window and you should see the familiar &gt;&gt;&gt; waiting for a command."
  },
  {
    "objectID": "intro-prog.html#script-mode",
    "href": "intro-prog.html#script-mode",
    "title": "2  Programming First Steps",
    "section": "\n2.5 Script Mode",
    "text": "2.5 Script Mode\nIn the last section, we played with interactive mode by typing R and python commands into a console or running code chunks interactively using the Run button or Ctrl/Cmd + Enter (which is the keyboard shortcut).\nYou may be learning to program in R and python because it’s a required part of the curriculum, but hopefully, you also have some broader ideas of what you might do with either language - process data, make pretty pictures, write a program to trigger the computer uprising…\nScripts are best used when you have a thing you want to do, and you will need to do that thing many times, perhaps with different input data. Suppose that I have a text file and I want to pull out the most common word in that file. In the next few examples, I will show you how to do this in R and python, and at the same time, demonstrate the difference between interactive mode and script mode in both languages. In each example, try to compare to the previous example to identify whether something is running as a full script or in interactive mode, and how it is launched (in R? at the command line?).\n[6] provides a handy python program to count words. This program is meant to be run on the command line, and it will run for any specified text file.\n\n\n\n\n\n\nExample: Counting Words\n\n\n\n\n\nPython Terminal\nPython (RStudio)\nInteractive Python\nR (RStudio)\nInteractive R\nR Terminal\n\n\n\nDownload words.py to your computer and open up a command line terminal in the location where you saved the file.\nBefore you run the script, save Oliver Twist to the same folder as dickens-oliver-627.txt (you can use another file name, but you will have to adjust your response to the program)\n\nname = input('Enter file:')\nhandle = open(name, 'r')\ncounts = dict()\n\nfor line in handle:\n    words = line.split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\nfor word, count in list(counts.items()):\n    if bigcount is None or count &gt; bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n\n# Code: http://www.py4e.com/code3/words.py\n\nIn your terminal, type in python words.py. If all goes well, you should get a prompt that tells you to enter a file name. Type in dickens-oliver-627.txt, and the program will read in the file and execute the program according to the instructions shown above. You don’t need to understand what is happening in this program (just like you don’t need to understand what is happening in the R code above either) – you get the answer anyways: the most common word, according to the output from the program, is\nthe 8854\nThat is, the word the occurs 8854 times in the text.\n\n\nScreenshot of folder and python script evaluation, showing how to run the python script in the terminal and get the count of the most common word, ‘the’, in the file dickens-oliver-627.txt\n\n\n\nWe can run this script in interactive mode in RStudio if we want to: Open the words.py file you downloaded in RStudio.\n\n\nRstudio screenshot showing the words.py file opened, with a green highlighted rectangle around the button “Source Script” which allows you to run the file in RStudio.\n\nClick the “Source Script” button highlighted in green above, and then look at the console below the script window:\n Once you enter the path to the text file – this time, from the project working directory – you get the same answer. It can be a bit tricky to figure out what your current working directory is in RStudio, but in the R console you can get that information with the getwd() command.\n\n\nRStudio screenshot of console window with getwd() command and result\n\nSince I know that I have stored the text file in the data subdirectory of the stat151book folder, I can type in data/dickens-oliver-627.txt and the python program can find my file.\nIn the above example, RStudio is functioning essentially like a terminal window - it runs the script as a single file, and once it has your input, all commands are executed one after the other automatically. This is convenient if you want to test the whole block of code at once, but it can be more useful to test each line individually and “play” with the output a bit (or modify code line-by-line).\n\n\nSuppose we want to modify this python script to be more like the R script, where we tell python what the file name is in the file itself, instead of waiting for user input at the terminal.\nInstead of using the input command, I just provide python with a string that contains the path to the file. If you have downloaded the text file to a different folder and RStudio’s working directory is set to that folder, you would change the first line to name = \"dickens-oliver-627.txt\" - I have set things up to live in a data folder because if I had all of the files in the same directory where this book lives, I would never be able to find anything.\nCreate a new python script file in RStudio (File -&gt; New File -&gt; Python Script) and paste in the following lines of code, adjusting the path to the text file appropriately.\n\nname = \"data/dickens-oliver-627.txt\"\nhandle = open(name, 'r')\ncounts = dict()\n\nfor line in handle:\n    words = line.split()\n    for word in words:\n        counts[word] = counts.get(word, 0) + 1\n\nbigcount = None\nbigword = None\nfor word, count in list(counts.items()):\n    if bigcount is None or count &gt; bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n\n# Code: http://www.py4e.com/code3/words.py\n## the 8854\n\n\n\nAbove script in the RStudio text editor window, with the first 3 lines of code highlighted\n\nWith the first 3 lines highlighted, click the Run button.\n We can examine the objects that we have defined this far in the program by typing their names into the console directly.\n\n\nRStudio python console allows us to examine the objects we have defined after the first 3 lines of code have been run. We can see that counts is an empty object, handle is a pointer to a text file, and name is a string with the path to the text file – so far, so good.\n\nIf we want to continue walking through the program chunk by chunk, we can run the next four lines of code. Lines 5-8 are a for loop, so we should run them all at once unless we want to fiddle with how the for loop works.\n\n\nRStudio editor window with the next four lines of the code chunk highlighted. If we click the Run button, we can tell python to evaluate these few lines of code, and then we can see what the objects we’ve defined look like once that has been done\n\nSelect lines 5-8 as shown above, and click the Run button. Your console window should update with additional lines of code. You can type in counts after that has been evaluated to see what the counts object looks like now.\n\n\nRStudio python console with lines 5-8 run and the counts object displayed. Counts is now filled with words and corresponding integer counts of the frequency of that word’s appearance in the text\n\nThe next few lines of code determine which word has the highest count. We won’t get into the details here, but to finish out the running of the program, select lines 10-17 and run them in RStudio.\n\n\nRStudio editor window and console showing the results when lines 10-17 are evaluated. It is clear that line 17 results in the console output of the 8854\n\nRunning scripts in interactive mode or within RStudio is much more convenient if you are still working on the script - it allows you to debug the script line-by-line if necessary. Running a script at the terminal (like we did above) is sometimes more convenient if you have a pre-written script that you know already works. Both modes are useful, but for the time being you will probably be running scripts within your development environment (RStudio or VSCode or any other IDE you prefer) more often than at the command line.\n\n\nJust for fun, let’s work with Oliver Twist, by Charles Dickens, which I have saved here.\n\n# Read in the file\ntext &lt;- readLines(\"dickens-oliver-627.txt\")\n\n# Split the lines of text into separate words\ntext &lt;- strsplit(text, \" \")\n\n# Simplify the list\ntext &lt;- unlist(text)\n\n# Count up the number of occurrences of each word\nword_freq &lt;- table(text)\n\n# Sort the table by decreasing frequency\nword_freq &lt;- sort(word_freq, decreasing = T)\n\n# Show the counts for the most common 10 words\nword_freq[1:10]\n\nMake a new R script (File -&gt; New File -&gt; R script) and copy the above code into R, or download the file to your computer directly and open the downloaded file in RStudio.\nIn the R console, run the command getwd() to see where R is running from. This is your “working directory”.\n\n\nR editor window with relevant script, with R console shown below. My working directory is /home/susan/Projects/Class/unl-stat151/stat151book/demo; yours will be different.\n\nSave the copy of Oliver Twist to the file dickens-oliver-627.txt in the folder that getwd() spit out. You can test that you have done this correctly by typing list.files() into the R console window and hitting enter. It is very important that you know where on your computer R is looking for files - otherwise, you will constantly get “file not found” errors, and that will be very annoying.\n\n\nR editor window with relevant script, with R console shown below. dickens-oliver-627.txt is in the working directory, so we can proceed.\n\nUse the “Run” button to run the script and see what the output is. How many times does ‘the’ appear in the file?\n\n\nUsing the file you created above, let’s examine what each line does in interactive mode.\n\n# Read in the file\ntext &lt;- readLines(\"dickens-oliver-627.txt\")\n\nSelect the above line and click the “Run” button in RStudio. Once you’ve done that, type in text[1:5] in the R console to see the first 5 lines of the file.\n\n\nRStudio editor window with the first 2 lines of the words-noinput.R file selected. The screenshot also shows the console window after running the first 2 lines of the R file, with the text[1:5] command run interactively afterwards showing the first 5 lines of the text file we read in.\n\nRun the next line of code using the run button (or click on the line of code and hit Ctrl/Cmd + Enter).\n\n# Split the lines of text into separate words\ntext &lt;- strsplit(text, \" \")\n\nType in text[[1]] to see what the text object looks like now.\n\ntext[[1]]\n##  [1] \"The\"       \"Project\"   \"Gutenberg\" \"Etext\"     \"of\"        \"Oliver\"   \n##  [7] \"Twist\"     \"by\"        \"Charles\"   \"Dickens\"\n\n\n\nScreenshot of RStudio editor window with lines of code highlighted, plus RStudio console with the code as run and text[[1]] showing the first entry in the text object - a list of the separate words in the first line of the text file.\n\n\n# Simplify the list\ntext &lt;- unlist(text)\ntext[[1]]\n## [1] \"The\"\ntext[1:20]\n##  [1] \"The\"       \"Project\"   \"Gutenberg\" \"Etext\"     \"of\"        \"Oliver\"   \n##  [7] \"Twist\"     \"by\"        \"Charles\"   \"Dickens\"   \"#13\"       \"in\"       \n## [13] \"our\"       \"series\"    \"by\"        \"Charles\"   \"Dickens\"   \"Copyright\"\n## [19] \"laws\"      \"are\"\n\nRunning unlist on text simplifies the object so that it is now a single vector of every word in the file, without regard for which line it appears on.\n\n# Count up the number of occurrences of each word\nword_freq &lt;- table(text)\nword_freq[1:5]\n## text\n##             _I_     --'    --by --kneel \n##    4558       4       1       1       1\n\nThe next line assembles a table of frequency counts in text. There are 4558 spaces, 4 occurrences of the string _I_, and so on.\n\n# Sort the table by decreasing frequency\nword_freq &lt;- sort(word_freq, decreasing = T)\nword_freq[1:5]\n## text\n##  the  and        to   of \n## 8854 4902 4558 3767 3763\n\nWe can then sort word_freq so that the most frequent words are listed first. The final line just prints out the first 10 words instead of the first 5.\n\n\nDownload the following file to your working directory: words.R, or paste the following code into a new R script and save it as words.R\n\n# Take arguments from the command line\nargs &lt;- commandArgs(TRUE)\n\n# Read in the file\ntext &lt;- readLines(args[1])\n\n# Split the lines of text into separate words\ntext &lt;- strsplit(text, \" \")\n\n# Simplify the list\ntext &lt;- unlist(text)\n\n# Count up the number of occurrences of each word\nword_freq &lt;- table(text)\n\n# Sort the table by decreasing frequency\nword_freq &lt;- sort(word_freq, decreasing = T)\n\n# Show the counts for the most common 10 words\nword_freq[1:10]\n\nIn a terminal window opened at the location you saved the file (and the corresponding text file), enter the following: Rscript words.R dickens-oliver-627.txt.\nHere, Rscript is the command that tells R to evaluate the file, words.R is the R code to run, and dickens-oliver-627.txt is an argument to your R script that tells R where to find the text file. This is similar to the python code, but instead, the user passes the file name in at the same time as the script instead of having to wait around a little bit.\n\n\n\n\n\n\n2.5.1 Comparing Python and R\nThis is one good example of the difference in culture between python and R: python is a general-purpose programming language, where R is a domain specific programming language. In both languages, I’ve shown you how I would run the script by default first - in python, I would use a pre-built script to run things, and in R I would open things up in RStudio and source the script rather than running R from the command line.\nThis is a bit of a cultural difference – because python is a general purpose programming language, it is easy to use for a wide variety of tasks, and is a common choice for creating scripts that are used on the command line. R is a domain-specific language, so it is extremely easy to use R for data analysis, but that tends to take place (in my experience) in an interactive or script-development setting using RStudio. It is less natural to me to write an R script that takes input from the user on the command line, even though obviously R is completely capable of doing that task. More commonly, I will write an R script for my own use, and thus there is no need to make it easy to use on the command line, because I can just change it in interactive mode. Python scripts, on the other hand, may be written for a novice to use at the command line with no idea of how to write or modify python code. This is a subtle difference, and may not make a huge impression on you now, but it is something to keep in mind as you learn to write code in each language – the culture around python and the culture around R are slightly different, and this affects how each language is used in practice."
  },
  {
    "objectID": "intro-prog.html#scripts-and-notebooks",
    "href": "intro-prog.html#scripts-and-notebooks",
    "title": "2  Programming First Steps",
    "section": "\n2.6 Scripts and Notebooks",
    "text": "2.6 Scripts and Notebooks\nWe’ve talked about organizing code in scripts and running them at the command line, but there is a (somewhat) more recent way to program that involves mixing code and text together in a document, usually called a notebook. This interweaving of code and text is called literate programming and was first proposed by [7].\nThere are many advantages to each workflow, and they are useful in different situations. In this section, try to figure out when you might want to use a script and when you might want to use a notebook.\n\n2.6.1 Scripts\nScripts are files of code that are meant to be run on their own. They may produce results, or format data and save it somewhere, or scrape data from the web – scripts can do just about anything. As you saw in the previous section, a script can be used in interactive mode as well, but their primary purpose is generally to run autonomously, even if they are sometimes written interactively.\nScripts can have documentation within the file, using # characters (at least, in R and python) at the beginning of a line. # indicates a comment – that is, that the line does not contain code and should be ignored by the computer when the program is run. Comments are incredibly useful to help humans understand what the code does and why it does it.\n\n\n\n\n\n\nPlotting a logarithmic spiral\n\n\n\nThis code will use concepts we have not yet introduced - feel free to tinker with it if you want, but know that you’re not responsible for being able to write this code yet. You just need to read it and get a sense for what it does. I have heavily commented it to help with this process.\n\n\nR\nPython\n\n\n\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta &lt;- seq(0, 4*pi, .01) \n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta\nr &lt;- seq(0, 5, length.out = length(theta))\n\n# Now define x and y in cartesian coordinates\nx &lt;- r * cos(theta)\ny &lt;- r * sin(theta)\n\nplot(x, y, type = \"l\")\n\n\n\nFigure 2.1: A Cartesian Spiral in R\n\n\n\nI have saved this script here. You can download it and open it up in RStudio (File -&gt; Open -&gt; Navigate to file location).\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the angle of the spiral (polar coords)\n# go around two full times (2*pi = one revolution)\ntheta = np.arange(0, 4 * np.pi, 0.01)\n# Define the distance from the origin of the spiral\n# Needs to have the same length as theta \n# (get length of theta with theta.size, \n#  and then divide 5 by that to get the increment)\nr = np.arange(0, 5, 5/theta.size)\n\n# Now define x and y in cartesian coordinates\nx = r * np.cos(theta)\ny = r * np.sin(theta)\n\n# Define the axes\nfig, ax = plt.subplots()\n# Plot the line\nax.plot(x, y)\nplt.show()\n\n\n\nFigure 2.2: A Cartesian Spiral in python\n\n\n\nI have saved this script here. You can download it and open it up in RStudio (File -&gt; Open -&gt; Navigate to file location).\n\n\n\n\n\nScripts can be run in Rstudio by clicking the Run button  at the top of the editor window when the script is open.\n\n\n\n\n\n\nTry it out!\n\n\n\n\nDownload the R and python scripts in the above example, open them in RStudio, and run each script using the Run button. What do you see?\n(Advanced) Open a terminal in RStudio (Tools -&gt; Terminal -&gt; New Terminal) and see if you can run the R script from the terminal usingR CMD BATCH path/to/file/markdown-spiral-script.R\n(You will have to modify this command to point to the file on your machine)\nNotice that two new files appear in your working directory: Rplots.pdf and markdown-spiral-script.Rout\n(Advanced) Open a terminal in RStudio (Tools -&gt; Terminal -&gt; New Terminal) and see if you can run the R script from the terminal usingpython3 path/to/file/markdown-spiral-script.py\n(You will have to modify this command to point to the file on your machine)\n\n\n\nMost of the time, you will run scripts interactively - that is, you’ll be sitting there watching the script run and seeing what the results are as you are modifying the script. However, one advantage to scripts over notebooks is that it is easy to write a script and schedule it to run without supervision to complete tasks which may be repetitive.\nHere are some examples of scripts I have running on my machine:\n\nA script that runs daily at midnight, 6am, noon, and 6pm to pull images of shoes off of shopping sites online for research purposes\nA script that runs every 10 minutes to push updates from a dataset on my server to github so that my students can access the latest data\nA script that runs every 10 minutes to pull updates from a github repository so that the web application on my server has the latest updates\nA script that runs daily to pull the day’s weather off of the National Weather Service site to maintain a local log (because I’m a weather nerd)\n\nThese scheduled tasks make my life easier and make it possible for me to do research more effectively without requiring me to remember to do tasks on a regular schedule.\n\n2.6.2 Notebooks\nNotebooks are an implementation of literate programming. Both R and python have native notebooks that are cross-platform and allow you to code in R or python. This book is written using Quarto markdown, which is an extension of Rmarkdown, but it is also possible to use jupyter notebooks to write R and python code.\nThis book will focus on the use of Quarto/R markdown, because it is a much better tool for creating polished reports than Jupyter (in my opinion). My goal is that you learn something useful for your own class work that can be easily applied when you go to work as an analyst somewhere to produce impressive documents. Jupyter notebooks are great for interactive coding, but aren’t so wonderful for producing polished results. They also don’t allow you to switch between languages mid-notebook, and since I’m writing this in both R and python (and will be teaching that way as well), I want you to have both languages available.\n\n\n\n\n\n\nLearn more about notebooks\n\n\n\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).1\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nR markdown\nJupyter\nQuarto markdown\n\n\n\nTake a look at the R markdown sample file I’ve created to go with the R script above. You can see the HTML file it generates here.\n\nDownload the Rmd file and open it with RStudio.\nChange the output to output: word_document and hit the Render button . Can you find the markdown-demo.docx file that was generated? What does it look like?\nChange the output to output: pdf_document and hit the Render button . Can you find the markdown-demo.pdf file that was generated? What does it look like?\n\nRmarkdown tries very hard to preserve your formatted text appropriately regardless of the output document type. While things may not look exactly the same, the goal is to allow you to focus on the content and the formatting will “just work”.\n\n\nTake a look at the jupyter notebook sample file I’ve created to go with the R script above. You can see the HTML file it generates here.\n\nDownload the ipynb file and open it with jupyter.\nExport the notebook as a pdf file (File -&gt; Save as -&gt; PDF via HTML). Can you find the jupyter-demo.pdf file that was generated? What does it look like?\nExport the notebook as an html file (File -&gt; Save as -&gt; HTML). Can you find the jupyter-demo.html file that was generated? What does it look like?\n\n\n\nThe nice thing about quarto is that it will work with python and R seamlessly, and you can compile the document using python or R. R markdown will also allow you to use python chunks, but you must compile the document using R; quarto can be compiled using the command line and is more platform-agnostic.\nTake a look at the Qmd notebook sample file I’ve created to go with the scripts above. You’ll notice that it is basically the script portion of this textbook – that’s because I’m writing the textbook in Quarto.\n\nDownload the qmd file and open it with RStudio\nTry to compile the file by hitting the Render button \n(Advanced) In the terminal, type in quarto render path/to/file/quarto-demo.qmd. Does that render the HTML file? One advantage of this is that using quarto to render the file doesn’t require R at the command line. As the document contains R chunks, R is still required to compile the document, but the biggest difference between qmd and rmd is that qmd files are workflow agnostic - you can generate them in e.g. MS Visual Studio Code, compile them in that workflow, and never have to use RStudio.\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Material\n\n\n\nTextbooks\n\n\nR for Data Science [8]\n\n\nAdvanced R [9]\n\n\nNon Programmer’s Tutorial for Python 3 [10]\n\nOther Course Websites\n\nStat 850 at UNL (Bilder): [11]\n\n\nStat 579 at Iowa State: [12]\n\n\nStat 545 at Univ. British Columbia: [13]\n\n\nIntroduction to SAS - Undergraduate course at Penn State\n\nIntermediate SAS - Undergraduate course at Penn State\n\nAdvanced SAS - Undergraduate course at Penn State\nHelpful Debugging Resources\n\nCommon R Problems"
  },
  {
    "objectID": "intro-prog.html#intro-prog-refs",
    "href": "intro-prog.html#intro-prog-refs",
    "title": "2  Programming First Steps",
    "section": "\n2.7 References",
    "text": "2.7 References\n\n\n\n\n[1] \nT. Page, “Basic programming concepts,” Coders Campus. Mar. 2021 [Online]. Available: https://www.coderscampus.com/basic-programming-concepts/\n\n\n\n[2] \nA. Mukit, “Basic programming concepts for beginners,” DEV Community. Mar. 2018 [Online]. Available: https://dev.to/lucpattyn/basic-programming-concepts-for-beginners-2o73\n\n\n\n[3] \nR. Holowczak, “Programming concepts: A brief tutorial for new programmers,” Holowczak.com Tutorials. Mar. 2013 [Online]. Available: http://holowczak.com/programming-concepts-tutorial-programmers/\n\n\n\n[4] \nD. Fenjves, “Computer language fundamentals: Five core concepts,” upperlinecode. Jun. 2016 [Online]. Available: https://medium.com/upperlinecode/computer-language-fundamentals-five-core-concepts-1aa43e929f40\n\n\n\n[5] \nD. Asay, “Basic programming concepts,” BYU LiveCode Lessons Gateway, DigHT 210. 2005 [Online]. Available: https://livecode.byu.edu/programmingconcepts/ControlStruct.php\n\n\n\n[6] \nD. C. R. Severance, Python for Everybody: Exploring Data in Python 3. Ann Arbor, MI: CreateSpace Independent Publishing Platform, 2016 [Online]. Available: https://www.py4e.com/html3/\n\n\n\n[7] \nD. E. Knuth, “Literate programming,” Comput. J., vol. 27, no. 2, pp. 97–111, May 1984, doi: 10.1093/comjnl/27.2.97. [Online]. Available: https://doi.org/10.1093/comjnl/27.2.97\n\n\n\n[8] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[10] \nJosh Cogliati et al., Non-Programmer’s Tutorial for Python 3. Wikibooks, 2021 [Online]. Available: https://en.wikibooks.org/wiki/Non-Programmer%27s_Tutorial_for_Python_3. [Accessed: May 28, 2022]\n\n\n[11] \nC. Bilder, “Stat 850 Course Site,” Stat 850 Course Site. 2019 [Online]. Available: https://www.chrisbilder.com/stat850/. [Accessed: May 09, 2022]\n\n\n[12] \nH. Hofmann, “Stat 579 at Iowa State,” Stat 579 at ISU. 2020 [Online]. Available: https://stat579-at-isu.github.io/index.html. [Accessed: May 09, 2022]\n\n\n[13] \nJ. Bryan and The STAT 545 TAs, Stat 545 at UBC. 2019 [Online]. Available: https://stat545.com/. [Accessed: May 09, 2022]"
  },
  {
    "objectID": "intro-prog.html#footnotes",
    "href": "intro-prog.html#footnotes",
    "title": "2  Programming First Steps",
    "section": "",
    "text": "Yihui Xie is the person responsible for knitr and Rmarkdown and has been heavily involved in the creation of quarto as well.↩︎"
  },
  {
    "objectID": "basic-types.html#module-3-objectives",
    "href": "basic-types.html#module-3-objectives",
    "title": "3  Basic Variable Types",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nKnow the basic data types and what their restrictions are\nKnow how to test to see if a variable is a given data type\nUnderstand the basics of implicit and explicit type conversion\nWrite code that assigns values to variables"
  },
  {
    "objectID": "basic-types.html#vocabulary",
    "href": "basic-types.html#vocabulary",
    "title": "3  Basic Variable Types",
    "section": "\n3.1 Vocabulary",
    "text": "3.1 Vocabulary\nFor a general overview, [1] is an excellent introduction to data types:\n\n\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, “Hello, World”, and so on.\nvalues have types - 2 is an integer, “Hello, World” is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn most languages, there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric - decimal numbers.\n\nfloat is short for floating-point value.\ndouble is a floating-point value with more precision (“double precision”).\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\nIf you don’t know what type a value is, there are usually functions to help you with that.\n\n\nR\nPython\n\n\n\n\nclass(FALSE)\nclass(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nclass(2)\nclass(\"Hello, programmer!\")\n## [1] \"logical\"\n## [1] \"integer\"\n## [1] \"numeric\"\n## [1] \"character\"\n\n\n\n\ntype(False)\ntype(2)\ntype(3.1415)\ntype(\"This is python code\")\n## &lt;class 'bool'&gt;\n## &lt;class 'int'&gt;\n## &lt;class 'float'&gt;\n## &lt;class 'str'&gt;\n\n\n\n\n\n\n\n\n\n\nCapitalization matters!\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data."
  },
  {
    "objectID": "basic-types.html#assignment",
    "href": "basic-types.html#assignment",
    "title": "3  Basic Variable Types",
    "section": "Assignment",
    "text": "Assignment\nIn R, &lt;- is used for assigning a value to a variable. So x &lt;- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’”.\nIn Python, = is used for assigning a value to a variable. This tends to be much easier to say out loud, but lacks any indication of directionality.\n\n\nCharacter\nLogical\nInteger\nDouble\nNumeric\n\n\n\n\nx &lt;- \"R is awesome\"\ntypeof(x)\n## [1] \"character\"\nis.character(x)\n## [1] TRUE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\n\nx = \"python is awesome\"\ntype(x)\n## &lt;class 'str'&gt;\nisinstance(x, str)\n## True\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- FALSE\ntypeof(x)\n## [1] \"logical\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] TRUE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\nIn R, is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false. 1\n\nx = False\ntype(x)\n## &lt;class 'bool'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## True\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\nNote that in python, boolean variables are also integers. If your goal is to test whether something is a T/F value, you may want to e.g. test whether its value is one of 0 or 1, rather than testing whether it is a boolean variable directly, since integers can also function directly as bools in Python.\n\n\n\nx &lt;- 2\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\n\nWait, 2 is an integer, right?\n2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically.\n\nx &lt;- 2L # The L immediately after the 2 indicates that it is an integer.\ntypeof(x)\n## [1] \"integer\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] TRUE\nis.double(x)\n## [1] FALSE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2\ntype(x)\n## &lt;class 'int'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\n\n\n\nx &lt;- 2.45\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2.45\ntype(x)\n## &lt;class 'float'&gt;\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## True\n\n\n\nA fifth common “type”2, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing.\nThe numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work.\nThe general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double."
  },
  {
    "objectID": "basic-types.html#testing-types",
    "href": "basic-types.html#testing-types",
    "title": "3  Basic Variable Types",
    "section": "\n3.2 Testing Types",
    "text": "3.2 Testing Types\nYou can use different functions to test whether a variable has a specific type as well:\n\n\nR\nPython\n\n\n\n\nis.logical(FALSE)\nis.integer(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nis.integer(2)\nis.numeric(2)\nis.character(\"Hello, programmer!\")\n## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] TRUE\n## [1] TRUE\n\nIn R, you use is.xxx functions, where xxx is the name of the type in question.\n\n\n\nisinstance(False, bool)\nisinstance(2, int)\nisinstance(2, (int, float)) # Test for one of multiple types\nisinstance(3.1415, float)\nisinstance(\"This is python code\", str)\n## True\n## True\n## True\n## True\n## True\n\nIn python, test for types using the isinstance function with an argument containing one or more data types in a tuple ((int, float) is an example of a tuple - a static set of multiple values)."
  },
  {
    "objectID": "basic-types.html#sec-type-conversions",
    "href": "basic-types.html#sec-type-conversions",
    "title": "3  Basic Variable Types",
    "section": "\n3.3 Type Conversions",
    "text": "3.3 Type Conversions\nProgramming languages will generally work hard to seamlessly convert variables to different types. So, for instance,\n\n\nR\nPython\n\n\n\n\nTRUE + 2\n## [1] 3\n\n2L + 3.1415\n## [1] 5.1415\n\n\"abcd\" + 3\n## Error in \"abcd\" + 3: non-numeric argument to binary operator\n\n\n\n\nTrue + 2\n## 3\nint(2) + 3.1415\n## 5.141500000000001\n\"abcd\" + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nThis conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R or python will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops.\nWhen you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be:\n\n\nR\nPython\n\n\n\n\nas.double(TRUE) + 2\n## [1] 3\n\nas.double(2L) + 3.1415\n## [1] 5.1415\n\nas.numeric(\"abcd\") + 3\n## [1] NA\n\n\n\n\nint(True) + 2\n## 3\nfloat(2) + 3.1415\n## 5.141500000000001\nfloat(\"abcd\") + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: could not convert string to float: 'abcd'\nimport pandas as pd # Load pandas library\npd.to_numeric(\"abcd\", errors = 'coerce') + 3\n## nan\n\n\n\n\nWhen we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value - in R. In Python, we get a more descriptive error by default, but we can use the pandas library (which adds some statistical functionality) to get a similar result to the result we get in R.\nThere’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA."
  },
  {
    "objectID": "basic-types.html#determining-a-variables-type",
    "href": "basic-types.html#determining-a-variables-type",
    "title": "3  Basic Variable Types",
    "section": "\n3.4 Determining a Variable’s Type",
    "text": "3.4 Determining a Variable’s Type\n\n\nR\nPython\n\n\n\nIf you are unsure what the type of a variable is, use the typeof() function to find out.\n\nw &lt;- \"a string\"\nx &lt;- 3L\ny &lt;- 3.1415\nz &lt;- FALSE\n\ntypeof(w)\n## [1] \"character\"\ntypeof(x)\n## [1] \"integer\"\ntypeof(y)\n## [1] \"double\"\ntypeof(z)\n## [1] \"logical\"\n\n\n\nIf you are unsure what the type of a variable is, use the type() function to find out.\n\nw = \"a string\"\nx = 3\ny = 3.1415\nz = False\n\ntype(w)\n## &lt;class 'str'&gt;\ntype(x)\n## &lt;class 'int'&gt;\ntype(y)\n## &lt;class 'float'&gt;\ntype(z)\n## &lt;class 'bool'&gt;\n\n\n\n\n\n\n\n\n\n\nTry It Out: Variables and Types\n\n\n\n\n\nR\nPython\nR Solution\nPython Solution\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring &lt;- \ninteger &lt;- \ndecimal &lt;- \nlogical &lt;- \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring = \ninteger = \ndecimal = \nlogical = \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nstring &lt;- \"hi, I'm a string\"\ninteger &lt;- 4L\ndecimal &lt;- 5.412\nlogical &lt;- TRUE\n\nlogical + decimal\n## [1] 6.412\ninteger + decimal\n## [1] 9.412\nas.numeric(string) + integer\n## [1] NA\n\n\"abcd\" + \"efgh\"\n## Error in \"abcd\" + \"efgh\": non-numeric argument to binary operator\nTRUE + TRUE\n## [1] 2\n\nIn R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value.\nTo concatenate strings in R (like the default behavior in python), we would use the paste0 function: paste0(\"abcd\", \"efgh\"), which returns abcdefgh.\n\n\n\nimport pandas as pd\n\nstring = \"hi, I'm a string\"\ninteger = 4\ndecimal = 5.412\nlogical = True\n\nlogical + decimal\n## 6.412\ninteger + decimal\n## 9.411999999999999\npd.to_numeric(string, errors='coerce') + integer\n## nan\n\"abcd\" + \"efgh\"\n## 'abcdefgh'\nTrue + True\n## 2\n\nIn Python, when a string is added to another string, the two strings are concatenated. This differs from the result in R, which is a “non-numeric argument to binary operator” error."
  },
  {
    "objectID": "basic-types.html#references",
    "href": "basic-types.html#references",
    "title": "3  Basic Variable Types",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \n\nWhy TRUE + TRUE = 2: Data Types. (Feb. 03, 2020) [Online]. Available: https://www.youtube.com/watch?v=6otW6OXjR8c. [Accessed: May 18, 2022]"
  },
  {
    "objectID": "basic-types.html#footnotes",
    "href": "basic-types.html#footnotes",
    "title": "3  Basic Variable Types",
    "section": "",
    "text": "There is also an R package dedicated to pure evil that will set F and T randomly on startup. Use this information wisely.↩︎\nnumeric is not really a type, it’s a mode. Run ?mode for more information.↩︎"
  },
  {
    "objectID": "data-structures.html#module-4-objectives",
    "href": "data-structures.html#module-4-objectives",
    "title": "\n4  Data Structures\n",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nKnow the types of data structures and what restrictions each has\nUse indexing to pull information out of data structures\nDo basic mathematical calculations on scalars, vectors, and matrices\nUnderstand which data structures are included in the base language and which require add-on packages"
  },
  {
    "objectID": "data-structures.html#introduction",
    "href": "data-structures.html#introduction",
    "title": "\n4  Data Structures\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nData structures are more complex arrangements of information than single variables. Of primary interest in statistical programming are the following types of structures, illustrated with Lego pieces (piece size indicates “data type”):\n\n\nTable 4.1: Common data structures as implemented in various statistical programming languages. Where applicable, specific package names are listed (e.g. in Python, Numpy and Pandas).\n\n\n\n\n\n\n \nHomogeneous\nHeterogeneous\n\n\n\n1d\n\n\n\n\n\nvector (R, python)Series (Pandas)\nlist (R, python)dict (python)tuple (python)\n\n\n2d\n\n\n\n\n\nmatrix (R)ndarray(Numpy)\ndata frame (R, Pandas)data set (SAS)\n\n\nnd\n\n\n\n\n\narray (R)ndarray(Numpy)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFiguring out what to call these types with multiple languages is hard: in SAS, an array is a group of columns of a data set, but in R, it’s a multi-dimensional matrix, and in Python it’s a ndarray. I’ve attempted to separate out language-specific content (e.g. how to do X in python or R) from the general concepts where possible."
  },
  {
    "objectID": "data-structures.html#homogeneous-data-structures",
    "href": "data-structures.html#homogeneous-data-structures",
    "title": "\n4  Data Structures\n",
    "section": "Homogeneous data structures",
    "text": "Homogeneous data structures"
  },
  {
    "objectID": "data-structures.html#scalars",
    "href": "data-structures.html#scalars",
    "title": "\n4  Data Structures\n",
    "section": "\n4.2 Scalars",
    "text": "4.2 Scalars\nR does not have scalar types - even single-value variables are technically vectors of length 1. SAS and Python do have scalar types. Creating scalars is as simple as defining a variable, which you have already seen before."
  },
  {
    "objectID": "data-structures.html#vectors",
    "href": "data-structures.html#vectors",
    "title": "\n4  Data Structures\n",
    "section": "\n4.3 Vectors",
    "text": "4.3 Vectors\nVectors are constrained so that all items in a vector must be of the same type. When necessary, this means that variables will be type-converted, as shown above.\n\n\nR\nPython\n\n\n\nIn R, creating a vector is as easy as using the c() (concatenate) function. To create a vector of sequential integers, we use start:end, where start and end are integers. To create a finer-grained sequence, such as counting from 1 to 10 by 0.05, we would use the seq() function. seq() also has an argument named length.out which allows you to specify the vector length; the interval between successive numbers is then automatically calculated for you.\n\ndigits_of_pi &lt;- c(3, 1, 4, 1, 5, 9)\n\nseq_10 &lt;- 1:10\n\ntmp &lt;- seq(1, 10, by = 0.05)\n\ntmp2 &lt;- seq(1, 10, length.out = 100)\n\n\n\nIn python, we will use the NumPy package to create arrays of any dimension. First, we have to tell Python we want to use the numpy package, and what we’re going to call it. Usually, this means that we add the line import numpy as np so that we call any functions in the package using np.function_name().\nThe next thing to realize is that by default, python doesn’t have a vector/matrix/array type - this is something added in numpy. So we’ll be using lists to create our default data structures, and then converting them into numpy objects.\n\nimport numpy as np\n\nlst = [3, 1, 4, 1, 5, 9] # this is a list\n\ndigits_of_pi = np.array(lst) # this creates a vector\n\nseq_10 = np.array(range(1, 10))\n\ntmp = np.arange(1, 10, step = 0.05)\n\ntmp2 = np.linspace(1, 10, 100)\n\nThe range function defines a range from 1 to 10, and the np.array() function converts that into a vector of integers counting from 1 to 10. In the next step, we use the np.arange function to specify what the step size we want is. Finally, we specify a vector of length 100 using the np.linspace function.\n\n\n\n\n\n\n\n\n\nTry it out: Vectors\n\n\n\n\n\nProblem\nSolution\n\n\n\nIf we try to create a heterogeneous vector in R, using the concatenate function, c(), which combines scalar entries into a vector, what happens?\n\n\n\nc(1, 2, \"a\", \"b\", \"c\")\n## [1] \"1\" \"2\" \"a\" \"b\" \"c\"\n\nBecause there were 3 character entries, the entire vector is now a character vector."
  },
  {
    "objectID": "data-structures.html#matrices-and-arrays",
    "href": "data-structures.html#matrices-and-arrays",
    "title": "\n4  Data Structures\n",
    "section": "\n4.4 Matrices and Arrays",
    "text": "4.4 Matrices and Arrays\nMatrices (2 dimensional) and arrays (&gt;2 dimensional) are homogeneous, rectangular arrangements of data. They may have dimension names in R (but not in python).\n\n\nR\nPython\n\n\n\nLet’s start out by creating a 3x4 matrix (3 rows, 4 columns) of the digits 1:12.\n\nmatrix(1:12, 3, 4)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\n\nBy default, R fills in everything column-by-column. If we want to change that behavior, we use the byrow option:\n\nmatrix(1:12, 3, 4, byrow = T)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n\nIf we want to create a higher-order array in R, we use the array command:\n\narray(1:12, dim = c(2, 2, 3))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    5    7\n## [2,]    6    8\n## \n## , , 3\n## \n##      [,1] [,2]\n## [1,]    9   11\n## [2,]   10   12\n\nNote that higher-dimension arrays are hard to print and visualize.\nIt is easy enough to create an array that has all 0’s or 1’s in it as well - I have used 1:12 here to show you how things get populated to different slices of the array, but any vector will work to initialize your array.\n\n\nIf a vector and a list are relatively similar in structure, we can think up an analogue of the matrix as a list of lists of the same length. Note that this is only necessary because by default, Python has a list type but not a vector type; numpy adds the ability to deal with vectors and arrays directly, but we must first construct them out of something that makes sense to base python.\n\nimport numpy as np\n\n# Different shapes matter\n[[1, 2, 3], [4, 5, 6]] \n## [[1, 2, 3], [4, 5, 6]]\n[[1,2], [3,4], [5,6]] \n# Row by row inside the first-level list\n## [[1, 2], [3, 4], [5, 6]]\nnp.array([[1, 2, 3], [4, 5, 6]]) \n## array([[1, 2, 3],\n##        [4, 5, 6]])\nnp.array([[1,2], [3,4], [5,6]])\n## array([[1, 2],\n##        [3, 4],\n##        [5, 6]])\n\nIf we instead want to create a matrix of 0s, we can do that using the np.zeros function.\n\nnp.zeros(shape=(2, 2, 3), dtype=float)\n## array([[[0., 0., 0.],\n##         [0., 0., 0.]],\n## \n##        [[0., 0., 0.],\n##         [0., 0., 0.]]])\n\nFilling the matrix with non-zero values is a different challenge that requires us to talk about indexing (discussed below)."
  },
  {
    "objectID": "data-structures.html#overgrown-calculators",
    "href": "data-structures.html#overgrown-calculators",
    "title": "\n4  Data Structures\n",
    "section": "\n4.5 Overgrown Calculators",
    "text": "4.5 Overgrown Calculators\nWhile R, SAS, and Python are all extremely powerful statistical programming languages, the core of most programming languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in whatever language you are planning to use to work with data.\nIn this section, we will essentially be using our programming language as overgrown calculators.\nIn the next chapters we’ll talk about data types and structures, so you’ll get to see more about matrices and arrays, but for now, let’s confine ourselves to using R and python to do basic math calculations.\n\n\nTable 4.2: Table of common mathematical and matrix operations in R, SAS, and Python [1].\n\n\n\n\n\n\n\nOperation\nR\nSAS\nPython\n\n\n\nAddition\n+\n+\n+\n\n\nSubtraction\n-\n-\n-\n\n\nElementwise Multiplication\n*\n#\n*\n\n\nDivision\n/\n/\n/\n\n\nModulo (Remainder)\n%%\nMOD\n%\n\n\nInteger Division\n%/%\nFLOOR(x\\y)\n//\n\n\nElementwise Exponentiation\n^\n##\n**\n\n\nMatrix/Vector Multiplication\n%*%\n*\nnp.dot()\n\n\nMatrix Exponentiation\n^\n**\nnp.exp()\n\n\nMatrix Transpose\nt(A)\n\nA`\nnp.transpose(A)\n\n\nMatrix Determinant\ndet(A)\ndet(A)\nnp.linalg.det(A)\n\n\nMatrix Diagonal\ndiag(A)\ndiag(A)\nnp.linalg.diag(A)\n\n\nMatrix Inverse\nsolve(A)\nsolve(A, diag({...}))\nnp.linalg.inv(A)\n\n\n\n\n\n4.5.1 Basic Mathematical Operators\n\n\nR\nPython\nSAS\n\n\n\n\nx &lt;- 1:10\ny &lt;- seq(3, 30, by = 3)\n\nx + y\n##  [1]  4  8 12 16 20 24 28 32 36 40\nx - y\n##  [1]  -2  -4  -6  -8 -10 -12 -14 -16 -18 -20\nx * y\n##  [1]   3  12  27  48  75 108 147 192 243 300\nx / y\n##  [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n##  [8] 0.3333333 0.3333333 0.3333333\nx^2\n##  [1]   1   4   9  16  25  36  49  64  81 100\nt(x) %*% y\n##      [,1]\n## [1,] 1155\n\n\n\n\nimport numpy as np\n\nx = np.array(range(1, 11))\ny = np.array(range(3, 33, 3)) # python indexes are not inclusive\n\nx + y\n## array([ 4,  8, 12, 16, 20, 24, 28, 32, 36, 40])\nx - y\n## array([ -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20])\nx * y\n## array([  3,  12,  27,  48,  75, 108, 147, 192, 243, 300])\nx / y\n## array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n##        0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])\nx ** 2\n## array([  1,   4,   9,  16,  25,  36,  49,  64,  81, 100])\nnp.dot(x.T, y)\n## 1155\n\n\n\nBy default, SAS creates row vectors with do(a, b, by = c) syntax. The transpose operator (a single backtick) can be used to transform A into A`.\nproc iml; \n  x = do(1, 10, 1);\n  y = do(3, 30, 3);\n\n  z = x + y;\n  z2 = x - y;\n  z3 = x # y;\n  z4 = x/y;\n  z5 = x##2;\n  z6 = x` * y;\n  print z, z2, z3, z4, z5, z6;\nquit;\n\n\n\n\n4.5.2 Matrix Operations\nOther matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy:\n\n\nR\nPython\nSAS\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\nmat\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    6    4    5\n## [3,]    7    8    9\nt(mat) # transpose\n##      [,1] [,2] [,3]\n## [1,]    1    6    7\n## [2,]    2    4    8\n## [3,]    3    5    9\ndet(mat) # get the determinant\n## [1] 18\ndiag(mat) # get the diagonal\n## [1] 1 4 9\ndiag(diag(mat)) # get a square matrix with off-diag 0s\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    4    0\n## [3,]    0    0    9\ndiag(1:3) # diag() also will create a diagonal matrix if given a vector\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    2    0\n## [3,]    0    0    3\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nmat\n## array([[1, 2, 3],\n##        [6, 4, 5],\n##        [7, 8, 9]])\nmat.T\n## array([[1, 6, 7],\n##        [2, 4, 8],\n##        [3, 5, 9]])\nnp.linalg.det(mat) # numerical precision...\n## 18.000000000000004\nnp.diag(mat)\n## array([1, 4, 9])\nnp.diag(np.diag(mat))\n## array([[1, 0, 0],\n##        [0, 4, 0],\n##        [0, 0, 9]])\nnp.diag(range(1, 4))\n## array([[1, 0, 0],\n##        [0, 2, 0],\n##        [0, 0, 3]])\n\n\n\nproc iml;\n  mat = {1 2 3, 6 4 5, 7 8 9}; \n  tmat = mat`; /* transpose */\n  determinant = det(mat); /* get the determinant */\n  diagonal_vector = vecdiag(mat); /* get the diagonal as a vector */\n  diagonal_mat = diag(mat); /* get the diagonal as a square matrix */\n                            /* with 0 on off-diagonal entries */\n  \n  dm = diag({1 2 3}); /* make a square matrix with vector as the diagonal */\n  \n  print tmat, determinant, diagonal_vector, diagonal_mat, dm;\nquit;\n\n\n\n\n4.5.3 Matrix Inverse\nThe other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, in R and SAS, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. In Python, we use the np.linalg.inv() function to invert a matrix, which is a bit more linguistically familiar.\n\n\nR\nPython\nSAS\n\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\n\nminv &lt;- solve(mat) # get the inverse\n\nminv\n##            [,1]       [,2]       [,3]\n## [1,] -0.2222222  0.3333333 -0.1111111\n## [2,] -1.0555556 -0.6666667  0.7222222\n## [3,]  1.1111111  0.3333333 -0.4444444\nmat %*% minv \n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nminv = np.linalg.inv(mat)\nminv\n## array([[-0.22222222,  0.33333333, -0.11111111],\n##        [-1.05555556, -0.66666667,  0.72222222],\n##        [ 1.11111111,  0.33333333, -0.44444444]])\nnp.dot(mat, minv)\n## array([[ 1.00000000e+00,  0.00000000e+00,  1.11022302e-16],\n##        [-8.88178420e-16,  1.00000000e+00, -5.55111512e-16],\n##        [ 0.00000000e+00,  2.22044605e-16,  1.00000000e+00]])\nnp.round(np.dot(mat, minv), 2)\n## array([[ 1.,  0.,  0.],\n##        [-0.,  1., -0.],\n##        [ 0.,  0.,  1.]])\n\n\n\nDocumentation\n    proc iml;\n      mat = {1 2 3, 6 4 5, 7 8 9};\n\n      mat_inv = solve(mat, diag({1 1 1})); /* get the inverse */\n      mat_inv2 = inv(mat); /* less efficient and less accurate */\n      print mat_inv, mat_inv2;\n\n      id = mat * mat_inv;\n      id2 = mat * mat_inv2;\n      print id, id2; \n    quit;"
  },
  {
    "objectID": "data-structures.html#heterogeneous-data-structures",
    "href": "data-structures.html#heterogeneous-data-structures",
    "title": "\n4  Data Structures\n",
    "section": "Heterogeneous data structures",
    "text": "Heterogeneous data structures\nThe heterogeneous data types are not much harder to grasp, as they’re mostly different ways to combine various homogeneous data types."
  },
  {
    "objectID": "data-structures.html#one-dimensional-structures",
    "href": "data-structures.html#one-dimensional-structures",
    "title": "\n4  Data Structures\n",
    "section": "\n4.6 One-dimensional structures",
    "text": "4.6 One-dimensional structures\nIn R and SAS, lists are the basic building block of one-dimensional data structures. Python makes this a bit more complicated, as it has several different one-dimensional heterogeneous data structures that we’ll have to cover. I’ve restructured this section considerably during the transition to R + Python as the main focus of this class, but I’d encourage you to read this section with a focus on lists and just keep in mind what the differences are between the structures - you don’t have to remember them, but know where to look it up.\n\n\n\n\n\n\n\n\n\nType\nLanguage\nNamed?\nOrdered/Indexable\nOther Notes\n\n\n\nList\nR\nyes\nyes\nbasic heterogeneous data type\n\n\nData frame\nR\nyes\nyes\nessentially a list of vectors of the same length\n\n\nList\npython\nno\nyes\n\n\n\nTuple\npython\nno\nyes\ncannot be edited once created\n\n\nDictionary\npython\nyes: key-value pairs\nyes (Python 3.7) no (Python &lt; 3.7)\ncannot have duplicate keys\n\n\n\n\n4.6.1 Lists\nA list is a sequence of different-typed values.\nUnlike when concatenating values, the list() command in R allows each value to keep its natural type. In Python, the default complex data type is a list; more strict data types are then converted from lists to more structured formats using e.g. np.array or pd.DataFrame.\n\n\nR\nPython\n\n\n\n\nx &lt;- list(\"a\", \"b\", \"c\", 1, 2, 3)\n\nx\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] \"b\"\n## \n## [[3]]\n## [1] \"c\"\n## \n## [[4]]\n## [1] 1\n## \n## [[5]]\n## [1] 2\n## \n## [[6]]\n## [1] 3\nx[[1]] # Indexes start at 1 in R\n## [1] \"a\"\n\nx[[4]] + x[[5]]\n## [1] 3\n\nx[1:2] # This will work\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] \"b\"\n\nx[[1:2]] # This won't work\n## Error in x[[1:2]]: subscript out of bounds\n\nSome lists (and data frames) consist of named variables. These list components can be accessed either by index (as above) or by name, using the $ operator. Names which have spaces or special characters must be enclosed in backticks (next to the 1 on the keyboard). Named components can also be accessed using the [[ ]] operator.\n\ndog &lt;- list(name = \"Edison Vanderplas\", age = 9, \n            breed = \"Jack Russell Terrorist\", \n            `favorite toy` = \"a blue and orange stuffed duck. Or rawhide.\",\n            `link(video)` = \"https://youtu.be/zVeoQTOTIuQ\")\n\ndog\n## $name\n## [1] \"Edison Vanderplas\"\n## \n## $age\n## [1] 9\n## \n## $breed\n## [1] \"Jack Russell Terrorist\"\n## \n## $`favorite toy`\n## [1] \"a blue and orange stuffed duck. Or rawhide.\"\n## \n## $`link(video)`\n## [1] \"https://youtu.be/zVeoQTOTIuQ\"\n\ndog$name\n## [1] \"Edison Vanderplas\"\ndog$breed\n## [1] \"Jack Russell Terrorist\"\ndog$`favorite toy`\n## [1] \"a blue and orange stuffed duck. Or rawhide.\"\ndog[[\"link(video)\"]]\n## [1] \"https://youtu.be/zVeoQTOTIuQ\"\n\nYou can get a sense of the structure of a list (or any other object) in R using the str() command.\n\n\nstr(dog)\n## List of 5\n##  $ name        : chr \"Edison Vanderplas\"\n##  $ age         : num 9\n##  $ breed       : chr \"Jack Russell Terrorist\"\n##  $ favorite toy: chr \"a blue and orange stuffed duck. Or rawhide.\"\n##  $ link(video) : chr \"https://youtu.be/zVeoQTOTIuQ\"\n\n\n\nUnlike in R, lists in python cannot have named entries. Lists are a basic data structure in python, so they exist in base Python without needing to load e.g. Numpy or Pandas. As a reminder, Python is a 0-indexed language, so you will need to count starting at 0 instead of 1 to determine what elements you want to pull out of a list.\n\nx = ['a', 'b', 'c', 1, 2, 3]\n\nx\n## ['a', 'b', 'c', 1, 2, 3]\nx[0] # Indexes start at 0 in python\n## 'a'\nx[1:2] # Python indexes are [a, b); if you want b, use [a, b+1)\n## ['b']\nx[2:4]\n## ['c', 1]\n\n\n\n\n\n4.6.2 Tuple\nPython contains a data type called a Tuple used to store multiple items in a single variable. Tuples are ordered and unchangeable - Once stored, you cannot update them. Tuples can store more than one data type.\n\nmytuple = (\"apple\", \"banana\", 3)\nmytuple[0]\n## 'apple'\nmytuple[1]\n## 'banana'\nmytuple[2]\n## 3\nmytuple\n## ('apple', 'banana', 3)\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the syntax for making a tuple is very similar to the syntax for making a list - the only difference is that tuples use () and lists use [].\n\n\nIt’s worth knowing about tuples, but I wouldn’t worry about using them frequently until you get better at programming. You can check out this article about Lists, Arrays, and Tuples for more information.\n\n4.6.3 Dictionary\nAnother basic data type in Python used to store multiple pieces of data is a dictionary.\nA dictionary is ordered (in Python 3.7+), changeable, and does not allow duplicates.\nDictionaries are written with curly brackets, and have keys and values, where we use the syntax key:value (with quotes around strings).\n\ndogdict = {\n  \"name\": \"Edison\",\n  \"age\": 9,\n  \"breed\": \"Jack Russell Terrorist\",\n  \"energy_level\": \"high\"\n}\nprint(dogdict)\n## {'name': 'Edison', 'age': 9, 'breed': 'Jack Russell Terrorist', 'energy_level': 'high'}\ndogdict[\"name\"]\n## 'Edison'\ndogdict[0] # dicts can be indexed by key but not by position\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 0\ndogdict[\"color\"] = \"white\" # add a new item by adding a new key and assigning a value\ndogdict\n## {'name': 'Edison', 'age': 9, 'breed': 'Jack Russell Terrorist', 'energy_level': 'high', 'color': 'white'}\n\n\n4.6.4 Series\nThe Pandas package contains a data type called a Series - essentially, it’s a one-dimensional vector with axis labels. This is roughly equivalent to a named vector in R.\n\nimport pandas as pd\n\ns1 = pd.Series([3,1,4,1,5])\ns1 # by default, the key is just the numeric index\n## 0    3\n## 1    1\n## 2    4\n## 3    1\n## 4    5\n## dtype: int64\ns1[0]\n\n# we can set labeled keys by converting a dict to a series\n## 3\ns2 = pd.Series({\"susan\":1987,\"ryan\":1986,\"alex\":2016,\"zoey\":2021})\ns2 \n\n# Series can be indexed by position or by key\n## susan    1987\n## ryan     1986\n## alex     2016\n## zoey     2021\n## dtype: int64\ns2[0]\n## 1987\ns2[\"zoey\"]\n## 2021\n\n\n4.6.5 Level Up: Recursive structures\nLists, dicts, and other one-dimensional structures can also contain containers - vectors, lists, or other multi-element items. When accessing a list-within-a-list, just add another index or name reference (see below).\n\n\n\n\n\n\nExample: Recursive lists\n\n\n\n\n\nR\nPython\n\n\n\n\ngrocery_list &lt;- list(\n  dairy = c(\"asiago\", \"fontina\", \"mozzarella\", \"blue cheese\"),\n  baking = c(\"flour\", \"yeast\", \"salt\"),\n  canned_goods = c(\"pepperoni\", \"pizza sauce\", \"olives\"),\n  meat = c(\"bacon\", \"sausage\", \"anchovies\"),\n  veggies = c(\"bell pepper\", \"onion\", \"scallions\", \"tomatoes\", \"basil\")\n)\n\nick &lt;- c(grocery_list[[4]][2:3], grocery_list$canned_goods[[3]])\nick\n## [1] \"sausage\"   \"anchovies\" \"olives\"\n\ncrust_ingredients &lt;- c(grocery_list$baking, \"water\")\ncrust_ingredients\n## [1] \"flour\" \"yeast\" \"salt\"  \"water\"\n\nessential_toppings &lt;- c(grocery_list$dairy[3], grocery_list$canned_goods[2])\nessential_toppings\n## [1] \"mozzarella\"  \"pizza sauce\"\n\nyummy_toppings &lt;- c(grocery_list$dairy[c(1, 2, 4)], grocery_list$meat[1], grocery_list[[5]][c(3, 5)])\nyummy_toppings\n## [1] \"asiago\"      \"fontina\"     \"blue cheese\" \"bacon\"       \"scallions\"  \n## [6] \"basil\"\n\n\n\nIn python we have to use a dict of lists, which doesn’t quite work as well - but we have to do that mostly because the pizza example makes sense with named lists. If we used unnamed lists, we could do the whole thing as a list of lists, just like in R.\n\ngrocery_list = {\n  \"dairy\": [\"asiago\", \"fontina\", \"mozzarella\", \"blue cheese\"],\n  \"baking\": [\"flour\", \"yeast\", \"salt\"],\n  \"canned_goods\": [\"pepperoni\", \"pizza sauce\", \"olives\"],\n  \"meat\": [\"bacon\", \"sausage\", \"anchovies\"],\n  \"veggies\": [\"bell pepper\", \"onion\", \"scallions\", \"tomatoes\", \"basil\"]\n  }\n  \nick = [grocery_list[\"meat\"][1:3], [grocery_list[\"canned_goods\"][2]]]\nick # this is a nested list, which isn't ideal\n## [['sausage', 'anchovies'], ['olives']]\nfrom itertools import chain\nick = list(chain(*ick)) # this unnests the list to a single level\n\ncrust_ingredients = [grocery_list[\"baking\"], [\"water\"]]\ncrust_ingredients = list(chain.from_iterable(crust_ingredients))\ncrust_ingredients\n## ['flour', 'yeast', 'salt', 'water']\nessential_toppings = [grocery_list[\"dairy\"][2], grocery_list[\"canned_goods\"][1]]\nessential_toppings\n## ['mozzarella', 'pizza sauce']\nyummy_toppings = [\n  grocery_list[\"dairy\"][0:2], \n  [\n    grocery_list[\"dairy\"][3], \n    grocery_list[\"meat\"][0], \n    grocery_list[\"veggies\"][2], \n    grocery_list[\"veggies\"][4]\n  ]# it works best to put individual entries in a list so that \n   # chain.from_iterable() works out\n  ]\n\nyummy_toppings = list(chain.from_iterable(yummy_toppings))\nyummy_toppings\n## ['asiago', 'fontina', 'blue cheese', 'bacon', 'scallions', 'basil']\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Lists\n\n\n\nUsing the list of pizza toppings above as a starting point, make your own list of pizza toppings organized by grocery store section (approximately). Create your own vectors of yummy, essential, and ick toppings, using R and Python."
  },
  {
    "objectID": "data-structures.html#data-frames",
    "href": "data-structures.html#data-frames",
    "title": "\n4  Data Structures\n",
    "section": "\n4.7 Data frames",
    "text": "4.7 Data frames\nA data frame is a special type of list - one in which each element in the list is a vector of the same length. If you put these vectors side-by-side, you get a table of data that looks like a spreadsheet. In Python, a DataFrame is a dict of Series.\nThe lego version of a data frame looks like this:\n\n\nA data frame with 4 columns. A data frame is essentially a list where all of the components are vectors or lists, and are constrained to have the same length.\n\n\n\nR\nPython\n\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\nhead(mtcars) ## A data frame included in base R\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nstr(mtcars)\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nYou can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations.\n\nmtcars$gpm &lt;- 1/mtcars$mpg # gpm is sometimes used to assess efficiency\n\nsummary(mtcars$gpm)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615\nsummary(mtcars$mpg)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\n\nOften, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary().\n\nsummary(mtcars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb            gpm         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02950  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04386  \n##  Median :0.0000   Median :4.000   Median :2.000   Median :0.05208  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.05423  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.06483  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.09615\ndim(mtcars)\n## [1] 32 12\nnrow(mtcars)\n## [1] 32\nncol(mtcars)\n## [1] 12\n\nMissing variables in an R data frame are indicated with NA.\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The df.head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\nmtcars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmtcars.head(5)\n##           Unnamed: 0   mpg  cyl   disp   hp  ...   qsec  vs  am  gear  carb\n## 0          Mazda RX4  21.0    6  160.0  110  ...  16.46   0   1     4     4\n## 1      Mazda RX4 Wag  21.0    6  160.0  110  ...  17.02   0   1     4     4\n## 2         Datsun 710  22.8    4  108.0   93  ...  18.61   1   1     4     1\n## 3     Hornet 4 Drive  21.4    6  258.0  110  ...  19.44   1   0     3     1\n## 4  Hornet Sportabout  18.7    8  360.0  175  ...  17.02   0   0     3     2\n## \n## [5 rows x 12 columns]\nmtcars.info()\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 32 entries, 0 to 31\n## Data columns (total 12 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   Unnamed: 0  32 non-null     object \n##  1   mpg         32 non-null     float64\n##  2   cyl         32 non-null     int64  \n##  3   disp        32 non-null     float64\n##  4   hp          32 non-null     int64  \n##  5   drat        32 non-null     float64\n##  6   wt          32 non-null     float64\n##  7   qsec        32 non-null     float64\n##  8   vs          32 non-null     int64  \n##  9   am          32 non-null     int64  \n##  10  gear        32 non-null     int64  \n##  11  carb        32 non-null     int64  \n## dtypes: float64(5), int64(6), object(1)\n## memory usage: 3.1+ KB\n\nYou can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. You can access a column named xyz using df.xyz or using df[\"xyz\"]. To create a new column, you must use df[\"xyz\"].\n\nmtcars[\"gpm\"] = 1/mtcars.mpg # gpm is sometimes used to assess efficiency\n\nmtcars.gpm.describe()\n## count    32.000000\n## mean      0.054227\n## std       0.016424\n## min       0.029499\n## 25%       0.043860\n## 50%       0.052083\n## 75%       0.064834\n## max       0.096154\n## Name: gpm, dtype: float64\nmtcars.mpg.describe()\n## count    32.000000\n## mean     20.090625\n## std       6.026948\n## min      10.400000\n## 25%      15.425000\n## 50%      19.200000\n## 75%      22.800000\n## max      33.900000\n## Name: mpg, dtype: float64\n\nOften, it is useful to know the dimensions of a data frame. The dimensions of a data frame (rows x columns) can be accessed using df.shape. There is also an easy way to get a summary of each column in the data frame, using df.describe().\n\nmtcars.describe()\n##              mpg        cyl        disp  ...       gear     carb        gpm\n## count  32.000000  32.000000   32.000000  ...  32.000000  32.0000  32.000000\n## mean   20.090625   6.187500  230.721875  ...   3.687500   2.8125   0.054227\n## std     6.026948   1.785922  123.938694  ...   0.737804   1.6152   0.016424\n## min    10.400000   4.000000   71.100000  ...   3.000000   1.0000   0.029499\n## 25%    15.425000   4.000000  120.825000  ...   3.000000   2.0000   0.043860\n## 50%    19.200000   6.000000  196.300000  ...   4.000000   2.0000   0.052083\n## 75%    22.800000   8.000000  326.000000  ...   4.000000   4.0000   0.064834\n## max    33.900000   8.000000  472.000000  ...   5.000000   8.0000   0.096154\n## \n## [8 rows x 12 columns]\nmtcars.shape\n## (32, 13)\n\nMissing variables in a pandas data frame are indicated with nan or NULL.\n\n\n\n\n\n\n\n\n\nTry it out: Data Frames\n\n\n\n\n\nSetup\nProblem\nR Solution\nPython Solution\n\n\n\nThe dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below.\n\ndata(state)\nstate_facts &lt;- data.frame(state.x77)\nstate_facts &lt;- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) \n# State names were stored as row labels\n# Store them in a variable instead, and add it to the data frame\n\nrow.names(state_facts) &lt;- NULL # get rid of row names\n\nhead(state_facts)\n##        state Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## 1    Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## 2     Alaska        365   6315        1.5    69.31   11.3    66.7   152 566432\n## 3    Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n## 4   Arkansas       2110   3378        1.9    70.66   10.1    39.9    65  51945\n## 5 California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## 6   Colorado       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n# Write data out so that we can read it in using Python\nwrite.csv(state_facts, file = \"data/state_facts.csv\", row.names = F)\n\nWe can write out the built in R data and read it in using pd.read_csv, which creates a DataFrame in pandas.\n\nimport pandas as pd\n\nstate_facts = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/state_facts.csv\")\n\n\n\n\nHow many rows and columns does it have? Can you find different ways to get that information?\nThe Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands.\nCalculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density?\n\n\n\n\n# 3 ways to get rows and columns\nstr(state_facts)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Population: num  3615 365 2212 2110 21198 ...\n##  $ Income    : num  3624 6315 4530 3378 5114 ...\n##  $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n##  $ Life.Exp  : num  69 69.3 70.5 70.7 71.7 ...\n##  $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n##  $ HS.Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n##  $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n##  $ Area      : num  50708 566432 113417 51945 156361 ...\ndim(state_facts)\n## [1] 50  9\nnrow(state_facts)\n## [1] 50\nncol(state_facts)\n## [1] 9\n\n# Illiteracy\nstate_facts$TotalNumIlliterate &lt;- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) \n\n# Population Density\nstate_facts$PopDensity &lt;- state_facts$Population * 1e3/state_facts$Area \n# in people per square mile\n\n# minimum population\nstate_facts$state[which.min(state_facts$PopDensity)]\n## [1] \"Alaska\"\n\n\n\n\n# Ways to get rows and columns\nstate_facts.shape\n## (50, 9)\nstate_facts.index.size # rows\n## 50\nstate_facts.columns.size # columns\n## 9\nstate_facts.info() # columns + rows + missing counts + data types\n\n# Illiteracy\n## &lt;class 'pandas.core.frame.DataFrame'&gt;\n## RangeIndex: 50 entries, 0 to 49\n## Data columns (total 9 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   state       50 non-null     object \n##  1   Population  50 non-null     int64  \n##  2   Income      50 non-null     int64  \n##  3   Illiteracy  50 non-null     float64\n##  4   Life.Exp    50 non-null     float64\n##  5   Murder      50 non-null     float64\n##  6   HS.Grad     50 non-null     float64\n##  7   Frost       50 non-null     int64  \n##  8   Area        50 non-null     int64  \n## dtypes: float64(4), int64(4), object(1)\n## memory usage: 3.6+ KB\nstate_facts[\"TotalNumIlliterate\"] = state_facts[\"Population\"] * 1e3 * state_facts[\"Illiteracy\"]/100\n\n# Population Density\nstate_facts[\"PopDensity\"] = state_facts[\"Population\"] * 1e3/state_facts[\"Area\"] \n# in people per square mile\n\n# minimum population\nmin_dens = state_facts[\"PopDensity\"].min()\n# Get location of minimum population\nloc_min_dens = state_facts.PopDensity.isin([min_dens])\n# Pull out matching state\nstate_facts.state[loc_min_dens]\n## 1    Alaska\n## Name: state, dtype: object\n\n\n\n\n\n\n\n4.7.1 Creating Data Frames\nIt is also possible to create data frames from scratch by building them out of simpler components, such as vectors or dicts of lists. This tends to be useful for small datasets, but it is more common to read data in from e.g. CSV files, which I’ve used several times already but haven’t yet shown you how to do (give it a couple of weeks).\n\n\nR\nPython\n\n\n\n\nmath_and_lsd &lt;- data.frame(\n  lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41),\n  test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97))\nmath_and_lsd\n##   lsd_conc test_score\n## 1     1.17      78.93\n## 2     2.97      58.20\n## 3     3.26      67.47\n## 4     4.69      37.47\n## 5     5.83      45.65\n## 6     6.00      32.92\n## 7     6.41      29.97\n\n# add a column - character vector\nmath_and_lsd$subjective &lt;- c(\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\")\n\nmath_and_lsd\n##   lsd_conc test_score                             subjective\n## 1     1.17      78.93                    finally coming back\n## 2     2.97      58.20                         getting better\n## 3     3.26      67.47                    it's totally better\n## 4     4.69      37.47                    really tripping out\n## 5     5.83      45.65                            is it over?\n## 6     6.00      32.92                              whoa, man\n## 7     6.41      29.97 I can taste color, but I can't do math\n\n\n\n\nmath_and_lsd = pd.DataFrame({\n  \"lsd_conc\": [1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41],\n  \"test_score\": [78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97]})\nmath_and_lsd\n\n# add a column - character vector\n##    lsd_conc  test_score\n## 0      1.17       78.93\n## 1      2.97       58.20\n## 2      3.26       67.47\n## 3      4.69       37.47\n## 4      5.83       45.65\n## 5      6.00       32.92\n## 6      6.41       29.97\nmath_and_lsd[\"subjective\"] = [\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\"]\n\nmath_and_lsd\n##    lsd_conc  test_score                              subjective\n## 0      1.17       78.93                     finally coming back\n## 1      2.97       58.20                          getting better\n## 2      3.26       67.47                     it's totally better\n## 3      4.69       37.47                     really tripping out\n## 4      5.83       45.65                             is it over?\n## 5      6.00       32.92                               whoa, man\n## 6      6.41       29.97  I can taste color, but I can't do math"
  },
  {
    "objectID": "data-structures.html#sec-indexing",
    "href": "data-structures.html#sec-indexing",
    "title": "\n4  Data Structures\n",
    "section": "\n4.8 Indexing",
    "text": "4.8 Indexing\nNow that you’ve seen some complex data structures, it’s probably a good time to talk about indexing.\nIn order to access sub-elements of a complex data structure, we have to index the data type. There are two main ways to index something:\n\nBy name\nBy position\n\nNot every data type has both of these options - vectors, for instance, are generally not named, so we typically only index vectors by position1.\nDifferences in how things are indexed are one reason that heterogeneous data structures are a bit more complicated in Python than in R - different structures have different indexing rules and/or names attached.\n\n\nLanguage\nData Type\nIndex by position\nIndex by Name\n\n\n\nR\nvector\nyes\nyes (uncommon)\n\n\nR\narray/matrix\nyes\nyes (uncommon)\n\n\nR\nlist\nyes\nyes\n\n\nR\ndata frame\nyes\nyes\n\n\nR\ntibble\nyes\nyes\n\n\nPython\nvector\nyes\nno\n\n\nPython\nndarray (numpy)\nyes\nno\n\n\nPython\ndict\nyes\nyes\n\n\nPython\nSeries (pandas)\nyes\nyes\n\n\nPython\nDataFrame (pandas)\nyes\nyes\n\n\n\nIn R, you can usually index things using one of $, [], and [[]].\nIn Python, you can usually index things using one of [] or .loc or .iloc.\nSee the examples below for which index methods work with each data type (and why).\n\n\nR vector\nR matrix\nR array\nR list\nR data frame\n\n\n\n\nmyvec &lt;- c(d1 = 3, d2 = 1, d3 = 4, d4 = 1, d5 = 5, d6 = 9) # this is a named vector\n\nmyvec\n## d1 d2 d3 d4 d5 d6 \n##  3  1  4  1  5  9\nmyvec[1:3] # first 3 entries\n## d1 d2 d3 \n##  3  1  4\nmyvec[\"d1\"] # indexing a vector by name\n## d1 \n##  3\nmyvec$d1 # \"atomic vector\" = homogeneous vector; can't index by name using $\n## Error in myvec$d1: $ operator is invalid for atomic vectors\n\n\n\n\nmymat &lt;- matrix(\n  myvec, nrow = 2, \n  dimnames = list(\n    c('r1', 'r2'), # row names\n    c('c1', 'c2', 'c3') # column names\n  )\n) # this creates a matrix with named rows and columns\n\nmymat\n##    c1 c2 c3\n## r1  3  4  5\n## r2  1  1  9\n\nmymat[1,] # get the first row\n## c1 c2 c3 \n##  3  4  5\nmymat[,1] # get the first column\n## r1 r2 \n##  3  1\nmymat['r1'] # this doesn't find anything\n## [1] NA\nmymat['r1',] # if you tell R to look for the row named 'r1' it actually works\n## c1 c2 c3 \n##  3  4  5\nmymat['c1'] # this doesn't find anything\n## [1] NA\nmymat[,'c1'] # if you tell R to look for the col named 'c1' it actually works\n## r1 r2 \n##  3  1\n\nmymat[,2:3] # get multiple columns and all rows\n##    c2 c3\n## r1  4  5\n## r2  1  9\nmymat[1, 2:3] # get multiple columns and the first row\n## c2 c3 \n##  4  5\n\nmymat$r1 # matrices are still atomic vectors\n## Error in mymat$r1: $ operator is invalid for atomic vectors\n\n\n\n\nmyarray &lt;- array(\n  1:12, \n  dim = c(2, 2, 3), \n  dimnames = list(\n    c('a1', 'a2'), \n    c('b1', 'b2'), \n    c('c1', 'c2', 'c3')\n  )\n)\n\n# adding another dimension just means we add another entry to our [a, b, c]\n\nmyarray[1, 2, 3]\n## [1] 11\nmyarray[1, 2, ] # leaving out a number provides everything\n## c1 c2 c3 \n##  3  7 11\nmyarray[, 1, 2] \n## a1 a2 \n##  5  6\nmyarray[,,2]\n##    b1 b2\n## a1  5  7\n## a2  6  8\nmyarray[, 3, ] # if you don't keep track of your dimensions you'll get an error\n## Error in myarray[, 3, ]: subscript out of bounds\nmyarray['a1', 'b1', ] # names still work IF you define dimnames (not common)\n## c1 c2 c3 \n##  1  5  9\n\nmyarray$a1 # arrays are still atomic vectors\n## Error in myarray$a1: $ operator is invalid for atomic vectors\n\n\n\n\n# Create a named list\n\ndog &lt;- list(name = \"Edison Vanderplas\", \n            age = 9, \n            breed = \"Jack Russell Terrorist\", \n            fav_toy = \"stuffed duck\",\n            video = \"https://youtu.be/zVeoQTOTIuQ\")\n\n# Getting things out of the list as single objects\ndog$name # use $ to index by name, with no quotes\n## [1] \"Edison Vanderplas\"\ndog$sex # if you use a name that isn't there, you get NULL\n## NULL\n\ndog[[\"name\"]] # use [[]] to get to a single element, then index by name or position\n## [1] \"Edison Vanderplas\"\ndog[[\"sex\"]]\n## NULL\n\ndog[[2]]\n## [1] 9\ndog[[7]] # if you use a position that isn't there you get an error\n## Error in dog[[7]]: subscript out of bounds\n\n# Getting a sub-list out (still as a list)\ndog[c(1, 3, 5)] # use [] to get a subset of the list\n## $name\n## [1] \"Edison Vanderplas\"\n## \n## $breed\n## [1] \"Jack Russell Terrorist\"\n## \n## $video\n## [1] \"https://youtu.be/zVeoQTOTIuQ\"\n\ndog[c(1, 3, 5)][[3]] # you can still use [[]] to get an item from this sub list\n## [1] \"https://youtu.be/zVeoQTOTIuQ\"\n\n\n\n\nmydf &lt;- data.frame(\n  name = c(\"Edison\", \"Wishbone\"), \n  age = c(9, 2), \n  sex = \"M\" # this will be repeated for all entries in the data frame\n)\n\nmydf\n##       name age sex\n## 1   Edison   9   M\n## 2 Wishbone   2   M\n\nmydf$name\n## [1] \"Edison\"   \"Wishbone\"\nmydf$age\n## [1] 9 2\nmydf$sex\n## [1] \"M\" \"M\"\n\nmydf[2, 3]\n## [1] \"M\"\n\nmydf[\"name\"] # this is a data frame!\n##       name\n## 1   Edison\n## 2 Wishbone\nmydf[[\"name\"]] # this is a vector\n## [1] \"Edison\"   \"Wishbone\"\n\nmydf[[\"name\"]][1] # this is a single entry (but still a vector of length 1)\n## [1] \"Edison\"\n\n\n\n\n\n\nPython lists and tuples\nPython dict\nNumpy vector\nNumpy ndarray\nPandas Series\nPandas DataFrame\n\n\n\n\nmylist = ['the', 'answer', 'to', 'life', 'is', 42]\n\nmylist[5]\n## 42\nmylist[0:5]\n## ['the', 'answer', 'to', 'life', 'is']\n\nList indexing and tuple indexing works the same way as vector indexing in python.\n\nmytuple = (3, 5, 'dog')\nmytuple[0]\n## 3\nmytuple[0:2] # can index with slices\n## (3, 5)\n\n\n\nPython dicts can be indexed by name directly and by location if you convert the keys or values to a list first.\n\ndog = {\"name\": \"Edison Vanderplas\", \n       \"age\": 9, \n       \"breed\": \"Jack Russell Terrorist\", \n       \"fav_toy\": \"stuffed duck\",\n       \"video\": \"https://youtu.be/zVeoQTOTIuQ\"}\n       \ndog[\"name\"]\n## 'Edison Vanderplas'\nlist(dog.keys()) # create a list from the keys of the dog dict\n## ['name', 'age', 'breed', 'fav_toy', 'video']\nlist(dog) # this does the same thing\n## ['name', 'age', 'breed', 'fav_toy', 'video']\nlist(dog)[0] # lists can then be indexed by location\n## 'name'\nlist(dog.values()) # create a list from the values of the dog dict\n## ['Edison Vanderplas', 9, 'Jack Russell Terrorist', 'stuffed duck', 'https://youtu.be/zVeoQTOTIuQ']\nlist(dog.values())[3]\n## 'stuffed duck'\n\n\n\nVectors in python can’t be named, so we can’t try to index them by name.\n\nimport numpy as np\n\nmyvec = np.array([3, 1, 4, 1, 5, 9])\n\nmyvec[0] # indexing in python starts at 0 instead of 1\n## 3\nmyvec[0:4] # select [a, b) elements in the vector\n## array([3, 1, 4, 1])\n\n\n\n\nimport numpy as np\n\n# generate a 3d array of random integers\nmyndarray = np.random.randint(12, size = (2, 2, 3))\nmyndarray\n## array([[[10,  6,  3],\n##         [ 5,  9,  0]],\n## \n##        [[ 0,  3,  2],\n##         [10,  1,  3]]])\nmyndarray[0, 0, 0]\n## 10\nmyndarray[0, :, 2] # use ':' to get all numbers in a \"slice\"\n## array([3, 0])\nmyndarray[:, :, 2]\n## array([[3, 0],\n##        [2, 3]])\nmyndarray[1, 1, 0:3] # a:b gets you an integer list between [a, b) (not including b)\n## array([10,  1,  3])\n\n\n\nPandas Series and Data Frames are the only types that can be indexed by name and location.\n\nimport pandas as pd\n\ndog_series = pd.Series(dog)\n\ndog_series['name']\n## 'Edison Vanderplas'\ndog_series['name':'age']\n## name    Edison Vanderplas\n## age                     9\n## dtype: object\ndog_series[1]\n## 9\ndog_series[0:3] # slicing works with location and name based indexing.\n## name          Edison Vanderplas\n## age                           9\n## breed    Jack Russell Terrorist\n## dtype: object\n\n\n\n\nimport pandas as pd\n\n\ndogs = {\n  \"name\": [\"Edison Vanderplas\", \"Tesla Vanderplas\"],\n  \"age\": [9, 14], \n  \"breed\": [\"Jack Russell Terrorist\", \"Lhasa Apso\"],\n  \"fav_toy\": [\"stuffed duck\", \"a human to pet me\"],\n  \"video\": [\"https://youtu.be/zVeoQTOTIuQ\", \"none\"]\n  }\ndog_df = pd.DataFrame(dogs, index = [\"Eddie\", \"Tez\"])\n\ndog_df.name # can use .varname notation sometimes, but not as robust\n## Eddie    Edison Vanderplas\n## Tez       Tesla Vanderplas\n## Name: name, dtype: object\ndog_df['name']\n## Eddie    Edison Vanderplas\n## Tez       Tesla Vanderplas\n## Name: name, dtype: object\ndog_df['name'][1] # access the series entry within\n\n# .iloc provides integer-based indexing\n## 'Tesla Vanderplas'\ndog_df.iloc[0, :]\n## name                  Edison Vanderplas\n## age                                   9\n## breed            Jack Russell Terrorist\n## fav_toy                    stuffed duck\n## video      https://youtu.be/zVeoQTOTIuQ\n## Name: Eddie, dtype: object\ndog_df.iloc[1, 2]\n## 'Lhasa Apso'\ndog_df.iloc[:, 2]\n\n# .loc provides name-based indexing for ROW labels\n## Eddie    Jack Russell Terrorist\n## Tez                  Lhasa Apso\n## Name: breed, dtype: object\ndog_df.loc[\"Tez\"] \n## name        Tesla Vanderplas\n## age                       14\n## breed             Lhasa Apso\n## fav_toy    a human to pet me\n## video                   none\n## Name: Tez, dtype: object\ndog_df.loc[\"Eddie\"] # this returns a Series\n## name                  Edison Vanderplas\n## age                                   9\n## breed            Jack Russell Terrorist\n## fav_toy                    stuffed duck\n## video      https://youtu.be/zVeoQTOTIuQ\n## Name: Eddie, dtype: object\ndog_df.loc[[\"Eddie\"]] # double brackets returns a DataFrame\n# note, this is basically the opposite of what [[ ]] does in R\n##                     name  age  ...       fav_toy                         video\n## Eddie  Edison Vanderplas    9  ...  stuffed duck  https://youtu.be/zVeoQTOTIuQ\n## \n## [1 rows x 5 columns]\ndog_df.loc[\"Eddie\", 'age':'fav_toy'] # can be used with slices for columns\n## age                             9\n## breed      Jack Russell Terrorist\n## fav_toy              stuffed duck\n## Name: Eddie, dtype: object\n\n\n\n\nThe 1, 2, and multi-dimensional homogeneous data types should be familiar from e.g. linear algebra and calculus. Single elements of a vector can be extracted using single square brackets, e.g. x[1] will get the first element of the vector x. In a matrix, elements are indexed as row, column, so to get the (2, 2) entry of a matrix x, you would use x[2,2]. This is extended for multi-dimensional arrays in both R and python, with each dimension added, e.g. x[3,1,2] or x[4, 3, 2, 1].\nTo get a full row or column from a matrix (in both SAS and R) you would use x[1,] (get the first row) or x[,3] (get the 3rd column). In python, you would use x[1,:], where : indicates that you want the whole row or column.\nTo select multiple rows or columns from a matrix, you would use x[, c(1, 3)] in R or x[,{1 3}] in SAS - both options get the first and third column of the matrix, with all rows of data included. In Python, you would use x.loc[:,[1,3]].\n\n\n\n\n\n\nImportant\n\n\n\nIn both R and SAS, a:b where a and b are numbers will form a sequence from a to b by 1s. So 1:4 is 1, 2, 3, 4. This is often used to get a set of rows or columns: x[3:4, 1:2]. In Python, a similar notation is used, but instead of counting from a to b when using a:b, Python will count from a to b-1.\nBoth R and SAS are 1-indexed languages, so the elements of a list or vector are indexed as 1, 2, 3, 4, …. Python is 0-indexed, like most general-purpose programming languages.2\n\n\nIn both R and Python, it is possible to index a vector using a logical vector of the same length. This is called logical indexing and is a very handy way to collect only the data you need for a task.\n\n\n\n\n\n\nTry it out: Logical Indexing\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\n(From project Euler)\nIf we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000.\nHint: The modulo operator, %%, gives the integer remainder of one number divided by another. So a %% b gives the integer remainder when dividing a by b. Modular division is often used to find multiples of a number.\n\n\n\n\nx &lt;- 1:999 # all nums below 1000\n\nm3 &lt;- (x %% 3) == 0 # multiple of 3\nm5 &lt;- (x %% 5) == 0 # multiple of 5\nm3or5 &lt;- m3 | m5\n\nsum(x[m3or5])\n## [1] 233168\n\n\n\n\nx = np.array(list(range(1, 1000)))\n\nm3 = (x % 3) == 0\nm5 = (x % 5) == 0\nm3or5 = m3|m5\n\nsum(x[m3or5])\n## 233168\n\n\n\n\n\n\nMost complicated structures in R (and python) are actually lists underneath. You should be able to access any of the pieces of a list using a combination of named references (where appropriate) and indexing.\n\n\n\n\n\n\nR Indexing Analogy\n\n\n\nIf you have trouble distinguishing between $, [, and [[, you’re not alone. [2] has an excellent illustration, which I will summarize for you here in abbreviated, tabular form (pictures directly lifted from the book).\n\n\n\n\n\n\n\n\nx\nx[1]\nx[[1]]\nx[[1]][[1]]"
  },
  {
    "objectID": "data-structures.html#references",
    "href": "data-structures.html#references",
    "title": "\n4  Data Structures\n",
    "section": "References",
    "text": "References\n\n\n\n\n[1] \nQuartz25, Jesdisciple, H. Röst, D. Ross, L. D’Oliveiro, and BLibrestez55, Python Programming. Wikibooks, 2016 [Online]. Available: https://en.wikibooks.org/wiki/Python_Programming. [Accessed: May 28, 2022]\n\n\n[2] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]"
  },
  {
    "objectID": "data-structures.html#footnotes",
    "href": "data-structures.html#footnotes",
    "title": "\n4  Data Structures\n",
    "section": "",
    "text": "In R, you can technically index vectors by name as well, but it’s not that common↩︎\nMost languages are 0-indexed languages: C, C++, python, Java, javascript. Vectors in these languages are indexed as 0, 1, 2, 3. Other 1-indexed languages include FORTRAN, Matlab, Julia, Mathematica, and Lua, many of which were intended for mathematical processing or data analysis.↩︎"
  },
  {
    "objectID": "control-structures.html#module-5-objectives",
    "href": "control-structures.html#module-5-objectives",
    "title": "\n5  Control Structures\n",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nUse foundational mathematical logic to write conditional statements\nCreate program flow diagrams for different control structures\nDetermine which iterative structure is more appropriate for a task\nIdentify the arguments, return statement, and body of a function\nWrite functions to perform simple tasks and use them appropriately"
  },
  {
    "objectID": "control-structures.html#mathematical-logic",
    "href": "control-structures.html#mathematical-logic",
    "title": "\n5  Control Structures\n",
    "section": "\n5.1 Mathematical Logic",
    "text": "5.1 Mathematical Logic\nBefore we start talking about data structures and control structures, though, we’re going to take a minute to review some concepts from mathematical logic. This will be useful for both data structures and control structures, so stick with me for a few minutes.\n\n5.1.1 And, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT, in Python, we use ~ for vector-wise negation (NOT).\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\n\nR\nPython\n\n\n\n\nx &lt;- c(TRUE, FALSE, TRUE, FALSE)\ny &lt;- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n## [1]  TRUE FALSE FALSE FALSE\nx | y # OR\n## [1]  TRUE  TRUE  TRUE FALSE\n!x & y # NOT X AND Y\n## [1] FALSE  TRUE FALSE FALSE\nx & !y # X AND NOT Y\n## [1] FALSE FALSE  TRUE FALSE\n\n\n\n\nimport numpy as np\nx = np.array([True, False, True, False])\ny = np.array([True, True, False, False])\n\nx & y\n## array([ True, False, False, False])\nx | y\n## array([ True,  True,  True, False])\n~x & y\n## array([False,  True, False, False])\nx & ~y\n## array([False, False,  True, False])\n\n\n\n\n\n5.1.2 De Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan’s First Law\nDeMorgan’s Second Law\n\n\n\n\n\nVenn Diagram of Set A and Set B\n\nSuppose that we set the convention that\n\n\nShaded regions are TRUE, unshaded regions are FALSE\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)\n\n\n\n\nDeMorgan’s laws become very useful when you are writing complex conditional statements. They’re basically the distributive rule of conditional statements and can help you simplify complex conditions."
  },
  {
    "objectID": "control-structures.html#conditional-if-statements",
    "href": "control-structures.html#conditional-if-statements",
    "title": "\n5  Control Structures\n",
    "section": "\n5.2 Conditional (If) Statements",
    "text": "5.2 Conditional (If) Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\nLet’s try this out:\n\n\nR\nPython\n\n\n\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) { \n  y &lt;- 8\n} else {\n  y &lt;- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x &gt; 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx &lt;- 3\ny &lt;- 1\n\nif (x &gt; 2) y &lt;- 8 else y &lt;- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x &gt; 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n## x = 3 ; y = 8\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x &gt; 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it “belongs”.\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python… but it’s a consequence of trying to make the code more readable.\n\n\n\n\n\n5.2.1 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\n\n\n\n\nTry it out!\n\n\n\n\n\nProblem\nR Solution\nPython Solution\nProgram Flow Chart\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n# Start with total income\nincome &lt;- 200000\n\n# x will hold income that hasn't been taxed yet\nx &lt;- income\n# y will hold taxes paid\ny &lt;- 0\n\nif (x &lt;= 10275) {\n  y &lt;- x*.1 # tax paid\n  x &lt;- 0 # All money has been taxed\n} else {\n  y &lt;- y + 10275 * .1\n  x &lt;- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x &lt;= (41775 - 10275)) {\n  y &lt;- y + x * .12\n  x &lt;- 0\n} else {\n  y &lt;- y + (41775 - 10275) * .12\n  x &lt;- x - (41775 - 10275) \n}\n\nif (x &lt;= (89075 - 41775)) {\n  y &lt;- y + x * .22\n  x &lt;- 0\n} else {\n  y &lt;- y + (89075 - 41775) * .22\n  x &lt;- x - (89075 - 41775)\n}\n\nif (x &lt;= (170050 - 89075)) {\n  y &lt;- y + x * .24\n  x &lt;- 0\n} else {\n  y &lt;- y + (170050 - 89075) * .24\n  x &lt;- x - (170050 - 89075)\n}\n\nif (x &lt;= (215950 - 170050)) {\n  y &lt;- y + x * .32\n  x &lt;- 0\n} else {\n  y &lt;- y + (215950 - 170050) * .32\n  x &lt;- x - (215950 - 170050)\n}\n\nif (x &lt;= (539900 - 215950)) {\n  y &lt;- y + x * .35\n  x &lt;- 0\n} else {\n  y &lt;- y + (539900 - 215950) * .35\n  x &lt;- x - (539900 - 215950)\n}\n\nif (x &gt; 0) {\n  y &lt;- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed &lt;= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed &lt;= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed &lt;= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed &lt;= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed &lt;= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed &lt;= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed &gt; 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n## Total Tauntaxed Rate on $ 200000  in income =  22.12 %\n\nWe will find a better way to represent this calculation once we discuss loops - we can store each bracket’s start and end point in a vector and loop through them. Any time you find yourself copy-pasting code and changing values, you should consider using a loop (or eventually a function) instead.\n\n\nLet’s explore using program flow maps for a slightly more complicated problem: The tax bracket example that we just used to try out if statement syntax.\n\n\n\nThe control flow diagram for the code in the previous example\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n\n\n\n5.2.2 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\n\n\n\n\n\nExample: Age Brackets\n\n\n\nFor instance, suppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\n\nProgram Flow Map\nR\nPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as &lt;18, 18-25, 26-40, 41-55, 56-65, and &gt;65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\nThe important thing to realize when examining this program flow map is that if age &lt;= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age &lt;= 18), (age &lt;= 25), and (age &lt;= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - &lt;40 is evaluated first, and so &lt;= 25 and &lt;= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage &lt;- 40 # change this as you will to see how the code works\n\nif (age &lt; 18) {\n  bracket &lt;- \"&lt;18\"\n} else if (age &lt;= 25) {\n  bracket &lt;- \"18-25\"\n} else if (age &lt;= 40) {\n  bracket &lt;- \"26-40\"\n} else if (age &lt;= 55) {\n  bracket &lt;- \"41-55\" \n} else if (age &lt;= 65) {\n  bracket &lt;- \"56-65\"\n} else {\n  bracket &lt;- \"&gt;65\"\n}\n\nbracket\n## [1] \"26-40\"\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don’t indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age &lt; 18:\n  bracket = \"&lt;18\"\nelif age &lt;= 25:\n  bracket = \"18-25\"\nelif age &lt;= 40:\n  bracket = \"26-40\"\nelif age &lt;= 55:\n  bracket = \"41-55\"\nelif age &lt;= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \"&gt;65\"\n  \nbracket\n## '26-40'"
  },
  {
    "objectID": "control-structures.html#loops",
    "href": "control-structures.html#loops",
    "title": "\n5  Control Structures\n",
    "section": "\n5.3 Loops",
    "text": "5.3 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n5.3.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true.\nThe simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\nFlow map showing while-loop pseudocode (while x &lt;= N) { # code that changes x in some way} and the program flow map expansion where we check if x &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\nR\nPython\n\n\n\n\nx &lt;- 0\n\nwhile (x &lt; 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x &lt;- x + 1\n}\n## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n\n\n\n\nx = 0\n\nwhile x &lt; 10:\n  print(x)\n  x = x + 1\n## 0\n## 1\n## 2\n## 3\n## 4\n## 5\n## 6\n## 7\n## 8\n## 9\n\n\n\n\n\n\n\n\n\n\nWhile Loops: Try it Out!\n\n\n\n\n\nProblem\nMath Notation\nR Solution\nPython solution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk &lt;- 1\nprod &lt;- 1\nans &lt;- (exp(pi) - exp(-pi))/(2*pi)\ndelta &lt;- 0.0001\n\nwhile (abs(prod - ans) &gt;= 0.0001) {\n  prod &lt;- prod * (1 + 1/k^2)\n  k &lt;- k + 1\n}\n\nk\n## [1] 36761\nprod\n## [1] 3.675978\nans\n## [1] 3.676078\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) &gt;= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k &gt; 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n## At  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n5.3.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j &gt; N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\n\n\n\nfor i in range(5):\n  print(i)\n## 0\n## 1\n## 2\n## 3\n## 4\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is run for every value of a vector (which is why loops are included in the same chapter as vectors).\n\n\n\n\n\n\nExample: For Loops\n\n\n\n\n\nR\nPython\n\n\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n## [1] \"January\"\n## [1] \"February\"\n## [1] \"March\"\n## [1] \"April\"\n## [1] \"May\"\n## [1] \"June\"\n## [1] \"July\"\n## [1] \"August\"\n## [1] \"September\"\n## [1] \"October\"\n## [1] \"November\"\n## [1] \"December\"\n\n\n\nIn python, we have to define our vector or list to start out with, but that’s easy enough:\n\nfuturama_crew = ['Fry', 'Leela', 'Bender', 'Amy', 'the Professor', 'Hermes', 'Zoidberg', 'Nibbler']\nfor i in futurama_crew:\n  print(i)\n## Fry\n## Leela\n## Bender\n## Amy\n## the Professor\n## Hermes\n## Zoidberg\n## Nibbler\n\n\n\n\n\n\n\n\n\n\n\n\nAvoiding Infinite Loops\n\n\n\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\n\nR\nPython\n\n\n\nThis while loop runs until either x &lt; 10 or n &gt; 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n&gt;50 check to the loop so that we don’t tie up the computer for eternity.\n\nx &lt;- 0\nn &lt;- 0 # count the number of times the loop runs\n\nwhile (x &lt; 10) { \n  print(x)\n  x &lt;- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n &lt;- n + 1\n  if (n &gt; 50) \n    break # this stops the loop if n &gt; 50\n}\n## [1] 0\n## [1] -0.7745874\n## [1] -1.535922\n## [1] -1.993021\n## [1] -3.32165\n## [1] -3.474433\n## [1] -3.643159\n## [1] -3.701863\n## [1] -5.011182\n## [1] -5.373117\n## [1] -5.192366\n## [1] -5.255273\n## [1] -5.446177\n## [1] -4.780911\n## [1] -2.821758\n## [1] -3.584013\n## [1] -3.240015\n## [1] -3.044352\n## [1] -1.855305\n## [1] -2.772381\n## [1] -1.596237\n## [1] -3.35068\n## [1] -2.804475\n## [1] -2.157194\n## [1] -2.68864\n## [1] -3.790945\n## [1] -4.159149\n## [1] -5.241374\n## [1] -4.447307\n## [1] -4.358774\n## [1] -6.367994\n## [1] -7.572604\n## [1] -7.862042\n## [1] -7.411637\n## [1] -8.516083\n## [1] -7.166801\n## [1] -6.294134\n## [1] -7.523757\n## [1] -8.125321\n## [1] -7.558365\n## [1] -6.263312\n## [1] -6.854931\n## [1] -5.240731\n## [1] -3.635126\n## [1] -4.441197\n## [1] -5.090125\n## [1] -5.610236\n## [1] -5.198105\n## [1] -6.071903\n## [1] -5.714642\n## [1] -7.026634\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x &lt; 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n &gt; 50:\n    break # this stops the loop if n &gt; 50\n## 0\n## [0.3166942]\n## [-0.75770066]\n## [0.60278608]\n## [-0.45888973]\n## [-1.29756657]\n## [-2.01140893]\n## [-3.68048599]\n## [-2.39997161]\n## [-3.4965905]\n## [-4.70053513]\n## [-4.1975302]\n## [-4.36264791]\n## [-2.55277547]\n## [-4.15077682]\n## [-4.74717417]\n## [-3.38929829]\n## [-4.10482674]\n## [-3.85361605]\n## [-2.17836759]\n## [-1.08360137]\n## [-2.0859616]\n## [-2.23402255]\n## [-4.07983682]\n## [-2.87343183]\n## [-3.96333104]\n## [-4.63766452]\n## [-3.86024952]\n## [-3.49001955]\n## [-4.08125286]\n## [-4.40708664]\n## [-3.01207288]\n## [-5.13422253]\n## [-4.42003627]\n## [-4.57304572]\n## [-5.87336508]\n## [-6.12089297]\n## [-6.51479922]\n## [-6.44478546]\n## [-6.24415652]\n## [-5.94114308]\n## [-4.84206464]\n## [-4.18970003]\n## [-4.92344889]\n## [-3.88350879]\n## [-4.97072704]\n## [-5.32757183]\n## [-5.62869927]\n## [-4.80464804]\n## [-5.07231628]\n## [-6.02080227]\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\n\n5.3.3 Controlling Loops\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n\nBreak Statement\nNext/Continue Statement\n\n\n\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\n\n\n\n\nExample: Next/Continue statements\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\n\nR\nPython\n\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n## [1] 1\n## [1] 2\n## [1] \"Divisible by 3\"\n## [1] 4\n## [1] \"Divisible by 5\"\n## [1] \"Divisible by 3\"\n## [1] 7\n## [1] 8\n## [1] \"Divisible by 3\"\n## [1] \"Divisible by 5\"\n## [1] 11\n## [1] \"Divisible by 3\"\n## [1] 13\n## [1] 14\n## [1] \"Exiting now\"\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n## 1\n## 2\n## Divisible by 3\n## 4\n## Divisible by 5\n## Divisible by 3\n## 7\n## 8\n## Divisible by 3\n## Divisible by 5\n## 11\n## Divisible by 3\n## 13\n## 14\n## Exiting now\n\n\n\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "control-structures.html#functions",
    "href": "control-structures.html#functions",
    "title": "\n5  Control Structures\n",
    "section": "\n5.4 Functions",
    "text": "5.4 Functions\nA function is a set of actions that we group together and name. Throughout this course, you’ve used a bunch of different functions in R and python that are built into the language or added through packages: mean, ggplot, length, print. In this chapter, we’ll be writing our own functions.\n\n5.4.1 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more about functions\n\n\n\nThere is some extensive material on this subject in R for Data Science [1] on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\n\n\n\n\n\n\nExample: Turning Code into Functions\n\n\n\nThis example is modified from R for Data Science [2, Ch. 19].\nWhat does this code do? Does it work as intended?\n\n\nR\nPython\n\n\n\n\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has\n\n\nR\nPython\n\n\n\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we start by rewriting it using general names\n\n\nR\nPython\n\n\n\nIn this case, it might help to replace df$a with x.\n\nx &lt;- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n##  [1] 0.8141778 0.4314973 1.0000000 0.5247025 0.5596544 0.3667815 0.2707588\n##  [8] 0.0000000 0.5295683 0.2324731\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n## 0    0.000000\n## 1    0.922919\n## 2    0.592004\n## 3    0.336827\n## 4    1.000000\n## 5    0.402345\n## 6    0.723887\n## 7    0.346991\n## 8    0.892496\n## 9    0.490739\n## Name: a, dtype: float64\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\n\nR\nPython\n\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng &lt;- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n##  [1] 0.8141778 0.4314973 1.0000000 0.5247025 0.5596544 0.3667815 0.2707588\n##  [8] 0.0000000 0.5295683 0.2324731\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n## 0    0.000000\n## 1    0.922919\n## 2    0.592004\n## 3    0.336827\n## 4    1.000000\n## 5    0.402345\n## 6    0.723887\n## 7    0.346991\n## 8    0.892496\n## 9    0.490739\n## Name: a, dtype: float64\n\n\n\n\nFinally, we turn this code into a function\n\n\nR\nPython\n\n\n\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n##  [1] 0.8141778 0.4314973 1.0000000 0.5247025 0.5596544 0.3667815 0.2707588\n##  [8] 0.0000000 0.5295683 0.2324731\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n## 0    0.000000\n## 1    0.922919\n## 2    0.592004\n## 3    0.336827\n## 4    1.000000\n## 5    0.402345\n## 6    0.723887\n## 7    0.346991\n## 8    0.892496\n## 9    0.490739\n## Name: a, dtype: float64\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function.\n\n\n\n5.4.2 Syntax\n\n\nR and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\nIn R, functions are defined as other variables, using &lt;-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return &lt;value&gt;, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don’t return anything; in R, this is a bit less common, for reasons we won’t get into here.\n\n5.4.3 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\n\n\n\n\n\nExample: Parts of a Function\n\n\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a puppy’s name and returns “ is a good pup!”.\n\n\nR\nPython\n\n\n\n\ndog &lt;- \"Eddie\"\n\ngoodpup &lt;- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n## [1] \"Eddie is a good pup!\"\n\nIn this example R function, when we call goodpup(dog), dog is the argument. name is the parameter.\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n## 'Eddie is a good pup!'\n\nIn this example python function, when we call goodpup(dog), dog is the argument. name is the parameter.\n\n\n\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\n\n\n\n\n\nTry it out: Function Parts\n\n\n\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be. Also determine whether the function output is stored in memory or just printed to the command line.\n\n\nFunction 1\nAnswer\n\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\n\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n## Hello World\n\n\nFunction output is not stored in memory and is printed to the command line.\n\n\n\n\n\n\nFunction 2\nAnswer\n\n\n\n\n\nmy_mean &lt;- function(x) {\n  censor_x &lt;- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n## [1] 6\n\n\nFunction output is saved to memory (x) and printed to the command line\n\n\n\n\n\n\n\n5.4.4 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\n\nR\nPython\n\n\n\n\n\ndivide &lt;- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\n\nR\nPython\n\n\n\n\ndivide(3, 6)\n## [1] 0.5\n\ndivide(x = 3, y = 6)\n## [1] 0.5\n\ndivide(y = 6, x = 3)\n## [1] 0.5\n\ndivide(6, 3)\n## [1] 2\n\ndivide(x = 6, y = 3)\n## [1] 2\n\ndivide(y = 3, x = 6)\n## [1] 2\n\n\n\n\ndivide(3, 6)\n## 0.5\ndivide(x = 3, y = 6)\n## 0.5\ndivide(y = 6, x = 3)\n## 0.5\ndivide(6, 3)\n## 2.0\ndivide(x = 6, y = 3)\n## 2.0\ndivide(y = 3, x = 6)\n## 2.0\n\n\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters.\n\n5.4.5 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\n\n\n\n\n\nInput Validation Example\n\n\n\n\n\nR\nPython\n\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd &lt;- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in x + y: non-numeric argument to binary operator\n\nadd &lt;- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in add(\"tmp\", 3): is.numeric(x) is not TRUE\nadd(3, 4)\n## [1] 7\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we’re good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n## x and y must be add-able\nadd(3, 4)\n## 7\n\nYou can read more about error handling in Python here\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused [3].\n\n5.4.6 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\nName Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\n\n\n\n\n\nCaution\n\n\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\na &lt;- 10\n\nmyfun &lt;- function() {\n  a &lt;- 20\n  a\n}\n\nmyfun()\n## [1] 20\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n## 20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\nEnvironments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\nmyfun &lt;- function() {\n  if (!exists(\"aa\")) {\n    aa &lt;- 1\n  } else {\n    aa &lt;- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n## [1] 1\nmyfun()\n## [1] 1\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n## 1\nmyfun()\n## 1\n\nNote that the try command here is used to handle the case where a doesn’t exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used [4], Ch 17.\n\n\n\n\n\nDynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\n\n\n\nCaution\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\nmyfun &lt;- function() {\n  x + 1\n}\n\nx &lt;- 14\n\nmyfun()\n## [1] 15\n\nx &lt;- 20\n\nmyfun()\n## [1] 21\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n## 15\nx = 20\n\nmyfun()\n## 21\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Function Scope\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. (Taken from [2, Ch. 6])\n\n\nR code\nR solution\nPython code\nPython solution\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf &lt;- function(x) {\n  f &lt;- function(x) {\n    f &lt;- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n## [1] 202\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ^ 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n## 202"
  },
  {
    "objectID": "control-structures.html#references",
    "href": "control-structures.html#references",
    "title": "\n5  Control Structures\n",
    "section": "\n5.5 References",
    "text": "5.5 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[3] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[4] \nA. Martelli and D. Ascher, Python Cookbook. O’Reilly Media, 2002 [Online]. Available: https://learning.oreilly.com/library/view/python-cookbook/0596001673/ch05s24.html. [Accessed: May 31, 2022]"
  },
  {
    "objectID": "control-structures.html#footnotes",
    "href": "control-structures.html#footnotes",
    "title": "\n5  Control Structures\n",
    "section": "",
    "text": "This is not strictly true, you can of course use pass-by-reference, but we will not be covering that in this class as we are strictly dealing with the bare minimum of learning how to write a function here.↩︎"
  },
  {
    "objectID": "data-programming.html#module6-objectives",
    "href": "data-programming.html#module6-objectives",
    "title": "6  Programming with Data",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nWrite basic functions and procedures to create simple plots and data summaries\nApply syntax knowledge to reference variables and observations in common data structures\nCreate new variables and columns or reformat existing columns in provided data structures"
  },
  {
    "objectID": "data-programming.html#introduction",
    "href": "data-programming.html#introduction",
    "title": "6  Programming with Data",
    "section": "Introduction",
    "text": "Introduction\nAt this point, you’ve learned how to write functions. You know the basics of how to create new variables, how data frames and lists work, and how to use markdown.\nAnd yet… these are skills that take some practice when applied to new data. We’re going to take a break from the fire-hose of syntax you’ve learned and focus on applying what you’ve learned to problems related to data. The goal is to reinforce the skills you’ve already learned and help you find your feet a bit as you work through data analysis. I’ll provide sample code for tasks like basic plots and tables that we haven’t covered yet - you should feel free to modify and tinker with these chunks as you go along. This chapter will also provide a preview of some of the packages we’re going to work with in the next few sections (because I’m going to show you some code for e.g. summarizing a dataset and plot a few things, even without having covered that material).\n\n\nIt’s 100% expected that you would be oscillating between just maybe understanding something and feeling completely lost again during this chapter. Hopefully, that feeling will get better over the next few weeks… but for now, just stick with it.\n\nAs you’ve probably guessed by now, this week’s reading will primarily be focused on examples."
  },
  {
    "objectID": "data-programming.html#example-artwork-dimensions",
    "href": "data-programming.html#example-artwork-dimensions",
    "title": "6  Programming with Data",
    "section": "\n6.1 Example: Artwork Dimensions",
    "text": "6.1 Example: Artwork Dimensions\nThe Tate Art Museum assembled a collection of 70,000 artworks (last updated in 2014). They cataloged information including accession number, artwork dimensions, units, title, date, medium, inscription, and even URLs for images of the art.\n\n6.1.1 Reading in the Data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\nartwork &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\nimport pandas as pd\nartwork = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-12/artwork.csv')\n\n\n\n\n\n6.1.2 Visual Summaries\nWhen you first access a new dataset, it’s fun to explore it a bit. I’ve shown a summary of the variables (character variables summarized with completion rates and # unique values, numeric variables summarized with quantiles and mean/sd) generated using the R skimr and Python skimpy packages (which we’ll talk about in the next chapter).\n\n\nR\nPython (pandas)\nPython (skimpy)\n\n\n\nYou may need to run install.packages(\"skimr\") in the R terminal if you have not used the package before.\n\nlibrary(skimr)\nskim(artwork)\n\n\nData summary\n\n\nName\nartwork\n\n\nNumber of rows\n69201\n\n\nNumber of columns\n20\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n12\n\n\nlogical\n1\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\naccession_number\n0\n1.00\n6\n7\n0\n69201\n0\n\n\nartist\n0\n1.00\n4\n120\n0\n3336\n0\n\n\nartistRole\n0\n1.00\n5\n24\n0\n19\n0\n\n\ntitle\n0\n1.00\n1\n320\n0\n43529\n0\n\n\ndateText\n0\n1.00\n4\n75\n0\n2736\n0\n\n\nmedium\n6384\n0.91\n3\n120\n0\n3401\n0\n\n\ncreditLine\n3\n1.00\n14\n820\n0\n3209\n0\n\n\ndimensions\n2433\n0.96\n4\n248\n0\n25575\n0\n\n\nunits\n3341\n0.95\n2\n2\n0\n1\n0\n\n\ninscription\n62895\n0.09\n14\n14\n0\n1\n0\n\n\nthumbnailUrl\n10786\n0.84\n55\n57\n0\n58415\n0\n\n\nurl\n0\n1.00\n48\n134\n0\n69201\n0\n\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nthumbnailCopyright\n69201\n0\nNaN\n:\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nid\n0\n1.00\n39148.03\n25980.47\n3\n19096.00\n37339\n54712\n129068\n▇▇▅▁▁\n\n\nartistId\n0\n1.00\n1201.06\n2019.42\n0\n558.00\n558\n1137\n19232\n▇▁▁▁▁\n\n\nyear\n5397\n0.92\n1867.23\n72.01\n1545\n1817.00\n1831\n1953\n2012\n▁▁▇▆▆\n\n\nacquisitionYear\n45\n1.00\n1910.65\n64.20\n1823\n1856.00\n1856\n1982\n2013\n▇▁▁▁▅\n\n\nwidth\n3367\n0.95\n323.47\n408.81\n3\n118.00\n175\n345\n11960\n▇▁▁▁▁\n\n\nheight\n3342\n0.95\n346.44\n538.04\n6\n117.00\n190\n359\n37500\n▇▁▁▁▁\n\n\ndepth\n66687\n0.04\n479.20\n1051.14\n1\n48.25\n190\n450\n18290\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n# Base pandas\nartwork.describe()\n##                   id      artistId  ...         depth  thumbnailCopyright\n## count   69201.000000  69201.000000  ...   2514.000000                 0.0\n## mean    39148.026213   1201.063251  ...    479.197772                 NaN\n## std     25980.468687   2019.422535  ...   1051.141734                 NaN\n## min         3.000000      0.000000  ...      1.000000                 NaN\n## 25%     19096.000000    558.000000  ...     48.250000                 NaN\n## 50%     37339.000000    558.000000  ...    190.000000                 NaN\n## 75%     54712.000000   1137.000000  ...    450.000000                 NaN\n## max    129068.000000  19232.000000  ...  18290.000000                 NaN\n## \n## [8 rows x 8 columns]\n\n\n\nYou may need to run pip install skimpy in the terminal if you have not used the package before.\n\n\n# Skimpy package - like skimr\nfrom skimpy import skim\nskim(artwork)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 69201  │ │ object      │ 12    │                       │\n## │ │ Number of columns │ 20     │ │ float64     │ 6     │                       │\n## │ └───────────────────┴────────┘ │ int64       │ 2     │                       │\n## │                                └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┓  │\n## │ ┃      ┃ missi ┃ comp ┃ mean  ┃ sd   ┃ p0   ┃ p25   ┃ p75  ┃ p100  ┃ hist ┃  │\n## │ ┃      ┃ ng    ┃ lete ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┃      ┃       ┃      ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┃      ┃       ┃ rate ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┡━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━┩  │\n## │ │ id   │     0 │    1 │ 39000 │ 2600 │    3 │ 19000 │ 5500 │ 13000 │ ▇██▁ │  │\n## │ │      │       │      │       │    0 │      │       │    0 │     0 │  ▁▁  │  │\n## │ │ arti │     0 │    1 │  1200 │ 2000 │    0 │   560 │ 1100 │ 19000 │  █   │  │\n## │ │ stId │       │      │       │      │      │       │      │       │      │  │\n## │ │ year │  5400 │ 0.92 │  1900 │   72 │ 1500 │  1800 │ 2000 │  2000 │    █ │  │\n## │ │      │       │      │       │      │      │       │      │       │  ▁▃  │  │\n## │ │ acqu │    45 │    1 │  1900 │   64 │ 1800 │  1900 │ 2000 │  2000 │   █  │  │\n## │ │ isit │       │      │       │      │      │       │      │       │ ▁▂▄  │  │\n## │ │ ionY │       │      │       │      │      │       │      │       │      │  │\n## │ │ ear  │       │      │       │      │      │       │      │       │      │  │\n## │ │ widt │  3400 │ 0.95 │   320 │  410 │    3 │   120 │  340 │ 12000 │  █   │  │\n## │ │ h    │       │      │       │      │      │       │      │       │      │  │\n## │ │ heig │  3300 │ 0.95 │   350 │  540 │    6 │   120 │  360 │ 38000 │  █   │  │\n## │ │ ht   │       │      │       │      │      │       │      │       │      │  │\n## │ │ dept │ 67000 │ 0.03 │   480 │ 1100 │    1 │    48 │  450 │ 18000 │  █   │  │\n## │ │ h    │       │    6 │       │      │      │       │      │       │      │  │\n## │ │ thum │ 69000 │    0 │   nan │  nan │  nan │   nan │  nan │   nan │      │  │\n## │ │ bnai │       │      │       │      │      │       │      │       │      │  │\n## │ │ lCop │       │      │       │      │      │       │      │       │      │  │\n## │ │ yrig │       │      │       │      │      │       │      │       │      │  │\n## │ │ ht   │       │      │       │      │      │       │      │       │      │  │\n## │ └──────┴───────┴──────┴───────┴──────┴──────┴───────┴──────┴───────┴──────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/numpy/lib/histograms.py:906: RuntimeWarning: invalid value encountered in true_divide\n##   return n/db/n.sum(), bin_edges\n\n\n\n\n\n6.1.3 Accessing one column\nFirst, let’s pull out the year for each piece of artwork in the dataset and see what we can do with it…\n\n\nR\nPython\n\n\n\n\nhead(artwork$year)\n## [1]   NA   NA 1785   NA 1826 1826\n\nWe reference a column of the dataset by name using dataset_name$column_name, and since our data is stored in artwork, and we want the column named year, we use artwork$year to get access to the data we want.\n\n\n\nartwork.year.head()\n## 0       NaN\n## 1       NaN\n## 2    1785.0\n## 3       NaN\n## 4    1826.0\n## Name: year, dtype: float64\n\nWe reference a column of the dataset by name using dataset_name.column_name or dataset_name['column_name'], and since our data is stored in artwork and we want the column year, we use artwork.year or artwork['year'] to access the data we want.\n\n\n\nI’ve used the head command to show only the first few values (so that the output isn’t overwhelming).\n\n6.1.4 Variable Summary\nWhen we have output like this, it is useful to summarize the output in some way:\n\n\nR\nPython\n\n\n\n\nsummary(artwork$year)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    1545    1817    1831    1867    1953    2012    5397\n\nThat’s much less output, but we might want to instead make a chart:\n\nhist(artwork$year, breaks = 30)\n\n\n\n\n\n\n\nartwork.year.describe()\n## count    63804.000000\n## mean      1867.227823\n## std         72.012718\n## min       1545.000000\n## 25%       1817.000000\n## 50%       1831.000000\n## 75%       1953.000000\n## max       2012.000000\n## Name: year, dtype: float64\n\nThe df.describe() command provides us with a 5-number summary and then some additional statistics.\nWe can also create a chart:\n\nartwork.year.hist(bins = 30)\n\n\n\n\n\n\n\nPersonally, I much prefer the graphical version. It’s informative (though it does leave out NA values) and shows that there are pieces going back to the 1500s, but that most pieces were made in the early 1800s or late 1900s.\n\n6.1.5 Create a Histogram (base graphics/matplotlib)\nWe might be interested in the aspect ratio of the artwork - let’s take a look at the input variables and define new variables related to aspect ratio(s).\n\nPython\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=1, ncols=3) # 3 subplots\n\nartwork.width.hist(bins = 30, ax = axes[0])\nartwork.depth.hist(bins = 30, ax = axes[1])\nartwork.height.hist(bins = 30, ax= axes[2])\n\n# Set subplot titles\naxes[0].title.set_text(\"width\")\naxes[1].title.set_text(\"depth\")\naxes[2].title.set_text(\"height\")\n\nplt.show()\n\n\n\n\n\n\n\nSo all of our variables are skewed quite a bit, and we know from the existence of the units column that they may not be in the same unit, either.\n\n6.1.6 Summary Tables\nLet’s make a table of the units column so that we can see what the frequency of various units are in the dataset.\n\n\nR\nPython\n\n\n\n\ntable(artwork$units, useNA = 'ifany')\n## \n##    mm  &lt;NA&gt; \n## 65860  3341\n\n\n\n\nartwork.units.value_counts(dropna=False)\n## mm     65860\n## NaN     3341\n## Name: units, dtype: int64\n\n\n\n\nEverything that has specified units is in mm. That makes things easier.\n\n6.1.7 Defining a new variable\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\naspect_hw &lt;- artwork$height/artwork$width\npar(mfrow = c(1, 2))\nhist(aspect_hw, breaks = 30)\nhist(log(aspect_hw), breaks = 30)\n\n\n\n\n\n\n\nimport numpy as np\n\nfig, axes = plt.subplots(nrows=1, ncols=2) # 2 subplots\n\naspect_hw = artwork.height/artwork.width\naspect_hw.hist(bins = 30, ax = axes[0])\nnp.log(aspect_hw).hist(bins = 30, ax = axes[1])\n\n\n\n\n\n\n\nMost things are pretty square-ish, but there are obviously quite a few exceptions in both directions.\nThe one problem with how we’ve done this is that we now have a data frame with all of our data in it, and a separate variable aspect_hw, that is not attached to our data frame. That’s not ideal - it’s easy to lose track of the variable, it’s easy to accidentally “sort” the variable so that the row order isn’t the same as in the original data frame… there are all sorts of potential issues.\n\n6.1.8 Adding a new column\nThe better way to define a new variable is to add a new column to the data frame:\n\n\nR\nPython\n\n\n\nTo define a new variable that exists on its own, we might do something like this:\n\nartwork$aspect_hw &lt;- artwork$height/artwork$width\n\n\n\n\nartwork['aspect_hw'] = artwork.height/artwork.width\n\nNote that when you create a new column in a pandas dataframe, you have to use df['colname'] on the left hand side, even if you use df.colname syntax on the right hand side.\n\n\n\n(We’ll learn a shorter way to do this later, but this is functional, if not pretty, for now).\nThe downside to this is that we have to write out artwork$aspect_hw or artwork.aspect_hw each time we want to reference the variable. That is a pain, but one that’s relatively temporary (we’ll get to a better way to do this in a couple of weeks). A little bit of extra typing is definitely worth it if you don’t lose data you want to keep.\n\n\n\n\n\n\nAssign your calculations to a variable or column!\n\n\n\nOne mistake I see people make frequently is to calculate height/width, but then not assign that value to a variable.\nIf you’re not using &lt;- in R1 or = in Python, then you’re not saving that information to be referenced later - you’re just calculating values temporarily and possibly printing them as output.\n\n\n\n6.1.9 Conclusions\nIt’s important to keep track of where you’re putting the pieces you create during an analysis - just as important as keeping track of the different sub-components when you’re putting a lego set together or making a complex recipe in the kitchen. Forgetting to assign your calculation to a variable is like dumping your glaze down the sink or throwing that small lego component into the trash."
  },
  {
    "objectID": "data-programming.html#example-2-dogs-of-nyc",
    "href": "data-programming.html#example-2-dogs-of-nyc",
    "title": "6  Programming with Data",
    "section": "\n6.2 Example 2: Dogs of NYC",
    "text": "6.2 Example 2: Dogs of NYC\nNew York City provides a whole host of open-data resources, including a dataset of dogs licensed in the city on an annual basis (link is to the NYC Open Data Page).\nCSV link (this data is ~23 MB)\n\n6.2.1 Read in data\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\n\nif (!file.exists(\"data/NYC_dogs.csv\")) {\n  # if the file doesn't exist, download it!\n  download.file(\n    \"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\", # url for download\n    destfile = \"data/NYC_dogs.csv\", # location to store the file\n    mode = \"wb\" # need this to get downloads to work on windows\n  )\n}\n\ndogs &lt;- read_csv(\"data/NYC_dogs.csv\")\nhead(dogs)\n## # A tibble: 6 × 8\n##   AnimalName AnimalGender AnimalBirthY…¹ Breed…² ZipCode Licen…³ Licen…⁴ Extra…⁵\n##   &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n## 1 PAIGE      F                      2014 Americ…   10035 09/12/… 09/12/…    2016\n## 2 YOGI       M                      2010 Boxer     10465 09/12/… 10/02/…    2016\n## 3 ALI        M                      2014 Basenji   10013 09/12/… 09/12/…    2016\n## 4 QUEEN      F                      2013 Akita …   10013 09/12/… 09/12/…    2016\n## 5 LOLA       F                      2009 Maltese   10028 09/12/… 10/09/…    2016\n## 6 IAN        M                      2006 Unknown   10013 09/12/… 10/30/…    2016\n## # … with abbreviated variable names ¹​AnimalBirthYear, ²​BreedName,\n## #   ³​LicenseIssuedDate, ⁴​LicenseExpiredDate, ⁵​`Extract Year`\n\n\n\n\nfrom os.path import exists # to test whether files exist\nimport requests # to download a file\n\nif ~exists(\"data/NYC_dogs.csv\"):\n  response = requests.get(\"https://data.cityofnewyork.us/api/views/nu7n-tubp/rows.csv?accessType=DOWNLOAD\")\n  open(\"data/NYC_dogs.csv\", \"wb\").write(response.content)\n## 31849901\ndogs = pd.read_csv(\"data/NYC_dogs.csv\")\ndogs.head()\n##   AnimalName AnimalGender  ...  LicenseExpiredDate Extract Year\n## 0      PAIGE            F  ...          09/12/2017         2016\n## 1       YOGI            M  ...          10/02/2017         2016\n## 2        ALI            M  ...          09/12/2019         2016\n## 3      QUEEN            F  ...          09/12/2017         2016\n## 4       LOLA            F  ...          10/09/2017         2016\n## \n## [5 rows x 8 columns]\n\n\n\n\n\n6.2.2 Work with Dates\nOne thing we might want to do first is to transform the license dates (LicenseIssuedDate, LicenseExpiredDate) into actual dates instead of characters.\n\n\nR\nPython\n\n\n\nWe will use the lubridate package to do this, because it is designed to make working with dates and times very easy.\nYou may need to run install.packages(\"lubridate\") in the R console if you have not used the package before.\n\nlibrary(lubridate)\nhead(dogs$LicenseExpiredDate) # Dates are in month-day-year format\n## [1] \"09/12/2017\" \"10/02/2017\" \"09/12/2019\" \"09/12/2017\" \"10/09/2017\"\n## [6] \"10/30/2019\"\n\ndogs$LicenseExpiredDate &lt;- mdy(dogs$LicenseExpiredDate)\ndogs$LicenseIssuedDate &lt;- mdy(dogs$LicenseIssuedDate)\n\nhead(dogs$LicenseExpiredDate)\n## [1] \"2017-09-12\" \"2017-10-02\" \"2019-09-12\" \"2017-09-12\" \"2017-10-09\"\n## [6] \"2019-10-30\"\n\n\n\nYou may need to run pip install datetime in the terminal if you have not used the package before.\n\nfrom datetime import date\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # Before\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         09/12/2017        09/12/2014\n## 1         10/02/2017        09/12/2014\n## 2         09/12/2019        09/12/2014\n## 3         09/12/2017        09/12/2014\n## 4         10/09/2017        09/12/2014\nformat_str = \"%m/%d/%Y\" # date format in the dataset\n\ndogs['LicenseExpiredDate'] = pd.to_datetime(dogs.LicenseExpiredDate, format = format_str)\ndogs['LicenseIssuedDate'] = pd.to_datetime(dogs.LicenseIssuedDate, format = format_str)\n\ndogs[['LicenseExpiredDate','LicenseIssuedDate']].head() # After\n##   LicenseExpiredDate LicenseIssuedDate\n## 0         2017-09-12        2014-09-12\n## 1         2017-10-02        2014-09-12\n## 2         2019-09-12        2014-09-12\n## 3         2017-09-12        2014-09-12\n## 4         2017-10-09        2014-09-12\n\n\n\n\nIt might be interesting to see when licenses have been issued over time, so let’s make a histogram. This time, I’m going to use ggplot graphics with the ggplot2 package in R and the plotnine package in python (which is the python version of the R package).\n\n6.2.3 Create a Histogram (ggplot2/plotnine)\n\n\nR\nPython\n\n\n\nYou may need to run install.packages(\"ggplot2\") in the R console if you have not used ggplot2 before.\n\nlibrary(ggplot2)\n\nggplot(\n  data = dogs, \n  aes(x = LicenseIssuedDate) # Specify we want LicenseIssueDate on the x-axis\n) + \n  geom_histogram() # Create a histogram\n\n\n\n\n\n\nYou may need to run pip install plotnine in the terminal if you have not used the package before.\n\nfrom plotnine import *\n\n(\n  ggplot(aes(x = 'LicenseIssuedDate'), data = dogs) + \n  geom_histogram() # Create a histogram\n)\n## &lt;ggplot: (8741671606544)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 88'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\nThere is an interesting periodicity to the license issue dates.\n\n6.2.4 Compute License Length\nI’m also curious about how long a license tends to be held for - we can get this information by subtracting the issue date from the expiration date.\n\n\nR\nPython\n\n\n\n\ndogs$LicenseLength &lt;- dogs$LicenseExpiredDate - dogs$LicenseIssuedDate\nsummary(dogs$LicenseLength)\n##   Length    Class     Mode \n##   508196 difftime  numeric\nhead(dogs$LicenseLength)\n## Time differences in days\n## [1] 1096 1116 1826 1096 1123 1874\n\nWe can see that directly subtracting date-times gives us a license length in days. That’s useful enough, I guess, but it might be more useful in years… unfortunately, that’s not an option for difftime()\n\nlibrary(ggplot2)\ndogs$LicenseLength &lt;- difftime(dogs$LicenseExpiredDate, dogs$LicenseIssuedDate, units = \"weeks\")\n\n# 52 weeks in a year so we'll just convert as we plot\nggplot(data = dogs, aes(x = LicenseLength / 52 )) + geom_histogram() + \n  scale_x_continuous(limits = c(0,10))\n\n\n\n\n\n\n\ndogs[\"License_length\"] = dogs.LicenseExpiredDate - dogs.LicenseIssuedDate\n\ndogs.License_length.describe()\n## count                         508119\n## mean     476 days 01:27:18.898761920\n## std      333 days 22:58:59.771381512\n## min                  1 days 00:00:00\n## 25%                365 days 00:00:00\n## 50%                366 days 00:00:00\n## 75%                405 days 00:00:00\n## max               7305 days 00:00:00\n## Name: License_length, dtype: object\ndogs.License_length.head()\n## 0   1096 days\n## 1   1116 days\n## 2   1826 days\n## 3   1096 days\n## 4   1123 days\n## Name: License_length, dtype: timedelta64[ns]\ndogs[\"License_length_yr\"] = dogs.License_length.dt.days/365.25\n\n\n(\n  ggplot(aes(x = \"License_length_yr\"), data = dogs) + \n  geom_histogram(bins = 30)+\n  scale_x_continuous(limits = (0,10))\n)\n## &lt;ggplot: (8741671549236)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:324: PlotnineWarning: stat_bin : Removed 98 rows containing non-finite values.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_histogram : Removed 2 rows containing missing values.\n\n\n\n\nIn python, we have to first access the “days” attribute of the timedelta64 data type (this gives us a number) using dogs.Licence_length.dt.days and then divide by 365.25 (number of days in a year, on average).\n\n\n\n\n6.2.5 Explore Boroughs\nAnother question that I have when looking at this dataset is a bit more superficial - are the characteristics of different areas different? The dogs data frame has a Borough column, but it’s not actually filled in, so we’ll need to get rid of it and then add Borough back in by zip code.\nTo look at this, we’ll need a bit more data. I found a list of NYC zip codes by borough, which we can merge in with the data we already have to get puppy registrations by borough. Then, we can see if e.g. the top 10 breeds are different for different boroughs. To simplify this, I’m going to link to a file to merge in, and not show you the specifics of how I read the table from this site.\n\n\nR\nPython\n\n\n\n\nborough_zip &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/nyc_zip_borough.csv\")\n\n# Remove the Borough column from dogs\ndogs &lt;- dogs[, which(names(dogs) != \"Borough\")]\ndogs &lt;- merge(dogs, borough_zip, by = \"ZipCode\")\nhead(dogs)\n##   ZipCode AnimalName AnimalGender AnimalBirthYear                     BreedName\n## 1   10001       NAMA            F            2019             Poodle Crossbreed\n## 2   10001     COWBOY            M            2008                   wheaton mix\n## 3   10001      PENNY            F            2016                      Havanese\n## 4   10001     COOKIE            F            2013                      Shih Tzu\n## 5   10001     TUCKER            M            2006            Labrador Retriever\n## 6   10001       MINI            M            2018 Miniature Australian Shepherd\n##   LicenseIssuedDate LicenseExpiredDate Extract Year   LicenseLength   Borough\n## 1        2020-08-10         2021-06-21         2022  45.00000 weeks Manhattan\n## 2        2021-11-12         2022-04-18         2022  22.42857 weeks Manhattan\n## 3        2020-03-02         2021-01-10         2022  44.85714 weeks Manhattan\n## 4        2020-08-18         2022-09-18         2022 108.71429 weeks Manhattan\n## 5        2015-03-25         2016-04-05         2016  53.85714 weeks Manhattan\n## 6        2018-08-08         2019-08-08         2018  52.14286 weeks Manhattan\n\n\n\n\nborough_zip = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/nyc_zip_borough.csv\")\n\ndogs = dogs.drop('Borough', axis = 1) # drop borough column\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['Borough'] not found in axis\"\ndogs = pd.merge(dogs, borough_zip, on = 'ZipCode')\ndogs.head()\n##    AnimalName AnimalGender  ...  License_length_yr    Borough\n## 0       PAIGE            F  ...           3.000684  Manhattan\n## 1       BRODY            M  ...           2.001369  Manhattan\n## 2       ROCKO            M  ...           3.000684  Manhattan\n## 3   SENSATION            F  ...           2.001369  Manhattan\n## 4  TEMPTATION            M  ...           2.001369  Manhattan\n## \n## [5 rows x 11 columns]\n\n\n\n\nNow that we have borough, let’s write a function that will take a dataset and spit out a list of the top 5 dog breeds registered in that area.\n\n6.2.6 Custom Summary Function\n\n\nR\nPython\n\n\n\n\ntop_5_breeds &lt;- function(data) {\n  # Inside the function, our dataset is called data, not dogs\n  tmp &lt;- table(data$BreedName) \n  return(sort(tmp, decreasing = T)[1:5]) # top 5 breeds with counts\n}\n\n\n\n\n\ndef top_5_breeds(data):\n  tmp = pd.value_counts(data.BreedName)\n  return tmp.iloc[0:5]\n\n\n\n\n\n6.2.7 For Loop Summary\nNow, using that function, lets write a for loop that loops through the 5 boroughs and spits out the top 5 breeds in each borough:\n\n\nR\nPython\n\n\n\n\nboroughs &lt;- unique(borough_zip$Borough) # get a list of the 5 boroughs\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- dogs[dogs$Borough == i,]\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  print(result)\n}\n##                Breed  Freq   Borough rank\n## 1            Unknown 14115 Manhattan    1\n## 2  Yorkshire Terrier  6993 Manhattan    2\n## 3          Chihuahua  6745 Manhattan    3\n## 4           Shih Tzu  6044 Manhattan    4\n## 5 Labrador Retriever  5570 Manhattan    5\n##                Breed Freq Borough rank\n## 1            Unknown 5204  Staten    1\n## 2           Shih Tzu 3146  Staten    2\n## 3  Yorkshire Terrier 3043  Staten    3\n## 4 Labrador Retriever 2062  Staten    4\n## 5            Maltese 1609  Staten    5\n##                                Breed Freq Borough rank\n## 1                  Yorkshire Terrier 4959   Bronx    1\n## 2                            Unknown 4413   Bronx    2\n## 3                           Shih Tzu 4254   Bronx    3\n## 4                          Chihuahua 2873   Bronx    4\n## 5 American Pit Bull Terrier/Pit Bull 2155   Bronx    5\n##               Breed  Freq Borough rank\n## 1           Unknown 10291  Queens    1\n## 2 Yorkshire Terrier  6827  Queens    2\n## 3          Shih Tzu  6107  Queens    3\n## 4           Maltese  4265  Queens    4\n## 5         Chihuahua  4241  Queens    5\n##                           Breed  Freq  Borough rank\n## 1                       Unknown 12208 Brooklyn    1\n## 2             Yorkshire Terrier  7954 Brooklyn    2\n## 3                      Shih Tzu  7647 Brooklyn    3\n## 4                     Chihuahua  5513 Brooklyn    4\n## 5 Labrador Retriever Crossbreed  4332 Brooklyn    5\n\n\n\n\nboroughs = borough_zip.Borough.unique()\nfor i in boroughs:\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough == @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n\n  print(result)\n##             BreedName  count\n## 0             Unknown  14115\n## 1   Yorkshire Terrier   6993\n## 2           Chihuahua   6745\n## 3            Shih Tzu   6044\n## 4  Labrador Retriever   5570\n##                 index  BreedName    Borough  rank\n## 0             Unknown      14115  Manhattan     1\n## 1   Yorkshire Terrier       6993  Manhattan     2\n## 2           Chihuahua       6745  Manhattan     3\n## 3            Shih Tzu       6044  Manhattan     4\n## 4  Labrador Retriever       5570  Manhattan     5\n##             BreedName  count\n## 0             Unknown   5204\n## 1            Shih Tzu   3146\n## 2   Yorkshire Terrier   3043\n## 3  Labrador Retriever   2062\n## 4             Maltese   1609\n##                 index  BreedName Borough  rank\n## 0             Unknown       5204  Staten     1\n## 1            Shih Tzu       3146  Staten     2\n## 2   Yorkshire Terrier       3043  Staten     3\n## 3  Labrador Retriever       2062  Staten     4\n## 4             Maltese       1609  Staten     5\n##                             BreedName  count\n## 0                   Yorkshire Terrier   4959\n## 1                             Unknown   4413\n## 2                            Shih Tzu   4254\n## 3                           Chihuahua   2873\n## 4  American Pit Bull Terrier/Pit Bull   2155\n##                                 index  BreedName Borough  rank\n## 0                   Yorkshire Terrier       4959   Bronx     1\n## 1                             Unknown       4413   Bronx     2\n## 2                            Shih Tzu       4254   Bronx     3\n## 3                           Chihuahua       2873   Bronx     4\n## 4  American Pit Bull Terrier/Pit Bull       2155   Bronx     5\n##            BreedName  count\n## 0            Unknown  10291\n## 1  Yorkshire Terrier   6827\n## 2           Shih Tzu   6107\n## 3            Maltese   4265\n## 4          Chihuahua   4241\n##                index  BreedName Borough  rank\n## 0            Unknown      10291  Queens     1\n## 1  Yorkshire Terrier       6827  Queens     2\n## 2           Shih Tzu       6107  Queens     3\n## 3            Maltese       4265  Queens     4\n## 4          Chihuahua       4241  Queens     5\n##                        BreedName  count\n## 0                        Unknown  12208\n## 1              Yorkshire Terrier   7954\n## 2                       Shih Tzu   7647\n## 3                      Chihuahua   5513\n## 4  Labrador Retriever Crossbreed   4332\n##                            index  BreedName   Borough  rank\n## 0                        Unknown      12208  Brooklyn     1\n## 1              Yorkshire Terrier       7954  Brooklyn     2\n## 2                       Shih Tzu       7647  Brooklyn     3\n## 3                      Chihuahua       5513  Brooklyn     4\n## 4  Labrador Retriever Crossbreed       4332  Brooklyn     5\n\nMore information on pandas query function (use \\@varname to use a variable in a query).\n\n\n\n\n6.2.8 Summary Data Frame\nIf we wanted to save these results as a summary data frame, we could totally do that!\n\n\nR\nPython\n\n\n\n\nbreeds_by_borough &lt;- data.frame() # create a blank data frame\n\nfor (i in boroughs) {\n  # Get subset of data frame corresponding to the Borough\n  dogs_sub &lt;- subset(dogs, Borough == i)\n  # Get top 5 dog breeds\n  result &lt;- as.data.frame(top_5_breeds(dogs_sub))\n  # set names\n  names(result) &lt;- c(\"Breed\", \"Freq\")\n  # Add Borough as a new column\n  result$Borough &lt;- i\n  # Add rank as a new column\n  result$rank &lt;- 1:5\n  \n  breeds_by_borough &lt;- rbind(breeds_by_borough, result)\n}\n\nbreeds_by_borough\n##                                 Breed  Freq   Borough rank\n## 1                             Unknown 14115 Manhattan    1\n## 2                   Yorkshire Terrier  6993 Manhattan    2\n## 3                           Chihuahua  6745 Manhattan    3\n## 4                            Shih Tzu  6044 Manhattan    4\n## 5                  Labrador Retriever  5570 Manhattan    5\n## 6                             Unknown  5204    Staten    1\n## 7                            Shih Tzu  3146    Staten    2\n## 8                   Yorkshire Terrier  3043    Staten    3\n## 9                  Labrador Retriever  2062    Staten    4\n## 10                            Maltese  1609    Staten    5\n## 11                  Yorkshire Terrier  4959     Bronx    1\n## 12                            Unknown  4413     Bronx    2\n## 13                           Shih Tzu  4254     Bronx    3\n## 14                          Chihuahua  2873     Bronx    4\n## 15 American Pit Bull Terrier/Pit Bull  2155     Bronx    5\n## 16                            Unknown 10291    Queens    1\n## 17                  Yorkshire Terrier  6827    Queens    2\n## 18                           Shih Tzu  6107    Queens    3\n## 19                            Maltese  4265    Queens    4\n## 20                          Chihuahua  4241    Queens    5\n## 21                            Unknown 12208  Brooklyn    1\n## 22                  Yorkshire Terrier  7954  Brooklyn    2\n## 23                           Shih Tzu  7647  Brooklyn    3\n## 24                          Chihuahua  5513  Brooklyn    4\n## 25      Labrador Retriever Crossbreed  4332  Brooklyn    5\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\n\nbreeds_by_borough[order(breeds_by_borough$rank, \n                        breeds_by_borough$Borough),]\n##                                 Breed  Freq   Borough rank\n## 11                  Yorkshire Terrier  4959     Bronx    1\n## 21                            Unknown 12208  Brooklyn    1\n## 1                             Unknown 14115 Manhattan    1\n## 16                            Unknown 10291    Queens    1\n## 6                             Unknown  5204    Staten    1\n## 12                            Unknown  4413     Bronx    2\n## 22                  Yorkshire Terrier  7954  Brooklyn    2\n## 2                   Yorkshire Terrier  6993 Manhattan    2\n## 17                  Yorkshire Terrier  6827    Queens    2\n## 7                            Shih Tzu  3146    Staten    2\n## 13                           Shih Tzu  4254     Bronx    3\n## 23                           Shih Tzu  7647  Brooklyn    3\n## 3                           Chihuahua  6745 Manhattan    3\n## 18                           Shih Tzu  6107    Queens    3\n## 8                   Yorkshire Terrier  3043    Staten    3\n## 14                          Chihuahua  2873     Bronx    4\n## 24                          Chihuahua  5513  Brooklyn    4\n## 4                            Shih Tzu  6044 Manhattan    4\n## 19                            Maltese  4265    Queens    4\n## 9                  Labrador Retriever  2062    Staten    4\n## 15 American Pit Bull Terrier/Pit Bull  2155     Bronx    5\n## 25      Labrador Retriever Crossbreed  4332  Brooklyn    5\n## 5                  Labrador Retriever  5570 Manhattan    5\n## 20                          Chihuahua  4241    Queens    5\n## 10                            Maltese  1609    Staten    5\n\n\n\n\nbreeds_by_borough = pd.DataFrame() # Create a blank dataframe\n\nfor i in boroughs:\n  print(i)\n  # get subset of data frame corresponding to the borough\n  dogs_sub = dogs.query(\"Borough== @i\")\n  # Get top 5 breeds\n  result = top_5_breeds(dogs_sub)\n  # Convert to DataFrame and make the index another column\n  result = result.to_frame().reset_index()\n  # Rename columns\n  result.rename(columns = {'index':'BreedName','BreedName':'count'})\n  # Add Borough column\n  result[\"Borough\"] = i\n  # Add rank column\n  result[\"rank\"] = range(1, 6)\n  # Append to blank dataframe\n  breeds_by_borough = breeds_by_borough.append(result)\n## Manhattan\n##             BreedName  count\n## 0             Unknown  14115\n## 1   Yorkshire Terrier   6993\n## 2           Chihuahua   6745\n## 3            Shih Tzu   6044\n## 4  Labrador Retriever   5570\n## Staten\n##             BreedName  count\n## 0             Unknown   5204\n## 1            Shih Tzu   3146\n## 2   Yorkshire Terrier   3043\n## 3  Labrador Retriever   2062\n## 4             Maltese   1609\n## Bronx\n##                             BreedName  count\n## 0                   Yorkshire Terrier   4959\n## 1                             Unknown   4413\n## 2                            Shih Tzu   4254\n## 3                           Chihuahua   2873\n## 4  American Pit Bull Terrier/Pit Bull   2155\n## Queens\n##            BreedName  count\n## 0            Unknown  10291\n## 1  Yorkshire Terrier   6827\n## 2           Shih Tzu   6107\n## 3            Maltese   4265\n## 4          Chihuahua   4241\n## Brooklyn\n##                        BreedName  count\n## 0                        Unknown  12208\n## 1              Yorkshire Terrier   7954\n## 2                       Shih Tzu   7647\n## 3                      Chihuahua   5513\n## 4  Labrador Retriever Crossbreed   4332\n## \n## &lt;string&gt;:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## &lt;string&gt;:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## &lt;string&gt;:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## &lt;string&gt;:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## &lt;string&gt;:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\nbreeds_by_borough.head()\n##                 index  BreedName    Borough  rank\n## 0             Unknown      14115  Manhattan     1\n## 1   Yorkshire Terrier       6993  Manhattan     2\n## 2           Chihuahua       6745  Manhattan     3\n## 3            Shih Tzu       6044  Manhattan     4\n## 4  Labrador Retriever       5570  Manhattan     5\nbreeds_by_borough.tail()\n##                            index  BreedName   Borough  rank\n## 0                        Unknown      12208  Brooklyn     1\n## 1              Yorkshire Terrier       7954  Brooklyn     2\n## 2                       Shih Tzu       7647  Brooklyn     3\n## 3                      Chihuahua       5513  Brooklyn     4\n## 4  Labrador Retriever Crossbreed       4332  Brooklyn     5\n\nWe could even sort our data by the rank and Borough for easier comparisons:\n\n\nbreeds_by_borough.sort_values(['rank', 'Borough'])\n##                                 index  BreedName    Borough  rank\n## 0                   Yorkshire Terrier       4959      Bronx     1\n## 0                             Unknown      12208   Brooklyn     1\n## 0                             Unknown      14115  Manhattan     1\n## 0                             Unknown      10291     Queens     1\n## 0                             Unknown       5204     Staten     1\n## 1                             Unknown       4413      Bronx     2\n## 1                   Yorkshire Terrier       7954   Brooklyn     2\n## 1                   Yorkshire Terrier       6993  Manhattan     2\n## 1                   Yorkshire Terrier       6827     Queens     2\n## 1                            Shih Tzu       3146     Staten     2\n## 2                            Shih Tzu       4254      Bronx     3\n## 2                            Shih Tzu       7647   Brooklyn     3\n## 2                           Chihuahua       6745  Manhattan     3\n## 2                            Shih Tzu       6107     Queens     3\n## 2                   Yorkshire Terrier       3043     Staten     3\n## 3                           Chihuahua       2873      Bronx     4\n## 3                           Chihuahua       5513   Brooklyn     4\n## 3                            Shih Tzu       6044  Manhattan     4\n## 3                             Maltese       4265     Queens     4\n## 3                  Labrador Retriever       2062     Staten     4\n## 4  American Pit Bull Terrier/Pit Bull       2155      Bronx     5\n## 4       Labrador Retriever Crossbreed       4332   Brooklyn     5\n## 4                  Labrador Retriever       5570  Manhattan     5\n## 4                           Chihuahua       4241     Queens     5\n## 4                             Maltese       1609     Staten     5\n\n\n\n\nSoon we’ll learn a much shorter set of commands to get these types of summaries, but it’s important to know how a for loop connects to the concept of summarizing data by a factor (in this case, by borough).\n\n\n\n\n\n\nTry it out: NYC dogs\n\n\n\nLook at the name, age, or gender of dogs registered in NYC and see if you can come up with a similar function and way of summarizing the data in a for-loop. You may want to calculate the mean or quantiles (for numeric variables), or list the most common dog names/genders in each borough."
  },
  {
    "objectID": "data-programming.html#footnotes",
    "href": "data-programming.html#footnotes",
    "title": "6  Programming with Data",
    "section": "",
    "text": "(or = or -&gt; if you’re a total heathen)↩︎"
  },
  {
    "objectID": "exploratory-data-analysis.html#module7-objectives",
    "href": "exploratory-data-analysis.html#module7-objectives",
    "title": "7  Exploratory Data Analysis",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nUnderstand the main goals of exploratory data analysis\nGenerate and answer questions about a new dataset using charts, tables, and numerical summaries\n\n\n\n\n\n\n\nExtra Reading\n\n\n\nThe EDA chapter in R for Data Science [1] is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. Much of the material in this chapter is based at least in part on the R4DS chapter.\n\n\n\nMajor components of Exploratory Data Analysis (EDA):\n\ngenerating questions about your data\nlook for answers to the questions (visualization, transformation, modeling)\nuse answers to refine the questions and generate new questions\n\nEDA is an iterative process. It is like brainstorming - you start with an idea or question you might have about the data, investigate, and then generate new ideas. EDA is useful even when you are relatively familiar with the type of data you’re working with: in any dataset, it is good to make sure that you know the quality of the data as well as the relationships between the variables in the dataset.\nEDA is important because it helps us to know what challenges a particular data set might bring, what we might do with it. Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence.\nWhile in many classes you’ll be given mostly clean data, you do need to know how to clean your own data up so that you can use more interesting data sets for projects (and for fun!). EDA is an important component to learning how to work with messy data.\n\nIn this section, I will mostly be using the plot commands that come with base R/python and require no extra packages. The R for Data Science book [1] shows plot commands which use the ggplot2 library. I’ll show you some plots from ggplot here as well, but you don’t have to understand how to generate them yet. We will learn more about ggplot2 later, though if you want to start using it now, you may."
  },
  {
    "objectID": "exploratory-data-analysis.html#a-note-on-language-philosophies",
    "href": "exploratory-data-analysis.html#a-note-on-language-philosophies",
    "title": "7  Exploratory Data Analysis",
    "section": "A Note on Language Philosophies",
    "text": "A Note on Language Philosophies\nIt is usually relatively easy to get summary statistics from a dataset, but the “flow” of EDA is somewhat different depending on the language patterns.\n\nYou must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of “give me every possible statistic that could be calculated from this model, whether or not it makes sense”. The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. – Douglas Bates\n\nI provide this as a historical artifact, but it does explain the difference between the approach to EDA and model output in R and Python, and the approach in SAS, which you may see in your other statistics classes. This is not (at least, in my opinion) a criticism – the SAS philosophy dates back to the mainframe and punch card days, and the syntax and output still bear evidence of that – but it is worth noting.\nIn R and in Python, you will have to specify each piece of output you want, but in SAS you will get more than you ever wanted with a single command. Neither approach is wrong, but sometimes one is preferable over the other for a given problem."
  },
  {
    "objectID": "exploratory-data-analysis.html#generating-eda-questions",
    "href": "exploratory-data-analysis.html#generating-eda-questions",
    "title": "7  Exploratory Data Analysis",
    "section": "\n7.1 Generating EDA Questions",
    "text": "7.1 Generating EDA Questions\nI very much like the two quotes in the [1] section on EDA Questions:\n\nThere are no routine statistical questions, only questionable statistical routines. — Sir David Cox\n\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. — John Tukey\n\nAs statisticians, we are concerned with variability by default. This is also true during EDA: we are interested in variability (or sometimes, lack thereof) in the variables in our dataset, including the co-variability between multiple variables.\nWe may assess variability using pictures or numerical summaries:\n\nhistograms or density plots (continuous variables)\ncolumn plots (categorical variables)\nboxplots\n5 number summaries (min, 25%, mean, 75%, max)\ntabular data summaries (for categorical variables)\n\nIn many cases, this gives us a picture of both variability and the “typical” value of our variable.\nSometimes we may also be interested in identifying unusual values: outliers, data entry errors, and other points which don’t conform to our expectations. These unusual values may show up when we generate pictures and the axis limits are much larger than expected.\nWe also are usually concerned with missing values - in many cases, not all observations are complete, and this missingness can interfere with statistical analyses. It can be helpful to keep track of how much missingness there is in any particular variable and any patterns of missingness that would impact the eventual data analysis1.\nIf you are having trouble getting started on EDA, [3] provides a nice checklist to get you thinking:\n\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat’s missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?"
  },
  {
    "objectID": "exploratory-data-analysis.html#useful-eda-techniques",
    "href": "exploratory-data-analysis.html#useful-eda-techniques",
    "title": "7  Exploratory Data Analysis",
    "section": "\n7.2 Useful EDA Techniques",
    "text": "7.2 Useful EDA Techniques\nIn this chapter, we’ll explore the pokemon data in shahinrostami’s github repository. This data has a number of categorical and continuous variables that should allow for a reasonable demonstration of a number of techniques for exploring data.\n\n\nR\nPython\n\n\n\n\nlibrary(readr)\nurl &lt;- \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\npoke &lt;- read_csv(url)[,-c(1, 4, 5)] # skip extra columns\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\")\npoke = poke.drop([\"Unnamed: 0\", \"german_name\", \"japanese_name\"], axis = 1) # Drop some extra cols\n\n\n\n\n\n7.2.1 Numerical Summary Statistics\n\n\nR: summary\nPython: describe\nR: skimr\npython: skimpy\n\n\n\nThe first, and most basic EDA command in R is summary().\nFor numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all “other” category.\n\n# Make types into factors to demonstrate the difference\npoke$type_1 &lt;- factor(poke$type_1)\npoke$type_2 &lt;- factor(poke$type_2)\n\nsummary(poke[,3:12])\n##    generation       status            species           type_number   \n##  Min.   :1.000   Length:1028        Length:1028        Min.   :1.000  \n##  1st Qu.:2.000   Class :character   Class :character   1st Qu.:1.000  \n##  Median :4.000   Mode  :character   Mode  :character   Median :2.000  \n##  Mean   :4.034                                         Mean   :1.527  \n##  3rd Qu.:6.000                                         3rd Qu.:2.000  \n##  Max.   :8.000                                         Max.   :2.000  \n##                                                                       \n##      type_1        type_2       height_m         weight_kg     \n##  Water  :134   Flying :109   Min.   :  0.100   Min.   :  0.10  \n##  Normal :115   Fairy  : 41   1st Qu.:  0.600   1st Qu.:  8.80  \n##  Grass  : 91   Ground : 39   Median :  1.000   Median : 28.50  \n##  Bug    : 81   Poison : 38   Mean   :  1.368   Mean   : 69.75  \n##  Psychic: 76   Psychic: 38   3rd Qu.:  1.500   3rd Qu.: 69.10  \n##  Fire   : 65   (Other):277   Max.   :100.000   Max.   :999.90  \n##  (Other):466   NA's   :486                     NA's   :1       \n##  abilities_number  ability_1        \n##  Min.   :0.000    Length:1028       \n##  1st Qu.:2.000    Class :character  \n##  Median :2.000    Mode  :character  \n##  Mean   :2.284                      \n##  3rd Qu.:3.000                      \n##  Max.   :3.000                      \n## \n\nOne common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis.\nThere is one pokemon who appears to not have a weight specified. Let’s investigate further:\n\npoke[is.na(poke$weight_kg),] # Show any rows where weight.kg is NA\n## # A tibble: 1 × 48\n##   pokedex_n…¹ name  gener…² status species type_…³ type_1 type_2 heigh…⁴ weigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n## 1         890 Eter…       8 Legen… Gigant…       2 Poison Dragon     100      NA\n## # … with 38 more variables: abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;,\n## #   ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;,\n## #   attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;dbl&gt;, base_friendship &lt;dbl&gt;,\n## #   base_experience &lt;dbl&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;dbl&gt;,\n## #   egg_cycles &lt;dbl&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\nThis is the last row of our data frame, and this pokemon appears to have many missing values.\n\n\nThe most basic EDA command in pandas is df.describe() (which operates on a DataFrame named df). Like summary() in R, describe() provides a 5-number summary for numeric variables. For categorical variables, describe() provides the number of unique values, the most common value, and the frequency of that common value.\n\npoke.iloc[:,2:11].describe() # describe only shows numeric variables by default\n\n# You can get categorical variables too if that's all you give it to show\n##         generation  type_number     height_m    weight_kg  abilities_number\n## count  1028.000000  1028.000000  1028.000000  1027.000000       1028.000000\n## mean      4.034047     1.527237     1.368093    69.753749          2.284047\n## std       2.234937     0.499501     3.380126   129.221230          0.794981\n## min       1.000000     1.000000     0.100000     0.100000          0.000000\n## 25%       2.000000     1.000000     0.600000     8.800000          2.000000\n## 50%       4.000000     2.000000     1.000000    28.500000          2.000000\n## 75%       6.000000     2.000000     1.500000    69.100000          3.000000\n## max       8.000000     2.000000   100.000000   999.900000          3.000000\npoke['status'].describe()\n## count       1028\n## unique         4\n## top       Normal\n## freq         915\n## Name: status, dtype: object\n\n\n\nAn R package that is incredibly useful for this type of dataset exploration is skimr.\n\nlibrary(skimr)\nskim(poke)\n\n\nData summary\n\n\nName\npoke\n\n\nNumber of rows\n1028\n\n\nNumber of columns\n48\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nfactor\n2\n\n\nnumeric\n37\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n3\n33\n0\n1028\n0\n\n\nstatus\n0\n1.00\n6\n13\n0\n4\n0\n\n\nspecies\n0\n1.00\n11\n21\n0\n641\n0\n\n\nability_1\n3\n1.00\n4\n16\n0\n202\n0\n\n\nability_2\n515\n0.50\n4\n16\n0\n126\n0\n\n\nability_hidden\n218\n0.79\n4\n16\n0\n154\n0\n\n\ngrowth_rate\n1\n1.00\n4\n11\n0\n6\n0\n\n\negg_type_1\n3\n1.00\n3\n12\n0\n15\n0\n\n\negg_type_2\n746\n0.27\n5\n10\n0\n11\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ntype_1\n0\n1.00\nFALSE\n18\nWat: 134, Nor: 115, Gra: 91, Bug: 81\n\n\ntype_2\n486\n0.53\nFALSE\n18\nFly: 109, Fai: 41, Gro: 39, Poi: 38\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\npokedex_number\n0\n1.00\n437.71\n259.37\n1.00\n213.75\n433.5\n663.25\n890.0\n▇▇▇▇▇\n\n\ngeneration\n0\n1.00\n4.03\n2.23\n1.00\n2.00\n4.0\n6.00\n8.0\n▇▅▇▂▅\n\n\ntype_number\n0\n1.00\n1.53\n0.50\n1.00\n1.00\n2.0\n2.00\n2.0\n▇▁▁▁▇\n\n\nheight_m\n0\n1.00\n1.37\n3.38\n0.10\n0.60\n1.0\n1.50\n100.0\n▇▁▁▁▁\n\n\nweight_kg\n1\n1.00\n69.75\n129.22\n0.10\n8.80\n28.5\n69.10\n999.9\n▇▁▁▁▁\n\n\nabilities_number\n0\n1.00\n2.28\n0.79\n0.00\n2.00\n2.0\n3.00\n3.0\n▁▃▁▅▇\n\n\ntotal_points\n0\n1.00\n437.57\n121.66\n175.00\n330.00\n455.0\n510.00\n1125.0\n▅▇▂▁▁\n\n\nhp\n0\n1.00\n69.58\n26.39\n1.00\n50.00\n66.5\n80.00\n255.0\n▃▇▁▁▁\n\n\nattack\n0\n1.00\n80.12\n32.37\n5.00\n55.00\n76.0\n100.00\n190.0\n▂▇▇▂▁\n\n\ndefense\n0\n1.00\n74.48\n31.30\n5.00\n50.00\n70.0\n90.00\n250.0\n▃▇▂▁▁\n\n\nsp_attack\n0\n1.00\n72.73\n32.68\n10.00\n50.00\n65.0\n95.00\n194.0\n▅▇▅▂▁\n\n\nsp_defense\n0\n1.00\n72.13\n28.08\n20.00\n50.00\n70.0\n90.00\n250.0\n▇▇▁▁▁\n\n\nspeed\n0\n1.00\n68.53\n29.80\n5.00\n45.00\n65.0\n90.00\n180.0\n▃▇▆▂▁\n\n\ncatch_rate\n104\n0.90\n93.17\n75.24\n3.00\n45.00\n60.0\n127.00\n255.0\n▇▃▂▂▂\n\n\nbase_friendship\n104\n0.90\n64.14\n21.46\n0.00\n70.00\n70.0\n70.00\n140.0\n▁▁▇▁▁\n\n\nbase_experience\n104\n0.90\n153.81\n79.27\n36.00\n67.00\n159.0\n201.50\n608.0\n▇▇▂▁▁\n\n\negg_type_number\n0\n1.00\n1.27\n0.45\n0.00\n1.00\n1.0\n2.00\n2.0\n▁▁▇▁▃\n\n\npercentage_male\n236\n0.77\n55.00\n20.18\n0.00\n50.00\n50.0\n50.00\n100.0\n▁▁▇▁▂\n\n\negg_cycles\n1\n1.00\n30.32\n28.94\n5.00\n20.00\n20.0\n25.00\n120.0\n▇▁▁▁▁\n\n\nagainst_normal\n0\n1.00\n0.87\n0.29\n0.00\n1.00\n1.0\n1.00\n1.0\n▁▁▁▁▇\n\n\nagainst_fire\n0\n1.00\n1.13\n0.72\n0.00\n0.50\n1.0\n2.00\n4.0\n▆▇▅▁▁\n\n\nagainst_water\n0\n1.00\n1.05\n0.61\n0.00\n0.50\n1.0\n1.00\n4.0\n▅▇▂▁▁\n\n\nagainst_electric\n0\n1.00\n1.03\n0.65\n0.00\n0.50\n1.0\n1.00\n4.0\n▅▇▃▁▁\n\n\nagainst_grass\n0\n1.00\n1.00\n0.75\n0.00\n0.50\n1.0\n1.00\n4.0\n▇▇▃▁▁\n\n\nagainst_ice\n0\n1.00\n1.20\n0.76\n0.00\n0.50\n1.0\n2.00\n4.0\n▅▇▅▁▁\n\n\nagainst_fight\n0\n1.00\n1.08\n0.75\n0.00\n0.50\n1.0\n2.00\n4.0\n▇▇▅▁▁\n\n\nagainst_poison\n0\n1.00\n0.95\n0.54\n0.00\n0.50\n1.0\n1.00\n4.0\n▃▇▂▁▁\n\n\nagainst_ground\n0\n1.00\n1.08\n0.78\n0.00\n0.50\n1.0\n1.62\n4.0\n▅▇▃▁▁\n\n\nagainst_flying\n0\n1.00\n1.17\n0.59\n0.25\n1.00\n1.0\n1.00\n4.0\n▇▁▂▁▁\n\n\nagainst_psychic\n0\n1.00\n0.98\n0.50\n0.00\n1.00\n1.0\n1.00\n4.0\n▃▇▂▁▁\n\n\nagainst_bug\n0\n1.00\n0.99\n0.60\n0.00\n0.50\n1.0\n1.00\n4.0\n▇▇▃▁▁\n\n\nagainst_rock\n0\n1.00\n1.24\n0.70\n0.25\n1.00\n1.0\n2.00\n4.0\n▇▁▂▁▁\n\n\nagainst_ghost\n0\n1.00\n1.01\n0.56\n0.00\n1.00\n1.0\n1.00\n4.0\n▂▇▂▁▁\n\n\nagainst_dragon\n0\n1.00\n0.98\n0.38\n0.00\n1.00\n1.0\n1.00\n2.0\n▁▁▇▁▁\n\n\nagainst_dark\n0\n1.00\n1.07\n0.45\n0.25\n1.00\n1.0\n1.00\n4.0\n▇▁▁▁▁\n\n\nagainst_steel\n0\n1.00\n0.98\n0.50\n0.00\n0.50\n1.0\n1.00\n4.0\n▅▇▂▁▁\n\n\nagainst_fairy\n0\n1.00\n1.08\n0.53\n0.00\n1.00\n1.0\n1.00\n4.0\n▂▇▂▁▁\n\n\n\n\n\nskim provides a beautiful table of summary statistics along with a sparklines-style histogram of values, giving you a sneak peek at the distribution.\n\n\nThere is a similar package to skimr in R called skimpy in Python.\n\nfrom skimpy import skim\nskim(poke)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 1028   │ │ float64     │ 25    │                       │\n## │ │ Number of columns │ 48     │ │ int64       │ 12    │                       │\n## │ └───────────────────┴────────┘ │ object      │ 11    │                       │\n## │                                └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━┳━━━━━┳━━━━━━┳━━━━━━━┓  │\n## │ ┃        ┃ missi ┃ comple ┃ mean ┃ sd   ┃ p0   ┃ p25 ┃ p75 ┃ p100 ┃ hist  ┃  │\n## │ ┃        ┃ ng    ┃ te     ┃      ┃      ┃      ┃     ┃     ┃      ┃       ┃  │\n## │ ┃        ┃       ┃ rate   ┃      ┃      ┃      ┃     ┃     ┃      ┃       ┃  │\n## │ ┡━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━╇━━━━━╇━━━━━━╇━━━━━━━┩  │\n## │ │ pokede │     0 │      1 │  440 │  260 │    1 │ 210 │ 660 │  890 │ █▇▇▇▇ │  │\n## │ │ x_numb │       │        │      │      │      │     │     │      │   ▇   │  │\n## │ │ er     │       │        │      │      │      │     │     │      │       │  │\n## │ │ genera │     0 │      1 │    4 │  2.2 │    1 │   2 │   6 │    8 │ █▄▃▅▂ │  │\n## │ │ tion   │       │        │      │      │      │     │     │      │   ▅   │  │\n## │ │ type_n │     0 │      1 │  1.5 │  0.5 │    1 │   1 │   2 │    2 │   ▇   │  │\n## │ │ umber  │       │        │      │      │      │     │     │      │   █   │  │\n## │ │ height │     0 │      1 │  1.4 │  3.4 │  0.1 │ 0.6 │ 1.5 │  100 │   █   │  │\n## │ │ _m     │       │        │      │      │      │     │     │      │       │  │\n## │ │ weight │     1 │      1 │   70 │  130 │  0.1 │ 8.8 │  69 │ 1000 │  █▁   │  │\n## │ │ _kg    │       │        │      │      │      │     │     │      │       │  │\n## │ │ abilit │     0 │      1 │  2.3 │ 0.79 │    0 │   2 │   3 │    3 │    ▃  │  │\n## │ │ ies_nu │       │        │      │      │      │     │     │      │  ▅█   │  │\n## │ │ mber   │       │        │      │      │      │     │     │      │       │  │\n## │ │ total_ │     0 │      1 │  440 │  120 │  180 │ 330 │ 510 │ 1100 │ ▅█▆▁  │  │\n## │ │ points │       │        │      │      │      │     │     │      │       │  │\n## │ │ hp     │     0 │      1 │   70 │   26 │    1 │  50 │  80 │  260 │  ▁█▂  │  │\n## │ │ attack │     0 │      1 │   80 │   32 │    5 │  55 │ 100 │  190 │ ▂██▅▂ │  │\n## │ │ defens │     0 │      1 │   74 │   31 │    5 │  50 │  90 │  250 │ ▃█▄▁  │  │\n## │ │ e      │       │        │      │      │      │     │     │      │       │  │\n## │ │ sp_att │     0 │      1 │   73 │   33 │   10 │  50 │  95 │  190 │ ▄█▅▃▁ │  │\n## │ │ ack    │       │        │      │      │      │     │     │      │       │  │\n## │ │ sp_def │     0 │      1 │   72 │   28 │   20 │  50 │  90 │  250 │  ▆█▂  │  │\n## │ │ ense   │       │        │      │      │      │     │     │      │       │  │\n## │ │ speed  │     0 │      1 │   69 │   30 │    5 │  45 │  90 │  180 │ ▃█▇▄▁ │  │\n## │ │ catch_ │   100 │    0.9 │   93 │   75 │    3 │  45 │ 130 │  260 │  ▂█▂  │  │\n## │ │ rate   │       │        │      │      │      │     │     │      │  ▂▂   │  │\n## │ │ base_f │   100 │    0.9 │   64 │   21 │    0 │  70 │  70 │  140 │ ▁▁ █  │  │\n## │ │ riends │       │        │      │      │      │     │     │      │       │  │\n## │ │ hip    │       │        │      │      │      │     │     │      │       │  │\n## │ │ base_e │   100 │    0.9 │  150 │   79 │   36 │  67 │ 200 │  610 │  ▆█▃  │  │\n## │ │ xperie │       │        │      │      │      │     │     │      │       │  │\n## │ │ nce    │       │        │      │      │      │     │     │      │       │  │\n## │ │ egg_ty │     0 │      1 │  1.3 │ 0.45 │    0 │   1 │   2 │    2 │    █  │  │\n## │ │ pe_num │       │        │      │      │      │     │     │      │   ▃   │  │\n## │ │ ber    │       │        │      │      │      │     │     │      │       │  │\n## │ │ percen │   240 │   0.77 │   55 │   20 │    0 │  50 │  50 │  100 │    █  │  │\n## │ │ tage_m │       │        │      │      │      │     │     │      │   ▂   │  │\n## │ │ ale    │       │        │      │      │      │     │     │      │       │  │\n## │ │ egg_cy │     1 │      1 │   30 │   29 │    5 │  20 │  25 │  120 │  █▂   │  │\n## │ │ cles   │       │        │      │      │      │     │     │      │   ▁   │  │\n## │ │ agains │     0 │      1 │ 0.87 │ 0.29 │    0 │   1 │   1 │    1 │ ▁  ▁  │  │\n## │ │ t_norm │       │        │      │      │      │     │     │      │   █   │  │\n## │ │ al     │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.72 │    0 │ 0.5 │   2 │    4 │ ▆█ ▄  │  │\n## │ │ t_fire │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.61 │    0 │ 0.5 │   1 │    4 │ ▄█ ▂  │  │\n## │ │ t_wate │       │        │      │      │      │     │     │      │       │  │\n## │ │ r      │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │    1 │ 0.65 │    0 │ 0.5 │   1 │    4 │ ▅█ ▃  │  │\n## │ │ t_elec │       │        │      │      │      │     │     │      │       │  │\n## │ │ tric   │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │    1 │ 0.75 │    0 │ 0.5 │   1 │    4 │ █▇ ▃  │  │\n## │ │ t_gras │       │        │      │      │      │     │     │      │   ▁   │  │\n## │ │ s      │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.2 │ 0.76 │    0 │ 0.5 │   2 │    4 │ ▅█ ▅  │  │\n## │ │ t_ice  │       │        │      │      │      │     │     │      │   ▁   │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.75 │    0 │ 0.5 │   2 │    4 │ ▇█ ▅  │  │\n## │ │ t_figh │       │        │      │      │      │     │     │      │       │  │\n## │ │ t      │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │ 0.95 │ 0.54 │    0 │ 0.5 │   1 │    4 │ ▄█ ▂  │  │\n## │ │ t_pois │       │        │      │      │      │     │     │      │       │  │\n## │ │ on     │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.78 │    0 │ 0.5 │ 1.6 │    4 │ ▅█ ▄  │  │\n## │ │ t_grou │       │        │      │      │      │     │     │      │       │  │\n## │ │ nd     │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.2 │ 0.59 │ 0.25 │   1 │   1 │    4 │  ▂█▃  │  │\n## │ │ t_flyi │       │        │      │      │      │     │     │      │       │  │\n## │ │ ng     │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │ 0.98 │  0.5 │    0 │   1 │   1 │    4 │ ▃█ ▁  │  │\n## │ │ t_psyc │       │        │      │      │      │     │     │      │       │  │\n## │ │ hic    │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │ 0.99 │  0.6 │    0 │ 0.5 │   1 │    4 │ ▇█ ▃  │  │\n## │ │ t_bug  │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.2 │  0.7 │ 0.25 │   1 │   2 │    4 │  ▂█▃  │  │\n## │ │ t_rock │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │    1 │ 0.56 │    0 │   1 │   1 │    4 │ ▂█ ▂  │  │\n## │ │ t_ghos │       │        │      │      │      │     │     │      │       │  │\n## │ │ t      │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │ 0.98 │ 0.38 │    0 │   1 │   1 │    2 │ ▁▁ █  │  │\n## │ │ t_drag │       │        │      │      │      │     │     │      │   ▁   │  │\n## │ │ on     │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.45 │ 0.25 │   1 │   1 │    4 │  ▂█▂  │  │\n## │ │ t_dark │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │ 0.98 │  0.5 │    0 │ 0.5 │   1 │    4 │ ▅█ ▂  │  │\n## │ │ t_stee │       │        │      │      │      │     │     │      │       │  │\n## │ │ l      │       │        │      │      │      │     │     │      │       │  │\n## │ │ agains │     0 │      1 │  1.1 │ 0.53 │    0 │   1 │   1 │    4 │ ▂█ ▂  │  │\n## │ │ t_fair │       │        │      │      │      │     │     │      │       │  │\n## │ │ y      │       │        │      │      │      │     │     │      │       │  │\n## │ └────────┴───────┴────────┴──────┴──────┴──────┴─────┴─────┴──────┴───────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\n\n7.2.2 Assessing Distributions\nWe are often also interested in the distribution of values.\nCategorical Variables\nOne useful way to assess the distribution of values is to generate a cross-tabular view of the data. This is mostly important for variables with a relatively low number of categories - otherwise, it is usually easier to use a graphical summary method.\nTabular Summaries\n\n\nR\nPython\n\n\n\nWe can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). We can even generate cross-tabular views for a combination of two variables (or theoretically more, but this gets hard to read and track).\n\ntable(poke$generation)\n## \n##   1   2   3   4   5   6   7   8 \n## 192 107 165 121 171  85  99  88\n\ntable(poke$type_1, poke$type_2)\n##           \n##            Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass\n##   Bug        0    0      0        4     2        4    2     14     1     6\n##   Dark       0    0      4        0     3        2    3      5     2     0\n##   Dragon     0    0      0        1     1        2    1      6     3     0\n##   Electric   0    2      2        0     2        0    1      6     1     1\n##   Fairy      0    0      0        0     0        0    0      2     0     0\n##   Fighting   0    1      0        0     0        0    0      1     1     0\n##   Fire       2    1      2        0     0        7    0      7     2     0\n##   Flying     0    0      2        0     0        0    0      0     0     0\n##   Ghost      0    1      2        0     1        0    3      3     0    11\n##   Grass      0    3      5        0     5        3    0      7     1     0\n##   Ground     0    3      2        1     0        0    1      4     4     0\n##   Ice        2    0      0        0     1        0    1      2     1     0\n##   Normal     0    0      1        0     5        4    0     27     0     2\n##   Poison     1    5      4        0     1        2    2      3     0     0\n##   Psychic    0    1      1        0     9        3    1      7     3     1\n##   Rock       2    2      2        3     3        1    2      6     0     2\n##   Steel      0    0      2        0     4        1    0      2     4     0\n##   Water      2    7      3        2     4        3    0      7     2     3\n##           \n##            Ground Ice Normal Poison Psychic Rock Steel Water\n##   Bug           2   0      0     12       2    3     7     3\n##   Dark          0   2      5      0       2    0     2     0\n##   Dragon        7   3      0      0       4    0     0     0\n##   Electric      0   2      2      3       1    0     4     1\n##   Fairy         0   0      0      0       0    0     1     0\n##   Fighting      0   1      0      0       3    0     3     0\n##   Fire          3   0      2      0       2    1     1     1\n##   Flying        0   0      0      0       0    0     1     1\n##   Ghost         2   0      0      4       0    0     0     0\n##   Grass         1   3      0     15       2    0     3     0\n##   Ground        0   0      0      0       2    3     4     0\n##   Ice           3   0      0      0       2    0     2     3\n##   Normal        1   0      0      0       3    0     0     1\n##   Poison        2   0      0      0       0    0     0     3\n##   Psychic       0   2      2      0       0    0     2     0\n##   Rock          6   2      0      1       2    0     4     6\n##   Steel         2   0      0      0       7    3     0     0\n##   Water        10   4      0      3       6    5     1     0\n\n\n\n\nimport numpy as np\n# For only one factor, use .groupby('colname')['colname'].count()\npoke.groupby(['generation'])['generation'].count()\n\n# for two or more factors, use pd.crosstab\n## generation\n## 1    192\n## 2    107\n## 3    165\n## 4    121\n## 5    171\n## 6     85\n## 7     99\n## 8     88\n## Name: generation, dtype: int64\npd.crosstab(index = poke['type_1'], columns = poke['type_2'])\n## type_2    Bug  Dark  Dragon  Electric  ...  Psychic  Rock  Steel  Water\n## type_1                                 ...                             \n## Bug         0     0       0         4  ...        2     3      7      3\n## Dark        0     0       4         0  ...        2     0      2      0\n## Dragon      0     0       0         1  ...        4     0      0      0\n## Electric    0     2       2         0  ...        1     0      4      1\n## Fairy       0     0       0         0  ...        0     0      1      0\n## Fighting    0     1       0         0  ...        3     0      3      0\n## Fire        2     1       2         0  ...        2     1      1      1\n## Flying      0     0       2         0  ...        0     0      1      1\n## Ghost       0     1       2         0  ...        0     0      0      0\n## Grass       0     3       5         0  ...        2     0      3      0\n## Ground      0     3       2         1  ...        2     3      4      0\n## Ice         2     0       0         0  ...        2     0      2      3\n## Normal      0     0       1         0  ...        3     0      0      1\n## Poison      1     5       4         0  ...        0     0      0      3\n## Psychic     0     1       1         0  ...        0     0      2      0\n## Rock        2     2       2         3  ...        2     0      4      6\n## Steel       0     0       2         0  ...        7     3      0      0\n## Water       2     7       3         2  ...        6     5      1      0\n## \n## [18 rows x 18 columns]\n\n\n\n\nFrequency Plots\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\n\nplot(table(poke$generation)) # bar plot\n\n\n\n\n\n\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(generation))) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\")\n\n\n\n\n\n\nWe generate a bar chart using the contingency table we generated earlier combined with matplotlib’s plt.bar().\n\nimport matplotlib.pyplot as plt\n\ntab = poke.groupby(['generation'])['generation'].count()\n\nplt.bar(tab.keys(), tab.values, color = 'grey')\n## &lt;BarContainer object of 8 artists&gt;\nplt.xlabel(\"Generation\")\nplt.ylabel(\"# Pokemon\")\nplt.show()\n\n\n\n\n\n\nPlotnine is a ggplot2 clone for python, and for the most part, the code looks almost exactly the same, minus a few python-specific tweaks to account for different syntax conventions in each language.\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nfrom plotnine import *\n\n(ggplot(aes(x = \"factor(generation)\"), data = poke) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\"))\n## &lt;ggplot: (8786353543605)&gt;\n\n\n\n\n\n\n\nQuantitative Variables\nWe covered some numerical summary statistics in the numerical summary statistic section above. In this section, we will primarily focus on visualization methods for assessing the distribution of quantitative variables.\n\n\n\n\n\n\nNote: R pipe\n\n\n\nThe code in this section uses the R pipe, %&gt;%. The left side of the pipe is passed as an argument to the right side. This makes code easier to read because it becomes a step-wise “recipe” instead of a nested mess of functions and parentheses.\n\n\nIn each step, the left hand side of the pipe is put into the first argument of the function. Source: Arthur Welle (Github)\n\n\n\nWe can generate histograms2 or kernel density plots (a continuous version of the histogram) to show us the distribution of a continuous variable.\n\n\nBase R\nPython: matplotlib\nR: ggplot2\nPython: plotnine\n\n\n\nBy default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desirable result. If we do not specify breaks, R will pick them for us.\n\nhist(poke$hp)\n\n\n\n\nFor continuous variables, we can use histograms, or we can examine kernel density plots.\n\nlibrary(magrittr) # This provides the pipe command, %&gt;%\n\nhist(poke$weight_kg)\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Take the log - will transformation be useful w/ modeling?\n  hist(main = \"Histogram of Log10 Weight (Kg)\") # create a histogram\n\npoke$weight_kg %&gt;%\n  density(na.rm = T) %&gt;% # First, we compute the kernel density\n  # (na.rm = T says to ignore NA values)\n  plot(main = \"Density of Weight (Kg)\") # Then, we plot the result\n\n\npoke$weight_kg %&gt;%\n  log10() %&gt;% # Transform the variable\n  density(na.rm = T) %&gt;% # Compute the density ignoring missing values\n  plot(main = \"Density of Log10 pokemon weight in Kg\") # Plot the result,\n    # changing the title of the plot to a meaningful value\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots with separate axes\n# This uses python multi-assignment to assign figures, axes\n# variables all in one go\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\npoke.weight_kg.plot.hist(ax = ax1) # first plot\nax1.set_title(\"Histogram of Weight (kg)\")\n\n\nnp.log10(poke.weight_kg).plot.hist(ax = ax2)\nax2.set_title(\"Histogram of Log10 Weight (kg)\")\n\npoke.weight_kg.plot.density(ax = ax3)\nax3.set_title(\"Density of Weight (kg)\")\n\nnp.log10(poke.weight_kg).plot.density(ax = ax4)\nax4.set_title(\"Density of Log10 Weight (kg)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\nggplot(poke, aes(x = height_m)) +\n  geom_density()\nggplot(poke, aes(x = height_m)) +\n  geom_density() +\n  scale_x_log10()\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\nggplot(poke, aes(x = 'height_m')) + geom_histogram(bins = 30)\n## &lt;ggplot: (8786353213892)&gt;\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_histogram(bins = 30) +\n  scale_x_log10())\n## &lt;ggplot: (8786353213730)&gt;\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density())\n## &lt;ggplot: (8786348196280)&gt;\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density() +\n  scale_x_log10())\n## &lt;ggplot: (8786404166720)&gt;\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\n\n7.2.3 Relationships Between Variables\nCategorical - Categorical Relationships\n\n\nR: ggplot2\nBase R\nPython: matplotlib\nPython: plotnine\n\n\n\nWe can generate a (simple) mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I’ve flipped the axes using coord_flip so that you can read the labels more easily.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip()\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\nggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +\n  # Shade tiles according to the number of things in the bin\n  geom_tile(aes(fill = ..count..), stat = \"bin2d\") +\n  # Add the number of things in the bin to the top of the tile as text\n  geom_text(aes(label = ..count..), stat = 'bin2d') +\n  # Scale the tile fill\n  scale_fill_gradient2(limits = c(0, 100), low = \"white\", high = \"blue\", na.value = \"white\")\n\n\n\n\n\n\nBase R mosaic plots aren’t nearly as pretty as the ggplot version, but I will at least show you how to create them.\n\nplot(table(poke$type_1, poke$type_2)) \n\n\n\n# mosaic plot - hard to read b/c too many categories\n\n\n\nTo get a mosaicplot, we need an additional library, called statsmodels, which we install with pip install statsmodels in the terminal.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(poke, ['type_1', 'type_2'], title = \"Pokemon Types\")\n## (&lt;Figure size 700x500 with 3 Axes&gt;, {('Grass', 'Poison'): (0.0, 0.0, 0.08162293604502865, 0.29574132492113564), ('Grass', 'Flying'): (0.0, 0.2988958990536278, 0.08162293604502865, 0.13801261829652994), ('Grass', 'Dragon'): (0.0, 0.4400630914826498, 0.08162293604502865, 0.09858044164037857), ('Grass', 'Normal'): (0.0, 0.5417981072555206, 0.08162293604502865, 0.0), ('Grass', 'Psychic'): (0.0, 0.5449526813880126, 0.08162293604502865, 0.03943217665615138), ('Grass', 'Steel'): (0.0, 0.5875394321766562, 0.08162293604502865, 0.05914826498422713), ('Grass', 'Ground'): (0.0, 0.6498422712933754, 0.08162293604502865, 0.019716088328075747), ('Grass', 'Fairy'): (0.0, 0.6727129337539433, 0.08162293604502865, 0.09858044164037852), ('Grass', 'Grass'): (0.0, 0.7744479495268138, 0.08162293604502865, 0.0), ('Grass', 'Fighting'): (0.0, 0.777602523659306, 0.08162293604502865, 0.05914826498422713), ('Grass', 'Electric'): (0.0, 0.8399053627760252, 0.08162293604502865, 0.0), ('Grass', 'Ice'): (0.0, 0.8430599369085173, 0.08162293604502865, 0.05914826498422713), ('Grass', 'Dark'): (0.0, 0.9053627760252366, 0.08162293604502865, 0.05914826498422713), ('Grass', 'Ghost'): (0.0, 0.9676656151419558, 0.08162293604502865, 0.019716088328075747), ('Grass', 'Rock'): (0.0, 0.9905362776025236, 0.08162293604502865, 0.0), ('Grass', 'Water'): (0.0, 0.9936908517350158, 0.08162293604502865, 0.0), ('Grass', 'Fire'): (0.0, 0.9968454258675078, 0.08162293604502865, 0.0), ('Grass', 'Bug'): (0.0, 1.0, 0.08162293604502865, 0.0), ('Fire', 'Poison'): (0.08623123097590422, 0.0, 0.05271481286241434, 0.0), ('Fire', 'Flying'): (0.08623123097590422, 0.003154574132492114, 0.05271481286241434, 0.213696957362369), ('Fire', 'Dragon'): (0.08623123097590422, 0.2200061056273532, 0.05271481286241434, 0.06105627353210545), ('Fire', 'Normal'): (0.08623123097590422, 0.2842169532919508, 0.05271481286241434, 0.06105627353210542), ('Fire', 'Psychic'): (0.08623123097590422, 0.3484278009565483, 0.05271481286241434, 0.06105627353210542), ('Fire', 'Steel'): (0.08623123097590422, 0.41263864862114585, 0.05271481286241434, 0.030528136766052684), ('Fire', 'Ground'): (0.08623123097590422, 0.44632135951969065, 0.05271481286241434, 0.09158441029815811), ('Fire', 'Fairy'): (0.08623123097590422, 0.5410603439503409, 0.05271481286241434, 0.0), ('Fire', 'Grass'): (0.08623123097590422, 0.544214918082833, 0.05271481286241434, 0.0), ('Fire', 'Fighting'): (0.08623123097590422, 0.5473694922153252, 0.05271481286241434, 0.21369695736236902), ('Fire', 'Electric'): (0.08623123097590422, 0.7642210237101862, 0.05271481286241434, 0.0), ('Fire', 'Ice'): (0.08623123097590422, 0.7673755978426783, 0.05271481286241434, 0.0), ('Fire', 'Dark'): (0.08623123097590422, 0.7705301719751705, 0.05271481286241434, 0.030528136766052684), ('Fire', 'Ghost'): (0.08623123097590422, 0.8042128828737153, 0.05271481286241434, 0.06105627353210548), ('Fire', 'Rock'): (0.08623123097590422, 0.8684237305383128, 0.05271481286241434, 0.030528136766052684), ('Fire', 'Water'): (0.08623123097590422, 0.9021064414368577, 0.05271481286241434, 0.030528136766052684), ('Fire', 'Fire'): (0.08623123097590422, 0.9357891523354025, 0.05271481286241434, 0.0), ('Fire', 'Bug'): (0.08623123097590422, 0.9389437264678945, 0.05271481286241434, 0.06105627353210548), ('Bug', 'Poison'): (0.14355433876919416, 0.0, 0.10542962572482865, 0.18316882059631628), ('Bug', 'Flying'): (0.14355433876919416, 0.18632339472880838, 0.10542962572482865, 0.21369695736236902), ('Bug', 'Dragon'): (0.14355433876919416, 0.4031749262236695, 0.10542962572482865, 0.0), ('Bug', 'Normal'): (0.14355433876919416, 0.4063295003561616, 0.10542962572482865, 0.0), ('Bug', 'Psychic'): (0.14355433876919416, 0.4094840744886537, 0.10542962572482865, 0.030528136766052684), ('Bug', 'Steel'): (0.14355433876919416, 0.4431667853871985, 0.10542962572482865, 0.10684847868118455), ('Bug', 'Ground'): (0.14355433876919416, 0.5531698382008752, 0.10542962572482865, 0.030528136766052684), ('Bug', 'Fairy'): (0.14355433876919416, 0.58685254909942, 0.10542962572482865, 0.030528136766052684), ('Bug', 'Grass'): (0.14355433876919416, 0.6205352599979648, 0.10542962572482865, 0.09158441029815816), ('Bug', 'Fighting'): (0.14355433876919416, 0.7152742444286151, 0.10542962572482865, 0.06105627353210537), ('Bug', 'Electric'): (0.14355433876919416, 0.7794850920932126, 0.10542962572482865, 0.06105627353210548), ('Bug', 'Ice'): (0.14355433876919416, 0.8436959397578101, 0.10542962572482865, 0.0), ('Bug', 'Dark'): (0.14355433876919416, 0.8468505138903023, 0.10542962572482865, 0.0), ('Bug', 'Ghost'): (0.14355433876919416, 0.8500050880227944, 0.10542962572482865, 0.015264068383026342), ('Bug', 'Rock'): (0.14355433876919416, 0.8684237305383128, 0.10542962572482865, 0.04579220514907903), ('Bug', 'Water'): (0.14355433876919416, 0.917370509819884, 0.10542962572482865, 0.045792205149079133), ('Bug', 'Fire'): (0.14355433876919416, 0.9663172891014551, 0.10542962572482865, 0.030528136766052684), ('Bug', 'Bug'): (0.14355433876919416, 1.0, 0.10542962572482865, 0.0), ('Normal', 'Poison'): (0.2535922594248984, 0.0, 0.07482102470794294, 0.0), ('Normal', 'Flying'): (0.2535922594248984, 0.003154574132492114, 0.07482102470794294, 0.5807284198451391), ('Normal', 'Dragon'): (0.2535922594248984, 0.5870375681101234, 0.07482102470794294, 0.021508459994264394), ('Normal', 'Normal'): (0.2535922594248984, 0.6117006022368798, 0.07482102470794294, 0.0), ('Normal', 'Psychic'): (0.2535922594248984, 0.6148551763693719, 0.07482102470794294, 0.06452537998279329), ('Normal', 'Steel'): (0.2535922594248984, 0.6825351304846574, 0.07482102470794294, 0.0), ('Normal', 'Ground'): (0.2535922594248984, 0.6856897046171495, 0.07482102470794294, 0.021508459994264394), ('Normal', 'Fairy'): (0.2535922594248984, 0.7103527387439059, 0.07482102470794294, 0.10754229997132206), ('Normal', 'Grass'): (0.2535922594248984, 0.8210496128477202, 0.07482102470794294, 0.04301691998852879), ('Normal', 'Fighting'): (0.2535922594248984, 0.8672211069687411, 0.07482102470794294, 0.08603383997705767), ('Normal', 'Electric'): (0.2535922594248984, 0.9564095210782909, 0.07482102470794294, 0.0), ('Normal', 'Ice'): (0.2535922594248984, 0.9595640952107829, 0.07482102470794294, 0.0), ('Normal', 'Dark'): (0.2535922594248984, 0.9627186693432751, 0.07482102470794294, 0.0), ('Normal', 'Ghost'): (0.2535922594248984, 0.9658732434757671, 0.07482102470794294, 0.0), ('Normal', 'Rock'): (0.2535922594248984, 0.9690278176082593, 0.07482102470794294, 0.0), ('Normal', 'Water'): (0.2535922594248984, 0.9721823917407515, 0.07482102470794294, 0.021508459994264394), ('Normal', 'Fire'): (0.2535922594248984, 0.9968454258675078, 0.07482102470794294, 0.0), ('Normal', 'Bug'): (0.2535922594248984, 1.0, 0.07482102470794294, 0.0), ('Dark', 'Poison'): (0.33302157906371693, 0.0, 0.05101433502814293, 0.0), ('Dark', 'Flying'): (0.33302157906371693, 0.003154574132492114, 0.05101433502814293, 0.15772870662460567), ('Dark', 'Dragon'): (0.33302157906371693, 0.16403785488958988, 0.05101433502814293, 0.12618296529968454), ('Dark', 'Normal'): (0.33302157906371693, 0.29337539432176657, 0.05101433502814293, 0.1577287066246057), ('Dark', 'Psychic'): (0.33302157906371693, 0.45425867507886436, 0.05101433502814293, 0.06309148264984225), ('Dark', 'Steel'): (0.33302157906371693, 0.5205047318611988, 0.05101433502814293, 0.06309148264984225), ('Dark', 'Ground'): (0.33302157906371693, 0.5867507886435331, 0.05101433502814293, 0.0), ('Dark', 'Fairy'): (0.33302157906371693, 0.5899053627760252, 0.05101433502814293, 0.09463722397476339), ('Dark', 'Grass'): (0.33302157906371693, 0.6876971608832807, 0.05101433502814293, 0.0), ('Dark', 'Fighting'): (0.33302157906371693, 0.6908517350157729, 0.05101433502814293, 0.06309148264984236), ('Dark', 'Electric'): (0.33302157906371693, 0.7570977917981073, 0.05101433502814293, 0.0), ('Dark', 'Ice'): (0.33302157906371693, 0.7602523659305994, 0.05101433502814293, 0.06309148264984225), ('Dark', 'Dark'): (0.33302157906371693, 0.8264984227129338, 0.05101433502814293, 0.0), ('Dark', 'Ghost'): (0.33302157906371693, 0.8296529968454259, 0.05101433502814293, 0.06309148264984225), ('Dark', 'Rock'): (0.33302157906371693, 0.8958990536277602, 0.05101433502814293, 0.0), ('Dark', 'Water'): (0.33302157906371693, 0.8990536277602524, 0.05101433502814293, 0.0), ('Dark', 'Fire'): (0.33302157906371693, 0.9022082018927445, 0.05101433502814293, 0.09463722397476339), ('Dark', 'Bug'): (0.33302157906371693, 1.0, 0.05101433502814293, 0.0), ('Electric', 'Poison'): (0.38864420902273544, 0.0, 0.04761337935960002, 0.1013970256872465), ('Electric', 'Flying'): (0.38864420902273544, 0.1045515998197386, 0.04761337935960002, 0.20279405137449302), ('Electric', 'Dragon'): (0.38864420902273544, 0.3105002253267238, 0.04761337935960002, 0.06759801712483098), ('Electric', 'Normal'): (0.38864420902273544, 0.38125281658404686, 0.04761337935960002, 0.06759801712483103), ('Electric', 'Psychic'): (0.38864420902273544, 0.45200540784137, 0.04761337935960002, 0.03379900856241549), ('Electric', 'Steel'): (0.38864420902273544, 0.48895899053627767, 0.04761337935960002, 0.13519603424966206), ('Electric', 'Ground'): (0.38864420902273544, 0.6273095989184319, 0.04761337935960002, 0.0), ('Electric', 'Fairy'): (0.38864420902273544, 0.6304641730509238, 0.04761337935960002, 0.06759801712483098), ('Electric', 'Grass'): (0.38864420902273544, 0.701216764308247, 0.04761337935960002, 0.03379900856241549), ('Electric', 'Fighting'): (0.38864420902273544, 0.7381703470031546, 0.04761337935960002, 0.0), ('Electric', 'Electric'): (0.38864420902273544, 0.7413249211356467, 0.04761337935960002, 0.0), ('Electric', 'Ice'): (0.38864420902273544, 0.7444794952681387, 0.04761337935960002, 0.06759801712483098), ('Electric', 'Dark'): (0.38864420902273544, 0.815232086525462, 0.04761337935960002, 0.06759801712483109), ('Electric', 'Ghost'): (0.38864420902273544, 0.8859846777827851, 0.04761337935960002, 0.03379900856241549), ('Electric', 'Rock'): (0.38864420902273544, 0.9229382604776927, 0.04761337935960002, 0.0), ('Electric', 'Water'): (0.38864420902273544, 0.9260928346101849, 0.04761337935960002, 0.03379900856241549), ('Electric', 'Fire'): (0.38864420902273544, 0.9630464173050924, 0.04761337935960002, 0.03379900856241549), ('Electric', 'Bug'): (0.38864420902273544, 1.0, 0.04761337935960002, 0.0), ('Ice', 'Poison'): (0.44086588331321097, 0.0, 0.028908123182614334, 0.0), ('Ice', 'Flying'): (0.44086588331321097, 0.003154574132492114, 0.028908123182614334, 0.11133791055854518), ('Ice', 'Dragon'): (0.44086588331321097, 0.11764705882352941, 0.028908123182614334, 0.0), ('Ice', 'Normal'): (0.44086588331321097, 0.12080163295602153, 0.028908123182614334, 0.0), ('Ice', 'Psychic'): (0.44086588331321097, 0.12395620708851364, 0.028908123182614334, 0.11133791055854518), ('Ice', 'Steel'): (0.44086588331321097, 0.23844869177955094, 0.028908123182614334, 0.11133791055854521), ('Ice', 'Ground'): (0.44086588331321097, 0.35294117647058826, 0.028908123182614334, 0.16700686583781776), ('Ice', 'Fairy'): (0.44086588331321097, 0.5231026164408982, 0.028908123182614334, 0.055668955279272604), ('Ice', 'Grass'): (0.44086588331321097, 0.5819261458526628, 0.028908123182614334, 0.0), ('Ice', 'Fighting'): (0.44086588331321097, 0.585080719985155, 0.028908123182614334, 0.0), ('Ice', 'Electric'): (0.44086588331321097, 0.5882352941176471, 0.028908123182614334, 0.0), ('Ice', 'Ice'): (0.44086588331321097, 0.5913898682501392, 0.028908123182614334, 0.0), ('Ice', 'Dark'): (0.44086588331321097, 0.5945444423826314, 0.028908123182614334, 0.0), ('Ice', 'Ghost'): (0.44086588331321097, 0.5976990165151235, 0.028908123182614334, 0.055668955279272604), ('Ice', 'Rock'): (0.44086588331321097, 0.6565225459268881, 0.028908123182614334, 0.0), ('Ice', 'Water'): (0.44086588331321097, 0.6596771200593803, 0.028908123182614334, 0.1670068658378177), ('Ice', 'Fire'): (0.44086588331321097, 0.8298385600296901, 0.028908123182614334, 0.055668955279272604), ('Ice', 'Bug'): (0.44086588331321097, 0.8886620894414548, 0.028908123182614334, 0.11133791055854521), ('Poison', 'Poison'): (0.4743823014267009, 0.0, 0.03911099018824293, 0.0), ('Poison', 'Flying'): (0.4743823014267009, 0.003154574132492114, 0.03911099018824293, 0.12343985735838706), ('Poison', 'Dragon'): (0.4743823014267009, 0.12974900562337127, 0.03911099018824293, 0.16458647647784944), ('Poison', 'Normal'): (0.4743823014267009, 0.29749005623371283, 0.03911099018824293, 0.0), ('Poison', 'Psychic'): (0.4743823014267009, 0.3006446303662049, 0.03911099018824293, 0.0), ('Poison', 'Steel'): (0.4743823014267009, 0.3037992044986971, 0.03911099018824293, 0.0), ('Poison', 'Ground'): (0.4743823014267009, 0.30695377863118917, 0.03911099018824293, 0.0822932382389247), ('Poison', 'Fairy'): (0.4743823014267009, 0.39240159100260597, 0.03911099018824293, 0.041146619119462324), ('Poison', 'Grass'): (0.4743823014267009, 0.43670278425456044, 0.03911099018824293, 0.0), ('Poison', 'Fighting'): (0.4743823014267009, 0.43985735838705253, 0.03911099018824293, 0.0822932382389247), ('Poison', 'Electric'): (0.4743823014267009, 0.5253051707584694, 0.03911099018824293, 0.0), ('Poison', 'Ice'): (0.4743823014267009, 0.5284597448909614, 0.03911099018824293, 0.0), ('Poison', 'Dark'): (0.4743823014267009, 0.5316143190234536, 0.03911099018824293, 0.20573309559731173), ('Poison', 'Ghost'): (0.4743823014267009, 0.7405019887532575, 0.03911099018824293, 0.0), ('Poison', 'Rock'): (0.4743823014267009, 0.7436565628857494, 0.03911099018824293, 0.0), ('Poison', 'Water'): (0.4743823014267009, 0.7468111370182416, 0.03911099018824293, 0.12343985735838708), ('Poison', 'Fire'): (0.4743823014267009, 0.8734055685091209, 0.03911099018824293, 0.08229323823892476), ('Poison', 'Bug'): (0.4743823014267009, 0.9588533808805376, 0.03911099018824293, 0.041146619119462324), ('Ground', 'Poison'): (0.5181015865458195, 0.0, 0.040811468022514286, 0.0), ('Ground', 'Flying'): (0.5181015865458195, 0.003154574132492114, 0.040811468022514286, 0.15772870662460567), ('Ground', 'Dragon'): (0.5181015865458195, 0.16403785488958988, 0.040811468022514286, 0.07886435331230285), ('Ground', 'Normal'): (0.5181015865458195, 0.24605678233438488, 0.040811468022514286, 0.0), ('Ground', 'Psychic'): (0.5181015865458195, 0.24921135646687695, 0.040811468022514286, 0.07886435331230282), ('Ground', 'Steel'): (0.5181015865458195, 0.3312302839116719, 0.040811468022514286, 0.1577287066246057), ('Ground', 'Ground'): (0.5181015865458195, 0.49211356466876977, 0.040811468022514286, 0.0), ('Ground', 'Fairy'): (0.5181015865458195, 0.4952681388012618, 0.040811468022514286, 0.0), ('Ground', 'Grass'): (0.5181015865458195, 0.4984227129337539, 0.040811468022514286, 0.0), ('Ground', 'Fighting'): (0.5181015865458195, 0.501577287066246, 0.040811468022514286, 0.0), ('Ground', 'Electric'): (0.5181015865458195, 0.5047318611987381, 0.040811468022514286, 0.03943217665615138), ('Ground', 'Ice'): (0.5181015865458195, 0.5473186119873816, 0.040811468022514286, 0.0), ('Ground', 'Dark'): (0.5181015865458195, 0.5504731861198738, 0.040811468022514286, 0.11829652996845426), ('Ground', 'Ghost'): (0.5181015865458195, 0.6719242902208201, 0.040811468022514286, 0.15772870662460575), ('Ground', 'Rock'): (0.5181015865458195, 0.832807570977918, 0.040811468022514286, 0.11829652996845426), ('Ground', 'Water'): (0.5181015865458195, 0.9542586750788643, 0.040811468022514286, 0.0), ('Ground', 'Fire'): (0.5181015865458195, 0.9574132492113565, 0.040811468022514286, 0.03943217665615138), ('Ground', 'Bug'): (0.5181015865458195, 1.0, 0.040811468022514286, 0.0), ('Water', 'Poison'): (0.5635213494992093, 0.0, 0.10542962572482868, 0.04579220514907907), ('Water', 'Flying'): (0.5635213494992093, 0.04894677928157118, 0.10542962572482868, 0.1068484786811845), ('Water', 'Dragon'): (0.5635213494992093, 0.15894983209524777, 0.10542962572482868, 0.04579220514907908), ('Water', 'Normal'): (0.5635213494992093, 0.207896611376819, 0.10542962572482868, 0.0), ('Water', 'Psychic'): (0.5635213494992093, 0.2110511855093111, 0.10542962572482868, 0.09158441029815814), ('Water', 'Steel'): (0.5635213494992093, 0.30579016993996133, 0.10542962572482868, 0.015264068383026342), ('Water', 'Ground'): (0.5635213494992093, 0.32420881245547983, 0.10542962572482868, 0.15264068383026358), ('Water', 'Fairy'): (0.5635213494992093, 0.4800040704182355, 0.10542962572482868, 0.06105627353210537), ('Water', 'Grass'): (0.5635213494992093, 0.544214918082833, 0.10542962572482868, 0.045792205149079133), ('Water', 'Fighting'): (0.5635213494992093, 0.5931616973644043, 0.10542962572482868, 0.04579220514907903), ('Water', 'Electric'): (0.5635213494992093, 0.6421084766459754, 0.10542962572482868, 0.030528136766052684), ('Water', 'Ice'): (0.5635213494992093, 0.6757911875445202, 0.10542962572482868, 0.06105627353210548), ('Water', 'Dark'): (0.5635213494992093, 0.7400020352091178, 0.10542962572482868, 0.10684847868118451), ('Water', 'Ghost'): (0.5635213494992093, 0.8500050880227944, 0.10542962572482868, 0.030528136766052684), ('Water', 'Rock'): (0.5635213494992093, 0.8836877989213392, 0.10542962572482868, 0.07632034191513182), ('Water', 'Water'): (0.5635213494992093, 0.9631627149689631, 0.10542962572482868, 0.0), ('Water', 'Fire'): (0.5635213494992093, 0.9663172891014551, 0.10542962572482868, 0.0), ('Water', 'Bug'): (0.5635213494992093, 0.9694718632339473, 0.10542962572482868, 0.030528136766052684), ('Rock', 'Poison'): (0.6735592701549136, 0.0, 0.07482102470794294, 0.02150845999426441), ('Rock', 'Flying'): (0.6735592701549136, 0.024663034126756526, 0.07482102470794294, 0.12905075996558646), ('Rock', 'Dragon'): (0.6735592701549136, 0.1568683682248351, 0.07482102470794294, 0.043016919988528836), ('Rock', 'Normal'): (0.6735592701549136, 0.20303986234585605, 0.07482102470794294, 0.0), ('Rock', 'Psychic'): (0.6735592701549136, 0.20619443647834818, 0.07482102470794294, 0.04301691998852881), ('Rock', 'Steel'): (0.6735592701549136, 0.25236593059936907, 0.07482102470794294, 0.08603383997705762), ('Rock', 'Ground'): (0.6735592701549136, 0.3415543447089188, 0.07482102470794294, 0.12905075996558651), ('Rock', 'Fairy'): (0.6735592701549136, 0.47375967880699743, 0.07482102470794294, 0.06452537998279317), ('Rock', 'Grass'): (0.6735592701549136, 0.5414396329222827, 0.07482102470794294, 0.04301691998852889), ('Rock', 'Fighting'): (0.6735592701549136, 0.5876111270433038, 0.07482102470794294, 0.021508459994264394), ('Rock', 'Electric'): (0.6735592701549136, 0.6122741611700603, 0.07482102470794294, 0.06452537998279317), ('Rock', 'Ice'): (0.6735592701549136, 0.6799541152853456, 0.07482102470794294, 0.04301691998852889), ('Rock', 'Dark'): (0.6735592701549136, 0.7261256094063666, 0.07482102470794294, 0.04301691998852879), ('Rock', 'Ghost'): (0.6735592701549136, 0.7722971035273875, 0.07482102470794294, 0.0), ('Rock', 'Rock'): (0.6735592701549136, 0.7754516776598795, 0.07482102470794294, 0.0), ('Rock', 'Water'): (0.6735592701549136, 0.7786062517923718, 0.07482102470794294, 0.12905075996558646), ('Rock', 'Fire'): (0.6735592701549136, 0.9108115858904502, 0.07482102470794294, 0.04301691998852889), ('Rock', 'Bug'): (0.6735592701549136, 0.9569830800114713, 0.07482102470794294, 0.04301691998852879), ('Psychic', 'Poison'): (0.7529885897937321, 0.0, 0.05441529069668575, 0.0), ('Psychic', 'Flying'): (0.7529885897937321, 0.003154574132492114, 0.05441529069668575, 0.20701892744479497), ('Psychic', 'Dragon'): (0.7529885897937321, 0.21332807570977919, 0.05441529069668575, 0.029574132492113565), ('Psychic', 'Normal'): (0.7529885897937321, 0.24605678233438488, 0.05441529069668575, 0.05914826498422713), ('Psychic', 'Psychic'): (0.7529885897937321, 0.30835962145110407, 0.05441529069668575, 0.0), ('Psychic', 'Steel'): (0.7529885897937321, 0.3115141955835962, 0.05441529069668575, 0.05914826498422713), ('Psychic', 'Ground'): (0.7529885897937321, 0.37381703470031546, 0.05441529069668575, 0.0), ('Psychic', 'Fairy'): (0.7529885897937321, 0.37697160883280756, 0.05441529069668575, 0.2661671924290221), ('Psychic', 'Grass'): (0.7529885897937321, 0.6462933753943217, 0.05441529069668575, 0.029574132492113565), ('Psychic', 'Fighting'): (0.7529885897937321, 0.6790220820189274, 0.05441529069668575, 0.0887223974763407), ('Psychic', 'Electric'): (0.7529885897937321, 0.7708990536277602, 0.05441529069668575, 0.0), ('Psychic', 'Ice'): (0.7529885897937321, 0.7740536277602523, 0.05441529069668575, 0.05914826498422713), ('Psychic', 'Dark'): (0.7529885897937321, 0.8363564668769716, 0.05441529069668575, 0.029574132492113565), ('Psychic', 'Ghost'): (0.7529885897937321, 0.8690851735015773, 0.05441529069668575, 0.0887223974763407), ('Psychic', 'Rock'): (0.7529885897937321, 0.9609621451104101, 0.05441529069668575, 0.0), ('Psychic', 'Water'): (0.7529885897937321, 0.9641167192429023, 0.05441529069668575, 0.0), ('Psychic', 'Fire'): (0.7529885897937321, 0.9672712933753943, 0.05441529069668575, 0.029574132492113565), ('Psychic', 'Bug'): (0.7529885897937321, 1.0, 0.05441529069668575, 0.0), ('Ghost', 'Poison'): (0.8120121754212933, 0.0, 0.04591290152532861, 0.14020329477742727), ('Ghost', 'Flying'): (0.8120121754212933, 0.14335786890991936, 0.04591290152532861, 0.10515247108307045), ('Ghost', 'Dragon'): (0.8120121754212933, 0.2516649141254819, 0.04591290152532861, 0.07010164738871363), ('Ghost', 'Normal'): (0.8120121754212933, 0.3249211356466877, 0.04591290152532861, 0.0), ('Ghost', 'Psychic'): (0.8120121754212933, 0.32807570977917977, 0.04591290152532861, 0.0), ('Ghost', 'Steel'): (0.8120121754212933, 0.3312302839116719, 0.04591290152532861, 0.0), ('Ghost', 'Ground'): (0.8120121754212933, 0.33438485804416407, 0.04591290152532861, 0.07010164738871363), ('Ghost', 'Fairy'): (0.8120121754212933, 0.40764107956536977, 0.04591290152532861, 0.03505082369435682), ('Ghost', 'Grass'): (0.8120121754212933, 0.4458464773922187, 0.04591290152532861, 0.38555906063792506), ('Ghost', 'Fighting'): (0.8120121754212933, 0.8345601121626359, 0.04591290152532861, 0.0), ('Ghost', 'Electric'): (0.8120121754212933, 0.8377146862951279, 0.04591290152532861, 0.0), ('Ghost', 'Ice'): (0.8120121754212933, 0.84086926042762, 0.04591290152532861, 0.0), ('Ghost', 'Dark'): (0.8120121754212933, 0.8440238345601122, 0.04591290152532861, 0.03505082369435676), ('Ghost', 'Ghost'): (0.8120121754212933, 0.882229232386961, 0.04591290152532861, 0.0), ('Ghost', 'Rock'): (0.8120121754212933, 0.8853838065194531, 0.04591290152532861, 0.0), ('Ghost', 'Water'): (0.8120121754212933, 0.8885383806519453, 0.04591290152532861, 0.0), ('Ghost', 'Fire'): (0.8120121754212933, 0.8916929547844374, 0.04591290152532861, 0.1051524710830705), ('Ghost', 'Bug'): (0.8120121754212933, 1.0, 0.04591290152532861, 0.0), ('Dragon', 'Poison'): (0.8625333718774976, 0.0, 0.04761337935960012, 0.0), ('Dragon', 'Flying'): (0.8625333718774976, 0.003154574132492114, 0.04761337935960012, 0.202794051374493), ('Dragon', 'Dragon'): (0.8625333718774976, 0.2091031996394772, 0.04761337935960012, 0.0), ('Dragon', 'Normal'): (0.8625333718774976, 0.21225777377196936, 0.04761337935960012, 0.0), ('Dragon', 'Psychic'): (0.8625333718774976, 0.21541234790446145, 0.04761337935960012, 0.13519603424966203), ('Dragon', 'Steel'): (0.8625333718774976, 0.3537629562866156, 0.04761337935960012, 0.0), ('Dragon', 'Ground'): (0.8625333718774976, 0.35691753041910773, 0.04761337935960012, 0.23659305993690846), ('Dragon', 'Fairy'): (0.8625333718774976, 0.5966651644885083, 0.04761337935960012, 0.03379900856241559), ('Dragon', 'Grass'): (0.8625333718774976, 0.6336187471834159, 0.04761337935960012, 0.0), ('Dragon', 'Fighting'): (0.8625333718774976, 0.6367733213159081, 0.04761337935960012, 0.06759801712483098), ('Dragon', 'Electric'): (0.8625333718774976, 0.7075259125732312, 0.04761337935960012, 0.03379900856241549), ('Dragon', 'Ice'): (0.8625333718774976, 0.7444794952681387, 0.04761337935960012, 0.10139702568724647), ('Dragon', 'Dark'): (0.8625333718774976, 0.8490310950878774, 0.04761337935960012, 0.0), ('Dragon', 'Ghost'): (0.8625333718774976, 0.8521856692203695, 0.04761337935960012, 0.10139702568724657), ('Dragon', 'Rock'): (0.8625333718774976, 0.9567372690401083, 0.04761337935960012, 0.0), ('Dragon', 'Water'): (0.8625333718774976, 0.9598918431726002, 0.04761337935960012, 0.0), ('Dragon', 'Fire'): (0.8625333718774976, 0.9630464173050924, 0.04761337935960012, 0.03379900856241549), ('Dragon', 'Bug'): (0.8625333718774976, 1.0, 0.04761337935960012, 0.0), ('Fairy', 'Poison'): (0.9147550461679732, 0.0, 0.0051014335028142215, 0.0), ('Fairy', 'Flying'): (0.9147550461679732, 0.003154574132492114, 0.0051014335028142215, 0.6309148264984227), ('Fairy', 'Dragon'): (0.9147550461679732, 0.637223974763407, 0.0051014335028142215, 0.0), ('Fairy', 'Normal'): (0.9147550461679732, 0.6403785488958991, 0.0051014335028142215, 0.0), ('Fairy', 'Psychic'): (0.9147550461679732, 0.6435331230283912, 0.0051014335028142215, 0.0), ('Fairy', 'Steel'): (0.9147550461679732, 0.6466876971608833, 0.0051014335028142215, 0.3154574132492114), ('Fairy', 'Ground'): (0.9147550461679732, 0.9652996845425867, 0.0051014335028142215, 0.0), ('Fairy', 'Fairy'): (0.9147550461679732, 0.968454258675079, 0.0051014335028142215, 0.0), ('Fairy', 'Grass'): (0.9147550461679732, 0.9716088328075709, 0.0051014335028142215, 0.0), ('Fairy', 'Fighting'): (0.9147550461679732, 0.9747634069400631, 0.0051014335028142215, 0.0), ('Fairy', 'Electric'): (0.9147550461679732, 0.9779179810725553, 0.0051014335028142215, 0.0), ('Fairy', 'Ice'): (0.9147550461679732, 0.9810725552050473, 0.0051014335028142215, 0.0), ('Fairy', 'Dark'): (0.9147550461679732, 0.9842271293375395, 0.0051014335028142215, 0.0), ('Fairy', 'Ghost'): (0.9147550461679732, 0.9873817034700316, 0.0051014335028142215, 0.0), ('Fairy', 'Rock'): (0.9147550461679732, 0.9905362776025236, 0.0051014335028142215, 0.0), ('Fairy', 'Water'): (0.9147550461679732, 0.9936908517350158, 0.0051014335028142215, 0.0), ('Fairy', 'Fire'): (0.9147550461679732, 0.9968454258675078, 0.0051014335028142215, 0.0), ('Fairy', 'Bug'): (0.9147550461679732, 1.0, 0.0051014335028142215, 0.0), ('Steel', 'Poison'): (0.924464774601663, 0.0, 0.0425119458567858, 0.0), ('Steel', 'Flying'): (0.924464774601663, 0.003154574132492114, 0.0425119458567858, 0.07570977917981073), ('Steel', 'Dragon'): (0.924464774601663, 0.08201892744479496, 0.0425119458567858, 0.07570977917981073), ('Steel', 'Normal'): (0.924464774601663, 0.16088328075709782, 0.0425119458567858, 0.0), ('Steel', 'Psychic'): (0.924464774601663, 0.1640378548895899, 0.0425119458567858, 0.26498422712933756), ('Steel', 'Steel'): (0.924464774601663, 0.4321766561514196, 0.0425119458567858, 0.0), ('Steel', 'Ground'): (0.924464774601663, 0.4353312302839117, 0.0425119458567858, 0.07570977917981074), ('Steel', 'Fairy'): (0.924464774601663, 0.5141955835962145, 0.0425119458567858, 0.15141955835962148), ('Steel', 'Grass'): (0.924464774601663, 0.6687697160883281, 0.0425119458567858, 0.0), ('Steel', 'Fighting'): (0.924464774601663, 0.6719242902208202, 0.0425119458567858, 0.037854889589905294), ('Steel', 'Electric'): (0.924464774601663, 0.7129337539432177, 0.0425119458567858, 0.0), ('Steel', 'Ice'): (0.924464774601663, 0.7160883280757098, 0.0425119458567858, 0.0), ('Steel', 'Dark'): (0.924464774601663, 0.7192429022082019, 0.0425119458567858, 0.0), ('Steel', 'Ghost'): (0.924464774601663, 0.722397476340694, 0.0425119458567858, 0.15141955835962148), ('Steel', 'Rock'): (0.924464774601663, 0.8769716088328076, 0.0425119458567858, 0.11356466876971609), ('Steel', 'Water'): (0.924464774601663, 0.9936908517350158, 0.0425119458567858, 0.0), ('Steel', 'Fire'): (0.924464774601663, 0.9968454258675078, 0.0425119458567858, 0.0), ('Steel', 'Bug'): (0.924464774601663, 1.0, 0.0425119458567858, 0.0), ('Fighting', 'Poison'): (0.9715850153893244, 0.0, 0.01700477834271428, 0.0), ('Fighting', 'Flying'): (0.9715850153893244, 0.003154574132492114, 0.01700477834271428, 0.09463722397476342), ('Fighting', 'Dragon'): (0.9715850153893244, 0.10094637223974764, 0.01700477834271428, 0.0), ('Fighting', 'Normal'): (0.9715850153893244, 0.10410094637223975, 0.01700477834271428, 0.0), ('Fighting', 'Psychic'): (0.9715850153893244, 0.10725552050473187, 0.01700477834271428, 0.2839116719242903), ('Fighting', 'Steel'): (0.9715850153893244, 0.39432176656151424, 0.01700477834271428, 0.2839116719242902), ('Fighting', 'Ground'): (0.9715850153893244, 0.6813880126182965, 0.01700477834271428, 0.0), ('Fighting', 'Fairy'): (0.9715850153893244, 0.6845425867507886, 0.01700477834271428, 0.0), ('Fighting', 'Grass'): (0.9715850153893244, 0.6876971608832807, 0.01700477834271428, 0.0), ('Fighting', 'Fighting'): (0.9715850153893244, 0.6908517350157729, 0.01700477834271428, 0.0), ('Fighting', 'Electric'): (0.9715850153893244, 0.694006309148265, 0.01700477834271428, 0.0), ('Fighting', 'Ice'): (0.9715850153893244, 0.697160883280757, 0.01700477834271428, 0.0946372239747635), ('Fighting', 'Dark'): (0.9715850153893244, 0.7949526813880127, 0.01700477834271428, 0.09463722397476339), ('Fighting', 'Ghost'): (0.9715850153893244, 0.8927444794952681, 0.01700477834271428, 0.09463722397476339), ('Fighting', 'Rock'): (0.9715850153893244, 0.9905362776025236, 0.01700477834271428, 0.0), ('Fighting', 'Water'): (0.9715850153893244, 0.9936908517350158, 0.01700477834271428, 0.0), ('Fighting', 'Fire'): (0.9715850153893244, 0.9968454258675078, 0.01700477834271428, 0.0), ('Fighting', 'Bug'): (0.9715850153893244, 1.0, 0.01700477834271428, 0.0), ('Flying', 'Poison'): (0.9931980886629144, 0.0, 0.006801911337085732, 0.0), ('Flying', 'Flying'): (0.9931980886629144, 0.003154574132492114, 0.006801911337085732, 0.0), ('Flying', 'Dragon'): (0.9931980886629144, 0.006309148264984228, 0.006801911337085732, 0.47318611987381703), ('Flying', 'Normal'): (0.9931980886629144, 0.48264984227129337, 0.006801911337085732, 0.0), ('Flying', 'Psychic'): (0.9931980886629144, 0.48580441640378547, 0.006801911337085732, 0.0), ('Flying', 'Steel'): (0.9931980886629144, 0.48895899053627767, 0.006801911337085732, 0.23659305993690852), ('Flying', 'Ground'): (0.9931980886629144, 0.7287066246056783, 0.006801911337085732, 0.0), ('Flying', 'Fairy'): (0.9931980886629144, 0.7318611987381703, 0.006801911337085732, 0.0), ('Flying', 'Grass'): (0.9931980886629144, 0.7350157728706624, 0.006801911337085732, 0.0), ('Flying', 'Fighting'): (0.9931980886629144, 0.7381703470031546, 0.006801911337085732, 0.0), ('Flying', 'Electric'): (0.9931980886629144, 0.7413249211356467, 0.006801911337085732, 0.0), ('Flying', 'Ice'): (0.9931980886629144, 0.7444794952681387, 0.006801911337085732, 0.0), ('Flying', 'Dark'): (0.9931980886629144, 0.7476340694006309, 0.006801911337085732, 0.0), ('Flying', 'Ghost'): (0.9931980886629144, 0.750788643533123, 0.006801911337085732, 0.0), ('Flying', 'Rock'): (0.9931980886629144, 0.7539432176656151, 0.006801911337085732, 0.0), ('Flying', 'Water'): (0.9931980886629144, 0.7570977917981073, 0.006801911337085732, 0.23659305993690852), ('Flying', 'Fire'): (0.9931980886629144, 0.9968454258675078, 0.006801911337085732, 0.0), ('Flying', 'Bug'): (0.9931980886629144, 1.0, 0.006801911337085732, 0.0)})\nplt.show()\n\n\n\n\nThis obviously needs a bit of cleaning up to remove extra labels, but it’s easy to get to and relatively functional. Notice that it does not, by default, show NA values.\n\n\nWe can generate a mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I’ve flipped the axes using coord_flip so that you can read the labels more easily.\n\n# Convert everything to categorical/factor variable ahead of time\n# this stops an error: TypeError: '&lt;' not supported between instances of 'float' and 'str'\npoke['type_1'] = pd.Categorical(poke['type_1'].astype(str))\npoke['type_2'] = pd.Categorical(poke['type_2'].astype(str))\n\n( ggplot(aes(x = 'type_1', fill = 'type_2'), data = poke) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip() +\n  # This says 85% of the plot is for the main plot and 15% is for the legend.\n  theme(subplots_adjust={'right':0.85})\n  )\n## &lt;ggplot: (8786341787983)&gt;\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\n(ggplot(aes(x = 'type_1', y = 'type_2'), data = poke) +\n  # Shade tiles according to the number of things in the bin\n  stat_bin2d(aes(fill = after_stat('count')), geom = 'tile'))\n## &lt;ggplot: (8786348198967)&gt;\n\n\n\n\n\n\n\nCategorical - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\nIn R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms.\nTo examine the relationship between a categorical variable and a continuous variable, we might look at box plots:\n\npar(mfrow = c(1, 2)) # put figures in same row\nboxplot(log10(height_m) ~ status, data = poke)\nboxplot(total_points ~ generation, data = poke)\n\n\n\n\n\n\n\nIn the second box plot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn’t usually about creating pretty plots (or we’d be using ggplot right now) but rather about identifying things which may come up in the analysis later.\n\n\n\nggplot(data = poke, aes(x = status, y = height_m)) + \n  geom_boxplot() + \n  scale_y_log10()\n\nggplot(data = poke, aes(x = factor(generation), y = total_points)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure()\n\n# Create a list of vectors of height_m by status\npoke['height_m_log'] = np.log(poke.height_m)\nheight_by_status = poke.groupby('status', group_keys = True).height_m_log.apply(list)\n\n# Plot each object in the list\nplt.boxplot(height_by_status, labels = height_by_status.index)\n## {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x7fdbac8b8670&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac8b8f40&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe5573ac0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe55730d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe555c940&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe555c5b0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac8590a0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac859a00&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x7fdbac8b8f10&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbb76518e0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe55737f0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe5573340&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe55623a0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac812ee0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac859ee0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbb768f8b0&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x7fdbac8b8430&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe5573490&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe555ceb0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac859cd0&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x7fdbb7651190&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe7fccd00&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac7f7190&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe54e5df0&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x7fdbe55209d0&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe555cf40&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbac859a90&gt;, &lt;matplotlib.lines.Line2D object at 0x7fdbe54e5520&gt;], 'means': []}\nplt.show()\n\n\n\n\nXXX TODO XXX\n\n\n\nggplot(aes(x = \"status\", y = \"height_m\"), data = poke) +\\\ngeom_boxplot()\n## &lt;ggplot: (8786341807632)&gt;\n\n\n\n\n\n\n\nYou can find more on boxplots and ways to customize boxplots in the Data Visualization chapter.\nContinuous - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: pandas\nPython: plotnine\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nplot(defense ~ attack, data = poke, type = \"p\")\n\n\n\n\n\n\n\ncor(poke$defense, poke$attack)\n## [1] 0.4507656\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset.\n\nplot(x = poke$base_experience, y = poke$base_friendship, type = \"p\")\n\n\n\n\nA scatterplot matrix can also be a useful way to visualize relationships between several variables.\n\npairs(poke[,16:20]) # hp - sp_defense columns\n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s more information on how to customize base R scatterplot matrices here.\n\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nlibrary(ggplot2)\nggplot(poke, aes(x = attack, y = defense)) + geom_point()\n\n\n\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset. When this happens, it can be a good idea to use geom_jitter to provide some “wiggle” in the data so that you can still see the point density. Changing the point transparency (alpha = .5) can also help with overplotting.\n\nggplot(poke, aes(x = base_experience, y = base_friendship)) + geom_point()\n\n\n\nggplot(poke, aes(x = base_experience, y = base_friendship)) + geom_jitter(alpha = 0.5)\n\n\n\n\n\nlibrary(GGally) # an extension to ggplot2\nggpairs(poke[,16:20], # hp - sp_defense columns\n        lower = list(continuous = wrap(\"points\", alpha = .15)),\n        progress = F) \n\n\n\nA scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon.\n\n\n\nggpairs can also handle continuous variables, if you want to explore the options available.\n\n\nBelieve it or not, you don’t have to go to matplotlib to get plots in python - you can get some plots from pandas directly, even if you are still using matplotlib under the hood (this is why you have to run plt.show() to get the plot to appear if you’re working in markdown).\n\nimport matplotlib.pyplot as plt\n\npoke.plot.scatter(x = 'attack', y = 'defense')\nplt.show()\n\n\n\n\nPandas also includes a nice scatterplot matrix method.\n\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\n\nscatter_matrix(poke.iloc[:,15:19], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')\n## array([[&lt;AxesSubplot:xlabel='hp', ylabel='hp'&gt;,\n##         &lt;AxesSubplot:xlabel='attack', ylabel='hp'&gt;,\n##         &lt;AxesSubplot:xlabel='defense', ylabel='hp'&gt;,\n##         &lt;AxesSubplot:xlabel='sp_attack', ylabel='hp'&gt;],\n##        [&lt;AxesSubplot:xlabel='hp', ylabel='attack'&gt;,\n##         &lt;AxesSubplot:xlabel='attack', ylabel='attack'&gt;,\n##         &lt;AxesSubplot:xlabel='defense', ylabel='attack'&gt;,\n##         &lt;AxesSubplot:xlabel='sp_attack', ylabel='attack'&gt;],\n##        [&lt;AxesSubplot:xlabel='hp', ylabel='defense'&gt;,\n##         &lt;AxesSubplot:xlabel='attack', ylabel='defense'&gt;,\n##         &lt;AxesSubplot:xlabel='defense', ylabel='defense'&gt;,\n##         &lt;AxesSubplot:xlabel='sp_attack', ylabel='defense'&gt;],\n##        [&lt;AxesSubplot:xlabel='hp', ylabel='sp_attack'&gt;,\n##         &lt;AxesSubplot:xlabel='attack', ylabel='sp_attack'&gt;,\n##         &lt;AxesSubplot:xlabel='defense', ylabel='sp_attack'&gt;,\n##         &lt;AxesSubplot:xlabel='sp_attack', ylabel='sp_attack'&gt;]],\n##       dtype=object)\nplt.show()\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(poke, aes(x = \"base_experience\", y = \"base_friendship\")) + geom_point()\n# jitter in plotnine seems to use width and height jointly instead of \n# marginally\n## &lt;ggplot: (8786341842800)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 104 rows containing missing values.\n\n\n\nggplot(poke, aes(x = \"base_experience\", y = \"base_friendship\")) + geom_jitter(alpha = 0.5, height = 5)\n## &lt;ggplot: (8786401256870)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_jitter : Removed 104 rows containing missing values.\n\n\n\n\nWhile plotnine doesn’t have scatterplot matrices by default, you can create them using some clever code. This is obviously not as fancy as ggpairs but it works well enough.\n\nfrom plotnine import *\nimport itertools\n\ndef plot_matrix(df, columns):\n  pdf = []\n  for a1, b1 in itertools.combinations(columns, 2):\n    for (a,b) in ((a1, b1), (b1, a1)):\n      sub = df[[a, b]].rename(columns={a: \"x\", b: \"y\"}).assign(a=a, b=b)\n      pdf.append(sub)\n  \n  g = ggplot(pd.concat(pdf))\n  g += geom_point(aes('x','y'))\n  g += facet_grid('b~a', scales='free')\n  return g\n\n\nplot_matrix(poke, poke.columns[15:19])\n## &lt;ggplot: (8786353282645)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n\n\n\n\n\n\n\nIf you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I’m cheating a bit by mentioning it now, but it’s just so useful!)\n\n\n\n\n\n\nTry it out: EDA\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nExplore the variables present in the police violence data.\nNote that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\n\n\nimport pandas as pd\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\n\n\n\npolice_violence$`Victim's age` &lt;- as.numeric(police_violence$`Victim's age`)\n\nskim(police_violence)\n\n\nData summary\n\n\nName\npolice_violence\n\n\nNumber of rows\n9942\n\n\nNumber of columns\n63\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n45\n\n\nnumeric\n17\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nVictim’s name\n0\n1.00\n7\n49\n0\n9662\n0\n\n\nVictim’s gender\n0\n1.00\n4\n11\n0\n4\n0\n\n\nVictim’s race\n0\n1.00\n5\n16\n0\n7\n0\n\n\nURL of image of victim\n4594\n0.54\n27\n10527\n0\n5330\n0\n\n\nStreet Address of Incident\n128\n0.99\n3\n73\n0\n9685\n0\n\n\nCity\n11\n1.00\n3\n29\n0\n3438\n0\n\n\nState\n0\n1.00\n2\n2\n0\n51\n0\n\n\nZipcode\n32\n1.00\n5\n8\n0\n7088\n0\n\n\nCounty\n5\n1.00\n3\n31\n0\n1218\n0\n\n\nAgency responsible for death\n2\n1.00\n7\n181\n0\n3480\n0\n\n\nORI Agency Identifier (if available)\n0\n1.00\n3\n77\n0\n3571\n0\n\n\nCause of death\n0\n1.00\n4\n39\n0\n31\n0\n\n\nA brief description of the circumstances surrounding the death\n20\n1.00\n30\n1631\n0\n9849\n0\n\n\nOfficial disposition of death (justified or other)\n11\n1.00\n7\n193\n0\n183\n0\n\n\nCriminal Charges?\n0\n1.00\n16\n77\n0\n47\n0\n\n\nLink to news article or photo of official document\n0\n1.00\n21\n312\n0\n9834\n0\n\n\nSymptoms of mental illness?\n63\n0.99\n2\n19\n0\n7\n0\n\n\nArmed/Unarmed Status\n0\n1.00\n7\n34\n0\n5\n0\n\n\nAlleged Weapon (Source: WaPo and Review of Cases Not Included in WaPo Database)\n0\n1.00\n2\n32\n0\n180\n0\n\n\nAlleged Threat Level (Source: WaPo)\n2380\n0.76\n4\n27\n0\n11\n0\n\n\nFleeing (Source: WaPo)\n3311\n0.67\n3\n11\n0\n12\n0\n\n\nBody Camera (Source: WaPo)\n3241\n0.67\n2\n18\n0\n10\n0\n\n\nOff-Duty Killing?\n9700\n0.02\n8\n8\n0\n3\n0\n\n\nGeography (via Trulia methodology based on zipcode population density: http://jedkolko.com/wp-content/uploads/2015/05/full-ZCTA-urban-suburban-rural-classification.xlsx )\n39\n1.00\n5\n12\n0\n4\n0\n\n\nEncounter Type (DRAFT)\n4308\n0.57\n5\n69\n0\n25\n0\n\n\nInitial Reported Reason for Encounter (DRAFT)\n4308\n0.57\n4\n135\n0\n1699\n0\n\n\nNames of Officers Involved (DRAFT)\n8115\n0.18\n5\n283\n0\n1798\n0\n\n\nRace of Officers Involved (DRAFT)\n9497\n0.04\n5\n119\n0\n70\n0\n\n\nKnown Past Shootings of Officer(s) (DRAFT)\n9823\n0.01\n2\n132\n0\n33\n0\n\n\nCall for Service? (DRAFT)\n5563\n0.44\n2\n11\n0\n4\n0\n\n\nHUD UPSAI Geography\n543\n0.95\n5\n12\n0\n4\n0\n\n\nNCHS Urban-Rural Classification Scheme Codes (https://www.cdc.gov/nchs/data_access/urban_rural.htm)\n438\n0.96\n12\n23\n0\n7\n0\n\n\nCongressional District\n89\n0.99\n3\n4\n0\n436\n0\n\n\nCurrent Representative Last name\n385\n0.96\n3\n17\n0\n396\n0\n\n\nCurrent Representative First name\n385\n0.96\n2\n11\n0\n276\n0\n\n\nCurrent Representative Party\n385\n0.96\n8\n10\n0\n2\n0\n\n\nOfficer Prosecuted by (Chief Prosecutor)\n9763\n0.02\n7\n77\n0\n139\n0\n\n\nProsecutor Race\n9763\n0.02\n5\n16\n0\n6\n0\n\n\nProsecutor Gender\n9763\n0.02\n4\n12\n0\n3\n0\n\n\nChief Prosecutor Political Party\n9887\n0.01\n10\n38\n0\n3\n0\n\n\nChief Prosecutor Term\n9892\n0.01\n5\n50\n0\n28\n0\n\n\nOfficer Prosecuted by (Prosecutor in Court)\n9918\n0.00\n10\n74\n0\n24\n0\n\n\nSpecial Prosecutor?\n9776\n0.02\n2\n119\n0\n10\n0\n\n\nIndependent Investigation?\n9897\n0.00\n2\n115\n0\n27\n0\n\n\nProsecutor Source Link\n9814\n0.01\n36\n286\n0\n124\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nVictim’s age\n393\n0.96\n36.85\n13.05\n1.00\n27.00\n35.00\n45.00\n107.00\n▂▇▃▁▁\n\n\nWaPo ID (If included in WaPo database)\n3377\n0.66\n3674.26\n2110.07\n3.00\n1849.00\n3644.00\n5424.00\n7468.00\n▇▇▇▇▇\n\n\nMPV ID\n15\n1.00\n4973.85\n2876.17\n1.00\n2485.50\n4967.00\n7452.50\n9987.00\n▇▇▇▇▇\n\n\nFatal Encounters ID\n230\n0.98\n21370.52\n5960.04\n116.00\n15962.75\n20876.50\n26624.25\n31497.00\n▁▁▇▃▆\n\n\nCensus Tract Code\n88\n0.99\n217720.56\n330317.48\n100.00\n5501.25\n30301.50\n313102.00\n989300.00\n▇▁▁▁▂\n\n\nMedian household income ACS Census Tract\n2649\n0.73\n55360.62\n30082.94\n0.00\n35915.00\n50551.00\n69100.00\n250001.00\n▇▇▁▁▁\n\n\nLatitude\n74\n0.99\n36.60\n5.36\n19.04\n33.45\n36.06\n40.01\n71.30\n▁▇▃▁▁\n\n\nLongitude\n74\n0.99\n-96.78\n16.60\n-163.17\n-112.07\n-93.59\n-82.90\n-67.26\n▁▁▇▇▇\n\n\nTotal Population of Census Tract 2019 ACS 5-Year Estimates\n2480\n0.75\n1884.14\n1794.80\n0.00\n1037.25\n1520.00\n2205.00\n58705.00\n▇▁▁▁▁\n\n\nWhite Non-Hispanic Percent of the Population ACS\n2480\n0.75\n0.66\n0.31\n0.00\n0.45\n0.77\n0.93\n1.00\n▂▂▂▃▇\n\n\nBlack Non-Hispanic Percent of the Population ACS\n2480\n0.75\n0.22\n0.29\n0.00\n0.01\n0.07\n0.33\n1.00\n▇▂▁▁▁\n\n\nNative American Percent of the Population ACS\n2480\n0.75\n0.01\n0.06\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nAsian Percent of the Population ACS\n2480\n0.75\n0.06\n0.13\n0.00\n0.00\n0.01\n0.06\n1.00\n▇▁▁▁▁\n\n\nPacific Islander Percent of the Population ACS\n2480\n0.75\n0.00\n0.02\n0.00\n0.00\n0.00\n0.00\n0.69\n▇▁▁▁▁\n\n\nOther/Two or More Race Percent of the Population ACS\n2480\n0.75\n0.04\n0.06\n0.00\n0.00\n0.02\n0.05\n1.00\n▇▁▁▁▁\n\n\nHispanic Percent of the Population ACS\n2480\n0.75\n0.22\n0.26\n0.00\n0.03\n0.11\n0.33\n1.00\n▇▂▁▁▁\n\n\nKilled by Police 2013-21\n43\n1.00\n1.00\n0.00\n1.00\n1.00\n1.00\n1.00\n1.00\n▁▁▇▁▁\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nDate of Incident (month/day/year)\n0\n1\n2013-01-01\n2022-01-20\n2017-08-02\n3118\n\n\n\n\nLet’s examine the numeric and date variables first:\n\nhist(police_violence$`Victim's age`)\n\n\n\n\n# hist(police_violence$`Date of Incident (month/day/year)`)\n# This didn't work - it wants me to specify breaks\n\n# Instead, lets see if ggplot handles it better - from R4DS\nlibrary(ggplot2)\nggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) +\n  geom_histogram()\n\n\n\nggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) +\n  geom_density()\n\n\n\n\nLet’s look at the victims’ gender and race:\n\ntable(police_violence$`Victim's race`, useNA = 'ifany')\n## \n##            Asian            Black         Hispanic  Native American \n##              142             2498             1777              139 \n## Pacific Islander     Unknown race            White \n##               60              976             4350\ntable(police_violence$`Victim's gender`)\n## \n##      Female        Male Transgender     Unknown \n##         496        9419          10          17\ntable(police_violence$`Victim's race`, police_violence$`Victim's gender`)\n##                   \n##                    Female Male Transgender Unknown\n##   Asian                 9  132           0       1\n##   Black                82 2409           4       3\n##   Hispanic             67 1707           1       2\n##   Native American       8  130           0       1\n##   Pacific Islander      2   58           0       0\n##   Unknown race         37  931           0       8\n##   White               291 4052           5       2\n\nplot(table(police_violence$`Victim's race`, police_violence$`Victim's gender`),\n     main = \"Police Killing by Race, Gender\")\n\n\n\n\nWe can also look at the age range for each race:\n\npolice_violence %&gt;%\n  # get groups with at least 100 observations that aren't unknown\n  subset(`Victim's race` %in% c(\"Asian\", \"Black\", \"Native American\", \"Hispanic\", \"White\")) %&gt;%\n  boxplot(`Victim's age` ~ `Victim's race`, data = .)\n\n\n\n\nAnd examine the age range for each gender as well:\n\npolice_violence %&gt;%\n  boxplot(`Victim's age` ~ `Victim's gender`, data = .)\n\n\n\n\nThe thing I’m honestly most surprised at with this plot is that there are so many elderly individuals (of both genders) shot. That’s not a realization I’d normally construct this plot for, but the visual emphasis on the outliers in a boxplot makes it much easier to focus on that aspect of the data.\n\n\n\npolice_violence[\"Victim's age\"] = pd.to_numeric(police_violence[\"Victim's age\"], errors = 'coerce')\n\n# police_violence.describe()\nskim(police_violence)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 9942   │ │ object      │ 44    │                       │\n## │ │ Number of columns │ 63     │ │ float64     │ 18    │                       │\n## │ └───────────────────┴────────┘ │ datetime64  │ 1     │                       │\n## │                                └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┓  │\n## │ ┃      ┃ missi ┃ comp ┃ mean  ┃ sd   ┃ p0   ┃ p25   ┃ p75  ┃ p100  ┃ hist ┃  │\n## │ ┃      ┃ ng    ┃ lete ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┃      ┃       ┃      ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┃      ┃       ┃ rate ┃       ┃      ┃      ┃       ┃      ┃       ┃      ┃  │\n## │ ┡━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━┩  │\n## │ │ Vict │   390 │ 0.96 │    37 │   13 │    1 │    27 │   45 │   110 │ ▁█▅▂ │  │\n## │ │ im's │       │      │       │      │      │       │      │       │      │  │\n## │ │  age │       │      │       │      │      │       │      │       │      │  │\n## │ │ Zipc │    32 │    1 │ 60000 │ 2800 │ 1100 │ 34000 │ 8600 │ 33000 │  ▆█  │  │\n## │ │ ode  │       │      │       │    0 │      │       │    0 │     0 │      │  │\n## │ │ WaPo │  3400 │ 0.66 │  3700 │ 2100 │    3 │  1800 │ 5400 │  7500 │ ████ │  │\n## │ │ ID   │       │      │       │      │      │       │      │       │  ▇▇  │  │\n## │ │ (If  │       │      │       │      │      │       │      │       │      │  │\n## │ │ incl │       │      │       │      │      │       │      │       │      │  │\n## │ │ uded │       │      │       │      │      │       │      │       │      │  │\n## │ │ MPV  │    15 │    1 │  5000 │ 2900 │    1 │  2500 │ 7500 │ 10000 │ ████ │  │\n## │ │ ID   │       │      │       │      │      │       │      │       │  ██  │  │\n## │ │ Fata │   230 │ 0.98 │ 21000 │ 6000 │  120 │ 16000 │ 2700 │ 31000 │   ▇█ │  │\n## │ │ l En │       │      │       │      │      │       │    0 │       │  ▆█  │  │\n## │ │ coun │       │      │       │      │      │       │      │       │      │  │\n## │ │ ters │       │      │       │      │      │       │      │       │      │  │\n## │ │  ID  │       │      │       │      │      │       │      │       │      │  │\n## │ │ Cens │    88 │ 0.99 │ 22000 │ 3300 │  100 │  5500 │ 3100 │ 99000 │ █▁▁▁ │  │\n## │ │ us T │       │      │     0 │   00 │      │       │   00 │     0 │  ▁   │  │\n## │ │ ract │       │      │       │      │      │       │      │       │      │  │\n## │ │      │       │      │       │      │      │       │      │       │      │  │\n## │ │ Code │       │      │       │      │      │       │      │       │      │  │\n## │ │ Medi │  2600 │ 0.73 │ 55000 │ 3000 │    0 │ 36000 │ 6900 │ 25000 │ ▆█▂  │  │\n## │ │ an h │       │      │       │    0 │      │       │    0 │     0 │      │  │\n## │ │ ouse │       │      │       │      │      │       │      │       │      │  │\n## │ │ hold │       │      │       │      │      │       │      │       │      │  │\n## │ │  inc │       │      │       │      │      │       │      │       │      │  │\n## │ │ Lati │    74 │ 0.99 │    37 │  5.4 │   19 │    33 │   40 │    71 │ ▁█▇▁ │  │\n## │ │ tude │       │      │       │      │      │       │      │       │      │  │\n## │ │ Long │    74 │ 0.99 │   -97 │   17 │ -160 │  -110 │  -83 │   -67 │   ▄▃ │  │\n## │ │ itud │       │      │       │      │      │       │      │       │  █▆  │  │\n## │ │ e    │       │      │       │      │      │       │      │       │      │  │\n## │ │ Tota │  2500 │ 0.75 │  1900 │ 1800 │    0 │  1000 │ 2200 │ 59000 │  █   │  │\n## │ │ l Po │       │      │       │      │      │       │      │       │      │  │\n## │ │ pula │       │      │       │      │      │       │      │       │      │  │\n## │ │ tion │       │      │       │      │      │       │      │       │      │  │\n## │ │  of  │       │      │       │      │      │       │      │       │      │  │\n## │ │ Whit │  2500 │ 0.75 │  0.66 │ 0.31 │    0 │  0.45 │ 0.93 │     1 │ ▂▁▂▂ │  │\n## │ │ e No │       │      │       │      │      │       │      │       │  ▄█  │  │\n## │ │ n-Hi │       │      │       │      │      │       │      │       │      │  │\n## │ │ span │       │      │       │      │      │       │      │       │      │  │\n## │ │ ic P │       │      │       │      │      │       │      │       │      │  │\n## │ │ Blac │  2500 │ 0.75 │  0.22 │ 0.29 │    0 │ 0.006 │ 0.33 │     1 │ █▂▁▁ │  │\n## │ │ k No │       │      │       │      │      │       │      │       │  ▁▁  │  │\n## │ │ n-Hi │       │      │       │      │      │       │      │       │      │  │\n## │ │ span │       │      │       │      │      │       │      │       │      │  │\n## │ │ ic P │       │      │       │      │      │       │      │       │      │  │\n## │ │ Nati │  2500 │ 0.75 │ 0.014 │ 0.06 │    0 │     0 │ 0.00 │     1 │  █   │  │\n## │ │ ve A │       │      │       │    3 │      │       │    2 │       │      │  │\n## │ │ meri │       │      │       │      │      │       │      │       │      │  │\n## │ │ can  │       │      │       │      │      │       │      │       │      │  │\n## │ │ Perc │       │      │       │      │      │       │      │       │      │  │\n## │ │ Asia │  2500 │ 0.75 │ 0.061 │ 0.13 │    0 │     0 │ 0.05 │     1 │  █▁  │  │\n## │ │ n Pe │       │      │       │      │      │       │    8 │       │      │  │\n## │ │ rcen │       │      │       │      │      │       │      │       │      │  │\n## │ │ t of │       │      │       │      │      │       │      │       │      │  │\n## │ │ the  │       │      │       │      │      │       │      │       │      │  │\n## │ │ Paci │  2500 │ 0.75 │ 0.003 │ 0.02 │    0 │     0 │    0 │  0.69 │  █   │  │\n## │ │ fic  │       │      │     4 │    5 │      │       │      │       │      │  │\n## │ │ Isla │       │      │       │      │      │       │      │       │      │  │\n## │ │ nder │       │      │       │      │      │       │      │       │      │  │\n## │ │  Per │       │      │       │      │      │       │      │       │      │  │\n## │ │ Othe │  2500 │ 0.75 │ 0.037 │ 0.05 │    0 │     0 │ 0.05 │     1 │  █   │  │\n## │ │ r/Tw │       │      │       │    6 │      │       │    1 │       │      │  │\n## │ │ o or │       │      │       │      │      │       │      │       │      │  │\n## │ │ More │       │      │       │      │      │       │      │       │      │  │\n## │ │ Ra   │       │      │       │      │      │       │      │       │      │  │\n## │ │ Hisp │  2500 │ 0.75 │  0.22 │ 0.26 │    0 │ 0.025 │ 0.33 │     1 │ █▂▁▁ │  │\n## │ │ anic │       │      │       │      │      │       │      │       │  ▁▁  │  │\n## │ │  Per │       │      │       │      │      │       │      │       │      │  │\n## │ │ cent │       │      │       │      │      │       │      │       │      │  │\n## │ │  of  │       │      │       │      │      │       │      │       │      │  │\n## │ │ Kill │    43 │    1 │     1 │    0 │    1 │     1 │    1 │     1 │    █ │  │\n## │ │ ed   │       │      │       │      │      │       │      │       │      │  │\n## │ │ by P │       │      │       │      │      │       │      │       │      │  │\n## │ │ olic │       │      │       │      │      │       │      │       │      │  │\n## │ │ e    │       │      │       │      │      │       │      │       │      │  │\n## │ │ 201  │       │      │       │      │      │       │      │       │      │  │\n## │ └──────┴───────┴──────┴───────┴──────┴──────┴───────┴──────┴───────┴──────┘  │\n## │                                  datetime                                    │\n## │ ┏━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┓  │\n## │ ┃            ┃ missing ┃ complete   ┃ first      ┃ last       ┃ frequency ┃  │\n## │ ┃            ┃         ┃ rate       ┃            ┃            ┃           ┃  │\n## │ ┡━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━┩  │\n## │ │ Date of    │       0 │          1 │ 2013-01-01 │ 2022-01-20 │ None      │  │\n## │ │ Incident   │         │            │            │            │           │  │\n## │ │ (mo        │         │            │            │            │           │  │\n## │ └────────────┴─────────┴────────────┴────────────┴────────────┴───────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\nLet’s examine the numeric and date variables first:\n\npolice_violence[\"Victim's age\"].plot.hist()\nplt.show()\n\n\n\n(ggplot(police_violence, aes(x = \"Date of Incident (month/day/year)\")) +\n  geom_histogram())\n## &lt;ggplot: (8786342179819)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 22'. Pick better value with 'binwidth'.\n\n\n\n(ggplot(police_violence, aes(x = \"Date of Incident (month/day/year)\")) +\n  geom_density())\n## &lt;ggplot: (8786459493648)&gt;\n\n\n\n\nLet’s look at the victims’ gender and race:\n\npolice_violence[\"Victim's race\"].groupby(police_violence[\"Victim's race\"]).count()\n## Victim's race\n## Asian                142\n## Black               2498\n## Hispanic            1777\n## Native American      139\n## Pacific Islander      60\n## Unknown race         976\n## White               4350\n## Name: Victim's race, dtype: int64\npolice_violence[\"Victim's gender\"].groupby(police_violence[\"Victim's gender\"]).count()\n## Victim's gender\n## Female          496\n## Male           9418\n## Male              1\n## Transgender      10\n## Unknown          17\n## Name: Victim's gender, dtype: int64\npd.crosstab(index = police_violence[\"Victim's race\"], columns = police_violence[\"Victim's gender\"])\n\n## Victim's gender   Female  Male  Male   Transgender  Unknown\n## Victim's race                                              \n## Asian                  9   132      0            0        1\n## Black                 82  2409      0            4        3\n## Hispanic              67  1707      0            1        2\n## Native American        8   130      0            0        1\n## Pacific Islander       2    58      0            0        0\n## Unknown race          37   930      1            0        8\n## White                291  4052      0            5        2\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(police_violence, [\"Victim's race\", \"Victim's gender\"], title = \"Police Killing by Race, Gender\")\n## (&lt;Figure size 700x500 with 3 Axes&gt;, {('Unknown race', 'Male'): (0.0, 0.0, 0.09531008001749958, 0.9403311044003453), ('Unknown race', 'Unknown'): (0.0, 0.9436205780845558, 0.09531008001749958, 0.008088869715271797), ('Unknown race', 'Female'): (0.0, 0.9549989214840382, 0.09531008001749958, 0.03741102243313199), ('Unknown race', 'Transgender'): (0.0, 0.9956994176013806, 0.09531008001749958, 0.0), ('Unknown race', 'Male '): (0.0, 0.9989888912855911, 0.09531008001749958, 0.0010111087144089607), ('White', 'Male'): (0.10016444894953842, 0.0, 0.4247939017173393, 0.9192377495462795), ('White', 'Unknown'): (0.10016444894953842, 0.92252722323049, 0.4247939017173393, 0.00045372050816691156), ('White', 'Female'): (0.10016444894953842, 0.9262704174228674, 0.4247939017173393, 0.06601633393829408), ('White', 'Transgender'): (0.10016444894953842, 0.995576225045372, 0.4247939017173393, 0.0011343012704173885), ('White', 'Male '): (0.10016444894953842, 1.0, 0.4247939017173393, 0.0), ('Hispanic', 'Male'): (0.5298127195989165, 0.0, 0.17353075019579586, 0.9479681900305067), ('Hispanic', 'Unknown'): (0.5298127195989165, 0.9512576637147172, 0.17353075019579586, 0.0011106832923614344), ('Hispanic', 'Female'): (0.5298127195989165, 0.9556578206912892, 0.17353075019579586, 0.03720789029410893), ('Hispanic', 'Transgender'): (0.5298127195989165, 0.9961551846696086, 0.17353075019579586, 0.0005553416461807172), ('Hispanic', 'Male '): (0.5298127195989165, 1.0, 0.17353075019579586, 0.0), ('Black', 'Male'): (0.7081978387267512, 0.0, 0.24393911873331345, 0.9516823985504191), ('Black', 'Unknown'): (0.7081978387267512, 0.9549718722346296, 0.24393911873331345, 0.0011851586532384652), ('Black', 'Female'): (0.7081978387267512, 0.9594465045720787, 0.24393911873331345, 0.032394336521849046), ('Black', 'Transgender'): (0.7081978387267512, 0.9951303147781383, 0.24393911873331345, 0.0015802115376511408), ('Black', 'Male '): (0.7081978387267512, 1.0, 0.24393911873331345, 0.0), ('Pacific Islander', 'Male'): (0.9569913263921035, 0.0, 0.0058592262305839655, 0.9539473684210525), ('Pacific Islander', 'Unknown'): (0.9569913263921035, 0.957236842105263, 0.0058592262305839655, 0.0), ('Pacific Islander', 'Female'): (0.9569913263921035, 0.9605263157894737, 0.0058592262305839655, 0.032894736842105254), ('Pacific Islander', 'Transgender'): (0.9569913263921035, 0.9967105263157894, 0.0058592262305839655, 0.0), ('Pacific Islander', 'Male '): (0.9569913263921035, 1.0, 0.0058592262305839655, 0.0), ('Native American', 'Male'): (0.9677049215547263, 0.0, 0.013573874100852925, 0.9229458538432411), ('Native American', 'Unknown'): (0.9677049215547263, 0.9262353275274515, 0.013573874100852925, 0.00709958349110192), ('Native American', 'Female'): (0.9677049215547263, 0.9366243847027641, 0.013573874100852925, 0.05679666792881481), ('Native American', 'Transgender'): (0.9677049215547263, 0.9967105263157894, 0.013573874100852925, 0.0), ('Native American', 'Male '): (0.9677049215547263, 1.0, 0.013573874100852925, 0.0), ('Asian', 'Male'): (0.986133164587618, 0.0, 0.013866835412382128, 0.9173461823573016), ('Asian', 'Unknown'): (0.986133164587618, 0.9206356560415121, 0.013866835412382128, 0.0069495922905856095), ('Asian', 'Female'): (0.986133164587618, 0.9308747220163083, 0.013866835412382128, 0.0625463306152706), ('Asian', 'Transgender'): (0.986133164587618, 0.9967105263157894, 0.013866835412382128, 0.0), ('Asian', 'Male '): (0.986133164587618, 1.0, 0.013866835412382128, 0.0)})\nplt.show()\n\n\n\n\nWe can also look at the age range for each race:\n\n# get groups with at least 100 observations that aren't unknown\nrace_sub = [\"Asian\", \"Black\", \"Native American\", \"Hispanic\", \"White\"]\n\npolice_sub = police_violence.loc[police_violence[\"Victim's race\"].isin(race_sub)]\npolice_sub = police_sub.assign(v_race = pd.Categorical(police_sub[\"Victim's race\"], categories = race_sub))\n\npolice_sub.boxplot(\"Victim's age\", by = \"v_race\")\nplt.show()\n\n\n\n\nAnd examine the age range for each gender as well:\n\npolice_violence.boxplot(\"Victim's age\", \"Victim's gender\")\nplt.show()\n\nThe thing I’m honestly most surprised at with this plot is that there are so many elderly individuals (of both genders) shot. That’s not a realization I’d normally construct this plot for, but the visual emphasis on the outliers in a boxplot makes it much easier to focus on that aspect of the data.\n\n\n\n\n\n\n\n\n\n\n\nLearn More: Janitor R package\n\n\n\nThe janitor package [4] has some very convenient functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing.\n\n\njanitor and clean_names() by Allison Horst"
  },
  {
    "objectID": "exploratory-data-analysis.html#references",
    "href": "exploratory-data-analysis.html#references",
    "title": "7  Exploratory Data Analysis",
    "section": "\n7.3 References",
    "text": "7.3 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nN. Tierney, D. Cook, M. McBain, and C. Fay, Naniar: Data structures, summaries, and visualisations for missing data. 2021 [Online]. Available: https://CRAN.R-project.org/package=naniar\n\n\n\n[3] \nDaniel Bourke, “A Gentle Introduction to Exploratory Data Analysis,” Daniel Bourke. Jan. 2019 [Online]. Available: https://www.mrdbourke.com/a-gentle-introduction-to-exploratory-data-analysis/. [Accessed: Jun. 13, 2022]\n\n\n[4] \nS. Firke, Janitor: Simple tools for examining and cleaning dirty data. 2021 [Online]. Available: https://CRAN.R-project.org/package=janitor"
  },
  {
    "objectID": "exploratory-data-analysis.html#footnotes",
    "href": "exploratory-data-analysis.html#footnotes",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "One package for this process in R is naniar [2].↩︎\nA histogram is a chart which breaks up a continuous variable into ranges, where the height of the bar is proportional to the number of items in the range. A bar chart is similar, but shows the number of occurrences of a discrete variable.↩︎"
  },
  {
    "objectID": "reading-data.html#module7-objectives",
    "href": "reading-data.html#module7-objectives",
    "title": "8  Loading External Data",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nRead in data from common formats into R or Python\nIdentify delimiters, headers, and other essential components of files\n\nIt may be helpful to print out this data import with the tidyverse cheat sheet for your work in R in this module."
  },
  {
    "objectID": "reading-data.html#an-overview-of-external-data-formats",
    "href": "reading-data.html#an-overview-of-external-data-formats",
    "title": "8  Loading External Data",
    "section": "\n8.1 An Overview of External Data Formats",
    "text": "8.1 An Overview of External Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not completely binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there’s this amazing comic:\n\n\n\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs. To read data from R into SAS, the easiest way is probably to call R from PROC IML.\n\nHere is a very thorough explanation of why binary file formats exist, and why they’re not necessarily optimal.\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which may be more common:\n\nWeb related data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis.\nSpatial files: Shapefiles are by far the most common version of spatial files1. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. We’ll explore these a bit more when we talk about maps.\n\nTo be minimally functional in R and Python, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly cover binary files and databases, but it is less critical to remember how to read these in without consulting one or more online references."
  },
  {
    "objectID": "reading-data.html#text-files",
    "href": "reading-data.html#text-files",
    "title": "8  Loading External Data",
    "section": "\n8.2 Text Files",
    "text": "8.2 Text Files\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say?\n\n8.2.1 Fixed-width files\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory).\n\n\nBase R\nreadr\nPython\nSAS\n\n\n\nIn base R (no extra packages), you can read fwf files in using read.fwf, but you must specify the column breaks yourself, which can be painful.\n\n## url &lt;- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\ndata &lt;- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you.\n\n\n\n\n\n\nCode for counting field width\n\n\n\n\n\n\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp &lt;- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars &lt;- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars &lt;- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space &lt;- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space &lt;- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks &lt;- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths &lt;- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata &lt;- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names &lt;- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) &lt;- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n\n\n\n\nYou can also write fixed-width files if you really want to:\n\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\n\nlibrary(gdata)\n\nwrite.fwf(mtcars, file = \"data/04_mtcars-fixed-width.txt\")\n\n\n\nThe readr package creates data-frame like objects called tibbles (a souped-up data frame), but it is much friendlier to use.\n\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 × 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # … with 110 more rows, and 12 more variables: PRES &lt;dbl&gt;, SRAD &lt;dbl&gt;,\n## #   TA9M &lt;dbl&gt;, WS2M &lt;dbl&gt;, TS10 &lt;dbl&gt;, TB10 &lt;dbl&gt;, TS05 &lt;dbl&gt;, TS25 &lt;dbl&gt;,\n## #   TS60 &lt;dbl&gt;, TR05 &lt;dbl&gt;, TR25 &lt;dbl&gt;, TR60 &lt;dbl&gt;\n## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n\nBy default, pandas’ read_fwf will guess at the format of your fixed-width file.\n\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\n\n\n\nIn SAS, it’s a bit more complicated, but not that much - the biggest difference is that you generally have to specify the column names for SAS. For complicated data, as in R, you may also have to specify the column widths.\n/* This downloads the file to my machine */\n/* x \"curl https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/ \n&gt; data/mesodata.txt\" */\n/* only run this once */\n\n/* Specifying WORK.mesodata means the dataset will cease to exist after this chunk exits */\ndata WORK.mesodata;\n\ninfile  \"data/mesodata.txt\" firstobs = 4; \n/* Skip the first 3 rows */\n  length STID $ 4; /* define ID length */\n  input STID $ STNM TIME RELH TAIR \n        WSPD WVEC WDIR WDSD WSSD WMAX \n        RAIN PRES SRAD TA9M WS2M TS10 \n        TB10 TS05 TS25 TS60 TR05 TR25 TR60;\nrun;\n\nproc print data=mesodata (obs=10); /* print the first 10 observations */\n  run;\nIn SAS data statements, you generally need to specify the data names explicitly.\nIn theory you can also get SAS to write out a fixed-width file, but it’s much easier to just… not. You can generally use a CSV or format of your choice – and you should definitely do that, because delimited files are much easier to work with.\n\n\n\n\n8.2.2 Delimited Text Files\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file.\n\n\nBase R\nreadr\nPython\nSAS\n\n\n\n\nurl &lt;- \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info &lt;- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n##   X pokedex_number          name german_name            japanese_name\n## 1 0              1     Bulbasaur     Bisasam フシギダネ (Fushigidane)\n## 2 1              2       Ivysaur   Bisaknosp  フシギソウ (Fushigisou)\n## 3 2              3      Venusaur    Bisaflor フシギバナ (Fushigibana)\n## 4 3              3 Mega Venusaur    Bisaflor フシギバナ (Fushigibana)\n## 5 4              4    Charmander    Glumanda      ヒトカゲ (Hitokage)\n## 6 5              5    Charmeleon     Glutexo       リザード (Lizardo)\n##   generation\n## 1          1\n## 2          1\n## 3          1\n## 4          1\n## 5          1\n## 6          1\n\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = 'data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('data/NE_Features.zip', exdir = 'data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations &lt;- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\nThe most common delimited text format is CSV: comma-separated value.\n\nlibrary(readr)\n\nurl &lt;- \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info &lt;- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n## # A tibble: 6 × 6\n##    ...1 pokedex_number name          german_name japanese_name           gener…¹\n##   &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;                     &lt;dbl&gt;\n## 1     0              1 Bulbasaur     Bisasam     フシギダネ (Fushigidan…       1\n## 2     1              2 Ivysaur       Bisaknosp   フシギソウ (Fushigisou)       1\n## 3     2              3 Venusaur      Bisaflor    フシギバナ (Fushigiban…       1\n## 4     3              3 Mega Venusaur Bisaflor    フシギバナ (Fushigiban…       1\n## 5     4              4 Charmander    Glumanda    ヒトカゲ (Hitokage)           1\n## 6     5              5 Charmeleon    Glutexo     リザード (Lizardo)            1\n## # … with abbreviated variable name ¹​generation\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can’t let it keep us from accessing the data.\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = 'data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('data/NE_Features.zip', exdir = 'data/')\n# List files matching the file type and pick the first one\nfname &lt;- list.files(\"data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations &lt;- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\nWe can actually read in the file without unzipping it, so long as we download it first - readr does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\nnebraska_locations &lt;- read_delim(\"data/NE_Features.zip\", delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\n\nimport pandas as pd\n\nurl = \"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\"\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n##                                 name german_name  ... against_steel  against_fairy\n## 0                          Bulbasaur     Bisasam  ...           1.0            0.5\n## 1                            Ivysaur   Bisaknosp  ...           1.0            0.5\n## 2                           Venusaur    Bisaflor  ...           1.0            0.5\n## 3                      Mega Venusaur    Bisaflor  ...           1.0            0.5\n## 4                         Charmander    Glumanda  ...           0.5            0.5\n## ...                              ...         ...  ...           ...            ...\n## 1023     Zacian Hero of Many Battles         NaN  ...           2.0            1.0\n## 1024        Zamazenta Crowned Shield         NaN  ...           0.5            1.0\n## 1025  Zamazenta Hero of Many Battles         NaN  ...           1.0            2.0\n## 1026                       Eternatus         NaN  ...           1.0            1.0\n## 1027             Eternatus Eternamax         NaN  ...           1.0            1.0\n## \n## [1028 rows x 49 columns]\n\nPandas can actually access zipped data files and unzip them while reading the data in, so we don’t have to download the file and unzip it first.\n\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\nnebraska_locations\n##        FEATURE_ID  ... DATE_EDITED\n## 0          171013  ...         NaN\n## 1          171029  ...  03/08/2018\n## 2          182687  ...         NaN\n## 3          182688  ...  10/23/2009\n## 4          182689  ...  12/20/2017\n## ...           ...  ...         ...\n## 31473     2806916  ...  08/12/2021\n## 31474     2806917  ...  08/12/2021\n## 31475     2806918  ...  08/12/2021\n## 31476     2806919  ...         NaN\n## 31477     2806920  ...  08/12/2021\n## \n## [31478 rows x 20 columns]\n\n\n\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters).\n\n\n\n\n\n\nNote\n\n\n\nDon’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding!\n\n\nWhile writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS.\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv &gt; data/pokemon.csv\";\nonly run this once to download the file... */\nfilename pokeloc 'data/pokemon.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\nIf I fix the file in R (because I know how to fix it there… another option is to fix it manually),\n\nlibrary(readr)\nlibrary(dplyr)\ntmp &lt;- read_csv(\"data/pokemon.csv\")[,-1]\n# You'll learn how to do this later\ntmp &lt;- select(tmp, -japanese_name) %&gt;%\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\nwrite_csv(tmp, \"data/pokemon_ascii.csv\", na='.')\n\nThen, reading in the new file allows us to actually see the output.\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"data/pokemon_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \nThis trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate.\nTo read in a pipe delimited file (the ‘|’ character), we have to make some changes. Here is the proc import code. Note that I am reading in a version of the file that I’ve converted to ASCII (see details below) because while the import works with the original file, it causes the SAS -&gt; R pipeline that the book is built on to break.\n\ntmp &lt;- readLines(\"data/NE_Features_20200501.txt\")\ntmp_ascii &lt;- iconv(tmp, to = \"ASCII//TRANSLIT\")\nwriteLines(tmp_ascii, \"data/NE_Features_ascii.txt\")\n\n/* Without specifying the library to store the data in, it is stored in WORK */\nproc import datafile = \"data/NE_Features_ascii.txt\" out=nefeatures\n  DBMS = DLM /* delimited file */\n  replace;\n  GETNAMES = YES;\n  DELIMITER = '|';\n  GUESSINGROWS = 31582;\nrun;\n\nproc print data=nefeatures (obs=10); /* print the first 10 observations */\n  run;\n\nUnder the hood, proc import is just writing code for a data step. So when proc import doesn’t work, we can just write the code ourselves. It requires a bit more work (specifying column names, for example) but it also doesn’t fail nearly as often.\n/* x \"curl https://raw.githubusercontent.com/srvanderplas/unl-stat850/master/data/NE_Features_20200501.txt\n&gt; data/NE_Features_20200501.txt\"; */\n/* only run this once... */\n\ndata nefeatures;\n/*infile \"data/NE_Features_20200501.txt\"*/\ninfile \"data/NE_Features_ascii.txt\"\ndlm='|' /* specify delimiter */\n  encoding=\"utf-8\" /* specify encoding */\n  DSD /* delimiter sensitive data */\n  missover /* keep going if missing obs encountered */\n  firstobs=2; /* skip header row */\n  input FEATURE_ID $\n  FEATURE_NAME $\n  FEATURE_CLASS $\n  STATE_ALPHA $\n  STATE_NUMERIC\nCOUNTY_NAME $\n  COUNTY_NUMERIC $\n  PRIMARY_LAT_DMS $\n  PRIM_LONG_DMS $\n  PRIM_LAT_DEC\nPRIM_LONG_DEC\nSOURCE_LAT_DMS $\n  SOURCE_LONG_DMS $\n  SOURCE_LAT_DEC\nSOURCE_LONG_DEC\nELEV_IN_M\nELEV_IN_FT\nMAP_NAME $\n  DATE_CREATED $\n  DATE_EDITED $\n  ;\nrun;\n\nproc print data=nefeatures (obs=10); /* print the first 10 observations */\n  run;\n\n\n\n::: callout-tip\n\n8.2.3 Try it out: Reading CSV files\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveninently available at their download page. Because these datasets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV.\n\n\nProblem\nR Solution\nPython Solution\nSAS Problem\nSAS Solution\n\n\n\nThe readr package and pandas can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n\n\nlibrary(readr)\nlegosets &lt;- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"data/lego_sets.csv\")\n\n\n\n\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"data/lego_sets_py.csv\")\n\n\n\nIn SAS, it is also possible to read gzip files directly; however, it is tricky to get PROC IMPORT to work with gzip files. The code below will 1) download the file (uncomment that part), 2) create a link to the gzip file, 3) create a link to where the unzipped file will go, and 4) unzip the file to the link specified in (3).\n/* x \"curl https://cdn.rebrickable.com/media/downloads/sets.csv.gz &gt; \\ \ndata/lego_sets.csv.gz\";\nonly run this once... */\n\nfilename legofile ZIP \"data/lego_sets.csv.gz\" GZIP;\nfilename target \"data/lego_sets.csv\";\n\ndata _null_;\n  infile legofile;\n  file target;\n  input;\n  put _infile_;\nrun;\nCan you write two different statements (one using proc import on the unzipped file, one using a datastep on the zipped file) to read the data in? Note that you may have to specify the length of character fields in the data step version using length var_name $ 100; before the input statement to set variable var_name to have maximum length of 100 characters.\n\n\nlibname classdat \"sas/\";\n/* Work with the library of class data */\n\nfilename legofile ZIP \"data/lego_sets.csv.gz\" GZIP;\nfilename target \"data/lego_sets.csv\";\n\ndata _null_;\n  infile legofile;\n  file target;\n  input;\n  put _infile_;\nrun;\n\nproc import datafile = target out=classdat.legoset DBMS=csv replace;\nGETNAMES=YES;\nGUESSINGROWS=15424;\nrun;\n\n/* This dataset will be stored in WORK */\ndata legoset2;\n  infile legofile dsd firstobs=2\n  dlm=\",\";\n  length set_num $20;\n  length name $100;\n  input set_num $ name $ year theme_id num_parts;\n  run;\n  \nproc print data=classdat.legoset (obs=10);\n  run;\n  \nproc print data=legoset2 (obs=10);\n  run;"
  },
  {
    "objectID": "reading-data.html#spreadsheets",
    "href": "reading-data.html#spreadsheets",
    "title": "8  Loading External Data",
    "section": "\n8.3 Spreadsheets",
    "text": "8.3 Spreadsheets\nThis example uses data downloaded from “Mapping Police Violence”, which now links to a google sheet that has to be downloaded manually. Save that file to a path that you can find again - in the code below it lives in data/police_violence.xlsx.\n\n\nR readxl\nPython\nSAS\n\n\n\nIn R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, skip = 1)\npolice_violence[1:10, 1:6]\n## # A tibble: 10 × 6\n##    `Victim's name`         Victim'…¹ Victi…² Victi…³ URL o…⁴ Date of Incident …⁵\n##    &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dttm&gt;             \n##  1 Name withheld by police 27.0      Male    Unknow… &lt;NA&gt;    2022-01-20 00:00:00\n##  2 Name withheld by police Unknown   Unknown Unknow… &lt;NA&gt;    2022-01-20 00:00:00\n##  3 Name withheld by police Unknown   Male    Unknow… &lt;NA&gt;    2022-01-20 00:00:00\n##  4 Ronald W. Flowers II    Unknown   Male    Unknow… &lt;NA&gt;    2022-01-19 00:00:00\n##  5 Name withheld by police Unknown   Male    Unknow… &lt;NA&gt;    2022-01-19 00:00:00\n##  6 Name withheld by police 31.0      Male    Unknow… &lt;NA&gt;    2022-01-18 00:00:00\n##  7 Name withheld by police Unknown   Male    Unknow… &lt;NA&gt;    2022-01-18 00:00:00\n##  8 Name withheld by police Unknown   Unknown Unknow… &lt;NA&gt;    2022-01-17 00:00:00\n##  9 Name withheld by police Unknown   Male    Unknow… &lt;NA&gt;    2022-01-16 00:00:00\n## 10 Name withheld by police Unknown   Male    Unknow… &lt;NA&gt;    2022-01-15 00:00:00\n## # … with abbreviated variable names ¹​`Victim's age`, ²​`Victim's gender`,\n## #   ³​`Victim's race`, ⁴​`URL of image of victim`,\n## #   ⁵​`Date of Incident (month/day/year)`\n\n\n\n\nimport pandas as pd\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows = 1)\npolice_violence\n##                 Victim's name  ... Killed by Police 2013-21\n## 0     Name withheld by police  ...                      NaN\n## 1     Name withheld by police  ...                      NaN\n## 2     Name withheld by police  ...                      NaN\n## 3        Ronald W. Flowers II  ...                      NaN\n## 4     Name withheld by police  ...                      NaN\n## ...                       ...  ...                      ...\n## 9937               Tyree Bell  ...                      1.0\n## 9938             Abel Gurrola  ...                      1.0\n## 9939      Christopher Tavares  ...                      1.0\n## 9940        Andrew L. Closson  ...                      1.0\n## 9941            Andrew Layton  ...                      1.0\n## \n## [9942 rows x 63 columns]\n\n\n\nIn SAS, PROC IMPORT is one easy way to read in xlsx files. In this code chunk, we have to handle the fact that one of the columns in the spreadsheet contains dates. SAS and Excel handle dates a bit differently, so we have to transform the date variable – and we may as well relabel it at the same time. To do this, we use a DATA statement that outputs to the same dataset it references. We define a new variable date, adjust the Excel dates so that they conform to SAS’s standard, and tell SAS how to format the date. (We’ll talk more about dates and times later)\nlibname classdat \"sas/\";\n\nPROC IMPORT OUT=classdat.police \n    DATAFILE=\"data/police_violence.xlsx\" \n    DBMS=xlsx /* Tell SAS what type of file it's reading */ \n    REPLACE; /* replace the dataset if it already exists */\n  SHEET=\"2013-2019 Police Killings\"; /* SAS reads the first sheet by default */\n  GETNAMES=yes;\n    informat VAR6 mmddyy10.; /* tell SAS what format the date is in */\nRUN;\n\nDATA classdat.police;\n  SET classdat.police; /* modify the dataset and write back out to it */\n  \n  date = VAR6 - 21916; /* Conversion to SAS date standard from Excel */\n  FORMAT date MMDDYY10.; /* Tell SAS how to format the data when printing it */ \n  DROP VAR6; /* Get rid of the original data */\n  \n  num_age = INPUT(Victim_s_age, 3.); /* create numeric age variable */\n  \n  DROP \n    A_brief_description_of_the_circu \n    URL_of_image_of_victim \n    Link_to_news_article_or_photo_of; \n    /* drop longer variable to save space so the file fits on GitHub */\n    /* Size went from 100 MB to 6.7 MB without these 3 vars */\nRUN;\n\n\nPROC PRINT DATA=classdat.police (obs=10); /* print the first 10 observations */\n  VAR Victim_s_name Victim_s_age num_age Victim_s_gender Victim_s_race date;\nRUN;\n\n\n\n\n\n\nNote\n\n\n\nHere is some additional information about reading and writing Excel files in SAS.\n\n\n\n\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier.\nTry it out\n\n\nProblem\nR Solution\nPython Solution\nSAS Solution\n\n\n\nThe Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n\n\nurl &lt;- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates &lt;- read_xls(path = \"data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 × 6\n##    Age   `\\nOperator's \\nLicense -\\nClass O` Operator's\\…¹ Motor…² Comme…³ ...6 \n##    &lt;chr&gt;                               &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;\n##  1 &lt;NA&gt;                                   NA            NA      NA CDL A   CDL B\n##  2 14                                      0             0       0 0       0    \n##  3 15                                      0             0       0 0       0    \n##  4 16                                      0             0       0 0       0    \n##  5 17                                    961            33       0 0       0    \n##  6 18                                  18903           174       0 25      3    \n##  7 19                                  22159           251       0 97      32   \n##  8 20                                  22844           326       1 144     47   \n##  9 21                                  21589           428       0 233     57   \n## 10 22                                  22478           588       0 292     81   \n## # … with abbreviated variable names\n## #   ¹​`Operator's\\nLicense - \\nClass O/\\nMotorcycle\\nClass M`,\n## #   ²​`Motor-\\ncycle\\nLicense /\\nClass M`, ³​`Commercial Driver's License`\n\n\n\nYou may need to install xlrd via pip for this code to work.\n\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\nne_plates\n##     Unnamed: 0  ... Total\\nLicensed\\n Drivers\n## 0          NaN  ...                       NaN\n## 1          NaN  ...                    3279.0\n## 2          NaN  ...                   14902.0\n## 3          NaN  ...                   22339.0\n## 4          NaN  ...                   24341.0\n## 5          NaN  ...                   21447.0\n## 6          NaN  ...                   23761.0\n## 7          NaN  ...                   24269.0\n## 8          NaN  ...                   23039.0\n## 9          NaN  ...                   23990.0\n## 10         NaN  ...                   24717.0\n## 11         NaN  ...                   25283.0\n## 12         NaN  ...                  125153.0\n## 13         NaN  ...                  128563.0\n## 14         NaN  ...                  126432.0\n## 15         NaN  ...                  117878.0\n## 16         NaN  ...                  102261.0\n## 17         NaN  ...                  104622.0\n## 18         NaN  ...                  111702.0\n## 19         NaN  ...                  118860.0\n## 20         NaN  ...                  106939.0\n## 21         NaN  ...                   86649.0\n## 22         NaN  ...                   56675.0\n## 23         NaN  ...                   35481.0\n## 24         NaN  ...                   20288.0\n## 25         NaN  ...                    8555.0\n## 26         NaN  ...                    1864.0\n## 27         NaN  ...                      82.0\n## 28         NaN  ...                 1483162.0\n## \n## [29 rows x 16 columns]\n\n\n\nPROC IMPORT OUT=WORK.licplate \n    DATAFILE=\"data/2019_Vehicle_Registration_Plates_NE.xlsx\" \n    DBMS=xlsx /* Tell SAS what type of file it's reading */ \n    REPLACE; /* replace the dataset if it already exists */\n    RANGE=\"'Reg By Plate Type'$A2:0\"\n  GETNAMES=yes;\nRUN;\n\n/* just a few columns... way too many to handle */\nPROC PRINT DATA=WORK.licplate (obs=10); /* print the first 10 observations */\nVar County Amateur__Radio Breast__Cancer Choose__Life County__Gov Comm__Truck Passenger Total;\nRUN;\n\n\n\n\n8.3.1 Google Sheets\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We’re not going to get into the power of these packages just now, but it’s worth a look if you’re working with collaborators who use Google sheets.\n\n\n\n\n\n\nNote\n\n\n\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via GSpread or googlesheets4 in python and R respectively.\n\n\nFor now, let’s just demonstrate reading in data from google sheets in R and python using the Data Is Plural archive.\n\n\nPython\nR\nR: googlesheets4\nPython: GSpread\n\n\n\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in [1].\n\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n\nThis method would likely work just as well in R and would not require the googlesheets4 package.\n\n\nThis method is described in [1] for Python, but I have adapted the code to use in R.\n\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n\n\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\n\n\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n\n\n\nThese instructions are derived from [2]. We will have to install the GSpread package: type pip install gspread into the terminal.\nThen, you will need to obtain a client token JSON file following these instructions.\n\nimport gspread as gs\nimport pandas as pd\n\nI’ve stopped here because I can’t get the authentication working."
  },
  {
    "objectID": "reading-data.html#binary-files",
    "href": "reading-data.html#binary-files",
    "title": "8  Loading External Data",
    "section": "\n8.4 Binary Files",
    "text": "8.4 Binary Files\nBoth R and SAS have binary data files that store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see [3].\n\n\nR formats in R\nR formats in Python\nR format in SAS\nSAS format in R\nSAS format in Python\nSAS format in SAS\n\n\n\n.Rdata is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\nlegos &lt;- read_csv(\"data/lego_sets.csv\")\nmy_var &lt;- \"This variable contains a string\"\nsave(legos, my_var, file = \"data/R_binary.Rdata\")\n\nIf we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious.\nWe can load the R binary file back in using the load() function.\n\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"mesodata\"           \"mesodata_names\"    \n##  [7] \"ne_plates\"          \"nebraska_locations\" \"pokemon_info\"      \n## [10] \"police_violence\"    \"sheet_id\"           \"sheet_name\"        \n## [13] \"tmp\"                \"tmp_chars\"          \"tmp_chars_space\"   \n## [16] \"tmp_space\"          \"url\"                \"widths\"\n\nload(\"data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"mesodata\"          \n##  [7] \"mesodata_names\"     \"my_var\"             \"ne_plates\"         \n## [10] \"nebraska_locations\" \"pokemon_info\"       \"police_violence\"   \n## [13] \"sheet_id\"           \"sheet_name\"         \"tmp\"               \n## [16] \"tmp_chars\"          \"tmp_chars_space\"    \"tmp_space\"         \n## [19] \"url\"                \"widths\"\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\nsaveRDS(legos, \"data/RDSlego.rds\")\n\nother_lego &lt;- readRDS(\"data/RDSlego.rds\")\n\nBecause RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately.\n\n\nWe first need to install the pyreadr package by running pip install pyreadr in the terminal.\n\nimport pyreadr\n\nrdata_result = pyreadr.read_r('data/R_binary.Rdata')\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n##            set_num                                 name  ...  theme_id  num_parts\n## 0            001-1                                Gears  ...       1.0       43.0\n## 1           0011-2                    Town Mini-Figures  ...      84.0       12.0\n## 2           0011-3           Castle 2 for 1 Bonus Offer  ...     199.0        0.0\n## 3           0012-1                   Space Mini-Figures  ...     143.0       12.0\n## 4           0013-1                   Space Mini-Figures  ...     143.0       12.0\n## ...            ...                                  ...  ...       ...        ...\n## 15419      wwgp1-1  Wild West Limited Edition Gift Pack  ...     476.0        0.0\n## 15420   XMASTREE-1                       Christmas Tree  ...     410.0       26.0\n## 15421      XWING-1                  Mini X-Wing Fighter  ...     158.0       60.0\n## 15422      XWING-2                    X-Wing Trench Run  ...     158.0       52.0\n## 15423  YODACHRON-1      Yoda Chronicles Promotional Set  ...     158.0      413.0\n## \n## [15424 rows x 5 columns]\nrdata_result[\"my_var\"]\n##                             my_var\n## 0  This variable contains a string\nrds_result = pyreadr.read_r('data/RDSlego.rds')\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n##            set_num                                 name  ...  theme_id  num_parts\n## 0            001-1                                Gears  ...       1.0       43.0\n## 1           0011-2                    Town Mini-Figures  ...      84.0       12.0\n## 2           0011-3           Castle 2 for 1 Bonus Offer  ...     199.0        0.0\n## 3           0012-1                   Space Mini-Figures  ...     143.0       12.0\n## 4           0013-1                   Space Mini-Figures  ...     143.0       12.0\n## ...            ...                                  ...  ...       ...        ...\n## 15419      wwgp1-1  Wild West Limited Edition Gift Pack  ...     476.0        0.0\n## 15420   XMASTREE-1                       Christmas Tree  ...     410.0       26.0\n## 15421      XWING-1                  Mini X-Wing Fighter  ...     158.0       60.0\n## 15422      XWING-2                    X-Wing Trench Run  ...     158.0       52.0\n## 15423  YODACHRON-1      Yoda Chronicles Promotional Set  ...     158.0      413.0\n## \n## [15424 rows x 5 columns]\n\n\n\nThere are theoretically ways to read R data into SAS via the R subsystem [4]. Feel free to do that on your own machine.2\n\n\nFirst, let’s download the NHTS data.\n\nlibrary(httr)\n# Download the file and write to disk\nres &lt;- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"data/cen10pub.sas7bdat\", overwrite = T))\n\nYou can see more information about this data here [5].\n\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata &lt;- read.sas7bdat(\"data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured.\n\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"data/cen10pub.sas7bdat\")\n## 'data/cen10pub.sas7bdat'\n\nYou can see more information about this data here [5].\nTo read SAS files, we use the read_sas function in Pandas.\n\nimport pandas as pd\n\ndata = pd.read_sas(\"data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n\n\n\nLet’s read in the data from the 2009 National Household Travel Survey:\nlibname classdat \"data\";\n/* this tells SAS where to look for (a bunch of) data files */\n\nproc contents data=classdat.cen10pub; /* This tells sas to access the specific file */\nrun;\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured.\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\nIn RStudio, go to Session -&gt; Clear Workspace. (This will clear your environment)\nNow, using your RDS files, load the objects back into R with different names.\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious)\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n\n\nlibrary(readxl)\nlibrary(readr)\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000)\nlegos &lt;- read_csv(\"data/lego_sets.csv\")\n\nsave(police_violence, legos, file = \"data/04_Try_Binary.Rdata\")\nsaveRDS(police_violence, \"data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"data/04_Try_Binary2.rds\")\n\nrm(police_violence, legos) # Limited clearing of workspace... \n\n\nload(\"data/04_Try_Binary.Rdata\")\n\npv_compare &lt;- readRDS(\"data/04_Try_Binary1.rds\")\nlego_compare &lt;- readRDS(\"data/04_Try_Binary2.rds\")\n\nall.equal(police_violence, pv_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n\n\n\n\nimport pyreadr\n\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\npolice = robjs[\"police_violence\"]\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n\npolice_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n\npolice.equals(police_compare)\n## True\nlegos.equals(lego_compare)\n## True"
  },
  {
    "objectID": "reading-data.html#databases",
    "href": "reading-data.html#databases",
    "title": "8  Loading External Data",
    "section": "\n8.5 Databases",
    "text": "8.5 Databases\nThere are many different database formats. Some of the most common databases are SQL* related formats and Microsoft Access files.\n\n\n\n\n\n\nNote\n\n\n\nYou can get through this class without this section. Feel free to skip it and come back when/if you need it.\n\n\nThis excellent GitHub repo contains code to connect to multiple types of databases in R, python, PHP, Java, SAS, and VBA\n\n8.5.1 Microsoft Access\nTo get access to MS Access databases, you will need to become familiar with how to install ODBC drivers. These drivers tell your operating system how to connect to each type of database (so you need a different driver to get to MS Access databases than to get to SQL databases).\n\n\n\n\n\n\nInstall ODBC Drivers on your machine\n\n\n\n\n\n and  This set of instructions appears to contain all of the right steps for Windows and Mac and has been updated recently (Feb 2022) [6].\n I installed mdbtools on Ubuntu and have the following entry in my /etc/odbcinst.ini file:\n[MDBTools]\nDescription=MDBTools Driver\nDriver=libmdbodbc.so\nSetup=libmdbodbc.so\nFileUsage=1\nUsageCount=1\nAdding this entry to the file may be part of the mdbtools installation - I certainly have no memory of doing it myself, but this may help if you’re troubleshooting, so I’ve included it.\n\n\n\nFor this demo, we’ll be using the Scottish Witchcraft Database[7], which you can download from their website, or acquire from the course data folder if you don’t want to register with the authors. A description of the dataset is also available.\n\n\nR\nSAS\n\n\n\nIn R, we can read in MS Access files using the Hmisc package, as long as the mdbtools library is available on your computer3.\n\nif (!\"Hmisc\" %in% installed.packages()) install.packages(\"Hmisc\")\nlibrary(Hmisc)\ndb_loc &lt;- \"data/Witchcraftsurvey_download.mdb\"\n\nmdb.get(db_loc, tables = TRUE) # get table list\n##  [1] \"WDB_Accused\"             \"WDB_Accused_family\"     \n##  [3] \"WDB_Appeal\"              \"WDB_CalendarCustom\"     \n##  [5] \"WDB_Case\"                \"WDB_Case_person\"        \n##  [7] \"WDB_Commission\"          \"WDB_Complaint\"          \n##  [9] \"WDB_Confession\"          \"WDB_CounterStrategy\"    \n## [11] \"WDB_DemonicPact\"         \"WDB_Denunciation\"       \n## [13] \"WDB_DevilAppearance\"     \"WDB_Elf_FairyElements\"  \n## [15] \"WDB_Imprisonment\"        \"WDB_LinkedTrial\"        \n## [17] \"WDB_Malice\"              \"WDB_MentionedAsWitch\"   \n## [19] \"WDB_MovestoHLA\"          \"WDB_MusicalInstrument\"  \n## [21] \"WDB_Ordeal\"              \"WDB_OtherCharges\"       \n## [23] \"WDB_OtherNamedwitch\"     \"WDB_Person\"             \n## [25] \"WDB_PrevCommission\"      \"WDB_PropertyDamage\"     \n## [27] \"WDB_Ref_Parish\"          \"WDB_Reference\"          \n## [29] \"WDB_ReligiousMotif\"      \"WDB_RitualObject\"       \n## [31] \"WDB_ShapeChanging\"       \"WDB_Source\"             \n## [33] \"WDB_Torture\"             \"WDB_Trial\"              \n## [35] \"WDB_Trial_Person\"        \"WDB_WeatherModification\"\n## [37] \"WDB_WhiteMagic\"          \"WDB_WitchesMeetingPlace\"\nmdb.get(db_loc, tables = \"WDB_Trial\")[1:6,1:10] # get table of trials, print first 6 rows and 10 cols\n##    Trialref TrialId TrialSystemId    CaseRef TrialType Trial.settlement\n## 1    T/JO/1       1            JO C/EGD/2120         2                 \n## 2  T/JO/100     100            JO  C/JO/2669         2                 \n## 3 T/JO/1000    1000            JO C/EGD/1474         2                 \n## 4 T/JO/1001    1001            JO C/EGD/1558         2                 \n## 5 T/JO/1002    1002            JO C/EGD/1681         2                 \n## 6 T/JO/1003    1003            JO C/EGD/1680         2                 \n##   Trial.parish Trial.presbytery Trial.county Trial.burgh\n## 1                      Aberdeen     Aberdeen    Aberdeen\n## 2                                                       \n## 3                                                       \n## 4                                                       \n## 5                                                       \n## 6\n\nMany databases have multiple tables with keys that connect information in each table. We’ll spend more time on databases later in the semester - for now, it’s enough to be able to get data out of one. #### Python {-}\nThere are several tutorials out there to access MS Access databases using packages like pyodbc e.g. [8]. I couldn’t quite get these working on Linux, but it is possible you may have better luck on another OS. With that said, the solution using pandas_access seems to be much simpler and require less OS configuration, so it’s what I’ll show here.\nFirst, we have to install pandas_access using pip install pandas_access.\n\nimport pandas_access as mdb\ndb_filename = 'data/Witchcraftsurvey_download.mdb'\n\n# List tables\nfor tbl in mdb.list_tables(db_filename):\n  print(tbl)\n\n# Read a small table.\n## WDB_Accused\n## WDB_Accused_family\n## WDB_Appeal\n## WDB_CalendarCustom\n## WDB_Case\n## WDB_Case_person\n## WDB_Commission\n## WDB_Complaint\n## WDB_Confession\n## WDB_CounterStrategy\n## WDB_DemonicPact\n## WDB_Denunciation\n## WDB_DevilAppearance\n## WDB_Elf_FairyElements\n## WDB_Imprisonment\n## WDB_LinkedTrial\n## WDB_Malice\n## WDB_MentionedAsWitch\n## WDB_MovestoHLA\n## WDB_MusicalInstrument\n## WDB_Ordeal\n## WDB_OtherCharges\n## WDB_OtherNamedwitch\n## WDB_Person\n## WDB_PrevCommission\n## WDB_PropertyDamage\n## WDB_Ref_Parish\n## WDB_Reference\n## WDB_ReligiousMotif\n## WDB_RitualObject\n## WDB_ShapeChanging\n## WDB_Source\n## WDB_Torture\n## WDB_Trial\n## WDB_Trial_Person\n## WDB_WeatherModification\n## WDB_WhiteMagic\n## WDB_WitchesMeetingPlace\ntrials = mdb.read_table(db_filename, \"WDB_Trial_Person\")\n\nThis isn’t perfectly stable - I tried to read WDB_Trial and got errors about NA values in an integer field - but it does at least work.\n\n\nUnfortunately, it appears that SAS on Linux doesn’t allow you to read in Access files. So I can’t demonstrate that for you. But, since you know how to do it in R, worst case you can open up R and export all of the tables to separate CSV files, then read those into SAS. 😢\n\n\n\nMy hope is that you never actually need to get at data in an MS Access database - the format seems to be largely dying out.\n\n8.5.2 SQLite\nSQLite databases are contained in single files with the extension .SQLite. These files can still contain many different tables, though. They function as databases but are more portable than SQL databases that require a server instance to run and connecting over a network (or running a server on your machine locally). As a result, they provide an opportunity to demonstrate most of the skills required for working with databases without all of the configuration overhead.\n\n\nR\nPython\nSAS\n\n\n\nLet’s try working with a sqlite file that has only one table in R:\n\nif (!\"RSQLite\" %in% installed.packages()) install.packages(\"RSQLite\")\nif (!\"DBI\" %in% installed.packages()) install.packages(\"DBI\")\nlibrary(RSQLite)\nlibrary(DBI)\n\n# Download the baby names file:\ndownload.file(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", destfile = \"data/ssa-\")\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \"data/ssa-babynames-for-2015.sqlite\")\ndbListTables(con) # List all the tables\n## [1] \"babynames\"\nbabyname &lt;- dbReadTable(con, \"babynames\")\nhead(babyname, 10) # show the first 10 obs\n##    state year    name sex count rank_within_sex per_100k_within_sex\n## 1     AK 2015  Olivia   F    56               1              2367.9\n## 2     AK 2015    Liam   M    53               1              1590.6\n## 3     AK 2015    Emma   F    49               2              2071.9\n## 4     AK 2015    Noah   M    46               2              1380.6\n## 5     AK 2015  Aurora   F    46               3              1945.0\n## 6     AK 2015   James   M    45               3              1350.5\n## 7     AK 2015  Amelia   F    39               4              1649.0\n## 8     AK 2015     Ava   F    39               4              1649.0\n## 9     AK 2015 William   M    44               4              1320.5\n## 10    AK 2015  Oliver   M    41               5              1230.5\n\nYou can of course write formal queries using the DBI package, but for many databases, it’s easier to do the querying in R. We’ll cover both options later - the R version will be in the next module.\n\n\nThis example was created using [9] as a primary reference.\nIf you haven’t already downloaded the database file, you can do that automatically in python using this code:\n\nimport urllib.request\nurllib.request.urlretrieve(\"http://2016.padjo.org/files/data/starterpack/ssa-babynames/ssa-babynames-for-2015.sqlite\", \"data/ssa-babynames-for-2015.sqlite\")\n\nYou don’t have to install the sqlite3 module in python using pip because it’s been included in base python since Python 2.5.\n\nimport pandas as pd\nimport sqlite3\n\ncon = sqlite3.connect('data/ssa-babynames-for-2015.sqlite')\n\nbabyname = pd.read_sql_query(\"SELECT * from babynames\", con)\nbabyname\n##        state  year      name sex  count  rank_within_sex  per_100k_within_sex\n## 0         AK  2015    Olivia   F     56                1               2367.9\n## 1         AK  2015      Liam   M     53                1               1590.6\n## 2         AK  2015      Emma   F     49                2               2071.9\n## 3         AK  2015      Noah   M     46                2               1380.6\n## 4         AK  2015    Aurora   F     46                3               1945.0\n## ...      ...   ...       ...  ..    ...              ...                  ...\n## 127661    WY  2015  Sterling   M      5              159                248.9\n## 127662    WY  2015    Steven   M      5              159                248.9\n## 127663    WY  2015     Trace   M      5              159                248.9\n## 127664    WY  2015   Tristan   M      5              159                248.9\n## 127665    WY  2015     Tyson   M      5              159                248.9\n## \n## [127666 rows x 7 columns]\ncon.close() # You must close any connection you open!\n\n\n\nIn SAS, you can theoretically connect to SQLite databases, but there are very specific instructions for how to do that for each operating system.\nYou’ll need to acquire the SQLite ODBC Driver for your operating system. You may also need to set up a DSN (Data Source Name) (Windows, Mac and Linux).4\nHere is my .odbc.ini file as I’ve configured it for my Ubuntu 20.04 machine. A similar file should work for any Mac or Linux machine. In windows, you’ll need to use the ODBC Data Source Administrator to set this up.\n[babyname]\nDescription = 2015 SSA baby names\nDriver = SQLite3\nDatabase = data/ssa-babynames-for-2015.sqlite\n/* This code requires that Ive set up a DSN connecting the sqlite file to */\n/* a specific driver on my computer. Youll have to set up your machine to */\n/* have a configuration that is appropriate for your setup */\n  \nlibname mydat odbc complete = \"dsn=babyname; Database=data/ssa-babynames-for-2015.sqlite\"; \n\nproc print data=mydat.babynames (obs=10);\nrun; \n\n\n\n\n\n\n\n\n\nLearn more\n\n\n\n\nRSQLite vignette\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds."
  },
  {
    "objectID": "reading-data.html#references",
    "href": "reading-data.html#references",
    "title": "8  Loading External Data",
    "section": "\n8.6 References",
    "text": "8.6 References\n\n\n\n\n[1] \nM. Schäfer, “Read Data from Google Sheets into Pandas without the Google Sheets API,” Towards Data Science. Dec. 2020 [Online]. Available: https://towardsdatascience.com/read-data-from-google-sheets-into-pandas-without-the-google-sheets-api-5c468536550. [Accessed: Jun. 07, 2022]\n\n\n[2] \nM. Clarke, “How to read Google Sheets data in Pandas with GSpread,” Practical Data Science. Jun. 2021 [Online]. Available: https://practicaldatascience.co.uk/data-science/how-to-read-google-sheets-data-in-pandas-with-gspread. [Accessed: Jun. 07, 2022]\n\n\n[3] \nC. Maierle, “Loading binary data to NumPy/Pandas,” Towards Data Science. Jul. 2020 [Online]. Available: https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672. [Accessed: Jun. 07, 2022]\n\n\n[4] \nWicklin, “Calling R from SAS/IML software,” The DO Loop. May 2011 [Online]. Available: https://blogs.sas.com/content/iml/2011/05/13/calling-r-from-sasiml-software.html. [Accessed: Jun. 07, 2022]\n\n\n[5] \nUS Department of Transportation, “National Household Travel Survey (NHTS) 2009,” data.world. Mar. 2018 [Online]. Available: https://data.world/dot/national-household-travel-survey-nhts-2009. [Accessed: Jun. 13, 2022]\n\n\n[6] \nTeam Exploratory, “How to import Data from Microsoft Access Database with ODBC,” Exploratory.io. Feb. 2022 [Online]. Available: https://exploratory.io/note/exploratory/How-to-import-Data-from-Microsoft-Access-Database-with-ODBC-zIJ2bjs2. [Accessed: Jun. 13, 2022]\n\n\n[7] \nJulian Goodare, Lauren Martin, Joyce Miller, and Louise Yeoman, “The Survey of Scottish Witchcraft.” Jan. 2003 [Online]. Available: www.shc.ed.ac.uk/witches/. [Accessed: Jun. 13, 2022]\n\n\n[8] \nData to Fish, “How to Connect Python to MS Access Database using Pyodbc,” Data to Fish. Aug. 2021 [Online]. Available: https://datatofish.com/how-to-connect-python-to-ms-access-database-using-pyodbc/. [Accessed: Jun. 13, 2022]\n\n\n[9] \nData Carpentry, “Accessing SQLite Databases Using Python and Pandas – Data Analysis and Visualization in Python for Ecologists,” Data Analysis and Visualization in Python for Ecologists. Jun. 2019 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html. [Accessed: Jun. 13, 2022]"
  },
  {
    "objectID": "reading-data.html#footnotes",
    "href": "reading-data.html#footnotes",
    "title": "8  Loading External Data",
    "section": "",
    "text": "though there are a seemingly infinite number of actual formats, and they pop up at the most inconvenient times↩︎\nI tried, and it crashed SAS on my machine.↩︎\nA currently maintained version of the library is here and should work for UNIX platforms. It may be possible to install the library on Windows using the UNIX subsystem, per this thread↩︎\nOn one of my machines, I also had to make sure the file libodbc.so existed - it was named libodbc.so.1 on my laptop, so a symbolic link fixed the issue.↩︎"
  },
  {
    "objectID": "data-cleaning-verbs.html#module9-objectives",
    "href": "data-cleaning-verbs.html#module9-objectives",
    "title": "9  Data Cleaning and Manipulation",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nApply data manipulation verbs (filter, select, group by, summarize, mutate) to prepare data for analysis\nIdentify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately and understand the consequences of data cleaning steps\nCreate summaries of data appropriate for analysis or display using data manipulation techniques"
  },
  {
    "objectID": "data-cleaning-verbs.html#introduction",
    "href": "data-cleaning-verbs.html#introduction",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\n\n\nData wrangling (by Allison Horst)\n\nIn R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework).\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\n\nIn Python, most data manipulation tasks are handled using pandas[1]. In the interests of using a single consistent “language” for describing data manipulation tasks, I’ll use the tidyverse “verbs” to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\nThere is also the datar python package[2], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, datar may be more useful if you prefer the dplyr way of handling things using a data-first API.\n\n\nI haven’t had the chance to add the datar package to this book, but it looks promising and may be worth your time to figure out. It’s a bit too new for me to take the time to add it into this book - I want packages that will be maintained long-term if I’m going to teach them to others.\n\n\n\n\n\n\nNote\n\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.\nHere is a data wrangling with pandas cheatsheet that is formatted similarly to the dplyr cheat sheet."
  },
  {
    "objectID": "data-cleaning-verbs.html#tidy-data",
    "href": "data-cleaning-verbs.html#tidy-data",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.2 Tidy Data",
    "text": "9.2 Tidy Data\nThere are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\nThese attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your clothes are folded, but they aren’t clean just because they’re folded; you could have folded a pile of dirty clothes).\nWe’ll get more into how to work with different “messy” data configurations in the next module, but it’s worth keeping rules 1 and 3 in mind while working through this module."
  },
  {
    "objectID": "data-cleaning-verbs.html#filter-pick-cases-rows-based-on-their-values",
    "href": "data-cleaning-verbs.html#filter-pick-cases-rows-based-on-their-values",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.3 Filter: Pick cases (rows) based on their values",
    "text": "9.3 Filter: Pick cases (rows) based on their values\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\ndplyr filter() by Allison Horst\n\n\n\n\n\n\n\nExample: starwars\n\n\n\nLet’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies.\nIn the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by CSV files. You can access the csv data here.\n\n\nR\nPython\n\n\n\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 C-3PO          167    75 &lt;NA&gt;    gold    yellow    112   none  mascu… Tatooi…\n##  3 R2-D2           96    32 &lt;NA&gt;    white,… red        33   none  mascu… Naboo  \n##  4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  8 R5-D4           97    32 &lt;NA&gt;    white,… red        NA   none  mascu… Tatooi…\n##  9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n## # … with 77 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nWe have to use the exported CSV data in python.\n\nimport pandas as pd\nstarwars = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padmé Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\nfrom skimpy import skim\nskim(starwars)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 87     │ │ object      │ 8     │                       │\n## │ │ Number of columns │ 11     │ │ float64     │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━┳━━━━┳━━━━━┳━━━━━┳━━━━━━┳━━━━━━━━┓  │\n## │ ┃        ┃ missing ┃ comple ┃ mean ┃ sd  ┃ p0 ┃ p25 ┃ p75 ┃ p100 ┃ hist   ┃  │\n## │ ┃        ┃         ┃ te     ┃      ┃     ┃    ┃     ┃     ┃      ┃        ┃  │\n## │ ┃        ┃         ┃ rate   ┃      ┃     ┃    ┃     ┃     ┃      ┃        ┃  │\n## │ ┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━╇━━━━╇━━━━━╇━━━━━╇━━━━━━╇━━━━━━━━┩  │\n## │ │ height │       6 │   0.93 │  170 │  35 │ 66 │ 170 │ 190 │  260 │ ▁ ▁█▂  │  │\n## │ │ mass   │      28 │   0.68 │   97 │ 170 │ 15 │  56 │  84 │ 1400 │   █    │  │\n## │ │ birth_ │      44 │   0.49 │   88 │ 150 │  8 │  35 │  72 │  900 │   █    │  │\n## │ │ year   │         │        │      │     │    │     │     │      │        │  │\n## │ └────────┴─────────┴────────┴──────┴─────┴────┴─────┴─────┴──────┴────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Section 4.8 and Chapter 6, but here we’ll focus on using specific functions to perform the same operation.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nThe dplyr verb for selecting rows is filter. filter takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n## #   starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\n\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n\n# Get only the people who come from Tattoine\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cordé   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dormé   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padmé Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n\n\n\nIn base R, you would perform a filtering operation using subset\n\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    &lt;chr&gt;        &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species &lt;chr&gt;, films &lt;list&gt;,\n## #   vehicles &lt;list&gt;, starships &lt;list&gt;, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n## #   starships &lt;list&gt;, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nNotice that with subset, you have to use & to join two logical statements; it does not by default take multiple successive arguments.\n\n\n\n\n\n\n9.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I’ll do my best to show you easy ways to handle each task in each language.\nFiltering by row number\n\n\nR: dplyr\nPython\nBase R\n\n\n\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\npoke &lt;- read_csv(\"data/pokemon_ascii.csv\")\nfilter(poke, (row_number() %% 2 == 0)) \n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          2 Ivys… Bisakn…       1 Normal Seed P…       2 Grass  Poison     1  \n##  2          3 Mega… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2.4\n##  3          5 Char… Glutexo       1 Normal Flame …       1 Fire   .          1.1\n##  4          6 Mega… Glurak        1 Normal Flame …       2 Fire   Dragon     1.7\n##  5          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  6          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  7         10 Cate… Raupy         1 Normal Worm P…       1 Bug    .          0.3\n##  8         12 Butt… Smettbo       1 Normal Butter…       2 Bug    Flying     1.1\n##  9         14 Kaku… Kokuna        1 Normal Cocoon…       2 Bug    Poison     0.6\n## 10         15 Mega… Bibor         1 Normal Poison…       2 Bug    Poison     1.4\n## # … with 504 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n\n\n\nIn python, the easiest way to accomplish filtering by row number is by using .iloc. But, up until now, we’ve only talked about how Python creates slices using start:(end+1) notation. There is an additional option with slicing - start:(end+1):by. So if we want to get only even rows, we can use the index [::2], which will give us row 0, 2, 4, 6, … through the end of the dataset, because we didn’t specify the start and end portions of the slice.\nBecause Python is 0-indexed, using ::2 will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"data/pokemon_ascii.csv\")\npoke.iloc[0::2]\n##       pokedex_number                      name  ... against_steel  against_fairy\n## 0                  1                 Bulbasaur  ...           1.0            0.5\n## 2                  3                  Venusaur  ...           1.0            0.5\n## 4                  4                Charmander  ...           0.5            0.5\n## 6                  6                 Charizard  ...           0.5            0.5\n## 8                  6          Mega Charizard Y  ...           0.5            0.5\n## ...              ...                       ...  ...           ...            ...\n## 1018             884                 Duraludon  ...           0.5            1.0\n## 1020             886                  Drakloak  ...           1.0            2.0\n## 1022             888      Zacian Crowned Sword  ...           1.0            0.5\n## 1024             889  Zamazenta Crowned Shield  ...           0.5            1.0\n## 1026             890                 Eternatus  ...           1.0            1.0\n## \n## [514 rows x 49 columns]\n\nIf we want to get only odd rows, we can use the index [1::2], which will start at row 1 and give us 1, 3, 5, …\n\n\nIn base R, we’d use seq() to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use subset, which requires a logical condition, and use 1:nrow(poke) to create an index which we then use for deciding whether each row is even or odd.\n\npoke[seq(1, nrow(poke), 2),]\n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          1 Bulb… Bisasam       1 Normal Seed P…       2 Grass  Poison     0.7\n##  2          3 Venu… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2  \n##  3          4 Char… Gluman…       1 Normal Lizard…       1 Fire   .          0.6\n##  4          6 Char… Glurak        1 Normal Flame …       2 Fire   Flying     1.7\n##  5          6 Mega… Glurak        1 Normal Flame …       2 Fire   Flying     1.7\n##  6          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  7          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  8         11 Meta… Safcon        1 Normal Cocoon…       1 Bug    .          0.7\n##  9         13 Weed… Hornliu       1 Normal Hairy …       2 Bug    Poison     0.3\n## 10         15 Beed… Bibor         1 Normal Poison…       2 Bug    Poison     1  \n## # … with 504 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## # A tibble: 514 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          2 Ivys… Bisakn…       1 Normal Seed P…       2 Grass  Poison     1  \n##  2          3 Mega… Bisafl…       1 Normal Seed P…       2 Grass  Poison     2.4\n##  3          5 Char… Glutexo       1 Normal Flame …       1 Fire   .          1.1\n##  4          6 Mega… Glurak        1 Normal Flame …       2 Fire   Dragon     1.7\n##  5          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  6          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  7         10 Cate… Raupy         1 Normal Worm P…       1 Bug    .          0.3\n##  8         12 Butt… Smettbo       1 Normal Butter…       2 Bug    Flying     1.1\n##  9         14 Kaku… Kokuna        1 Normal Cocoon…       2 Bug    Poison     0.6\n## 10         15 Mega… Bibor         1 Normal Poison…       2 Bug    Poison     1.4\n## # … with 504 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\nThis is less fun than using dplyr because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you’re going. The real power of dplyr is in the collection of the full set of verbs with a consistent user interface; nothing done in dplyr is so special that it can’t be done in base R as well.\n\n\n\nSorting rows by variable values\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n\nR: dplyr\nPython\nBase R\n\n\n\narrange() is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\narrange(poke, desc(total_points))\n## # A tibble: 1,028 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 1,018 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\n\n\nIn pandas, we use the sort_values function, which has an argument ascending. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\npoke.sort_values(['total_points'], ascending = False)\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 1027             890   Eternatus Eternamax  ...           1.0            1.0\n## 190              150         Mega Mewtwo Y  ...           1.0            1.0\n## 189              150         Mega Mewtwo X  ...           1.0            2.0\n## 458              384         Mega Rayquaza  ...           1.0            2.0\n## 456              383        Primal Groudon  ...           0.5            0.5\n## ...              ...                   ...  ...           ...            ...\n## 351              298               Azurill  ...           2.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## \n## [1028 rows x 49 columns]\n\n\n\nThe sort() function in R can be used to sort a vector, but when sorting a data frame we usually want to use the order() function instead. This is because sort() orders the values of the argument directly, where order() returns a sorted index.\n\nx &lt;- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n\nWhen working with a data frame, we want to sort the entire data frame’s rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\npoke[order(poke$total_points, decreasing = T),]\n## # A tibble: 1,028 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 1,018 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\n\n\n\nKeep the top \\(n\\) values of a variable\n\n\nR: dplyr\nPython\nBase R\n\n\n\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nslice_max(poke, order_by = total_points, n = 5)\n## # A tibble: 6 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## 6         383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\nBy default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows.\n\nslice_max(poke, order_by = total_points, n = 5, with_ties = F) \n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\nOf course, there is a similar slice_min() function as well:\n\nslice_min(poke, order_by = total_points, n = 5)\n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n## 1         746 Wish… Lusard…       7 Normal Small …       1 Water  .          0.2\n## 2         191 Sunk… Sonnke…       2 Normal Seed P…       1 Grass  .          0.3\n## 3         824 Blip… .             8 Normal Larva …       1 Bug    .          0.4\n## 4         872 Snom  .             8 Normal Worm P…       2 Ice    Bug        0.3\n## 5         298 Azur… Azurill       3 Normal Polka …       2 Normal Fairy      0.2\n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nslice_max(poke, order_by = total_points, prop = .01)\n## # A tibble: 10 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\n\n\nIn Python, nlargest and nsmallest work roughly the same as dplyr’s slice_max and slice_min for integer counts.\n\npoke.nlargest(5, 'total_points')\n##       pokedex_number                 name  ... against_steel  against_fairy\n## 1027             890  Eternatus Eternamax  ...           1.0            1.0\n## 189              150        Mega Mewtwo X  ...           1.0            2.0\n## 190              150        Mega Mewtwo Y  ...           1.0            1.0\n## 458              384        Mega Rayquaza  ...           1.0            2.0\n## 454              382        Primal Kyogre  ...           0.5            1.0\n## \n## [5 rows x 49 columns]\npoke.nsmallest(5, 'total_points')\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 351              298               Azurill  ...           2.0            1.0\n## \n## [5 rows x 49 columns]\n\nTo get proportions, though, we have to do some math:\n\npoke.nlargest(int(len(poke)*0.01), 'total_points')\n##       pokedex_number                      name  ... against_steel  against_fairy\n## 1027             890       Eternatus Eternamax  ...           1.0            1.0\n## 189              150             Mega Mewtwo X  ...           1.0            2.0\n## 190              150             Mega Mewtwo Y  ...           1.0            1.0\n## 458              384             Mega Rayquaza  ...           1.0            2.0\n## 454              382             Primal Kyogre  ...           0.5            1.0\n## 456              383            Primal Groudon  ...           0.5            0.5\n## 930              800            Ultra Necrozma  ...           1.0            2.0\n## 584              493                    Arceus  ...           1.0            1.0\n## 1022             888      Zacian Crowned Sword  ...           1.0            0.5\n## 1024             889  Zamazenta Crowned Shield  ...           0.5            1.0\n## \n## [10 rows x 49 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total_points')\n##       pokedex_number                  name  ... against_steel  against_fairy\n## 871              746  Wishiwashi Solo Form  ...           0.5            1.0\n## 232              191               Sunkern  ...           1.0            1.0\n## 954              824               Blipbug  ...           1.0            1.0\n## 1003             872                  Snom  ...           2.0            1.0\n## 351              298               Azurill  ...           2.0            1.0\n## 478              401             Kricketot  ...           1.0            1.0\n## 13                10              Caterpie  ...           1.0            1.0\n## 16                13                Weedle  ...           1.0            0.5\n## 317              265               Wurmple  ...           1.0            1.0\n## 332              280                 Ralts  ...           2.0            1.0\n## \n## [10 rows x 49 columns]\n\n\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing. In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\npoke[order(poke$total_points, decreasing = T)[1:5],]\n## # A tibble: 5 × 49\n##   pokedex_n…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n## 1         890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n## 2         150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n## 3         150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n## 4         384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n## 5         382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\npoke[order(poke$total_points, decreasing = T)[1:round(nrow(poke)*.01)],]\n## # A tibble: 10 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1        890 Eter… .             8 Legen… Gigant…       2 Poison Dragon   100  \n##  2        150 Mega… Mewtu         1 Legen… Geneti…       2 Psych… Fight…     2.3\n##  3        150 Mega… Mewtu         1 Legen… Geneti…       1 Psych… .          1.5\n##  4        384 Mega… Rayqua…       3 Legen… Sky Hi…       2 Dragon Flying    10.8\n##  5        382 Prim… Kyogre        3 Legen… Sea Ba…       1 Water  .          9.8\n##  6        383 Prim… Groudon       3 Legen… Contin…       2 Ground Fire       5  \n##  7        800 Ultr… Necroz…       7 Legen… Prism …       2 Psych… Dragon     7.5\n##  8        493 Arce… Arceus        4 Mythi… Alpha …       1 Normal .          3.2\n##  9        888 Zaci… .             8 Legen… Warrio…       2 Fairy  Steel      2.8\n## 10        889 Zama… .             8 Legen… Warrio…       2 Fight… Steel      2.9\n## # … with 39 more variables: weight_kg &lt;chr&gt;, abilities_number &lt;dbl&gt;,\n## #   ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;, ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;,\n## #   hp &lt;dbl&gt;, attack &lt;dbl&gt;, defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;,\n## #   speed &lt;dbl&gt;, catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;,\n## #   base_experience &lt;chr&gt;, growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;,\n## #   egg_type_1 &lt;chr&gt;, egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;,\n## #   egg_cycles &lt;chr&gt;, against_normal &lt;dbl&gt;, against_fire &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\n\n\nTry it out: Filtering\n\n\n\n\n\nProblem\nR: dplyr\nPython\nBase R\n\n\n\nUsing the Pokemon data, can you create a new data frame that has only water type Pokemon? Can you write a filter statement that looks for any Pokemon which has water type for either type1 or type2?\n\n\n\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\nfilter(poke, type_1 == \"Water\")\n## # A tibble: 134 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 124 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\nfilter(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## # A tibble: 153 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 143 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n# The conditions have to be separated by |, which means \"or\"\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\npoke.query(\"type_1=='Water'\")\n##       pokedex_number            name  ... against_steel  against_fairy\n## 9                  7        Squirtle  ...           0.5            1.0\n## 10                 8       Wartortle  ...           0.5            1.0\n## 11                 9       Blastoise  ...           0.5            1.0\n## 12                 9  Mega Blastoise  ...           0.5            1.0\n## 72                54         Psyduck  ...           0.5            1.0\n## ...              ...             ...  ...           ...            ...\n## 964              834         Drednaw  ...           1.0            1.0\n## 976              846        Arrokuda  ...           0.5            1.0\n## 977              847     Barraskewda  ...           0.5            1.0\n## 1016             882       Dracovish  ...           0.5            2.0\n## 1017             883       Arctovish  ...           1.0            1.0\n## \n## [134 rows x 49 columns]\npoke.query(\"type_1=='Water'|type_2=='Water'\")\n# The conditions have to be separated by |, which means \"or\"\n##       pokedex_number            name  ... against_steel  against_fairy\n## 9                  7        Squirtle  ...           0.5            1.0\n## 10                 8       Wartortle  ...           0.5            1.0\n## 11                 9       Blastoise  ...           0.5            1.0\n## 12                 9  Mega Blastoise  ...           0.5            1.0\n## 72                54         Psyduck  ...           0.5            1.0\n## ...              ...             ...  ...           ...            ...\n## 975              845       Cramorant  ...           0.5            1.0\n## 976              846        Arrokuda  ...           0.5            1.0\n## 977              847     Barraskewda  ...           0.5            1.0\n## 1016             882       Dracovish  ...           0.5            2.0\n## 1017             883       Arctovish  ...           1.0            1.0\n## \n## [153 rows x 49 columns]\n\n\n\n\npoke &lt;- read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/pokemon_ascii.csv\")\n\nsubset(poke, type_1 == \"Water\")\n## # A tibble: 134 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 124 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n\nsubset(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## # A tibble: 153 × 49\n##    pokedex_…¹ name  germa…² gener…³ status species type_…⁴ type_1 type_2 heigh…⁵\n##         &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n##  1          7 Squi… Schiggy       1 Normal Tiny T…       1 Water  .          0.5\n##  2          8 Wart… Schill…       1 Normal Turtle…       1 Water  .          1  \n##  3          9 Blas… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  4          9 Mega… Turtok        1 Normal Shellf…       1 Water  .          1.6\n##  5         54 Psyd… Enton         1 Normal Duck P…       1 Water  .          0.8\n##  6         55 Gold… Entoron       1 Normal Duck P…       1 Water  .          1.7\n##  7         60 Poli… Quapsel       1 Normal Tadpol…       1 Water  .          0.6\n##  8         61 Poli… Quaput…       1 Normal Tadpol…       1 Water  .          1  \n##  9         62 Poli… Quappo        1 Normal Tadpol…       2 Water  Fight…     1.3\n## 10         72 Tent… Tentac…       1 Normal Jellyf…       2 Water  Poison     0.9\n## # … with 143 more rows, 39 more variables: weight_kg &lt;chr&gt;,\n## #   abilities_number &lt;dbl&gt;, ability_1 &lt;chr&gt;, ability_2 &lt;chr&gt;,\n## #   ability_hidden &lt;chr&gt;, total_points &lt;dbl&gt;, hp &lt;dbl&gt;, attack &lt;dbl&gt;,\n## #   defense &lt;dbl&gt;, sp_attack &lt;dbl&gt;, sp_defense &lt;dbl&gt;, speed &lt;dbl&gt;,\n## #   catch_rate &lt;chr&gt;, base_friendship &lt;chr&gt;, base_experience &lt;chr&gt;,\n## #   growth_rate &lt;chr&gt;, egg_type_number &lt;dbl&gt;, egg_type_1 &lt;chr&gt;,\n## #   egg_type_2 &lt;chr&gt;, percentage_male &lt;chr&gt;, egg_cycles &lt;chr&gt;, …\n# The conditions have to be separated by |, which means \"or\""
  },
  {
    "objectID": "data-cleaning-verbs.html#select-pick-columns",
    "href": "data-cleaning-verbs.html#select-pick-columns",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.4 Select: Pick columns",
    "text": "9.4 Select: Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\n\n\n\n\nAn exhaustive(?) list of ways to select variables in dplyr\n\n\n\n\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(&lt;set of variables&gt;) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(&lt;set of vars 1&gt;) & (&lt;set of vars 2&gt;) will give you any variables that are in both set 1 and set 2. (&lt;set of vars 1&gt;) | (&lt;set of vars 2&gt;) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\n\n\nLet’s try these selector functions out and see what we can accomplish!\n\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\nWe’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n\n\n\n\n\nTip\n\n\n\nYou might want to try out your EDA skills from the previous module to see what you can find out about the dataset, before seeing how select() works.\n\n\nWe could get a data frame of departure information for each flight:\n\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nflights %&gt;%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time &lt;int&gt;,\n## #   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n## #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nNote that everything() won’t duplicate columns you’ve already added.\nExploring the difference between bare name selection and all_of()/any_of()\n\nflights %&gt;%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 × 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;         &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, 1 more variable: time_hour &lt;dttm&gt;, and abbreviated\n## #   variable names ¹​sched_arr_time, ²​air_time\n\nvarlist &lt;- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %&gt;%\n  select(all_of(varlist))\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;         &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nvarlist &lt;- c(varlist, \"whoops\")\n\nflights %&gt;%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `whoops` doesn't exist.\n\nflights %&gt;%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;      &lt;int&gt;          &lt;int&gt;    &lt;int&gt;         &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\nFirst, let’s install the nycflights13 package[3] in python with pip install nycflights13.\n\nfrom nycflights13 import flights\n\nSelect operations are not as easy in python as they are using select() with helpers.\n\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\nList Comprehensions\nIn Python, there are certain shorthands called “list comprehensions” [4] that can perform similar functions to e.g. the matches() function in dplyr.\nSuppose we want to get all columns containing the word ‘time’. We could iterate through the list of columns (flights.columns) and add the column name any time we detect the word ‘time’ within. That is essentially what the following code does:\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n\nfor col in flights.columns iterates through the list of columns; if 'time' in col detects the presence of the word ‘time’, and the col out front adds the time-containing col to the array of columns to keep.\nSelecting columns in Python\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\nselcols.extend([\"whoops\"])\nselcols\n\n# Subset the data frame\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\nflights.loc[:,selcols]\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['whoops'] not in index\"\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\n\n\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. matches in dplyr.\ngrepl is a shorthand function for grep, which searches for a pattern in a vector of strings. grepl returns a logical vector indicating whether the pattern (\"dep\", in this case) was found in the vector (names(flights), in this case).\n\n\ndepcols &lt;- names(flights)[grepl(\"dep\", names(flights))]\ncollist &lt;- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nnew_order &lt;- names(flights)\nnew_order &lt;- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time &lt;int&gt;,\n## #   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n## #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nThis is less convenient than dplyr::everything in part because it depends on us to get the column indexes right.\n\n\n\n\n\n\n\n\n\ndplyr::relocate\n\n\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time &lt;int&gt;,\n## #   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n## #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\n# move numeric variables to the front\nflights %&gt;% relocate(where(is.numeric))\n## # A tibble: 336,776 × 19\n##     year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ flight\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;\n##  1  2013     1     1      517         515       2     830     819      11   1545\n##  2  2013     1     1      533         529       4     850     830      20   1714\n##  3  2013     1     1      542         540       2     923     850      33   1141\n##  4  2013     1     1      544         545      -1    1004    1022     -18    725\n##  5  2013     1     1      554         600      -6     812     837     -25    461\n##  6  2013     1     1      554         558      -4     740     728      12   1696\n##  7  2013     1     1      555         600      -5     913     854      19    507\n##  8  2013     1     1      557         600      -3     709     723     -14   5708\n##  9  2013     1     1      557         600      -3     838     846      -8     79\n## 10  2013     1     1      558         600      -2     753     745       8    301\n## # … with 336,766 more rows, 9 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n## #   hour &lt;dbl&gt;, minute &lt;dbl&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;,\n## #   dest &lt;chr&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names\n## #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay"
  },
  {
    "objectID": "data-cleaning-verbs.html#mutate-add-and-transform-variables",
    "href": "data-cleaning-verbs.html#mutate-add-and-transform-variables",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.5 Mutate: Add and transform variables",
    "text": "9.5 Mutate: Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate, but in base R and python, we’ll simply use assignment to add columns to our data frames.\n\n\nMutate (by Allison Horst)\n\nLets use the police violence data to demonstrate. There is an entry labeled “Unknown race” for victim’s race, but this is essentially equivalent to NA (missing data). So we should replace the Unknown race entry with NA so that those data points are handled consistently.\n\n\nBase R\nR: dplyr\nPython\n\n\n\n\nlibrary(readxl)\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\n\n# There are two categories for \"unknown race\"\ntable(police_violence$`Victim's race`, useNA = 'ifany')\n## \n##            Asian            Black         Hispanic  Native American \n##              142             2498             1777              139 \n## Pacific Islander     Unknown race            White \n##               60              976             4350\n\n# First, copy the original data into a race variable that is easy to access (no spaces)\npolice_violence$race &lt;- police_violence$`Victim's race`\n# Then, overwrite \"Unknown race\" with NA\npolice_violence$race[police_violence$`Victim's race` == \"Unknown race\"] &lt;- NA\n\n# Fixed!\ntable(police_violence$race, useNA = 'ifany')\n## \n##            Asian            Black         Hispanic  Native American \n##              142             2498             1777              139 \n## Pacific Islander            White             &lt;NA&gt; \n##               60             4350              976\n\nNotice that we had to type the name of the dataset at least 4 times to perform the operation we were looking for. I could reduce that a bit to 2x with the ifelse function, but it’s still a lot of typing.\n\n\n\nlibrary(readxl)\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\n\nlibrary(dplyr)\npolice_violence %&gt;%\n  mutate(race = ifelse(`Victim's race` == \"Unknown race\", NA, `Victim's race`)) %&gt;%\n  select(`Victim's race`, race) %&gt;%\n  table(useNA = 'ifany')\n##                   race\n## Victim's race      Asian Black Hispanic Native American Pacific Islander White\n##   Asian              142     0        0               0                0     0\n##   Black                0  2498        0               0                0     0\n##   Hispanic             0     0     1777               0                0     0\n##   Native American      0     0        0             139                0     0\n##   Pacific Islander     0     0        0               0               60     0\n##   Unknown race         0     0        0               0                0     0\n##   White                0     0        0               0                0  4350\n##                   race\n## Victim's race      &lt;NA&gt;\n##   Asian               0\n##   Black               0\n##   Hispanic            0\n##   Native American     0\n##   Pacific Islander    0\n##   Unknown race      976\n##   White               0\n\nThe last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab.\n\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with).\n\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence[\"race\"] = police_violence[\"Victim's race\"].replace(\"Unknown race\", pd.NA)\npolice_violence.race\n\n# This doesn't actually work :(\n## 0           &lt;NA&gt;\n## 1           &lt;NA&gt;\n## 2           &lt;NA&gt;\n## 3           &lt;NA&gt;\n## 4           &lt;NA&gt;\n##           ...   \n## 9937       Black\n## 9938    Hispanic\n## 9939    Hispanic\n## 9940       White\n## 9941       White\n## Name: race, Length: 9942, dtype: object\npolice_violence.groupby('race', dropna=False)['race'].count()\n## race\n## Asian                142\n## Black               2498\n## Hispanic            1777\n## Native American      139\n## Pacific Islander      60\n## White               4350\n## NaN                    0\n## Name: race, dtype: int64\n\nUnfortunately, for some reason, I can’t seem to get pandas to count the NAs once they’re in the data frame.\nAnother function that may be useful is the assign function, which can be used to create new variables if you don’t want to use the [\"new_col\"] notation. In some circumstances, .assign(var = ...) is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I’d like to not get into right now for simplicity’s sake).\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables.\n\n\n\n\n\n\nMutate and new challenges\n\n\n\n\n\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation and google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\nSource\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a quick table of places to look in R and python to solve some of the more common problems.\n\n\nProblem\nR\nPython\n\n\n\nDates and Times\n\nlubridate package (esp. ymd_hms() and variants, decimal_date(), and other convenience functions)\n\npandas has some date time support by default; see the datetime module for more functionality.\n\n\nString manipulation\n\nstringr package\nQuick Tips [5], Whirlwind Tour of Python chapter [6]"
  },
  {
    "objectID": "data-cleaning-verbs.html#summarize",
    "href": "data-cleaning-verbs.html#summarize",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.6 Summarize",
    "text": "9.6 Summarize\nThe next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we’ve used e.g. group_by + count in Python to perform certain tasks as well.\nHere (in a trivial example), I compute the overall average age of a victim of police violence, and then also compute the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes.\n\n\nR: dplyr\nPython\n\n\n\n\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\npolice_violence %&gt;%\n  mutate(age = as.numeric(`Victim's age`),\n         name_length = nchar(`Victim's name`)) %&gt;%\n  summarize(age = mean(age, na.rm = T), name_length = mean(name_length))\n## # A tibble: 1 × 2\n##     age name_length\n##   &lt;dbl&gt;       &lt;dbl&gt;\n## 1  36.8        16.7\n\n\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as mean. You can also build custom summary functions [7], or use the agg() function to define multiple summary variables.\n\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence = police_violence.assign(\n  age = pd.to_numeric(police_violence[\"Victim's age\"], errors='coerce'),\n  name_length = police_violence[\"Victim's name\"].str.len()\n)\n\npolice_violence[[\"age\", \"name_length\"]].mean()\n## age            36.846476\n## name_length    16.659022\n## dtype: float64\npolice_violence[[\"age\", \"name_length\"]].agg(['mean', 'min'])\n##             age  name_length\n## mean  36.846476    16.659022\n## min    1.000000     7.000000\n\n\n\n\nThe real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail."
  },
  {
    "objectID": "data-cleaning-verbs.html#group-by-power",
    "href": "data-cleaning-verbs.html#group-by-power",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.7 Group By + (?) = Power!",
    "text": "9.7 Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\nsplit my dataset into smaller datasets - one for each day\ncompute summary values for each smaller dataset\nput my summarized data back together into a single dataset\n\nThis is known as the split-apply-combine [9] or sometimes, map-reduce [10] strategy (though map-reduce is usually on specifically large datasets and performed in parallel).\nIn tidy parlance, group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well.\nLet’s see how things change when we calculate the average age and name length of victims of police violence by their recorded race.\n\n\nR: dplyr\nPython\n\n\n\n\npolice_violence &lt;- read_xlsx(\"data/police_violence.xlsx\", sheet = 1, guess_max = 7000, skip = 1)\npolice_violence %&gt;%\n  mutate(age = as.numeric(`Victim's age`),\n         name_length = nchar(`Victim's name`)) %&gt;%\n  group_by(`Victim's race`) %&gt;%\n  summarize(age = mean(age, na.rm = T), name_length = mean(name_length))\n## # A tibble: 7 × 3\n##   `Victim's race`    age name_length\n##   &lt;chr&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Asian             36.1        14.0\n## 2 Black             32.4        16.3\n## 3 Hispanic          33.6        17.2\n## 4 Native American   31.8        17.3\n## 5 Pacific Islander  33.6        15.8\n## 6 Unknown race      43.1        17.1\n## 7 White             39.7        16.6\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\npolice_violence = pd.read_excel(\"data/police_violence.xlsx\", skiprows=1)\n\npolice_violence = police_violence.assign(\n  age = pd.to_numeric(police_violence[\"Victim's age\"], errors='coerce'),\n  name_length = police_violence[\"Victim's name\"].str.len()\n)\n\npolice_violence.groupby(\"Victim's race\")[[\"age\", \"name_length\"]].agg(\"mean\")\n##                         age  name_length\n## Victim's race                           \n## Asian             36.107143    13.978873\n## Black             32.415209    16.345076\n## Hispanic          33.604389    17.214406\n## Native American   31.789474    17.266187\n## Pacific Islander  33.650000    15.816667\n## Unknown race      43.073292    17.081967\n## White             39.733943    16.597241\n\n\n\n\nWhen you group_by a variable, your result carries this grouping with it. In R, summarize will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command. In Python, you should consider using reset_index or grouped_thing.obj() to access the original information[11].\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\n\n\n\n\n\nStorms Example\n\n\n\nLet’s try a non-trivial example, using the storms dataset that is part of the dplyr package.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 11,859 × 13\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;ord&gt;   &lt;int&gt;   &lt;int&gt;\n##  1 Amy    1975     6    27     0  27.5 -79   tropical dep… -1         25    1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical dep… -1         25    1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical dep… -1         25    1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical dep… -1         25    1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical dep… -1         25    1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical dep… -1         25    1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical dep… -1         25    1011\n##  8 Amy    1975     6    28    18  34   -77   tropical dep… -1         30    1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical sto… 0          35    1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical sto… 0          40    1002\n## # … with 11,849 more rows, 2 more variables:\n## #   tropicalstorm_force_diameter &lt;int&gt;, hurricane_force_diameter &lt;int&gt;, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n\n\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n\n\n\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes).\nOne thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way:\n\nThe points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause).\nIf there’s a tie, we might want to know when the maximum wind speed occurred.\nIf that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred.\n\nLet’s start by translating these criteria into basic operations. I’ll use dplyr function names here, but I’ll also specify what I mean when there’s a conflict (e.g. filter in dplyr means something different than filter in python).\nInitial attempt:\n\n\nFor each storm (group_by),\nwe need the point where the storm has lowest atmospheric pressure. (filter - pick the row with the lowest pressure).\n\nThen we read the next part: “If there is a tie, pick the maximum wind speed.”\n\ngroup_by\n\narrange by ascending pressure and descending wind speed\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\ngroup_by\n\narrange by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nsummarize - compute the average time and category (if there are multiple rows)\n\nLet’s write the code, now that we have the order of operations straight!\n\n\nR\nPython\n\n\n\n\nmax_power_storm &lt;- storms %&gt;%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %&gt;%\n  filter(pressure == min(pressure, na.rm = T)) %&gt;%\n  filter(wind == max(wind, na.rm = T)) %&gt;%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %&gt;%\n  arrange(time) %&gt;%\n  ungroup()\nmax_power_storm\n## # A tibble: 512 × 7\n##    name      year pressure  wind category status         time               \n##    &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;    &lt;chr&gt;          &lt;dttm&gt;             \n##  1 Amy       1975      981    60 0        tropical storm 1975-07-02 12:00:00\n##  2 Caroline  1975      963   100 3        hurricane      1975-08-31 06:00:00\n##  3 Doris     1975      965    95 2        hurricane      1975-09-02 21:00:00\n##  4 Belle     1976      957   105 3        hurricane      1976-08-09 00:00:00\n##  5 Gloria    1976      970    80 1        hurricane      1976-09-30 00:00:00\n##  6 Anita     1977      926   150 5        hurricane      1977-09-02 06:00:00\n##  7 Clara     1977      993    65 1        hurricane      1977-09-08 12:00:00\n##  8 Evelyn    1977      994    65 1        hurricane      1977-10-15 00:00:00\n##  9 Amelia    1978     1005    45 0        tropical storm 1978-07-31 00:00:00\n## 10 Bess      1978     1005    40 0        tropical storm 1978-08-07 12:00:00\n## # … with 502 more rows\n\n\n\n\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n\n\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## &lt;ggplot: (8756197382509)&gt;\n\n\n\n\n\n\n\nWe could also look to see whether there has been any change over time in pressure.\n\n\nR\nPython\n\n\n\n\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n\n\n\n\n\n\n\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## &lt;ggplot: (8756333835469)&gt;\n\n\n\n\n\n\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic (see this coverage map from 1995).\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n\nR\nPython\n\n\n\n\nstorm_strength_duration &lt;- storms %&gt;%\n  group_by(name, year) %&gt;%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %&gt;%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n\n\n\n\n\n\n\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## &lt;ggplot: (8756333828127)&gt;\n\n\n\n\n\n\n\nYou don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with.\nWe could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate)\nDiameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables).\n\n\nR\nPython\n\n\n\nNote the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted.\n\nstorm_evolution &lt;- storms %&gt;%\n  filter(!is.na(hurricane_force_diameter)) %&gt;%\n  group_by(name, year) %&gt;%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %&gt;%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n\n\n\n\n\n\n\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## &lt;ggplot: (8756331529991)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/facets/facet.py:390: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\nFor this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms.\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n\nR\nPython\n\n\n\n\nstorm_evolution %&gt;%\n  filter(year == 2008) %&gt;%\n  arrange(desc(time_since_start))\n## # A tibble: 327 × 15\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;ord&gt;   &lt;int&gt;   &lt;int&gt;\n##  1 Ike    2008     9    14     6  35.5 -93.7 tropical sto… 0          35     985\n##  2 Ike    2008     9    14     0  33.5 -94.9 tropical sto… 0          35     980\n##  3 Ike    2008     9    13    18  31.7 -95.3 tropical sto… 0          50     974\n##  4 Ike    2008     9    13    12  30.3 -95.2 hurricane     2          85     959\n##  5 Ike    2008     9    13     7  29.3 -94.7 hurricane     2          95     950\n##  6 Ike    2008     9    13     6  29.1 -94.6 hurricane     2          95     951\n##  7 Ike    2008     9    13     0  28.3 -94   hurricane     2          95     952\n##  8 Fay    2008     8    27     0  35   -85.8 tropical dep… -1         15    1005\n##  9 Ike    2008     9    12    18  27.5 -93.2 hurricane     2          95     954\n## 10 Fay    2008     8    26    18  34.6 -86.5 tropical dep… -1         20    1004\n## # … with 317 more rows, 4 more variables: tropicalstorm_force_diameter &lt;int&gt;,\n## #   hurricane_force_diameter &lt;int&gt;, time &lt;dttm&gt;, time_since_start &lt;drtn&gt;, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorm_evolution %&gt;% filter(name == \"Ike\") %&gt;%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n\n\n\n\n\n\n\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n##      name  year  ...                time              age\n## 8000  Ike  2008  ... 2008-09-14 06:00:00 13 days 00:00:00\n## 7999  Ike  2008  ... 2008-09-14 00:00:00 12 days 18:00:00\n## 7998  Ike  2008  ... 2008-09-13 18:00:00 12 days 12:00:00\n## 7997  Ike  2008  ... 2008-09-13 12:00:00 12 days 06:00:00\n## 7996  Ike  2008  ... 2008-09-13 07:00:00 12 days 01:00:00\n## \n## [5 rows x 12 columns]\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## &lt;ggplot: (8756331290631)&gt;\n\n\n\n\n\n\n\n\n\n\n9.7.1 Summarizing Across Multiple Variables\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time.\n\n\ndplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)\n\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead.\n\n\nR\nPython\n\n\n\n\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms &lt;- storms %&gt;%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity &lt;- storms %&gt;%\n  filter(status == \"hurricane\") %&gt;%\n  group_by(name) %&gt;%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %&gt;%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %&gt;%\n  arrange(desc(wind)) %&gt;% \n  # get top 10\n  filter(row_number() &lt;= 10) %&gt;%\n  knitr::kable() # Make into a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nmonth\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\nAndrew\n1992\n8.000000\n118.2609\n946.6522\nNaN\nNaN\n\n\nMitch\n1998\n10.000000\n115.9091\n945.3182\nNaN\nNaN\n\n\nRita\n2005\n9.000000\n114.7368\n931.6316\n265.2941\n97.05882\n\n\nIsabel\n2003\n9.000000\n112.1875\n946.5417\nNaN\nNaN\n\n\nGilbert\n1988\n9.000000\n110.8929\n945.4286\nNaN\nNaN\n\n\nLuis\n1995\n8.928571\n110.5952\n948.6190\nNaN\nNaN\n\n\nWilma\n2005\n10.000000\n110.3030\n939.4242\n349.8333\n118.33333\n\n\nMatthew\n2016\n9.880952\n109.5238\n952.1190\n263.5714\n62.02381\n\n\nHugo\n1989\n9.000000\n106.5789\n950.9211\nNaN\nNaN\n\n\nDavid\n1979\n8.457143\n105.1429\n956.1429\nNaN\nNaN\n\n\n\n\n\n\n\nStackoverflow reference\nIn the interests of using the same example, I’ve exported dplyr’s storms data to CSV.\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a lambda function - basically, a “dummy” function that has some arguments but not all of the arguments. Here, our lambda function is a function of x, and we calculate x.mean(skipna=True) for each x passed in (so, for each column).\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]"
  },
  {
    "objectID": "data-cleaning-verbs.html#try-it-out",
    "href": "data-cleaning-verbs.html#try-it-out",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.8 Try it out",
    "text": "9.8 Try it out\nYou can read about the gapminder project here.\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. The gapminder package in python (install with pip install gapminder) is a port of the R package, but doesn’t contain the unfiltered data, so we’ll instead use a CSV export.\n\n\n\n\n\n\nRead in the Data\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 3,303 more rows\n\n\n\n\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://github.com/srvanderplas/unl-stat850/raw/main/data/gapminder_unfiltered.csv\")\n\n\n\n\n\n9.8.1 Task 1: How Bad is It?\n\n\nProblem\nR\nPython\n\n\n\nUsing your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information.\n\n\n\ngapminder_unfiltered %&gt;% \n  group_by(country) %&gt;% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %&gt;%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 × 5\n##    country        n missinglifeExp missingpop missingGDP\n##    &lt;fct&gt;      &lt;int&gt;          &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # … with 73 more rows\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n\n9.8.2 Task 2: Exclude any data which isn’t at 5-year increments\nStart in 1952 (so 1952, 1957, 1962, …, 2007).\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 2,003 more rows\n\n\n\nReminder about python list comprehensions\nExplanation of the query @ statement\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n\n\n\n\n\n9.8.3 Task 3: Exclude any countries that don’t have a full set of observations\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %&gt;%\n  filter(year %in% seq(1952, 2007, by = 5)) %&gt;%\n  group_by(country) %&gt;%\n  mutate(nobs = n()) %&gt;% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %&gt;%\n  select(-nobs)\n## # A tibble: 1,704 × 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,694 more rows\n\n\n\n\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nBase R data manipulation\n\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.\n\n\nTidyTuesday Python github repo - replicating Tidy Tuesday analyses in Python with Pandas"
  },
  {
    "objectID": "data-cleaning-verbs.html#references",
    "href": "data-cleaning-verbs.html#references",
    "title": "9  Data Cleaning and Manipulation",
    "section": "\n9.9 References",
    "text": "9.9 References\n\n\n\n\n[1] \nPandas, “Indexing and selecting data,” Pandas 1.4.3 Documentation. 2022 [Online]. Available: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing. [Accessed: Jun. 30, 2022]\n\n\n[2] \npwwang, “Datar: A Grammar of Data Manipulation in python.” May 2022 [Online]. Available: https://pwwang.github.io/datar/. [Accessed: Jun. 30, 2022]\n\n\n[3] \nM. Chow, “nycflights13: A data package for nyc flights (the nycflights13 R package).” 2020 [Online]. Available: https://github.com/machow/nycflights13. [Accessed: Jun. 30, 2022]\n\n\n[4] \nPython Foundation, “Data Structures,” Python 3.10.5 documentation. Jun. 2022 [Online]. Available: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions. [Accessed: Jun. 30, 2022]\n\n\n[5] \nC. Nguyen, “Tips for String Manipulation in Python,” Towards Data Science. Sep. 2021 [Online]. Available: https://towardsdatascience.com/tips-for-string-manipulation-in-python-92b1fc3f4d9f. [Accessed: Jul. 01, 2022]\n\n\n[6] \nJ. VanderPlas, “String Manipulation and Regular Expressions,” in A Whirlwind Tour of Python, O’Reilly Media, 2016 [Online]. Available: https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html. [Accessed: Jul. 01, 2022]\n\n\n[7] \nC. Whorton, “Applying Custom Functions to Groups of Data in Pandas,” Medium. Jul. 2021 [Online]. Available: https://towardsdatascience.com/applying-custom-functions-to-groups-of-data-in-pandas-928d7eece0aa. [Accessed: Jul. 01, 2022]\n\n\n[8] \nH. Wickham, “The split-apply-combine strategy for data analysis,” Journal of statistical software, vol. 40, pp. 1–29, 2011. \n\n\n[9] \n\n“Group by: Split-apply-combine,” in Pandas 1.4.3 documentation, Python, 2022 [Online]. Available: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html. [Accessed: Jul. 01, 2022]\n\n\n[10] \nJ. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008, doi: 10.1145/1327452.1327492. [Online]. Available: https://doi.org/10.1145/1327452.1327492. [Accessed: Jul. 01, 2022]\n\n\n[11] \nM. Dancho, “Answer to \"Is there an \"ungroup by\" operation opposite to .groupby in pandas?\",” Stack Overflow. Mar. 2021 [Online]. Available: https://stackoverflow.com/a/66879388/2859168. [Accessed: Jul. 01, 2022]"
  },
  {
    "objectID": "data-cleaning-verbs.html#footnotes",
    "href": "data-cleaning-verbs.html#footnotes",
    "title": "9  Data Cleaning and Manipulation",
    "section": "",
    "text": "See this twitter thread for some horror stories. This tweet is also pretty good at showing one type of messiness.↩︎\nThe philosophy includes a preference for pipes, but this preference stems from the belief that code should be readable in the same way that text is readable.↩︎\nIt accomplishes this through the magic of quasiquotation, which we will not cover in this course because it’s basically witchcraft.↩︎"
  },
  {
    "objectID": "data-transformations.html#module6-objectives",
    "href": "data-transformations.html#module6-objectives",
    "title": "10  Data Transformations",
    "section": "Module Objectives",
    "text": "Module Objectives\nBroadly, your objective while reading this chapter is to be able to identify datasets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should be master the following concepts:\n\nDetermine what data format is necessary to generate a desired plot or statistical model\nJoin and split data columns using string operations\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures\nUnderstand relational data formats and how to use data joins to assemble data from multiple tables into a single table."
  },
  {
    "objectID": "data-transformations.html#tidy-and-messy-data",
    "href": "data-transformations.html#tidy-and-messy-data",
    "title": "10  Data Transformations",
    "section": "\n10.1 Tidy and Messy Data",
    "text": "10.1 Tidy and Messy Data\n\n10.1.1 Motivating Example\nConsider the spreadsheet screenshot in Figure 10.1.\n\n\nFigure 10.1: Spreadsheet intended for human consumption, from [1] (Chapter 3)\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. Figure 10.2 shows a restructured version of this data in a more compact rectangular format.\n\n\nFigure 10.2: Spreadsheet reorganized for data analysis\n\nIn Figure 10.2, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as “tidy” data: it has a single column for each variable and a single row for each observation.\n\n10.1.2 Defining Tidy data\nThe illustrations below are lifted from an excellent blog post [2] about tidy data; they’re reproduced here because\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\nFigure 10.3: Tidy data format, illustrated.\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the dataset, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own dataset.\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\nFigure 10.4: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\n\n\n\n\n\nExamples: Messy Data\n\n\n\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Classifying Messy Data\n\n\n\n\n\nProblem\nTable 1\n2\n3\n4\n5\n\n\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nEach variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first).\n\n\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach dataset goes into its own table (or tibble, if you are using R)\nEach variable gets its own column\n\n\n\n\n\n\n\nAdditional reading\n\n\n\n[3] - IBM SPSS ad that talks about the perils of spreadsheets\n[4] - assembled news stories involving spreadsheet mishaps\n\n\nBy the end of this chapter, you will have the skills needed to wrangle the most common “messy” data sets into “tidy” form."
  },
  {
    "objectID": "data-transformations.html#string-operations",
    "href": "data-transformations.html#string-operations",
    "title": "10  Data Transformations",
    "section": "\n10.2 String operations",
    "text": "10.2 String operations\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\nThe stringr cheatsheet by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n\nTable 10.1: Table of string functions in R and python. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\nTask\nR\nPython\n\n\n\nReplace pattern with replacement\n\n\nbase: gsub(pattern, replacement, x)\nstringr: str_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\npandas: x.str.replace(pattern, replacement) (not vectorized over pattern or replacement)\n\n\nConvert case\n\nbase: tolower(x), toupper(x)\nstringr: str_to_lower(x), str_to_upper(x) , str_to_title(x)\n\npandas: x.str.lower(), x.str.upper()\n\n\n\nStrip whitespace from start/end\n\nbase: trimws(x)\nstringr: str_trim(x) , str_squish(x)\n\npandas: x.str.strip()\n\n\n\nPad strings to a specific length\n\nbase: sprintf(format, x)\nstringr: str_pad(x, …)\n\npandas: x.str.pad()\n\n\n\nTest if the string contains a pattern\n\nbase: grep(pattern, x) or grepl(pattern, x)\nstringr: str_detect(x, pattern)\n\npandas: x.str.contains(pattern)\n\n\n\nCount how many times a pattern appears in the string\n\nbase: gregexpr(pattern, x) + sapply to count length of the returned list\nstringi: stri_count(x, pattern)\nstringr: str_count(x, pattern)\n\npandas: x.str.count(pattern)\n\n\n\nFind the first appearance of the pattern within the string\n\nbase: regexpr(pattern, x)\nstringr: str_locate(x, pattern)\n\npandas: x.str.find(pattern)\n\n\n\nFind all appearances of the pattern within the string\n\nbase: gregexpr\nstringr: str_locate_all(x, pattern)\n\npandas: x.str.findall(pattern)\n\n\n\nDetect a match at the start/end of the string\n\nbase: use regular expr.\nstringr: str_starts(x, pattern) ,str_ends(x, pattern)\n\npandas: x.str.startswith(pattern) , x.str.endswith(pattern)\n\n\n\nSubset a string from index a to b\n\nbase: substr(x, a, b)\nstringr: str_sub(x, a, b)\n\npandas: x.str.slice(a, b, step)\n\n\n\nConvert string encoding\n\nbase: iconv(x, encoding)\nstringr: str_conv(x, encoding)\n\npandas: x.str.encode(encoding)\n\n\n\n\n\nIn Table 10.1, multiple functions are provided for e.g. common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the stringr package, which is included in the tidyverse1.\nConverting strings to numbers\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\n\nTable 10.2: Different “messy” number formats\n\n\nint_col\nfloat_col\nmix_col\nmissing_col\nmoney_col\neu_numbers\nboolean_col\ncustom\n\n\n\n0\n1\n1.1\na\n1\n£1,000.00\n1.000.000,00\nTrue\nY\n\n\n1\n2\n1.2\n2\n2\n£2,400.00\n2.000.342,00\nFalse\nY\n\n\n2\n3\n1.3\n3\n3\n£2,400.00\n3.141,59\nTrue\nN\n\n\n3\n4\n4.7\n4\nnan\n£2,400.00\n34,25\nTrue\nN\n\n\n\n\n\nNote that numbers, currencies, dates, and times are written differently based on what country you’re in [5]. In computer terms, this is the “locale”, and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [6].\nIf you’ve never had to deal with the complexities of working on a laptop designed for one country using another country’s conventions, know that it isn’t necessarily the easiest thing to do.\n\n\n\n\n\n\nOptional: Locales\n\n\n\n\n\nFind your locale\n\n\n Type Get-WinSystemLocale into your CMD or powershell terminal.\n\n (10.4 and later) and  Type locale into your terminal\nGet set up to work with locales\nWhile this isn’t required, it may be useful and is definitely good practice if you’re planning to work with data generated internationally.\nThis article tells you how to set things up in linux . The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n\n Installing languages\n\n\n Change locales. Installing or creating new locales seems to be more complicated, and since I do not have a mac, I can’t test this out easily myself.\n\n\n\n\nWe’ll use Table 10.2 to explore different string operations focused specifically on converting strings to numbers.\n\n\nGet the data: Python\nR\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\")\n\n\n\n\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\", colClasses = \"character\")\n\nBy default, R tries to outsmart us and read the data in as numbers. I’ve disabled this behavior by setting colClasses='character' so that you can see how these functions work… but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don’t know how to disable it.\n\n\n\n\n\n\n\n\n\nConverting Columns Using Your Best Guess\n\n\n\nBoth R and Python have ways to “guess” what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you’re working with data that is already read in, how do you get R and Python to guess what type something is?\n\n\nR\nPython\n\n\n\nHere, R gets everything “right” except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn’t clearly numeric or doesn’t match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle mix_col, we would have to explicitly convert to numeric, causing the a to be converted to NA\n\nlibrary(dplyr)\nlibrary(readr)\ndf_guess &lt;- type_convert(df)\nstr(df_guess)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\nThe type_convert function has a locale argument; readr includes a locale() function that you can pass to type_convert that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\nlibrary(dplyr)\nlibrary(readr)\nfixed_df &lt;- type_convert(df) \nfixed_df2 &lt;- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\nAs in R, we can set the locale in Python to change how things are read in.\n\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Columns Directly\n\n\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in Section 3.3. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of ‘missing’ or NA variables that are not necessarily compatible.\n\n\nR\nPython\n\n\n\nHere, we use the across helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don’t work here, because they contain characters R doesn’t recognize as numeric characters.\n\nlibrary(dplyr)\n\ndf_numeric &lt;- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  1 0 1 1\n##  $ custom     : num  NA NA NA NA\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\n\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Converting Y/N data\n\n\n\nThe next thing we might want to do is convert our custom column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\nWe could use factors/categorical variables, which have numeric values “under the hood”, but show up as labeled.\nWe could (in this particular case) test for equality with “Y”, but this approach would not generalize well if we had more than 2 categories.\nWe could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I’ve used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n\nR\nPython\n\n\n\n\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %&gt;%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %&gt;%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':    4 obs. of  11 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\nWe’ve already done a brief demonstration of string methods in Python when we trimmed off the £ character. In this situation, it’s better to use the pandas replace method, which allows you to pass in a list of values and a list of replacements.\n\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers      float64\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object\n\n\n\n\n\n\n\n10.2.1 Using find and replace\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution2, and may introduce bugs if you use the same code to analyze new data with characters you haven’t anticipated, but in so many cases it’s also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\nI’ll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading [7] so that you know why find-and-replace isn’t (necessarily) the best option for locale-specific formatting.\n\n\n\n\n\n\nExample: find and replace\n\n\n\nLet’s start with the money column.\n\n\nR\nPython\n\n\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn’t work by default with type_convert, but as long as we mutate and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ‘,’ for decimal and ‘.’ for thousands separators.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n\n\n\nIn python, a similar approach doesn’t work out, because the pound sign is not handled correctly.\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): babel.numbers.NumberFormatError: '£1,000.00' is not a valid decimal number\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n# Remove £ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"£\")\n# Then parse the number\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Locale find-and-replace\n\n\n\nWe could also handle the locale issues using find-and-replace, if we wanted to…\n\n\nR\nPython\n\n\n\nNote that str_remove is shorthand for str_replace(x, pattern, \"\"). There is a little bit of additional complexity in switching “,” for “.” and vice versa - we have to change “,” to something else first, so that we can replace “.” with “,”. This is not elegant but it does work. It also doesn’t generalize - it will mess up numbers formatted using the US/UK convention, and it won’t handle numbers formatted using other conventions from other locales.\n\nfixed_df = df %&gt;%\n  type_convert() %&gt;% # guess everything\n  mutate(money_col = str_remove(money_col, \"£\") %&gt;% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %&gt;%\n           parse_number())\n\n\n\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\n## &lt;string&gt;:1: FutureWarning: The default value of regex will change from True to False in a future version.\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"£\").\\\nstr.replace(',', '')\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...  eu_numbers  boolean_col  custom\n## 0        1        1.1       a  ...  1000000.00         True       Y\n## 1        2        1.2       2  ...  2000342.00        False       Y\n## 2        3        1.3       3  ...     3141.59         True       N\n## 3        4        4.7       4  ...       34.25         True       N\n## \n## [4 rows x 8 columns]\n\n\n\n\n\n\n\n10.2.2 Separating multi-variable columns\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\nIn pandas, we use x.str.split() to split columns in a DataFrame, in R we use the tidyr package’s separate function.\n\n\n\n\n\n\nExample: Separating columns\n\n\n\nWe’ll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to separate the rate column into two new columns, cases and population.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate(table3, rate, into = c('cases', 'pop'), sep = \"/\", remove = F)\n## # A tibble: 6 × 5\n##   country      year rate              cases  pop       \n##   &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;     \n## 1 Afghanistan  1999 745/19987071      745    19987071  \n## 2 Afghanistan  2000 2666/20595360     2666   20595360  \n## 3 Brazil       1999 37737/172006362   37737  172006362 \n## 4 Brazil       2000 80488/174504898   80488  174504898 \n## 5 China        1999 212258/1272915272 212258 1272915272\n## 6 China        2000 213766/1280428583 213766 1280428583\n\n\n\n\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country  year               rate   cases         pop\n## 0  Afghanistan  1999       745/19987071     745    19987071\n## 1  Afghanistan  2000      2666/20595360    2666    20595360\n## 2       Brazil  1999    37737/172006362   37737   172006362\n## 3       Brazil  2000    80488/174504898   80488   174504898\n## 4        China  1999  212258/1272915272  212258  1272915272\n## 5        China  2000  213766/1280428583  213766  1280428583\n\nThis uses python’s multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g. cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time.\n\n\n\n\n\n\n10.2.3 Joining columns\nIt’s also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a ‘name’ column that has both pieces of information together.\n\n\n\n\n\n\nExample: Joining columns\n\n\n\nWe’ll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to join the century and year columns into a new column, yyyy.\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %&gt;%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 × 5\n##   country      yyyy century year  rate             \n##   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n\n\n\n\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000\n\n\n\n\n\n\n\n10.2.4 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears &lt;- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\n\n\n\n\n\n\nOptional: Short Regular Expressions Primer\n\n\n\n\n\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\n\nR\nPython\n\n\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn &lt;- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n\n\n\nIn python, a regular expression is indicated by putting the character ‘r’ right before the quoted expression. This tells python that any backslashes in the string should be left alone – if R had that feature, we wouldn’t have to escape all the backslashes!\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\n\n\n\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times – most useful when you’re looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\n\nR\nPython\n\n\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n\n\nnum_string &lt;- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn &lt;- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone &lt;- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid &lt;- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance &lt;- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n\n\n\n\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## &lt;re.Match object; span=(0, 6), match='banana'&gt;\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## &lt;re.Match object; span=(0, 2), match='ba'&gt;\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## &lt;re.Match object; span=(1, 5), match='anan'&gt;\n\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## &lt;re.Match object; span=(42, 53), match='123-45-6789'&gt;\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## &lt;re.Match object; span=(7, 19), match='123-456-7890'&gt;\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## &lt;re.Match object; span=(27, 35), match='12345678'&gt;\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## &lt;re.Match object; span=(77, 91), match='$50,000,000.23'&gt;\n\n\n\n\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n\nR\nPython\n\n\n\n\naddress &lt;- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num &lt;- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet &lt;- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet &lt;- str_remove(street, house_num) %&gt;% str_trim() # remove house number\n\ncity &lt;- str_extract(address, \",.*,\") %&gt;% str_remove_all(\",\") %&gt;% str_trim()\n\nzip &lt;- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\nPython match objects contain 3 things: .span(), which has the start and end positions of the match, .string, which contains the original string passed into the function, and .group(), which contains the actual matching portion of the string.\n\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n\n\n\n\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\n\n\nR\nPython\n\n\n\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants &lt;- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk &lt;- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears &lt;- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n\n\n\n\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n\n\n\n\nPutting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n\nR\nPython\n\n\n\n\nstrings &lt;- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex &lt;- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex &lt;- \"(walk|treat)\"\naddr_regex &lt;- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex &lt;- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 × 5\n##   text                                                   phone dog   addr  abab \n##   &lt;chr&gt;                                                  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]\n## \n## &lt;string&gt;:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## &lt;string&gt;:4: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## &lt;string&gt;:5: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n## &lt;string&gt;:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract."
  },
  {
    "objectID": "data-transformations.html#pivot-operations",
    "href": "data-transformations.html#pivot-operations",
    "title": "10  Data Transformations",
    "section": "\n10.3 Pivot operations",
    "text": "10.3 Pivot operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\nThe two operations we’ll learn here are wide -&gt; long and long -&gt; wide.\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n10.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n## * &lt;chr&gt;        &lt;int&gt;  &lt;int&gt;\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n## * &lt;chr&gt;            &lt;int&gt;      &lt;int&gt;\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntba &lt;- table4a %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb &lt;- table4b %&gt;% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %&gt;%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\n\nselect variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x &lt;- table4a %&gt;% mutate(id = \"cases\")\ntable4b.x &lt;- table4b %&gt;% mutate(id = \"population\")\n# Create one table\ntable4 &lt;- bind_rows(table4a.x, table4b.x)\n\ntable4_long &lt;- table4 %&gt;%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %&gt;%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;int&gt;\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy &lt;- table4_long %&gt;%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   &lt;chr&gt;       &lt;chr&gt;  &lt;int&gt;      &lt;int&gt;\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\nIn Pandas, pandas.melt(...) takes id_vars, value_vars, var_name, and value_name. Otherwise, it functions nearly exactly the same as pivot_longer; the biggest difference is that column selection works differently in python than it does in the tidyverse.\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n\nHere’s the melt/pivot_longer + pivot/pivot_wider version:\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year   cases  population\n## 0   Afghanistan  1999     745    19987071\n## 1   Afghanistan  2000    2666    20595360\n## 2        Brazil  1999   37737   172006362\n## 3        Brazil  2000   80488   174504898\n## 4         China  1999  212258  1272915272\n## 5         China  2000  213766  1280428583\n\n\n\n\n\n10.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\ntable2 %&gt;%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;      &lt;int&gt;\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country  year   cases  population\n## 0     Afghanistan  1999     745    19987071\n## 1     Afghanistan  2000    2666    20595360\n## 2          Brazil  1999   37737   172006362\n## 3          Brazil  2000   80488   174504898\n## 4           China  1999  212258  1272915272\n## 5           China  2000  213766  1280428583\n\n\n\n\n\n\n\n\n\n\nTry it Out!\n\n\n\nIn the next section, we’ll be using the WHO surveillance of disease incidence data (link). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form (Excel link). For demonstration purposes, I’ll continue using the messier 2020 data, but the link is no longer available on the WHO’s site.\nIt will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest.\n\n\nPreprocessing\nProblem\nR solution\nPython solution\n\n\n\nYou don’t have to understand what this code is doing just yet.\n\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\n\nsheets &lt;- excel_sheets(\"data/incidence_series.xls\")\nsheets &lt;- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence &lt;- map_df(sheets, ~read_xls(path =\"data/incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 &lt;- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 &lt;- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"data/incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"data/who_disease_incidence.csv\")\n\n\n\nDownload the exported data here and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\nCan you make a line plot of cases of measles in Bangladesh over time?\n\nhead(disease_incidence)\n## # A tibble: 6 × 43\n##   WHO_R…¹ ISO_c…² Cname Disease `2018` `2017` `2016` `2015` `2014` `2013` `2012`\n##   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 EMR     AFG     Afgh… CRS         NA     NA     NA      0      0      0     NA\n## 2 EUR     ALB     Alba… CRS          0      0     NA     NA     NA      0      0\n## 3 AFR     DZA     Alge… CRS         NA     NA      0      0     NA     NA      0\n## 4 EUR     AND     Ando… CRS          0      0      0     NA     NA      0      0\n## 5 AFR     AGO     Ango… CRS         NA     NA     NA     NA     NA     NA     NA\n## 6 AMR     ATG     Anti… CRS          0      0      0      0      0      0      0\n## # … with 32 more variables: `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;,\n## #   `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;,\n## #   `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;, `1999` &lt;dbl&gt;,\n## #   `1998` &lt;dbl&gt;, `1997` &lt;dbl&gt;, `1996` &lt;dbl&gt;, `1995` &lt;dbl&gt;, `1994` &lt;dbl&gt;,\n## #   `1993` &lt;dbl&gt;, `1992` &lt;dbl&gt;, `1991` &lt;dbl&gt;, `1990` &lt;dbl&gt;, `1989` &lt;dbl&gt;,\n## #   `1988` &lt;dbl&gt;, `1987` &lt;dbl&gt;, `1986` &lt;dbl&gt;, `1985` &lt;dbl&gt;, `1984` &lt;dbl&gt;,\n## #   `1983` &lt;dbl&gt;, `1982` &lt;dbl&gt;, `1981` &lt;dbl&gt;, `1980` &lt;dbl&gt;, and abbreviated …\n## # ℹ Use `colnames()` to see all variable names\n\n\n\n\nwho_disease &lt;- read_csv(\"data/who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long &lt;- who_disease %&gt;%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %&gt;%\n  rename(Country = Cname) %&gt;%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %&gt;%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n## Error in ggplot(., aes(x = year, y = cases)): could not find function \"ggplot\"\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"data/who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\n##       WHO_REGION ISO_code  ...  year cases\n## 0            EMR      AFG  ...  2018   NaN\n## 1            EUR      ALB  ...  2018   0.0\n## 2            AFR      DZA  ...  2018   NaN\n## 3            EUR      AND  ...  2018   0.0\n## 4            AFR      AGO  ...  2018   NaN\n## ...          ...      ...  ...   ...   ...\n## 83221        AMR      VEN  ...  1980   4.0\n## 83222        WPR      VNM  ...  1980   NaN\n## 83223        EMR      YEM  ...  1980   NaN\n## 83224        AFR      ZMB  ...  1980   NaN\n## 83225        AFR      ZWE  ...  1980   NaN\n## \n## [83226 rows x 6 columns]\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()\n## &lt;ggplot: (8777230666506)&gt;"
  },
  {
    "objectID": "data-transformations.html#merging-tables",
    "href": "data-transformations.html#merging-tables",
    "title": "10  Data Transformations",
    "section": "\n10.4 Merging Tables",
    "text": "10.4 Merging Tables\nThe final essential data tidying and transformation skill you need to acquire is joining tables. It is common for data to be organized relationally - that is, certain aspects of the data apply to a group of data points, and certain aspects apply to individual data points, and there are relationships between the individual data points and the groups of data points that have to be documented.\n\n\n\n\n\n\nRelational Data Example: Primary School Records\n\n\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational: - each class has both a teacher and a set of students - each class is held in a specific location that has certain equipment\nIt would be silly to store this information in a single table (though it probably can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have.\n\n\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nkeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs\n\n\n10.4.1 Animating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nMutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\n\n\n\n\n\nDemonstration: Mutating Joins\n\n\n\n\n\nR\nPython\n\n\n\n\nt1 &lt;- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 &lt;- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n## # A tibble: 2 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n## # A tibble: 4 × 3\n##   x         y     z\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n\n\n\n\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n\nAn outer join keeps everything - all rows, all columns.\n\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\n## 3  C  NaN  4.0\n\n\n\n\n\n\nI’ve included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins here [8].\nFiltering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\nSet Operations\n\n\nUnion\nUnion All\nIntersection\nSet Difference\n\n\n\nAll unique rows from x and y\n\nOr, all unique rows from y and x.\n\n\n\nAll rows from x and y, keeping duplicate rows.\n\n\n\nCommon rows in x and y, keeping only unique rows.\n\n\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n\n\n\n\n\n10.4.2 Example: NYC Flights\nWe’ll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you’ll recall, I had to use a small sample the last time we played with this data…). So before we can work with this data, we have to load the tables into Python.\n\n\nLoading Data: R\nPython\n\n\n\n\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"data/\")\n## &lt;SQLiteConnection&gt;\n##   Path: /home/susan/Projects/Class/unl-stat850/stat850-textbook/data/nycflights13.sqlite\n##   Extensions: TRUE\n\n\n\n\nimport sqlite3\ncon = sqlite3.connect(\"data/nycflights13.sqlite\")\ncur = con.cursor()\n\n\n\n\nI am not going to cover SQLITE commands here - I’m just going to use the bare minimum, but you can find a very nice introduction to python and SQLITE at datacarpentry [9], and an introduction to the dbplyr package for a nice R-SQLITE interface.\n\n\n\n\n\n\nTry it out: Understanding Relational Data\n\n\n\n\n\nProblem\nSolution\n\n\n\nSketch a diagram of which fields in each table match fields in other tables. Use the data documentation to help you with your sketch.\n\n\nhere (scroll down a bit).\n\n\n\n\n\n\n\n\n\n\n\nMutating Joins\n\n\n\nThese functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let’s determine whether there’s a relationship between the age of a plane and its delays.\n\n\nR\nPython\n\n\n\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age &lt;- planes %&gt;%\n  mutate(age = 2013 - year) %&gt;% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane &lt;- flights %&gt;%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres &lt;- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n\n\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there’s much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren’t enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n\n\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## &lt;ggplot: (8777320556617)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:324: PlotnineWarning: stat_boxplot : Removed 9374 rows containing non-finite values.\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## &lt;ggplot: (8777229164978)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:324: PlotnineWarning: stat_boxplot : Removed 10317 rows containing non-finite values."
  },
  {
    "objectID": "data-transformations.html#example-gas-prices-data",
    "href": "data-transformations.html#example-gas-prices-data",
    "title": "10  Data Transformations",
    "section": "\n10.5 Example: Gas Prices Data",
    "text": "10.5 Example: Gas Prices Data\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table. \n\n\nGas prices at US EIA site\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\nBut looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n\n\n\n\n\nTry it out: Manual Formatting in Excel\n\n\n\n\n\nProblem\nSolution\nVideo\n\n\n\nAn excel spreadsheet of the data as downloaded in July 2022 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\nWhat steps are involved?\n\n\n\nCopy the year-month column, creating one vertical copy for every set of columns\nMove each block of two columns down to the corresponding vertical copy\nDelete empty rows\nFormat dates\nDelete empty columns\n\n\n\n\n\n\nFigure 10.5: Here is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n\n\n\n\n\n\n\n10.5.1 Setup: Gas Price Data Cleaning\nFor the next two examples, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl &lt;- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc &lt;- read_html(url)\ngas_prices_html &lt;- html_table(htmldoc, fill = T, trim = T)[[5]]\n\n\n\n\nFirst 6 rows of gas prices data as read into R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-Month\nWeek 1\nWeek 1\nWeek 2\nWeek 2\nWeek 3\nWeek 3\nWeek 4\nWeek 4\nWeek 5\nWeek 5\n\n\n\n\n\nYear-Month\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nNA\nNA\n\n\n1994-Nov\n\n\n\n\n\n\n11/28\n1.175\n\n\nNA\nNA\n\n\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\n\n\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nNA\nNA\n\n\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.110\n01/30\n1.109\nNA\nNA\n\n\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\n\n\nNA\nNA\n\n\n\n\n\n\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n\n\n\n\n(‘Year-Month’, ‘Year-Month’)\n(‘Week 1’, ‘End Date’)\n(‘Week 1’, ‘Value’)\n(‘Week 2’, ‘End Date’)\n(‘Week 2’, ‘Value’)\n(‘Week 3’, ‘End Date’)\n(‘Week 3’, ‘Value’)\n(‘Week 4’, ‘End Date’)\n(‘Week 4’, ‘Value’)\n(‘Week 5’, ‘End Date’)\n(‘Week 5’, ‘Value’)\n(‘Unnamed: 11_level_0’, ‘Unnamed: 11_level_1’)\n(‘Unnamed: 12_level_0’, ‘Unnamed: 12_level_1’)\n\n\n\n0\n1994-Nov\nnan\nnan\nnan\nnan\nnan\nnan\n11/28\n1.175\nnan\nnan\nnan\nnan\n\n\n1\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\nnan\nnan\nnan\nnan\n\n\n2\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n3\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.11\n01/30\n1.109\nnan\nnan\n\n\n4\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\nTry it out: Formatting with Pivot Operations\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\nSteps to work through the gas prices data cleaning process\n\n\n\n\nlibrary(tidyverse)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names &lt;- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %&gt;%\n    # trim leading/trailing spaces\n    str_trim() %&gt;%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 13\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 4 more variables: Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;, X &lt;lgl&gt;,\n## #   Date &lt;lgl&gt;, and abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date,\n## #   ³​Week.2.Value, ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n## # ℹ Use `colnames()` to see all variable names\n\n\ngas_prices_raw &lt;- select(gas_prices_raw, -c(X, Date))\ngas_prices_long &lt;- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 × 3\n##   Year.Month variable     value\n##   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n\n\ngas_prices_sep &lt;- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %&gt;%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 × 4\n##   Year.Month week  variable value\n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n\n\ngas_prices_wide &lt;- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 × 4\n##   Year.Month week  Date    Value  \n##   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n\n\ngas_prices_date &lt;- gas_prices_wide %&gt;%\n  filter(nchar(Value) &gt; 0) %&gt;%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %&gt;%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %&gt;%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 × 2\n##   Date       Value\n##   &lt;chr&gt;      &lt;chr&gt;\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n\n\nlibrary(lubridate)\ngas_prices &lt;- gas_prices_date %&gt;%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %&gt;%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       Price.per.gallon\n##   &lt;date&gt;                &lt;dbl&gt;\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## MultiIndex([(         'Year-Month',          'Year-Month'),\n##             (             'Week 1',            'End Date'),\n##             (             'Week 1',               'Value'),\n##             (             'Week 2',            'End Date'),\n##             (             'Week 2',               'Value'),\n##             (             'Week 3',            'End Date'),\n##             (             'Week 3',               'Value'),\n##             (             'Week 4',            'End Date'),\n##             (             'Week 4',               'Value'),\n##             (             'Week 5',            'End Date'),\n##             (             'Week 5',               'Value'),\n##             ('Unnamed: 11_level_0', 'Unnamed: 11_level_1'),\n##             ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')],\n##            )\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date', 'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date', 'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\ngas_prices_raw.head()\n##   Year-Month Week.1.Date  ...  Unnamed:.11_level_0 Unnamed:.12_level_0\n## 0   1994-Nov         NaN  ...                  NaN                 NaN\n## 1   1994-Dec       12/05  ...                  NaN                 NaN\n## 3   1995-Jan       01/02  ...                  NaN                 NaN\n## 4   1995-Feb       02/06  ...                  NaN                 NaN\n## 5   1995-Mar       03/06  ...                  NaN                 NaN\n## \n## [5 rows x 13 columns]\n\n\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\ngas_prices_long.head()\n##   Year-Month     variable  value\n## 0   1994-Nov  Week.1.Date    NaN\n## 1   1994-Dec  Week.1.Date  12/05\n## 2   1995-Jan  Week.1.Date  01/02\n## 3   1995-Feb  Week.1.Date  02/06\n## 4   1995-Mar  Week.1.Date  03/06\n\n\ngas_prices_sep = gas_prices_long\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\ngas_prices_sep.head()\n##   Year-Month variable  value week\n## 0   1994-Nov     Date    NaN    1\n## 1   1994-Dec     Date  12/05    1\n## 2   1995-Jan     Date  01/02    1\n## 3   1995-Feb     Date  02/06    1\n## 4   1995-Mar     Date  03/06    1\n\n\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\ngas_prices_wide.head()\n## variable          Date  Value\n## Year-Month week              \n## 1994-Dec   1     12/05  1.143\n##            2     12/12  1.118\n##            3     12/19  1.099\n##            4     12/26  1.088\n##            5       NaN    NaN\n\n\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n\ngas_prices_date.head()\n## variable Year-Month week       Date  Value  Year Month\n## 0          1994-Dec    1 1994-12-05  1.143  1994   Dec\n## 1          1994-Dec    2 1994-12-12  1.118  1994   Dec\n## 2          1994-Dec    3 1994-12-19  1.099  1994   Dec\n## 3          1994-Dec    4 1994-12-26  1.088  1994   Dec\n## 4          1994-Nov    4 1994-11-28  1.175  1994   Nov\n\n\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\ngas_prices['Price_per_gallon'] = gas_prices.Value\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\ngas_prices.head()\n## variable       Date Price_per_gallon\n## 0        1994-12-05            1.143\n## 1        1994-12-12            1.118\n## 2        1994-12-19            1.099\n## 3        1994-12-26            1.088\n## 4        1994-11-28            1.175\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting using merge + pivot\n\n\n\n\n\nProblem\nSketch\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\nYou can start with the gas_prices_raw\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n #### R solution\nWe’ll use the same data cleaning function as before:\n\n# Clean up the table a bit\ngas_prices_raw &lt;- gas_prices_html %&gt;%\n  set_names(fix_gas_names(names(.))) %&gt;%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %&gt;%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 13\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 4 more variables: Week.5.Date &lt;chr&gt;, Week.5.Value &lt;chr&gt;, X &lt;lgl&gt;,\n## #   Date &lt;lgl&gt;, and abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date,\n## #   ³​Week.2.Value, ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n## # ℹ Use `colnames()` to see all variable names\n\n\ngas_prices_dates &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\ngas_prices_values &lt;- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n\nhead(gas_prices_dates)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Date Week.2.Date Week.3.Date Week.4.Date Week.5.Date\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n## 1 1994-Nov   \"\"          \"\"          \"\"          11/28       \"\"         \n## 2 1994-Dec   \"12/05\"     \"12/12\"     \"12/19\"     12/26       \"\"         \n## 3 1995-Jan   \"01/02\"     \"01/09\"     \"01/16\"     01/23       \"01/30\"    \n## 4 1995-Feb   \"02/06\"     \"02/13\"     \"02/20\"     02/27       \"\"         \n## 5 1995-Mar   \"03/06\"     \"03/13\"     \"03/20\"     03/27       \"\"         \n## 6 1995-Apr   \"04/03\"     \"04/10\"     \"04/17\"     04/24       \"\"\nhead(gas_prices_values)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Value Week.2.Value Week.3.Value Week.4.Value Week.5.Value\n##   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;       \n## 1 1994-Nov   \"\"           \"\"           \"\"           1.175        \"\"          \n## 2 1994-Dec   \"1.143\"      \"1.118\"      \"1.099\"      1.088        \"\"          \n## 3 1995-Jan   \"1.104\"      \"1.111\"      \"1.102\"      1.110        \"1.109\"     \n## 4 1995-Feb   \"1.103\"      \"1.099\"      \"1.093\"      1.101        \"\"          \n## 5 1995-Mar   \"1.103\"      \"1.096\"      \"1.095\"      1.102        \"\"          \n## 6 1995-Apr   \"1.116\"      \"1.134\"      \"1.149\"      1.173        \"\"\n\n\ngas_prices_dates_long &lt;- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\ngas_prices_values_long &lt;- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n\nhead(gas_prices_dates_long)\n## # A tibble: 6 × 3\n##   Year.Month week        month_day\n##   &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;    \n## 1 1994-Nov   Week.1.Date \"\"       \n## 2 1994-Nov   Week.2.Date \"\"       \n## 3 1994-Nov   Week.3.Date \"\"       \n## 4 1994-Nov   Week.4.Date \"11/28\"  \n## 5 1994-Nov   Week.5.Date \"\"       \n## 6 1994-Dec   Week.1.Date \"12/05\"\nhead(gas_prices_values_long)\n## # A tibble: 6 × 3\n##   Year.Month week         price_per_gallon\n##   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;           \n## 1 1994-Nov   Week.1.Value \"\"              \n## 2 1994-Nov   Week.2.Value \"\"              \n## 3 1994-Nov   Week.3.Value \"\"              \n## 4 1994-Nov   Week.4.Value \"1.175\"         \n## 5 1994-Nov   Week.5.Value \"\"              \n## 6 1994-Dec   Week.1.Value \"1.143\"\n\n\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean &lt;- gas_prices_dates_long %&gt;%\n  filter(month_day != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %&gt;% \n           ymd())\n\ngas_prices_values_long_clean &lt;- gas_prices_values_long %&gt;%\n  filter(price_per_gallon != \"\") %&gt;%\n  mutate(week = str_extract(week, \"\\\\d\") %&gt;% as.numeric()) %&gt;%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n\nhead(gas_prices_dates_long_clean)\n## # A tibble: 6 × 5\n##   Year.Month  week month_day year  Date      \n##   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;date&gt;    \n## 1 1994-Nov       4 11/28     1994  1994-11-28\n## 2 1994-Dec       1 12/05     1994  1994-12-05\n## 3 1994-Dec       2 12/12     1994  1994-12-12\n## 4 1994-Dec       3 12/19     1994  1994-12-19\n## 5 1994-Dec       4 12/26     1994  1994-12-26\n## 6 1995-Jan       1 01/02     1995  1995-01-02\nhead(gas_prices_values_long_clean)\n## # A tibble: 6 × 3\n##   Year.Month  week price_per_gallon\n##   &lt;chr&gt;      &lt;dbl&gt;            &lt;dbl&gt;\n## 1 1994-Nov       4             1.18\n## 2 1994-Dec       1             1.14\n## 3 1994-Dec       2             1.12\n## 4 1994-Dec       3             1.10\n## 5 1994-Dec       4             1.09\n## 6 1995-Jan       1             1.10\n\n\ngas_prices &lt;- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %&gt;%\n  select(Date, price_per_gallon)\nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       price_per_gallon\n##   &lt;date&gt;                &lt;dbl&gt;\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\ngas_prices_raw = gas_prices_html\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## Index(['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date',\n##        'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date',\n##        'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0',\n##        'Unnamed:.12_level_0'],\n##       dtype='object')\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date.Date', 'Week.1.Value.Value', 'Week.2.Date.Date', 'Week.2.Value.Value', 'Week.3.Date.Date', 'Week.3.Value.Value', 'Week.4.Date.Date', 'Week.4.Value.Value', 'Week.5.Date.Date', 'Week.5.Value.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\ngas_prices_raw.head()\n##   Year-Month Week.1.Date.Date  ...  Unnamed:.11_level_0 Unnamed:.12_level_0\n## 0   1994-Nov              NaN  ...                  NaN                 NaN\n## 1   1994-Dec            12/05  ...                  NaN                 NaN\n## 3   1995-Jan            01/02  ...                  NaN                 NaN\n## 4   1995-Feb            02/06  ...                  NaN                 NaN\n## 5   1995-Mar            03/06  ...                  NaN                 NaN\n## \n## [5 rows x 13 columns]\n\n\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n\ngas_prices_dates.head()\n##   Year-Month Week.1.Date.Date  ... Week.4.Date.Date Week.5.Date.Date\n## 0   1994-Nov              NaN  ...            11/28              NaN\n## 1   1994-Dec            12/05  ...            12/26              NaN\n## 3   1995-Jan            01/02  ...            01/23            01/30\n## 4   1995-Feb            02/06  ...            02/27              NaN\n## 5   1995-Mar            03/06  ...            03/27              NaN\n## \n## [5 rows x 6 columns]\ngas_prices_values.head()\n##   Year-Month  Week.1.Value.Value  ...  Week.4.Value.Value  Week.5.Value.Value\n## 0   1994-Nov                 NaN  ...               1.175                 NaN\n## 1   1994-Dec               1.143  ...               1.088                 NaN\n## 3   1995-Jan               1.104  ...               1.110               1.109\n## 4   1995-Feb               1.103  ...               1.101                 NaN\n## 5   1995-Mar               1.103  ...               1.102                 NaN\n## \n## [5 rows x 6 columns]\n\n\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n\ngas_prices_dates_long.head()\n##   Year-Month              week month_day\n## 0   1994-Nov  Week.1.Date.Date       NaN\n## 1   1994-Dec  Week.1.Date.Date     12/05\n## 2   1995-Jan  Week.1.Date.Date     01/02\n## 3   1995-Feb  Week.1.Date.Date     02/06\n## 4   1995-Mar  Week.1.Date.Date     03/06\ngas_prices_values_long.head()\n##   Year-Month                week  price_per_gallon\n## 0   1994-Nov  Week.1.Value.Value               NaN\n## 1   1994-Dec  Week.1.Value.Value             1.143\n## 2   1995-Jan  Week.1.Value.Value             1.104\n## 3   1995-Feb  Week.1.Value.Value             1.103\n## 4   1995-Mar  Week.1.Value.Value             1.103\n\n\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna(subset = \"month_day\")\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## &lt;string&gt;:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## &lt;string&gt;:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## &lt;string&gt;:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n\n## &lt;string&gt;:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_values_long_clean = gas_prices_values_long.dropna(subset = \"price_per_gallon\")\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## &lt;string&gt;:1: SettingWithCopyWarning: \n## A value is trying to be set on a copy of a slice from a DataFrame.\n## Try using .loc[row_indexer,col_indexer] = value instead\n## \n## See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n\ngas_prices_dates_long_clean.head()\n##   Year-Month week month_day  year       Date\n## 1   1994-Dec    1     12/05  1994 1994-12-05\n## 2   1995-Jan    1     01/02  1995 1995-01-02\n## 3   1995-Feb    1     02/06  1995 1995-02-06\n## 4   1995-Mar    1     03/06  1995 1995-03-06\n## 5   1995-Apr    1     04/03  1995 1995-04-03\ngas_prices_values_long_clean.head()\n##   Year-Month week  price_per_gallon\n## 1   1994-Dec    1             1.143\n## 2   1995-Jan    1             1.104\n## 3   1995-Feb    1             1.103\n## 4   1995-Mar    1             1.103\n## 5   1995-Apr    1             1.116\n\n\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\ngas_prices.head()\n##         Date  price_per_gallon\n## 0 1994-12-05             1.143\n## 1 1995-01-02             1.104\n## 2 1995-02-06             1.103\n## 3 1995-03-06             1.103\n## 4 1995-04-03             1.116\n\n\n\n\n\n\n\n\n\n\n\n\nOther resources\n\n\n\n[10] - very nice task-oriented chapter that’s below the level addressed in this course but still useful"
  },
  {
    "objectID": "data-transformations.html#references",
    "href": "data-transformations.html#references",
    "title": "10  Data Transformations",
    "section": "\n10.6 References",
    "text": "10.6 References\n\n\n\n\n[1] \nQ. E. McCallum, Bad data handbook: Mapping the world of data problems, 1. ed. Beijing, Köln: O’Reilly, 2013. \n\n\n[2] \nJ. Lowndes and A. Horst, “Tidy data for efficiency, reproducibility, and collaboration,” Openscapes. Oct. 2020 [Online]. Available: https://www.openscapes.org/blog/2020/10/12/tidy-data//. [Accessed: Jul. 21, 2022]\n\n\n[3] \nInternational Business Machines, “The risks of using spreadsheets for statistical analysis,” The risks of using spreadsheets for statistical analysis. Nov. 2018 [Online]. Available: https://www.ibm.com/downloads/cas/7YEX9BKK. [Accessed: Jul. 21, 2022]\n\n\n[4] \nP. O’Beirne, F. Hermans, T. Cheng, and M. P. Campbell, “Horror Stories,” European Spreadsheet Risks Interest Group Horror Stories. Oct. 2020 [Online]. Available: http://www.eusprig.org/horror-stories.htm. [Accessed: Jul. 21, 2022]\n\n\n[5] \nM. Ashour, “A Concise Guide to Number Localization,” Phrase. Feb. 2022 [Online]. Available: https://phrase.com/blog/posts/number-localization/. [Accessed: Jul. 25, 2022]\n\n\n[6] \nWikipedia Contributors, “Locale (computer software),” Wikipedia. Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Locale_(computer_software)&oldid=1082900932. [Accessed: Jul. 25, 2022]\n\n\n[7] \nA. Herrmann, “How to deal with international data formats in Python,” herrmann.tech. Feb. 2021 [Online]. Available: https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html. [Accessed: Jul. 25, 2022]\n\n\n[8] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nThe Carpentries, “Accessing SQLite Databases Using Python and Pandas,” Data Analysis and Visualization in Python for Ecologists. 2022 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html. [Accessed: Jul. 26, 2022]\n\n\n[10] \nJ. Dougherty and I. Ilyankou, “Clean Up Messy Data,” in Hands-On Data Visualization, 1st ed., O’Reilly Media, 2021, p. 480 [Online]. Available: http://handsondataviz.github.io/. [Accessed: Jul. 21, 2022]"
  },
  {
    "objectID": "data-transformations.html#footnotes",
    "href": "data-transformations.html#footnotes",
    "title": "10  Data Transformations",
    "section": "",
    "text": "Many functions from stringr have somewhat faster functional equivalents in the stringi package, but the stringi package has a less “tidy” API, so it may be worth the slight slowdown to use stringr if your data isn’t huge because your code will be more readable.↩︎\nIt’s particularly hackish when you’re working with locale-specific settings [7], and in many cases you can handle locale issues much more elegantly.↩︎"
  },
  {
    "objectID": "data-vis.html#module11-objectives",
    "href": "data-vis.html#module11-objectives",
    "title": "11  Data Visualization",
    "section": "Module Objectives",
    "text": "Module Objectives\n\n\n\n\n\nCreate charts designed to communicate specific aspects of the data\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data\nEvaluate existing charts and develop new versions that improve accessibility and readability\n\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. I’m going to be opinionated on this one - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics specifically.\n\n\nVisualization and statistical graphics are also my research area, so I’m more passionate about this material, which means there’s going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested.\nThis is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That’s why I’m showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.\n\n\n\n\n\n\nGuides and Resources\n\n\n\n\n\nGraph galleries contain sample code to create many different types of charts. Similar galaries are available in R and Python.\nCheat Sheets:\n\nPython\nGgplot2\nBase R"
  },
  {
    "objectID": "data-vis.html#why-do-we-create-graphics",
    "href": "data-vis.html#why-do-we-create-graphics",
    "title": "11  Data Visualization",
    "section": "\n11.1 Why do we create graphics?",
    "text": "11.1 Why do we create graphics?\n\n\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John W. Tukey [1]\n\nCharts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. In very basic evolutionary terms, it’s important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out."
  },
  {
    "objectID": "data-vis.html#general-approaches-to-creating-graphics",
    "href": "data-vis.html#general-approaches-to-creating-graphics",
    "title": "11  Data Visualization",
    "section": "\n11.2 General approaches to creating graphics",
    "text": "11.2 General approaches to creating graphics\nThere are two general approaches to generating statistical graphics computationally:\n\nManually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.\nBase R, matplotlib, old-style SAS graphics\nDescribe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.\nggplot2, plotnine, seaborn (sort of)\n\n\n\nThere is a difference between low-level plotting libraries (base R, matplotlib) and high-level plotting libraries (ggplot2, plotnine, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it’s probably a low level library.\nIn the introduction to The Grammar of Graphics [2], Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects”. His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics”.\n\nBefore we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years[3], so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible at the end of this module in Creating Good charts.\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 8 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke &lt;- read_csv(\"data/pokemon_ascii.csv\", na = '.') %&gt;%\n  mutate(generation = factor(generation))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"data/pokemon_ascii.csv\")\npoke['generation'] = pd.Categorical(poke.generation)\n\n\n\n\nOnce the data is read in, we can start plotting:\n\n\nggplot2\nBase R\nMatplotlib\nPlotnine\n\n\n\nIn ggplot2, we start by specifying which variables we want to be mapped to which features of the data.\nIn a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack”. So it’s easiest to specify our x variable as a constant, ““. We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable.\nThen, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”).\n\nlibrary(ggplot2)\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") \n\n\n\n\nIf we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\).\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") + \n  coord_polar(\"y\", start = 0)\n\n\n\n\nNotice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\npie(tmp$n, labels = tmp$generation)\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\n# Create summary of pokemon by type\ntmp &lt;- poke %&gt;%\n  group_by(generation) %&gt;%\n  count() \n\n# Matrix is necessary for a stacked bar chart\nmatrix(tmp$n, nrow = 8, ncol = 1, dimnames = list(tmp$generation)) %&gt;%\nbarplot(beside = F, legend.text = T, main = \"Generations of Pokemon\")\n\n\n\n\nThere’s not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)\n## ([&lt;matplotlib.patches.Wedge object at 0x7ff4cf8b8b50&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf89a970&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf8cc8b0&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf8ccfd0&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf85a730&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf85ae50&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf8675b0&gt;, &lt;matplotlib.patches.Wedge object at 0x7ff4cf867cd0&gt;], [Text(-0.6090293244431805, 0.916014891783034, '1'), Text(-1.091376662290067, -0.1374662904445805, '2'), Text(-0.447205636991834, -1.004991103563971, '3'), Text(0.4837518599092984, -0.9879190948829235, '4'), Text(1.004991103563971, -0.44720563699183413, '5'), Text(1.0756351166663767, 0.23023704261936323, '6'), Text(0.780190906881347, 0.7754367471429643, '7'), Text(0.2825357301945357, 1.06309621444319, '8')], [Text(-0.33219781333264387, 0.4996444864271094, '18.7%'), Text(-0.5952963612491273, -0.07498161296977117, '16.6%'), Text(-0.24393034745009126, -0.5481769655803478, '16.1%'), Text(0.2638646508596173, -0.538864960845231, '11.8%'), Text(0.5481769655803477, -0.2439303474500913, '10.4%'), Text(0.5867100636362054, 0.12558384142874357, '9.6%'), Text(0.42555867648073464, 0.42296549844161685, '8.6%'), Text(0.15411039828792855, 0.5798706624235581, '8.3%')])\nax1.axis('equal')\n## (-1.1150076843840162, 1.1060059449729593, -1.1113209244032165, 1.1005390916382485)\nplt.show()\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts()\nsizes = sizes.sort_index()\n\n# Find location of bottom of the bar for each bar\ncumulative_sizes = sizes.cumsum() - sizes\nwidth = 1\n\nfig, ax = plt.subplots()\n\nfor i in sizes.index:\n  ax.bar(\"Generation\", sizes[i], width, label=i, bottom = cumulative_sizes[i])\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\n## &lt;BarContainer object of 1 artists&gt;\nax.set_ylabel('# Pokemon')\nax.set_title('Pokemon Distribution by Generation')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\nAs of September 2022, pie charts are still not supported in plotnine. So this demo will fall a bit flat.\n\nfrom plotnine import *\nplt.cla() # clear out matplotlib buffer\n\nggplot(aes(x = \"1\", fill = \"generation\"), data = poke) + geom_bar(position = \"stack\")\n## &lt;ggplot: (8793000082809)&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll talk first about the general idea behind the grammar of graphics. For each concept, I’ll provide you first with the ggplot grammar of graphics code, and then, where it is possible to replicate the chart easily in base R or Python graphics, I will provide code for that as well - so that you can compare the approaches, but also so that you get a sense for what is easy and what is possible in each plotting system."
  },
  {
    "objectID": "data-vis.html#the-grammar-of-graphics",
    "href": "data-vis.html#the-grammar-of-graphics",
    "title": "11  Data Visualization",
    "section": "\n11.3 The Grammar of Graphics",
    "text": "11.3 The Grammar of Graphics\nIn the grammar of graphics, a plot consists of several mostly independent specifications:\n\n\naesthetics - links between data variables and graphical features (position, color, shape, size)\n\nlayers - geometric elements (points, lines, rectangles, text, …)\n\ntransformations - transformations specify a functional link between the data and the displayed information (identity, count, bins, density, regression). Transformations act on the variables.\n\nscales - scales map values in data space to values in the aesthetic space. Scales change the coordinate space of an aesthetic, but don’t change the underlying value (so the change is at the visual level, not the mathematical level).\n\ncoordinate system - e.g. polar or Cartesian\n\nfaceting - facets allow you to split plots by other variables to produce many sub-plots.\n\ntheme - formatting items, such as background color, fonts, margins…\n\nWe can contrast this with other plotting systems (e.g. Base R, matplotlib, seaborn), where transformations and scales must be handled manually (or are handled differently in each type of plot), there may be separate plotting systems for different coordinate systems, etc.\n\n\n\n\n\n\nNote\n\n\n\nWorking with a full data frame can sometimes be a pain, because you may end up with labels that are repeated many, many times. As with any system, you should ensure you’re formatting your data in a way that is consistent and compatible with the underlying philosophy.\n\n\nFunctionally, the biggest difference between the two systems is that in the grammar of graphics system (as implemented in ggplot2), we work with a full tabular data set. As with the rest of the tidyverse, ggplot2 will allow you to reference bare column names as if they were variables, so long as you’ve passed in the data set to the data = argument.\n\n\n\n\nBuilding a masterpiece, by Allison Horst\n\n\n\nR data setup\nPython data setup\n\n\n\n\nif (!\"palmerpenguins\" %in% installed.packages())\n  install.packages(\"palmerpenguins\")\ndata(penguins, package = \"palmerpenguins\")\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n##   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n## 1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n## 2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n## 3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n## 4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n## 5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n## 6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nRemember to run pip3 install palmerpenguins if you haven’t already.\n\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\npenguins.head\n## &lt;bound method NDFrame.head of        species     island  bill_length_mm  ...  body_mass_g     sex  year\n## 0       Adelie  Torgersen            39.1  ...       3750.0    male  2007\n## 1       Adelie  Torgersen            39.5  ...       3800.0  female  2007\n## 2       Adelie  Torgersen            40.3  ...       3250.0  female  2007\n## 3       Adelie  Torgersen             NaN  ...          NaN     NaN  2007\n## 4       Adelie  Torgersen            36.7  ...       3450.0  female  2007\n## ..         ...        ...             ...  ...          ...     ...   ...\n## 339  Chinstrap      Dream            55.8  ...       4000.0    male  2009\n## 340  Chinstrap      Dream            43.5  ...       3400.0  female  2009\n## 341  Chinstrap      Dream            49.6  ...       3775.0    male  2009\n## 342  Chinstrap      Dream            50.8  ...       4100.0    male  2009\n## 343  Chinstrap      Dream            50.2  ...       3775.0  female  2009\n## \n## [344 rows x 8 columns]&gt;\n\n\n\n\n\n11.3.1 Basic Plot Components - Axes and Points\nLet’s start out with a basic scatterplot: we want to use x and y locations to show two variables, and we want to use points to indicate the space where the x location and y location meet in the Cartesian plane.\n\n\nggplot2\nplotnine\nBase R\nMatplotlib\n\n\n\nLet’s start out with a basic scatterplot: we want to use x and y locations to show two variables, and we want to use points to indicate the space where the x location and y location meet in the Cartesian plane.\n\nlibrary(ggplot2)\n# This defines a blank coordinate plane\nggplot(data = penguins) + \n  aes(x = bill_length_mm, y = body_mass_g)\n\n\n\n\n# This plot actually has points!\nggplot(data = penguins) + \n  aes(x = bill_length_mm, y = body_mass_g) + \n  geom_point() # add points\n\n\n\n\nThe aes() statement can also go inside of the ggplot() statement or inside of geom_point(aes(...)). It’s useful to show it outside of the ggplot() statement to show you exactly how the plot is built up, but most people write the code as ggplot(aes(x = ..., y = ...), data = ...) + geom_point() by convention.\n\n\n\nfrom plotnine import *\n# Define a blank coordinate plane\nggplot(data = penguins) + aes(x = \"bill_length_mm\", y = \"body_mass_g\")\n\n# Add a points layer\n## &lt;ggplot: (8793000062281)&gt;\n\n\n\nggplot(data = penguins) + aes(x = \"bill_length_mm\", y = \"body_mass_g\") + geom_point()\n## &lt;ggplot: (8793089384458)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n\nIn base R, we have to use df$var notation to reference the variables.\n\nplot(x = penguins$bill_length_mm, y = penguins$body_mass_g)\n\n\n\n\n# We can also use formula notation, which allows a data = ... argument\nplot(body_mass_g ~ bill_length_mm, data = penguins)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\nplt.scatter(penguins.bill_length_mm, penguins.body_mass_g)\nplt.show()\n\n\n\n\n\n\n\n\n11.3.2 Adding Labels and Titles\n\n\nggplot2\nplotnine\nBase R\nMatplotlib\n\n\n\n\nlibrary(ggplot2)\n# This defines a blank coordinate plane\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + \n  # add points\n  geom_point() + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n\n\n\n\nYou can even add labels that have math symbols if you are careful about how you do it (or if you use the latex2exp package [4]).\n\n\n\nfrom plotnine import *\n\n(ggplot(aes(x = \"bill_length_mm\", y = \"body_mass_g\"), data = penguins) + \n  geom_point() + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n)\n## &lt;ggplot: (8793048721924)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n\n\nplot(x = penguins$bill_length_mm, y = penguins$body_mass_g,\n     main = \"Penguin Bill Length and Body Mass\",\n     xlab = \"Bill Length (mm)\", ylab = \"Body Mass (g)\")\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.title(\"Penguin Bill Length and Body Mass\")\nplt.scatter(penguins.bill_length_mm, penguins.body_mass_g)\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n\n\n\n\n\n\n\n\n11.3.3 Mapping vs. Constant Aesthetics\nWhen considering other aesthetics, such as the color or size of plotted objects, the shape of points, line types, and alpha blending, it is important to differentiate between mapping these quantities to dataset variables and setting constant values.\nMapping quantities to a variable in the dataset requires considerably more work under the hood, as we have to set up a scale from the original values that maps to the aesthetic values, we have to generate the corresponding aesthetic values for each data point, and we usually want to create a legend.\nWhen we set constant values, as in Section 11.3.6, we just… set the value and we’re done. Constant values are usually set to adhere to style guides, to enhance visual appeal, and sometimes to “backwards-engineer” a scale (but this is only necessary in limited circumstances).\n\n11.3.4 Mapping Categorical Variables\n\n\nFor those of you used to British spellings, in ggplot2, all variations on both colour and color work for any parameter.\nIf the goal is to map an aesthetic to a variable in the dataset, then we need to ensure that each geometric object has an appropriately computed mapping (and each object is plotted with that mapping). This is more computationally complex than just setting a constant value for all objects plotted.\nIn grammar of graphics terminology, this is an aesthetic mapping, and in ggplot2 and plotnine, aesthetic mappings go inside aes() statements.\nHere, I’ll demonstrate mapping variables to color, shape, and size aesthetics, but the process is similar for other aesthetics, such as linetype, fill, etc.\n\n\nNote that color is the outside border of a shape, fill is the inside. So if you’re working on a bar plot, you probably want to map things to fill instead of color. If you’re working on a scatterplot, color is probably what you want instead of fill.\n\n\nggplot2\nplotnine\nBase R\nMatplotlib\n\n\n\n\nlibrary(ggplot2)\n# This defines a blank coordinate plane\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + \n  # add points\n  geom_point(aes(shape = species, color = species)) + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n\n\n\n\nNotice that we haven’t had to specify any sort of categorical mapping here - ggplot picks the shapes and colors we’re using based on the number of categories we have. If we want to customize these default mappings, we can use scale_color_discrete, scale_color_manual, scale_shape_discrete, and so on. Only if we want to override the default mapping do we have to specify that we’re working with a discrete variable.\n\n\n\nfrom plotnine import *\n\n(ggplot(aes(x = \"bill_length_mm\", y = \"body_mass_g\"), data = penguins) + \n  geom_point(aes(shape = \"species\", color = \"species\")) + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n)\n## &lt;ggplot: (8793045897966)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n\nI’ve used code from Jenny Bryan’s Stat 545 at UBC website [5] and modified it, since I’m not an expert in base graphics. Regardless, the whole thing feels very clunky to me relative to the grammar-of-graphics approach in ggplot2/plotnine.\n\nlibrary(RColorBrewer)\n# Create color and point mapping between data and plotted values\naes_mapping &lt;- data.frame(\n  species = unique(penguins$species), \n  color = brewer.pal(nlevels(penguins$species), name = 'Dark2'),\n  shape = 1:3)\n\nplot(x = penguins$bill_length_mm, y = penguins$body_mass_g,\n     col = aes_mapping$color[match(penguins$species, aes_mapping$species)],\n     pch = aes_mapping$shape[match(penguins$species, aes_mapping$species)],\n     main = \"Penguin Bill Length and Body Mass\",\n     xlab = \"Bill Length (mm)\", ylab = \"Body Mass (g)\")\n# Manually create legend\nlegend(x = 'bottomright', \n       legend = as.character(aes_mapping$species),\n       col = aes_mapping$color, pch = aes_mapping$shape, bty = 'n', xjust = 1)\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\npenguins['species'] = pd.Categorical(penguins.species)\n\n# Create categorical mapping to marker type\nmarker_types = {'Adelie':'o', 'Chinstrap':'*', 'Gentoo':'+'}\nmarker_color = {'Adelie':'orange', 'Chinstrap':'green', 'Gentoo':'purple'}\n\ngroups = penguins.groupby('species')\nfor name, group in groups:\n  plt.plot(group.bill_length_mm, group.body_mass_g, marker = marker_types[name], color = marker_color[name], linestyle = '', label = name)\n\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.legend()\nplt.show()\n\n\n\n\nPython’s dict type helps a lot here: we can create the mapping between species and shape/color a bit more naturally. But it’s still a lot of details to think about and customize, where ggplot2 tries very hard to give you sensible default color/shape mappings that you don’t have to customize unless the defaults aren’t what you want.\n\n\n\n\n11.3.5 Mapping Continuous Variables\nAbove, we demonstrated mapping to categorical variables, where we had to select an appropriate color for each unique value in the variable. With continuous mappings, however, we have to instead specify a range of output values (e.g. color, size) and map those values to the values in the dataset. This is understandably more complicated:\n\nWe need some sort of one-to-one function from our variable’s values to our color space, but it doesn’t have to be linear (and in many cases, we may want to use a transformation).\nWe need a way to interpolate between a vector of color values to get a continuous color space. Color spaces are complicated [6].\n\n\n\nggplot2\nplotnine\nBase R\nMatplotlib\n\n\n\n\nlibrary(ggplot2)\n# This defines a blank coordinate plane\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + \n  # add points\n  geom_point(aes(color = bill_depth_mm)) + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n\n\n\n\n\n\n\nfrom plotnine import *\n\n(ggplot(aes(x = \"bill_length_mm\", y = \"body_mass_g\"), data = penguins) + \n  geom_point(aes(color = \"bill_depth_mm\")) + \n  ggtitle(\"Penguin Bill Length and Body Mass\") + \n  xlab(\"Bill Length (mm)\") + \n  ylab(\"Body Mass (g)\")\n)\n## &lt;ggplot: (8793045894093)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n\nI found one solution at StackOverflow, but gave up getting it to work in the interests of finishing the rest of this chapter. \n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\nfig, ax = plt.subplots()\n\nscatter = ax.scatter(penguins.bill_length_mm, penguins.body_mass_g, c = penguins.bill_depth_mm, cmap = 'Greens')\n\n# Produce a legend for the ranking (colors).\nlegend1 = ax.legend(*scatter.legend_elements(num=8),\n                    loc=\"upper left\", title=\"Bill Depth\")\nax.add_artist(legend1)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nUse the Pokemon data to see if there is a relationship between a Pokemon’s attack and special attack points (attack and sp_attack, respectively). Can you map a Pokemon’s weight in kg to the point opacity (alpha) so that light Pokemon show up as semi-transparent?\n\n\n\nlibrary(ggplot2)\n\npoke &lt;- read_csv(\"data/pokemon_ascii.csv\", na = '.') \nggplot(data = poke, aes(x = attack, y = sp_attack, alpha = weight_kg)) + geom_point()\n\n\n\n\n\n\n\nfrom plotnine import *\n\npoke = pd.read_csv(\"data/pokemon_ascii.csv\")\npoke['weight_kg'] = pd.to_numeric(poke.weight_kg, errors='coerce')\n\n# Get rid of NA points - they mess up the scales\npoke_sub = poke.dropna(axis = 0, subset=['weight_kg', 'attack', 'sp_attack'])\nggplot(data = poke_sub) + geom_point(aes(x = \"attack\", y = \"sp_attack\", alpha = \"weight_kg\"))\n## &lt;ggplot: (8793045738319)&gt;\n\n\n\n\n\n\n\n\n\n\n11.3.6 Customizing Appearance with Constant Values\nHere I’m focusing on characteristics like color and alpha value, but you can customize all sorts of different parameters, depending on the plot type - line size and type, point size, and more.\nThese constant values are called attributes and they are different from aesthetics because aesthetics are mapped to variables (and thus are not constant).\n\n\nggplot2\nplotnine\nBase R\nMatplotlib\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point(color = \"blue\", alpha = .5, size = 4, shape = 6)\n\n\n\n\n\n\n\n(\n  ggplot(penguins, aes(x = \"bill_length_mm\", y = \"body_mass_g\")) +\n  geom_point(color = \"blue\", alpha = .5, size = 4, shape = 6)\n)\n## &lt;ggplot: (8793045739207)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/geoms/geom_point.py:61: UserWarning: You passed a edgecolor/edgecolors (['#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80', '#0000ff80']) for an unfilled marker (6).  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n\n\n\n\n\n\nBase R doesn’t support alpha blending by default, so we have to load the scales package in order to get that functionality.\n\nlibrary(scales)\n# Using constant alpha:\nplot(body_mass_g ~ bill_length_mm, \n     col = alpha(\"blue\", .5),\n     pch = 6,\n     cex = 3,\n     data = penguins) \n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\nplt.scatter(penguins.bill_length_mm, penguins.body_mass_g, c = 'blue', marker = '^', alpha = .5, s = 200)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Debugging plots\n\n\n\n\n\nProblem\nSolution\n\n\n\nCan you fix the following plot so that it has blue points?\nWhat mistake was made, and why did the plot end up having pink points?\n\npenguins %&gt;%\nggplot(aes(x = bill_length_mm, y = body_mass_g, color = \"blue\")) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) + geom_point(color = \"blue\")\n\n\n\n\nIn the original code, color = \"blue\" was inside the AES statement, which set the color aesthetic to a string “blue”. ggplot2 maps this string to a default value (the reddish-pink color). If we move the color = \"blue\" statement into geom_point(), then this specifies that we want all points to be a constant blue color, instead of mapping color to a variable that has constant value “blue” (which isn’t interpreted by ggplot2 to indicate a color).\n\n\n\n\n\n\n11.3.7 Layers\nOne of the main advantages of ggplot2 is that the syntax is basically consistent across very different types of plots. In base R and matpotlib, this is not the case - you have to look up the available options for each different plot type. In ggplot2, I might have to look up what the aesthetic names are for a specific geom, but I can guess most of the time. So let’s look a bit more into what ggplot2’s approach to graph specification is and what it allows us to do.\nYou’re fairly used to the syntax of the pipe by now; but ggplot works on a slightly different (but similar) concept that we’ve used implicitly up until this point. There is the initial plot statement, ggplot(), and successive layers are added using +.\nYou can specify a data set and aesthetic variables in the ggplot() statement (which is what we’ll usually do), but you can also have a completely blank ggplot() statement and specify your aesthetic mappings and data sets for each layer separately. This approach is more useful when you start creating complex plots, because you may need to plot summary information and raw data, or e.g. separate tables with city information, geographic boundaries, and rivers, all of which need to be represented in the same map. Technically, you can even specify the aes() statement outside of the ggplot() statement or a geom_...() layer call… but this is not typically done because auto complete then doesn’t work.\nIn this extended example, we’ll examine the different features we need to make a map and how to add new layers to a map. We’ll also look at some new geoms: geom_polygon and geom_path.\nData Setup\nLet’s use some data from the state of Nebraska to generate a state map with important geographic features: state parks, railroads, and counties.\n\n\nState parks (I downloaded the GeoJSON file)\n\nRailroads (I downloaded the GeoJSON file)\nCounty boundaries\n\n\n\nR\nPython\n\n\n\n\nif(!\"sf\" %in% installed.packages()) install.packages(\"sf\")\nlibrary(sf)\nlibrary(ggplot2)\n\nne &lt;- map_data(\"state\", \"nebraska\") # state outline, built into R\nne_parks &lt;- read_sf(\"data/State_Park_Areas.geojson\")\nne_railroads &lt;- read_sf(\"data/Railroads.geojson\")\nne_counties &lt;- read_sf(\"data/County_Boundaries.geojson\")\n\n\n\n\n# This installs the package in the current python environment\n# %pip install geopandas\n# (uncomment the line above to use)\nimport geopandas as gp\n\nne = r.ne # state outline (steal from R)\nne_parks = gp.read_file(\"data/State_Park_Areas.geojson\")\nne_railroads = gp.read_file(\"data/Railroads.geojson\")\nne_counties = gp.read_file(\"data/County_Boundaries.geojson\")\n\n\n\n\nData Exploration\nWe’re working with GIS data (geographical information systems), which has a different format than most of the data we’ve worked with thus far. Geographic data tends to be nested, so that e.g. a railroad segment will contain a set of lat/long coordinates that define the segment, but there may be many different segments in a continuous track.\nLet’s see how each of these data types are represented by doing a bit of EDA.\nFirst, let’s look at the ne object, which contains the coordinates for the border of the state.\n\n\nR\nPython\n\n\n\n\nhead(ne)\n##        long      lat group order   region subregion\n## 1 -104.0606 43.00621     1     1 nebraska      &lt;NA&gt;\n## 2 -103.5106 42.99475     1     2 nebraska      &lt;NA&gt;\n## 3 -103.0063 42.99475     1     3 nebraska      &lt;NA&gt;\n## 4 -102.7944 42.99475     1     4 nebraska      &lt;NA&gt;\n## 5 -102.0782 42.99475     1     5 nebraska      &lt;NA&gt;\n## 6 -101.2302 42.99475     1     6 nebraska      &lt;NA&gt;\n\n\n\n\nne.describe\n## &lt;bound method NDFrame.describe of            long        lat  group  order    region subregion\n## 0   -104.060593  43.006210    1.0      1  nebraska        NA\n## 1   -103.510551  42.994755    1.0      2  nebraska        NA\n## 2   -103.006348  42.994755    1.0      3  nebraska        NA\n## 3   -102.794357  42.994755    1.0      4  nebraska        NA\n## 4   -102.078163  42.994755    1.0      5  nebraska        NA\n## ..          ...        ...    ...    ...       ...       ...\n## 203 -104.060593  41.562355    1.0    204  nebraska        NA\n## 204 -104.060593  41.699867    1.0    205  nebraska        NA\n## 205 -104.054863  41.997807    1.0    206  nebraska        NA\n## 206 -104.060593  42.610874    1.0    207  nebraska        NA\n## 207 -104.060593  43.006210    1.0    208  nebraska        NA\n## \n## [208 rows x 6 columns]&gt;\n\n\n\n\nWe have a series of latitude and longitude values, with a group variable and an order. That is, group defines which piece of the state we’re working with (this is especially important in the case of states with disjoint regions, such as Michigan or Hawaii), and order defines the order in which the points are to be connected. This is important - it matters which order we connect the points.\nThe ne object is a bit lower-level than the objects containing the spatial information about parks, railroads, and counties. We’ve used the sf and geopandas packages, which store all of the data from the geojson file in a table-like structure, but under the hood, these objects are a bit more complicated. We’re going to completely ignore these complexities for the moment.\n\n\nR\nPython\n\n\n\n\nhead(ne_parks)\n\nSimple feature collection with 6 features and 22 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -102.1317 ymin: 40.23034 xmax: -95.87157 ymax: 42.42903\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 23\n  OBJECTID AreaName   Acres Distr…¹ Latit…² Longi…³ Notes DB_GI…⁴ AreaT…⁵ Status\n     &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; \n1        1 Alexandri… 101   SE         40.2   -97.3 &lt;NA&gt;  1R      SRA     Active\n2        2 Arbor Lod…  73.8 SE         40.7   -95.9 &lt;NA&gt;  4H      SHP     Active\n3        3 Ash Hollo… 389   NW         41.3  -102.  Hunt… 8H      SHP     Active\n4        4 Ashfall S… 360   NE         42.4   -98.2 Mana… 279H    SHP     Active\n5        5 Blue Rive…  14   SE         40.7   -97.1 &lt;NA&gt;  25R     SRA     Active\n6        6 Bluestem … 441.  SE         40.6   -96.8 &lt;NA&gt;  27R     SRA     Active\n# … with 13 more variables: GISAcres &lt;dbl&gt;, County &lt;chr&gt;,\n#   OFWAtlasDisplay &lt;chr&gt;, Park_ID &lt;chr&gt;, NGPC_Manager_Code &lt;chr&gt;,\n#   Display2Public &lt;chr&gt;, MgtAgreementType &lt;chr&gt;, ManagedAs &lt;chr&gt;,\n#   HuntGuide &lt;chr&gt;, GlobalID &lt;chr&gt;, Shape__Area &lt;dbl&gt;, Shape__Length &lt;dbl&gt;,\n#   geometry &lt;MULTIPOLYGON [°]&gt;, and abbreviated variable names ¹​District,\n#   ²​Latitude, ³​Longitude, ⁴​DB_GISID, ⁵​AreaType\n\nhead(ne_railroads)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: -102.0517 ymin: 40.002 xmax: -96.97689 ymax: 40.66054\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 8\n  OBJECTID   ID1 RR      RR_ID Oper_By Oper_…¹ Shape…²                  geometry\n     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;MULTILINESTRING [°]&gt;\n1        1     1 Burlin… BNSF  \" \"     \" \"       0.881 ((-101.652 40.51432, -10…\n2        2     2 Burlin… BNSF  \" \"     \" \"       2.50  ((-102.0517 40.06478, -1…\n3        3     3 Burlin… BNSF  \" \"     \" \"       0.125 ((-98.43093 40.45798, -9…\n4        4     4 Burlin… BNSF  \" \"     \" \"       0.545 ((-98.4006 40.58327, -98…\n5        5     5 Burlin… BNSF  \" \"     \" \"       0.401 ((-98.42898 40.08312, -9…\n6        6     6 Burlin… BNSF  \" \"     \" \"       2.82  ((-96.97689 40.62462, -9…\n# … with abbreviated variable names ¹​Oper_By_ID, ²​Shape_Length\n\nhead(ne_counties)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -104.0535 ymin: 40.35027 xmax: -97.82985 ymax: 42.43772\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 8\n  OBJECTID Cnty_Name Cnty_No CountyFIPS GlobalID                 Shape…¹ Shape…²\n     &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;   &lt;dbl&gt;\n1        1 Adams           1 001        {16352331-3262-4EBB-A78…    1.59   0.155\n2        2 Antelope        2 003        {FA9D8609-F002-4003-AF4…    1.98   0.242\n3        3 Arthur          3 005        {FF27D2CA-CA67-4D69-872…    1.85   0.201\n4        4 Banner          4 007        {85111E3B-7990-4C93-A22…    1.98   0.208\n5        5 Blaine          5 009        {AA433830-4A29-4E78-841…    1.85   0.201\n6        6 Boone           6 011        {7525569E-EDEB-4C3D-B8B…    1.80   0.193\n# … with 1 more variable: geometry &lt;MULTIPOLYGON [°]&gt;, and abbreviated variable\n#   names ¹​Shape_Length, ²​Shape_Area\n\n\n\n\n\nne_parks.describe\n\n&lt;bound method NDFrame.describe of     OBJECTID  ...                                           geometry\n0          1  ...  POLYGON ((-97.32632 40.23043, -97.32647 40.230...\n1          2  ...  MULTIPOLYGON (((-95.87223 40.67319, -95.87230 ...\n2          3  ...  POLYGON ((-102.12684 41.30910, -102.12681 41.3...\n3          4  ...  POLYGON ((-98.16506 42.41817, -98.16504 42.418...\n4          5  ...  POLYGON ((-97.11678 40.70553, -97.11655 40.705...\n..       ...  ...                                                ...\n71        72  ...  MULTIPOLYGON (((-99.27091 41.86783, -99.27065 ...\n72        73  ...  POLYGON ((-103.06751 42.46677, -103.06744 42.4...\n73        74  ...  MULTIPOLYGON (((-101.07141 40.15372, -101.0714...\n74        75  ...  POLYGON ((-96.84584 40.75942, -96.84826 40.761...\n75        76  ...  MULTIPOLYGON (((-97.58016 42.82993, -97.58343 ...\n\n[76 rows x 23 columns]&gt;\n\nne_railroads.describe\n\n&lt;bound method NDFrame.describe of     OBJECTID  ...                                           geometry\n0          1  ...  MULTILINESTRING ((-101.65200 40.51432, -101.64...\n1          2  ...  MULTILINESTRING ((-102.05175 40.06478, -102.04...\n2          3  ...  MULTILINESTRING ((-98.43093 40.45798, -98.4312...\n3          4  ...  MULTILINESTRING ((-98.40060 40.58327, -98.4017...\n4          5  ...  MULTILINESTRING ((-98.42898 40.08312, -98.4264...\n..       ...  ...                                                ...\n73        74  ...  MULTILINESTRING ((-97.58089 40.63769, -97.5803...\n74        75  ...  MULTILINESTRING ((-97.60241 40.63639, -97.6029...\n75        76  ...  MULTILINESTRING ((-95.92598 41.27184, -95.9258...\n76        77  ...  MULTILINESTRING ((-95.89146 41.27833, -95.8921...\n77        78  ...  MULTILINESTRING ((-99.06272 40.70339, -99.0633...\n\n[78 rows x 8 columns]&gt;\n\nne_counties.describe\n\n&lt;bound method NDFrame.describe of     OBJECTID  ...                                           geometry\n0          1  ...  MULTIPOLYGON (((-98.27811 40.69829, -98.27816 ...\n1          2  ...  MULTIPOLYGON (((-97.83445 42.43772, -97.83448 ...\n2          3  ...  MULTIPOLYGON (((-101.98542 41.74226, -101.9711...\n3          4  ...  MULTIPOLYGON (((-104.05273 41.69795, -104.0443...\n4          5  ...  MULTIPOLYGON (((-99.68688 42.08620, -99.68702 ...\n..       ...  ...                                                ...\n88        89  ...  MULTIPOLYGON (((-96.12204 41.68268, -96.12134 ...\n89        90  ...  MULTIPOLYGON (((-97.01781 42.35137, -97.01779 ...\n90        91  ...  MULTIPOLYGON (((-98.27357 40.35036, -98.27354 ...\n91        92  ...  MULTIPOLYGON (((-98.29554 42.08890, -98.29548 ...\n92        93  ...  MULTIPOLYGON (((-97.36815 41.04696, -97.36815 ...\n\n[93 rows x 8 columns]&gt;\n\n\n\n\n\nPlotting Regions\nStates are a good example of polygons - they’re a set of points which are connected and define the border of a region. Now, Nebraska is pretty boring, because we don’t have any islands (ok, you could probably fill in a lot of things instead of islands in that sentence). But, in general, it’s important to make sure that when dealing with generic polygons, we only connect points from the same polygon.\nIf you neglect point order and grouping, you may end up with a map like this:\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(mapproj)\n\nggplot() + \n  geom_polygon(data = ne, aes(x = long, y = lat, group = group), fill = \"white\", color = \"black\") + \n  coord_map() # This forces the right aspect ratio\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot() + geom_polygon(aes(x = \"long\", y = \"lat\", group = \"group\"), data = ne, fill = \"white\", color = \"black\")\n# coord_map isn't implemented in plotnine :(\n## &lt;ggplot: (8793045903869)&gt;\n\n\n\n\n\n\n\nPlotting counties\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(mapproj)\n\nggplot() + \n  # geom_sf automatically uses lat/long and the type of data to choose the right geom\n  geom_sf(data = ne_counties) + \n  coord_sf() # This forces the right aspect ratio\n\n\n\n\nWe could even add annotations to the counties showing what the county names are, if we want to do so - this requires a call to geom_sf_text, and an additional aesthetic mapping to label (the value of the text we’re printing). The geographic centroid operation is handled automatically behind the scenes.\n\nggplot() + \n  # geom_sf automatically uses lat/long and the type of data to choose the right geom\n  geom_sf(data = ne_counties) + \n  geom_sf_text(data = ne_counties, aes(label = Cnty_Name), size = 2) + \n  coord_sf() # This forces the right aspect ratio\n\n\n\n\n\n\n\nfrom plotnine import *\nimport geopandas as gp\n\nggplot() + geom_map(data = ne_counties)\n## &lt;ggplot: (8793087232058)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n\n\n\nIn python, it requires a bit more work to handle county names - we have to define a function to give us the centroid of each region.\n\nfrom plotnine import *\nimport geopandas as gp\n\n# from plotnine docs: https://plotnine.readthedocs.io/en/stable/generated/plotnine.geoms.geom_map.html?highlight=coord_map\ndef calculate_center(df):\n  \"\"\"\n  Calculate the centre of a geometry\n\n  This method first converts to a planar crs, gets the centroid\n  then converts back to the original crs. This gives a more\n  accurate\n  \"\"\"\n  original_crs = df.crs\n  planar_crs = 'EPSG:3857'\n  return df['geometry'].to_crs(planar_crs).centroid.to_crs(original_crs)\n\nne_counties['center'] = calculate_center(ne_counties)\n\nggplot() + geom_map(data = ne_counties, fill = \"#f0f0f0\") +\\\ngeom_text(ne_counties, aes(\"center.x\", \"center.y\", label = 'Cnty_Name'), size = 5)\n## &lt;ggplot: (8792999817915)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n\n\n\n\n\n\n\nAdding Polygons\nNext, let’s add some park information to our map.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(mapproj)\n\nggplot() + \n  # geom_sf automatically uses lat/long and the type of data to choose the right geom\n  geom_sf(data = ne_counties) + \n  geom_sf(data = ne_parks, fill = \"green\", color = \"darkgreen\") + \n  coord_sf() # This forces the right aspect ratio\n\n\n\n\n\n\n\nfrom plotnine import *\nimport geopandas as gp\n\nggplot() +\\\ngeom_map(data = ne_counties, fill = \"#fafafa\") +\\\ngeom_map(data = ne_parks, color = \"darkgreen\", fill = \"green\")\n## &lt;ggplot: (8793045807318)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n\n\n\n\n\n\nAdding Lines\nSo far, everything we’ve plotted on our map has been related to polygons. Not all geometric objects are polygons, though - railroads and rivers are (usually) lines, and points of interest may be actual points and not spatial regions. We can add this additional information by including extra layers on our plot.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nlibrary(mapproj)\n\nggplot() + \n  # geom_sf automatically uses lat/long and the type of data to choose the right geom\n  geom_sf(data = ne_counties, fill = \"white\", color = \"grey80\") + \n  geom_sf(data = ne_parks, fill = \"green\", color = \"darkgreen\") + \n  geom_sf(data = ne_railroads, color = \"black\") + \n  coord_sf() # This forces the right aspect ratio\n\n\n\n\n\n\n\nfrom plotnine import *\nimport geopandas as gp\n\nggplot() +\\\ngeom_map(data = ne_counties, fill = \"#ffffff\", color = \"#f0f0f0\") +\\\ngeom_map(data = ne_parks, color = \"darkgreen\", fill = \"green\") +\\\ngeom_map(data = ne_railroads, color = \"black\")\n## &lt;ggplot: (8792959545140)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:62: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/descartes/patch.py:46: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Maps of Middle Earth\n\n\n\n\n\nDedicated fans have re-created middle earth in digital format using ArcGIS files. These map file formats, called shape files, can be read into R and plotted.\nYou may need to install a few spatial packages first (Mac and Windows, Linux)\nThe sf package in R contains a special geom, geom_sf, which will plot map objects with an appropriate geom, whether they are points, lines, or polygons. In complicated maps with many layers, this is a really awesome feature. Similarly, if you have geopandas in python, plotnine’s geom_map function will work with geopandas tables.\nI’ve provided some code to get you started, but there are many other shapefiles in the dataset. Pick some layers which you think are interesting, and plot them with appropriate geoms to make a map of Middle Earth.\nUnfortunately, in this map there is not an underlying polygon (the coastline is a series of shorter segments). To resolve this, I have provided a theme statement that will have a white background, so that you can add useful layers without the grey grid background.\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(sf)\n\nif (!dir.exists(\"data\")) mkdir(\"data\")\n\nif (!file.exists(\"data/MiddleEarthMap.zip\")) {\n  download.file(\"https://github.com/jvangeld/ME-GIS/archive/master.zip\", \"data/MiddleEarthMap.zip\", mode = \"wb\")\n}\nif (!dir.exists(\"data/ME-GIS-main/\")) {\n  unzip(\"data/MiddleEarthMap.zip\", exdir = \"data/\")\n}\n\ncoastline &lt;- read_sf(\"data/ME-GIS-main/Coastline2.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Coastline2.shp\"; The file doesn't seem to exist.\ncities &lt;- read_sf(\"data/ME-GIS-main/Cities.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Cities.shp\"; The file doesn't seem to exist.\nforests &lt;- read_sf(\"data/ME-GIS-main/Forests.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Forests.shp\"; The file doesn't seem to exist.\nlakes &lt;- read_sf(\"data/ME-GIS-main/Lakes.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Lakes.shp\"; The file doesn't seem to exist.\nrivers &lt;- read_sf(\"data/ME-GIS-main/Rivers.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Rivers.shp\"; The file doesn't seem to exist.\nroads &lt;- read_sf(\"data/ME-GIS-main/Roads.shp\")\n## Error: Cannot open \"data/ME-GIS-main/Roads.shp\"; The file doesn't seem to exist.\n\nggplot() + \n  geom_sf(data = coastline) + \n  geom_sf(data = forests, color = NA, fill = \"darkgreen\", alpha = .2) + \n  geom_sf(data = rivers, color = \"blue\", alpha = .1) + \n  geom_sf(data = lakes, fill = \"blue\", color = NA, alpha = .2) + \n  theme_map()\n## Error in fortify(data): object 'coastline' not found\n\n\n\n\n\n11.3.8 Statistics and Plot Types\nAt this point, we’ve primarily looked at charts which have two continuous variables - scatter plots and line plots. There are a number of situations where these types of charts are inadequate. For one thing, we might want to only look at the distribution of a single variable. Or, we might want to look at how a continuous response variable changes when the level of a categorical variable changes. In this section, we’ll hit the most common types of plots, but there are almost infinite variations. Sites like the Data Viz Catalogue can be useful if you’re trying to accomplish a specific task and want to know what type of plot to use.\nIn all of the plots which we discuss in this section, there is a statistical function applied to the data before plotting. So while you may specify e.g. x = var1, what is plotted is f(var1), where f() might be the mean/median/quartiles, a binned count, or a computed kernel density.\nIn ggplot2, you can formally specify a statistic by using stat_xxx functions, but many geoms implicitly call these same functions.\n\n\n\n\n\n\nIntroducing Seaborn: higher level Python plotting\n\n\n\n\n\nIt’s at this point in the chapter that I gave up trying to do everything in matplotlib. Matplotlib is the low-level plotting library, and when computing statistics and creating these types of plots, it’s … not ideal. I will instead demonstrate plotnine, which is grammar-of-graphics style, and seaborn [7], which is another plotting library in python that isn’t strictly grammar-of-graphics but is at least somewhat higher level than matplotlib.\nWhile standard seaborn isn’t a grammar-of-graphics interface, it appears that the next generation of the package will be much more grammar-of-graphics like. [8]\nInstall seaborn with pip3 install seaborn in your terminal.\n\n# Load the seaborn package\n# the alias \"sns\" stands for Samuel Norman Seaborn \n# from \"The West Wing\" television show\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Initialize seaborn styling; context\nsns.set_style('white')\nsns.set_context('notebook')\n\n\n\n\n\n11.3.9 Visualizing Distributions\nBox plots\nA box plot can show some summary information about the distribution of a single continuous variable, and usually is used to show differences in the level of a response variable at different levels of a categorical variable.\nLet’s look at the distribution of penguin weight by species and sex. We’ll return to this plot later when discussing how to create good charts.\n\n\nggplot2\nplotnine\nBase R\nSeaborn\n\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;% # Remove unknown sex penguins\n  ggplot(aes(x = species, y = body_mass_g, color = sex)) + \n  geom_boxplot(outlier.shape = NA) + \n  # We can add another layer overlaying the actual points, \n  # with some random noise added to reduce overplotting\n  geom_jitter(position = position_jitterdodge(), shape = 1) + \n  ylab(\"Body Mass (g)\") + xlab(\"Species\") + ggtitle(\"Penguin Weight by Species and Sex\")\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\n\n# Remove NAs\ntmp = penguins.dropna(axis = 0, subset = 'sex')\n\nggplot(aes(x = \"species\", y = \"body_mass_g\", color = \"sex\"), data = tmp) +\\\ngeom_boxplot(outlier_shape='') +\\\ngeom_jitter(position = position_jitterdodge(), shape = 'o') +\\\nggtitle(\"Penguin Weight by Species and Sex\") +\\\nylab(\"Body Mass (g)\") +\\\nxlab(\"Species\")\n## &lt;ggplot: (8792953132656)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/geoms/geom_point.py:61: UserWarning: You passed a edgecolor/edgecolors (['#db5f57ff']) for an unfilled marker ('').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/geoms/geom_point.py:61: UserWarning: You passed a edgecolor/edgecolors (['#57d3dbff']) for an unfilled marker ('').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n\n\n\n\n\n\nBecause base R doesn’t have a default way to handle jittering or putting boxplots side-by-side, it’s not easy to add points on top of these boxplots.\n\nboxplot(body_mass_g ~ species + sex, data = penguins, \n        # Manually color boxplots\n        col = rep(c(\"red\", \"blue\"), times = 3))\n\n\n\n\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.boxplot(data=penguins, x = 'species', y = 'body_mass_g', hue = 'sex')\nplt.show()\n\n\n\n\n\n\n\nHistograms and Density Plots\nBox plots aren’t the only way to show distributions. If we want to, we can show distributions using histograms or density plots. A histogram is created by binning the variable, then counting the number of observations that fall within each specified range. Usually, these ranges have constant width (but not always).\nChanging the binwidth can radically change the appearance and usefulness of the histogram.\n\n\nggplot2\nplotnine\nBase R\nSeaborn\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = body_mass_g)) + \n  geom_histogram(color = \"black\", fill = \"grey\")\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = body_mass_g)) + \n  geom_histogram(color = \"black\", fill = \"grey\", binwidth = 60)\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = body_mass_g)) + \n  geom_histogram(color = \"black\", fill = \"grey\", binwidth = 300)\n\n\n\n\n\n\n\nggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = \"body_mass_g\")) + geom_histogram(color = \"black\", fill = \"grey\")\n  \n## &lt;ggplot: (8792950290116)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/stats/stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 11'. Pick better value with 'binwidth'.\n\n\n\nggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = \"body_mass_g\")) + geom_histogram(color = \"black\", fill = \"grey\", binwidth = 60)\n  \n## &lt;ggplot: (8792950274130)&gt;\n\n\n\nggplot(penguins.dropna(axis = 0, subset='body_mass_g'), aes(x = \"body_mass_g\")) + geom_histogram(color = \"black\", fill = \"grey\", binwidth = 300)\n## &lt;ggplot: (8792950261863)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/ggplot.py:365: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n\n\n\n\n\n\n\nhist(penguins$body_mass_g)\n\n\n\n\nhist(penguins$body_mass_g, breaks = 30) # number of bins\n\n\n\n\nhist(penguins$body_mass_g, \n     seq(min(penguins$body_mass_g, na.rm = T), \n         max(penguins$body_mass_g, na.rm = T), \n         by = 60)) # specific bin width\n\n\n\n\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.histplot(data = penguins, x = \"body_mass_g\")\nplt.show()\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.histplot(data = penguins, x = \"body_mass_g\", binwidth = 60)\nplt.show()\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.histplot(data = penguins, x = \"body_mass_g\", binwidth = 300)\nplt.show()\n\n\n\n\n\n\n\nViolin Plots\nOne interesting hybrid between a boxplot and a histogram is a violin plot, which shows the full density of the data, reflected (usually in y) to form an object which could potentially look like a violin for e.g. bimodal data.\n\n\nSource\nSeaborn has options to split the violin plots by sex - this is a bit trickier in ggplot2 but it can be done with the help of the introdataviz package available on GitHub.\n\n\nggplot2\nSeaborn\n\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;% # Remove unknown sex penguins\n  ggplot(aes(x = species, y = body_mass_g, color = sex)) + \n  geom_violin(outlier.shape = NA) + \n  # We can add another layer overlaying the actual points, \n  # with some random noise added to reduce overplotting\n  geom_jitter(position = position_jitterdodge(), shape = 1) + \n  ylab(\"Body Mass (g)\") + xlab(\"Species\") + ggtitle(\"Penguin Weight by Species and Sex\")\n\n\n\n\n\n# devtools::install_github(\"psyteachr/introdataviz\")\nlibrary(introdataviz)\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;% # Remove unknown sex penguins\n  ggplot(aes(x = species, y = body_mass_g, color = sex)) + \n  geom_split_violin() + \n  # We can add another layer overlaying the actual points, \n  # with some random noise added to reduce overplotting\n  geom_jitter(position = position_jitterdodge(), shape = 1) + \n  ylab(\"Body Mass (g)\") + xlab(\"Species\") + ggtitle(\"Penguin Weight by Species and Sex\")\n\n\n\n\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.violinplot(data = penguins, x = 'species', y = 'body_mass_g', hue = 'sex', split = True)\nplt.show()\n\n\n\n\n\n\n\nBar Charts\nA bar chart is a plot with a categorical variable on one axis and a summary statistic on the other (usually, this is a count). Note that a bar chart is NOT the same as a histogram (a histogram looks very similar, but has a binned numeric variable on one axis and counts on the other). Geometrically, bar charts are rectangles; typically each rectangle will have equal width and variable height.\n\n\nggplot2\nplotnine\nBase R\nSeaborn\n\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = species, fill = species)) + \n  geom_bar()\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = species, fill = sex)) + \n  geom_bar(position = \"dodge\")\n\n\n\npenguins %&gt;%\n  ggplot(aes(x = species, fill = sex)) + \n  geom_bar(position = \"stack\")\n\n\n\n\n\n\n\nggplot(penguins.dropna(axis = 0, subset='species'), aes(x = \"species\", fill = \"species\")) + geom_bar()\n## &lt;ggplot: (8792944617009)&gt;\n\n\n\nggplot(penguins.dropna(axis = 0, subset=['species', 'sex']), aes(x = \"species\", fill = \"sex\")) + geom_bar()\n## &lt;ggplot: (8792942487664)&gt;\n\n\n\nggplot(penguins.dropna(axis = 0, subset=['species', 'sex']), aes(x = \"species\", fill = \"sex\")) + geom_bar(position = \"dodge\")\n## &lt;ggplot: (8792942451086)&gt;\n\n\n\n\n\n\n\ncount_species &lt;- table(penguins$species)\nbarplot(count_species)\n\n\n\ncount_species2 &lt;- table(penguins$sex, penguins$species, useNA = 'ifany')\nbarplot(count_species2, beside = T)\n\n\n\n\n\n\ncountplot is a canonical barchart in seaborn. barplot shows the aggregated value of a y variable, with a confidence interval, which is not what is typically meant by a bar plot.\n\nplt.cla() # clear out matplotlib buffer\nsns.countplot(data = penguins, x = \"species\")\nplt.show()\n\n\n\nplt.cla() # clear out matplotlib buffer\nsns.countplot(data = penguins, x = \"species\", hue = 'sex')\nplt.show()\n\n\n\n\n\n\n\n\n11.3.10 Beyond Points: Lines, Rectangles, and Other Geoms\nThere are many situations where points aren’t the best way to display data. If we have several series of data connected over e.g. time, then we might want to join our individual observations with lines that suggest continuity. Or, we may want to display a range of values over each time point, at which point we might be better off with a ribbon-like area enclosing the maximum and minimum values over time.\nIn the grammar of graphics, we need to select a geometric object and then provide variable mappings for each required spatial dimension. When working with non-grammar approaches, however, these different plots sometimes use very different syntax.\nLines\nLet’s consider the lego set data and examine the number of sets over time.\n\n\nR data setup\nPython data setup\n\n\n\n\nlegos_time &lt;- readr::read_csv(\"data/lego_sets.csv\") %&gt;%\n  group_by(year) %&gt;%\n  count()\n\n\n\n\nlegos_time = pd.read_csv(\"data/lego_sets.csv\").\\\n  groupby(\"year\")[[\"theme_id\"]].\\\n  agg(\"count\").\\\n  reset_index().\\\n  rename(columns = {\"theme_id\": \"count\"})\n\n\n\n\n\n\nggplot2\nplotnine\nBase R\nSeaborn\n\n\n\n\nggplot(legos_time, aes(x = year, y = n)) + geom_line()\n\n\n\n\n\n\n\nggplot(legos_time, aes(x = \"year\", y = \"count\")) + geom_line()\n## &lt;ggplot: (8792942431781)&gt;\n\n\n\n\n\n\n\nplot(legos_time$year, legos_time$n, type = 'l')\n\n\n\n\n\n\n\nplt.cla()\nsns.lineplot(data = legos_time, x = \"year\", y = \"count\")\nplt.show()\n\n\n\n\n\n\n\nStatistics and the Grammar of Graphics\nLet’s explore the different ways we can represent data using the grammar of graphics and statistics. This section will be written in R, but of course, you can also do much the same thing in plotnine.\nLet’s consider the case where we want to understand the joint distribution between penguin flipper length and bill depth:\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_point()\n\n\n\n\n\n\n\nWe’re already getting some overplotting in this data (points on top of each other), which interferes with our ability to actually see how many points there are. We could instead create a grid for the data and fill in the grid based on how many points are in each cell. We would be using fill/color/hue as a third “dimension”, if we wanted to go that route. We lose some numerical accuracy, as we can’t map points directly to colors as accurately as we do location, but we might have a better intuitive feeling for the data distribution than we get from seeing the points (and this approach scales to data sets that are bigger than the penguins data).\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + geom_bin2d()\n\n\n\n\n\n\n\nWhile the points (or counts of points) are great, we might want to add some sort of density statistic to the mix. We can use geom_density2d to do this calculation. Note that we have to tell ggplot to wait until after the statistic is computed to calculate the fill (and what statistic to use).\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + \n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", \n                 contour = F)\n\n\n\n\nIf you want to preserve some numerical precision, we could instead work on contour lines – like elevation maps, contour plots show lines connecting densities of the same numerical value.\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + \n  geom_density2d()\n\n\n\n\nWe can also combine fill and contour lines and plot filled polygons:\n\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_depth_mm)) + \n  stat_density2d_filled()\n\n\n\n\nObviously, there’s a lot of ways to generate 3D-like charts in R, but can we actually generate 3D charts? Of course! We’ll use the rayshader package [9].\n\n\n\n\nThe rayshader package lets you create 3d maps and graphs (image by Allison Horst)\n\n\nlibrary(rayshader)\n\nmy_penguin_plot &lt;- penguins %&gt;% \n  ggplot(aes(x = flipper_length_mm, y = bill_depth_mm)) + \n  stat_density2d(aes(fill = after_stat(density)), \n                 geom = \"tile\", \n                 contour = F)\n\nplot_gg(my_penguin_plot) # Running this is very memory intensive\n\nIf you happen to want to get a 3D printed version of your plot, you can do that too using rayshader’s save_3dprint() function."
  },
  {
    "objectID": "data-vis.html#sec-creating-good-charts",
    "href": "data-vis.html#sec-creating-good-charts",
    "title": "11  Data Visualization",
    "section": "\n11.4 Creating Good Charts",
    "text": "11.4 Creating Good Charts\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n\nI’m going to provide a broad overview of considerations in this section, but if you have more questions, please contact me. This is my research area, and I love to talk about it.\n\n\n\n\n\n\nHow do we know a chart is good?\n\n\n\n\n\nWe do experiments on people! (evil cackle)\n\nA review of graphical testing in statistics [10]\n\nA really cool paper about testing competing graphical designs to figure out polar coordinates suck [11]\n\n\nIf you’re still curious after reading those, set up an appointment and let’s talk!\n\n\n\n\n11.4.1 General guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks. When making a judgement corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level. [12] is the major source of this ranking; other follow-up studies have been integrated, but the essential order is largely unchanged.\n\n\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to assess position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not recommended – they make it harder on the reader to accurately read information off of the charts\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n\n\n\n\n\n\nExample: Misleading Charts\n\n\n\n\n\nHere, the area and height both encode the same variable, leading to a far disproportionate number of pixels allocated to “Stocks” than “Cash Investments” [13]. In the first chart, stocks make up 60% of the portfolio, but have 67.5% of the pixels; Cash makes up 5% of the portfolio but those investments represent 2.3% of the pixels.\n\n[14] has some more great examples of misleading charts.\n\n\n\n\nHere’s a great Ted Ed talk about spotting misleading charts:\n\n\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration).\n\n11.4.2 Perceptual and Cognitive Factors\nShort Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time. Without rehearsing information, short term memory lasts a few seconds.\n\n\n\n\n\n\nTry it out\n\n\n\n\nClick here, read the information, and then click to hide\n\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\n\nWhat was the third number?\n\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\n\n\n\n\n\n\nImportant Takeaways\n\n\n\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n\n\nColor\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. We evolved to be able to distinguish different shades of green because it impacts our ability to find food. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient [15]. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, &lt;1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\n\n\n\n\n\n\nColorblindness Test & Info\n\n\n\n\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal.\nA different test is available here.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Color Deficiency\n\n\n\n\n\nColor “blindness” can arise from either a mutation in one of the three cones (causing the cone to function less optimally), or when the gene for a cone is deleted or mutated to the point where it is not functional. We can simulate what these conditions look like when a rainbow color scheme is used:\n\n\n\n\n\n\nOriginal image using a rainbow color scale\n\n\n\n\n\nRed deficient\n\n\n\n\n\nGreen deficient\n\n\n\n\n\nBlue deficient\n\n\n\n\n\n\n\nRed absent\n\n\n\n\n\nGreen absent\n\n\n\n\n\nBlue absent\n\n\n\n\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\n\n\n\n\n\nFigure 11.1: The color constancy illusion. The squares marked A and B are actually the same color\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\n\n\n\n\n\n\nImportant Takeaways\n\n\n\n\n\nDo not use rainbow color gradient schemes\n\nBecause of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nPalettes such as Viridis and those on colorcet.com are better alternatives because they are perceptually uniform [18].\n\n\nAvoid green-yellow-red signaling\nWhile these schemes are intuitive (go, caution, stop), they are very difficult for those with common types of colorblindness to read.\n\nBe conscious of implied meaning\nThis can be used to your advantage, but you also want to avoid adding context that isn’t supported by the data.\n\nCommon associations can make it easier for viewers\n(but may be culturally specific)\n\nblue for cold, orange/red for hot is a natural scale,\nred = Republican and blue = Democrat in the US\nwhite -&gt; blue gradients for rainfall totals\n\n\nSome colors can can provoke emotional responses that may not be desirable (red = blood, black = death, yellow = happy).\nSome color schemes have social baggage:\n\nthe pink/blue color scheme often used to denote gender can be unnecessarily polarizing\nInstead, consider using a dark color (blue or purple) for men and a light color (yellow, orange, lighter green) for women [19].\n\n\n\n\n\n\n\n\n\nWhen the COVID-19 outbreak started, many maps were using white-to-red gradients to show case counts and/or deaths. The emotional association between red and blood, danger, and death may have caused people to become more frightened than what was reasonable given the available information. [20]\n\n\n\n\n\n\nColor Palette Options\n\n\n\n\n\n\nPackages such as RColorBrewer and dichromat have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer).\n\nOther fun color palette generation methods [22] may also be useful.\n\nArtistic sources:\n\nTaylor Swift album colors [23]\n\nWes Anderson and Studio Ghibli films [25]\n\nfamous paintings and other images [26]\n\n\n\nNatural palettes:\n\nThe earthtones package generates palettes for elevation maps based on the actual location you’re showing. [27]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColorblind Friendly Graphics\n\n\n\n\n\n\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out. This also makes your plot more readable [28] and helps your mapping stand out more.\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -&gt; dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange.\nIn any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\n\n\nGrouping and Sense-making: Imposing order on visual chaos\nTake a look at each of the examples in the panels below.\n\n\nAmbiguous Image\nIllusory Contours\nFigure-background\n\n\n\n\n\nIs it a rabbit, or a duck?\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\n\nWhat do you see?\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\nYou can read about the gestalt rules in more depth if you want to, but they are also demonstrated in the figure above [29].\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\n\n\n\n\n\nExample: Grouping in Graphics\n\n\n\n\n\n\n\n\n\n\nA bar chart\n\n\n\n\n\nA line chart\n\n\n\n\n\nA box plot\n\n\n\n\n\nWhich chart best demonstrates that in every state and region, the murder rate decreased?\n\n\n\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\n\n\n\n\n\n\n\n\nImportant Takeaways\n\n\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) come to when examining those plots [28]."
  },
  {
    "objectID": "data-vis.html#useful-tricks",
    "href": "data-vis.html#useful-tricks",
    "title": "11  Data Visualization",
    "section": "\n11.5 Useful Tricks",
    "text": "11.5 Useful Tricks\n\n11.5.1 Saving figures\n\n\nggplot2\nBase R\nPlotnine\nMatplotlib\nSeaborn\n\n\n\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, y = body_mass_g, color = species)) + \n  geom_point()\nggsave(filename = \"images/graphics/test-penguin-ggplot2.png\", \n       width = 8, height = 6, dpi = 300)\n\n\n\n\npng(\"images/graphics/test-penguin-baseR.png\", width = 8, height = 6, units = \"in\", res = 300)\nplot(penguins$bill_length_mm, penguins$body_mass_g) \ndev.off()\n\nThere are other functions available, such as pdf, cairo_pdf (which may have better unicode support), and jpeg. Some other R packages, such as ragg, provide additional ways to save R plots to files.\n\n\nIn plotnine, we store the plot as an object and then use the .save() method, which has the same arguments as ggsave().\n\nfrom plotnine import *\n\nmyplot = ggplot(penguins, aes(x = \"bill_length_mm\", y = \"body_mass_g\", color = \"species\")) + geom_point()\nmyplot.save(\"images/graphics/test-penguin-plotnine.png\", width = 8, height = 6, dpi = 300, units = \"in\")\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/ggplot.py:719: PlotnineWarning: Saving 8 x 6 in image.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/ggplot.py:722: PlotnineWarning: Filename: images/graphics/test-penguin-plotnine.png\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/layer.py:401: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\nimport matplotlib as plt\n\nplt.cla() # clear out matplotlib buffer\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: module 'matplotlib' has no attribute 'cla'\nplt.figure(figsize=[8,6], dpi = 300) # set figure size\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: 'module' object is not callable\nplt.scatter(penguins.bill_length_mm, penguins.body_mass_g)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: module 'matplotlib' has no attribute 'scatter'\nplt.savefig(\"images/graphics/test-penguin-matplotlib.png\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: module 'matplotlib' has no attribute 'savefig'\n\n\n\nSeaborn is built on top of matplotlib, so it works the same way.\n\nimport matplotlib as plt\nimport seaborn as sns\n\nplt.cla() # clear out matplotlib buffer\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: module 'matplotlib' has no attribute 'cla'\nplt.figure(figsize=[8,6], dpi = 300) # set figure size\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: 'module' object is not callable\nsns.scatterplot(data = penguins, x = \"bill_length_mm\", y = \"body_mass_g\", hue = \"species\")\nplt.savefig(\"images/graphics/test-penguin-seaborn.png\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: module 'matplotlib' has no attribute 'savefig'\n\n\n\n\n\n11.5.2 Combining Figures\nWe often want to combine multiple figures (or pieces of figures) together. While this topic is outside the scope of this book, I do want to at least point you in the right direction.\n\n\n\n\nThe patchwork package is a great way to combine figures\n\n\n\nR\nPython\n\n\n\n\n\npatchwork [30]\n\n\ncowplot [31]\n\n\n\n\n\n\npatchwork (works with matplotlib, plotnine, and seaborn) [32]\n\nthe subplot_mosaic function in matplotlib\n\n\n\n\n\n\n11.5.3 Interactive Plots\nWe’ll get to interactive plots later, but for now, check out\n\n\nplotly (R and python)\n\nanimint2 (Takes ggplot2 graphics and creates interactive d3 charts)\n\nShiny (R originally, and as of July 2022, for python too!)"
  },
  {
    "objectID": "data-vis.html#references",
    "href": "data-vis.html#references",
    "title": "11  Data Visualization",
    "section": "\n11.6 References",
    "text": "11.6 References\n\n\n\n\n[1] \nJ. W. Tukey, “Data-Based Graphics: Visual Display in the Decades to Come,” Statistical Science, vol. 5, no. 3, pp. 327–339, Aug. 1990, doi: 10.1214/ss/1177012101. [Online]. Available: https://projecteuclid.org/journals/statistical-science/volume-5/issue-3/Data-Based-Graphics--Visual-Display-in-the-Decades-to/10.1214/ss/1177012101.full. [Accessed: Aug. 22, 2022]\n\n\n[2] \nL. Wilkinson, The grammar of graphics. New York: Springer, 1999 [Online]. Available: http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3085765. [Accessed: Jan. 29, 2020]\n\n\n[3] \nF. E. Croxton and R. E. Stryker, “Bar Charts Versus Circle Diagrams,” Journal of the American Statistical Association, vol. 22, no. 160, pp. 473–482, 1927, doi: 10.2307/2276829. [Online]. Available: https://www.jstor.org/stable/2276829. [Accessed: Aug. 22, 2022]\n\n\n[4] \nS. Meschiari, latex2exp: Use LaTeX expressions in plots. 2022 [Online]. Available: https://CRAN.R-project.org/package=latex2exp\n\n\n\n[5] \nJ. Bryan, “Putting colors to work for you in base graphics,” Stat 545 at UBC. [Online]. Available: https://www.stat.ubc.ca/~jenny/STAT545A/block15_colorMappingBase.html. [Accessed: Aug. 22, 2022]\n\n\n[6] \nWikipedia contributors, “Color space — Wikipedia, the free encyclopedia.” 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Color_space&oldid=1096489744\n\n\n\n[7] \nM. Waskom, “Seaborn,” Seaborn data visualization library. 2022 [Online]. Available: https://seaborn.pydata.org/. [Accessed: Aug. 29, 2022]\n\n\n[8] \nM. Waskom, “Next-generation seaborn interface,” seaborn nextgen documentation. 2022 [Online]. Available: https://seaborn.pydata.org/nextgen/. [Accessed: Aug. 29, 2022]\n\n\n[9] \nT. Morgan-Wall, Rayshader: Create maps and visualize data in 2D and 3D. 2021 [Online]. Available: https://CRAN.R-project.org/package=rayshader\n\n\n\n[10] \nS. Vanderplas, D. Cook, and H. Hofmann, “Testing Statistical Charts: What Makes a Good Graph?” Annual Review of Statistics and Its Application, vol. 7, no. 1, Mar. 2020, doi: ggnv7t. [Online]. Available: https://www.annualreviews.org/doi/10.1146/annurev-statistics-031219-041252. [Accessed: Feb. 10, 2020]\n\n\n[11] \nH. Hofmann, L. Follett, M. Majumder, and D. Cook, “Graphical Tests for Power Comparison of Competing Designs,” IEEE Transactions on Visualization and Computer Graphics, vol. 18, no. 12, pp. 2441–2448, Dec. 2012, doi: 10.1109/TVCG.2012.230. \n\n\n[12] \nW. S. Cleveland and R. McGill, “Graphical perception: Theory, experimentation, and application to the development of graphical methods,” Journal of the American statistical association, vol. 79, no. 387, pp. 531–554, 1984. \n\n\n[13] \nK. Fung, “When the pie chart is more complex than the data,” Junk Charts. Jun. 2020 [Online]. Available: https://junkcharts.typepad.com/junk_charts/2020/06/when-the-pie-chart-is-more-complex-than-the-data.html. [Accessed: Sep. 19, 2022]\n\n\n[14] \nW. Hickey, “The 27 Worst Charts Of All Time,” Business Insider. Jun. 2013 [Online]. Available: https://www.businessinsider.com/the-27-worst-charts-of-all-time-2013-6. [Accessed: Sep. 19, 2022]\n\n\n[15] \nWikipedia Contributors, “Color blindness,” Wikipedia. Sep. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Color_blindness. [Accessed: Sep. 19, 2022]\n\n\n[16] \nJ. R. Nuñez, C. R. Anderton, and R. S. Renslow, “Optimizing colormaps with consideration for color vision deficiency to enable accurate interpretation of scientific data,” PLOS ONE, vol. 13, no. 7, Aug. 2018, doi: 10.1371/journal.pone.0199239. [Online]. Available: https://dx.plos.org/10.1371/journal.pone.0199239. [Accessed: Sep. 19, 2022]\n\n\n[17] \nP. Kovesi, “Good Colour Maps: How to Design Them.” arXiv, Sep. 2015 [Online]. Available: http://arxiv.org/abs/1509.03700. [Accessed: Sep. 19, 2022]\n\n\n[18] \nP. Kovesi, “CET Perceptually Uniform Colour Maps,” ColorCET. 2021 [Online]. Available: https://colorcet.com/. [Accessed: Sep. 19, 2022]\n\n\n[19] \nL. C. Rost, “What to Consider When Choosing Colors for Data Visualization,” Dataquest. Aug. 2018 [Online]. Available: https://www.dataquest.io/blog/what-to-consider-when-choosing-colors-for-data-visualization/. [Accessed: Sep. 19, 2022]\n\n\n[20] \nK. Field, “Mapping coronavirus, responsibly,” ArcGIS Blog. Feb. 2020 [Online]. Available: https://www.esri.com/arcgis-blog/products/product/mapping/mapping-coronavirus-responsibly/. [Accessed: Aug. 31, 2022]\n\n\n[21] \nL. C. Muth, “Your Friendly Guide to Colors in Data visualization,” Lisa Charlotte Rost Muth. Apr. 2016 [Online]. Available: https://lisacharlottemuth.com/2016/04/22/Colors-for-DataVis/. [Accessed: Sep. 19, 2022]\n\n\n[22] \nM. Taylor, “Having trouble picking a color palette for your #Rstats visualization? Well here’s a MEGA thread about all the ways you can choose a palette! 🧵[1/22],” Twitter. May 2021 [Online]. Available: https://twitter.com/moriah_taylor58/status/1395431000977649665. [Accessed: Sep. 19, 2022]\n\n\n[23] \nA. Stephenson, tayloRswift: Color palettes generated by taylor swift albums. 2022 [Online]. Available: https://asteves.github.io/tayloRswift/\n\n\n\n[24] \nK. Ram and H. Wickham, Wesanderson: A wes anderson palette generator. 2018 [Online]. Available: https://CRAN.R-project.org/package=wesanderson\n\n\n\n[25] \nE. Henderson, Ghibli: Studio ghibli colour palettes. 2022 [Online]. Available: https://CRAN.R-project.org/package=ghibli\n\n\n\n[26] \nA. Cirillo, Paletter: Make a palette from your image. 2022 [Online]. Available: https://github.com/AndreaCirilloAC/paletter\n\n\n\n[27] \nW. Cornwell, M. Lyons, and N. Murray, Earthtones: Derive a color palette from a particular location on earth. 2019 [Online]. Available: https://CRAN.R-project.org/package=earthtones\n\n\n\n[28] \nS. VanderPlas and H. Hofmann, “Clusters Beat Trend⁉ Testing Feature Hierarchy in Statistical Graphics,” Journal of Computational and Graphical Statistics, vol. 26, no. 2, pp. 231–242, Apr. 2017, doi: 10.1080/10618600.2016.1209116. [Online]. Available: https://doi.org/10.1080/10618600.2016.1209116. [Accessed: Dec. 12, 2018]\n\n\n[29] \nWikipedia Contributors, “Principles of grouping,” Wikipedia. Aug. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Principles_of_grouping&oldid=1102190641. [Accessed: Sep. 19, 2022]\n\n\n[30] \nT. L. Pedersen, Patchwork: The composer of plots. 2022 [Online]. Available: https://CRAN.R-project.org/package=patchwork\n\n\n\n[31] \nC. O. Wilke, Cowplot: Streamlined plot theme and plot annotations for ’ggplot2’. 2020 [Online]. Available: https://CRAN.R-project.org/package=cowplot\n\n\n\n[32] \nH. Mori, “Patchworklib.” Sep. 2022 [Online]. Available: https://github.com/ponnhide/patchworklib. [Accessed: Sep. 19, 2022]"
  },
  {
    "objectID": "debugging.html#module12-objectives",
    "href": "debugging.html#module12-objectives",
    "title": "12  Debugging",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nCreate reproducible examples of problems\nUse built in debugging tools to trace errors\nUse online resources to research errors\n\nNote: The skills in this chapter take a lifetime to truly master. The real goal here is that you know how to ask for help appropriately (and in a way that people will respond positively to) and that you know how to do the research to get help yourself.\n\n\nThe faces of debugging (by Allison Horst)"
  },
  {
    "objectID": "debugging.html#avoiding-errors-defensive-programming",
    "href": "debugging.html#avoiding-errors-defensive-programming",
    "title": "12  Debugging",
    "section": "\n12.1 Avoiding Errors: Defensive Programming",
    "text": "12.1 Avoiding Errors: Defensive Programming\nOne of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively [1]. By that, I mean, code in a way that you will make debugging things easier later.\n\nModularize your code. Each function should do only one task, ideally in the least-complex way possible.\nMake your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly.\nComment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you.\nDon’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex.\nReduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse [2] is notorious for this.\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constituent packages are some of the most installed R packages on CRAN.)\n\n\nAnother way to handle dependency management is to use the renv package [3], which creates a local package library with the appropriate versions of your packages stored in the same directory as your project. renv was inspired by the python concept of virtual environments, and it does also work with python if you’re using both R and python inside a project (e.g. this book uses renv). renv will at the very least help you minimize issues with code not working after unintentional package updates.\n\nAdd safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a”.)\nDon’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression.\nCollect your often-reused code in packages that you can easily load and make available to “future you”. The process of making a package often encourages you to document your code better than you would a script. A good resource for getting started making R packages is [4], and a similar python book is [5]."
  },
  {
    "objectID": "debugging.html#working-through-errors",
    "href": "debugging.html#working-through-errors",
    "title": "12  Debugging",
    "section": "\n12.2 Working through Errors",
    "text": "12.2 Working through Errors\n\n12.2.1 First steps\nGet into the right mindset\nYou can’t debug something effectively if you’re upset. You have to be in a puzzle-solving, detective mindset to actually solve a problem. If you’re already stressed out, try to relieve that stress before you tackle the problem: take a shower, go for a walk, pet a puppy.\n\n\nA debugging manifesto [6]\n\nCheck your spelling\nI’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation.\n\n\nTitle: user.fist_name [7]\n\n\n12.2.2 General Debugging Strategies\n\n\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nWhile defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in [8]; I’ve added some steps that are commonly overlooked and modified the context from the original package development to introductory programming. I’ve also integrated some lovely illustrations from Julia Evans (@b0rk) to lighten the mood.\n\nRealize that you have a bug\nRead the error message\n\n\n\nDebugging strategy: Reread the error message[9]\n\n\n\nGoogle! Seriously, just google the whole error message.\nIn R you can automate this with the errorist and searcher packages. Python is so commonly used that you’ll likely be able to find help for your issue if you are specific enough.\n\n\n\n\n\nDebugging strategy: Shorten your feedback loop [10]\n\n\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. \nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nCome up with one question. If you’re stuck, it can be helpful to break it down a bit and ask one tiny question about the bug.\n\n\n\nDebugging strategy: Come up with one question [13]\n\n\n\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked.\n\n\n\n\n\nDebugging strategy: Write a unit test [14]\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed &lt;-- instead of &lt;- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck1. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. A more thorough explanation can be found at gitduck.com.\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea."
  },
  {
    "objectID": "debugging.html#dividing-problems-into-smaller-parts",
    "href": "debugging.html#dividing-problems-into-smaller-parts",
    "title": "12  Debugging",
    "section": "\n12.3 Dividing Problems into Smaller Parts",
    "text": "12.3 Dividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solvable.\n\n\n\n\n\n\nExample: Exhaustion\n\n\n\nThis example inspired by [15].\n\n\nGeneral problem\nSpecific problem\nSubproblems\nBrainstorm\nSubproblem solutions\n\n\n\n“I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\n\n\n“I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\n\nMoving through the list in the previous tab, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\n\n\n\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfullness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work."
  },
  {
    "objectID": "debugging.html#minimal-working-or-reproducible-examples",
    "href": "debugging.html#minimal-working-or-reproducible-examples",
    "title": "12  Debugging",
    "section": "\n12.4 Minimal Working (or Reproducible) Examples",
    "text": "12.4 Minimal Working (or Reproducible) Examples\n\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and really famous python developers browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\nThere are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working.\nOther resources:\n\nreprex package: Do’s and Don’ts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney\n\n\n12.4.1 Example: MWEs\n\n\nSAS markdown\nPython/Quarto\n\n\n\nWhen this book was written to teach R and SAS, at one point I had issues with SAS graphs rendering in black and white most of the time.\nI started debugging the issue with the following code chunk:\n{r sas-cat-aes-map-07, engine=\"sashtml\", engine.path=\"sas\", fig.path = \"image/\"}\nlibname classdat \"sas/\";\n\nPROC SGPLOT data=classdat.fbiwide; \nSCATTER x = Population y = Assault /\n  markerattrs=(size=8pt symbol=circlefilled) \n  group = Abb; /* maps to point color by default */\nRUN;\nQUIT; \n  \nPROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend */\nSCATTER x = Population y = Assault /\n  markercharattrs=(size=8) \n  markerchar = Abb /* specify marker character variable */\n    group = Abb\n  ; \nRUN;\nQUIT; \n(I moved the chunk header to the next line so that you can see the whole chunk)\nAfter running the code separately in SAS and getting a figure that looked like what I’d expected, I set out to construct a reproducible example so that I could post to the SASmarkdown github issues page and ask for help.\nThe first thing I did was strip out all of the extra stuff that didn’t need to be in the chunk - this chunk generates 2 pictures; I only need one. This chunk requires the fbiwide data; I replaced it with a dataset in the sashelp library.\nWhen I was done, the chunk looked like this:\nPROC SGPLOT data=sashelp.snacks;\nSCATTER x = date y = QtySold /\n  markerattrs=(size=8pt symbol=circlefilled)\n  group = product; /* maps to point color by default */\nRUN;\nQUIT;\nThen, I started constructing my reproducible example. I ran ?sas_enginesetup to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file).\nI copied the example from that page:\nindoc &lt;- '\n---\ntitle: \"Basic SASmarkdown Doc\"\nauthor: \"Doug Hemken\"\noutput: html_document\n---\n\n# I've deleted the intermediate chunks because they screw \n# everything up when I print this chunk out\n'\nknitr::knit(text=indoc, output=\"test.md\")\nrmarkdown::render(\"test.md\")\nThen, I created several chunks which would do the following: 1. Write the minimal example SAS code above to a file 2. Call that file in a SASmarkdown chunk using the %include macro, which dumps the listed file into the SAS program. This generates the plot using SASmarkdown. 3. Call the file using SAS batch mode\n(this runs the code and produces a plot outside of SASmarkdown, to prove that the issue is SASmarkdown itself)\nFinally, I included the image generated from the batch mode call manually.\nYou can see the resulting code here.\nI pasted my example into the issues page, and then included some additional information:\n\nA screenshot of the rendered page\nThe image files themselves\nA description of what happened\nMy suspicions (some obvious option I’m missing?)\nAn additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks.\n\nThis process took me about 45 minutes, but that was still much shorter than the time I’d spent rerunning code trying to get it to work with no success.\nIn less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. I had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do).\nThen, I had to tinker with the book a bit to figure out if there were easier ways to get the same result. Hopefully, at this point, all of the SAS graphs are now in full color, as modern graphics intended.\n\n\nWhile converting the book from Rmarkdown to quarto, I ran into an issue setting up GitHub Actions (basically, when I push changes, GitHub rebuilds the book from scratch automatically).\nI found an issue describing the same segfault issue I had been getting, and so I made a post there with a new github repository containing a minimal working example that I set up to test the problem.\nWithin 24h, I had gotten replies from people working at RStudio, and one of them had diagnosed the problem. After I asked a few more questions, one of them submitted a pull request to my repository with a solution.\nI didn’t know enough python or enough about GitHub Actions to diagnose the problem myself, but because I managed to create a reproducible example, I got the answers I needed from people with more experience.\n\n\n\n\n\n\n\n\n\nTry It Out\n\n\n\nUse this list of StackOverflow posts to try out your new debugging techniques. Can you figure out what’s wrong? What information would you need from the poster in order to come up with a solution?"
  },
  {
    "objectID": "debugging.html#debugging-tools",
    "href": "debugging.html#debugging-tools",
    "title": "12  Debugging",
    "section": "\n12.5 Debugging Tools",
    "text": "12.5 Debugging Tools\nNow that we’ve discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging.\n\n12.5.1 Low tech debugging with print() and other tools\nSometimes called “tracing” techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. When the code is executed, you get a window into what the variables look like during execution.\nThis is called print debugging and it is an incredibly useful tool.\n\n\n\n\n\n\nExample: Nested Functions\n\n\n\n\n\nR\nPython\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n      z + y\n    }\n    cc(3) + 2\n  }\n  x + bb(4)\n}\n\naa(5)\n## [1] 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\naa &lt;- function(x) {\n  print(paste(\"Entering aa(). x = \", x))\n  bb &lt;- function(y) {\n    print(paste(\"Entering bb(). x = \", x, \"y = \", y))\n    cc &lt;- function(z) {\n      print(paste(\"Entering cc(). x = \", x, \"y = \", y, \"z = \", z))\n      cres &lt;- z + y\n      print(paste(\"Returning\", cres, \"from cc()\"))\n      cres\n    }\n    bres &lt;- cc(3) + 2\n    print(paste(\"Returning\", bres, \"from bb()\"))\n    bres\n  }\n  ares &lt;- x + bb(4)\n  print(paste(\"Returning\",ares, \"from aa()\"))\n  ares\n}\n\naa(5)\n## [1] \"Entering aa(). x =  5\"\n## [1] \"Entering bb(). x =  5 y =  4\"\n## [1] \"Entering cc(). x =  5 y =  4 z =  3\"\n## [1] \"Returning 7 from cc()\"\n## [1] \"Returning 9 from bb()\"\n## [1] \"Returning 14 from aa()\"\n## [1] 14\n\n\n\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      return z + y\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  print(\"Entering aa(). x = \" + str(x))\n  def bb(y):\n    print(\"Entering bb(). x = \" + str(x) + \", y = \" + str(y))\n    def cc(z):\n      print(\"Entering cc(). x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))\n      cres = z + y\n      print(\"Returning \" + str(cres) + \" from cc()\")\n      return cres\n    bres = cc(3) + 2\n    print(\"Returning \" + str(bres) + \" from bb()\")\n    return bres\n  ares = x + bb(4)\n  print(\"Returning \" + str(ares) + \" from aa()\")\n  return ares\n\naa(5)\n## Entering aa(). x = 5\n## Entering bb(). x = 5, y = 4\n## Entering cc(). x = 5, y = 4, z = 3\n## Returning 7 from cc()\n## Returning 9 from bb()\n## Returning 14 from aa()\n## 14\n\n\n\n\n\n\nFor more complex data structures, it can be useful to add str(), head(), or summary() functions.\n\n\n\n\n\n\nReal world example: Web Scraping\n\n\n\nIn fall 2020, when I started teaching 850, I wrote a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I used print debugging to show the sequence of tags on the page.\nI was assuming that the order of the parents would be “html”, “body”, “div”, “table”, “tbody”, “tr” - descending from outer to inner (if you know anything about HTML/XML structure).\nTo prevent the site from changing on me (as websites tend to do…), I’ve saved the HTML file here.\n\n\nR\nPython\n\n\n\n\nlibrary(xml2) # read html\n\nsearch_for_parent &lt;- function(node, tag) {\n  # Get all of the parent nodes \n  parents &lt;- xml2::xml_parents(node)\n  # Get the tags of every parent node\n  tags &lt;- purrr::map_chr(parents, rvest::html_name)\n  print(tags)\n  \n  # Find matching tags\n  matches &lt;- which(tags == tag)\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match &lt;- min(matches)\n  if (length(matches) == 1) return(parents[min_match]) else return(NULL)\n}\n\npage &lt;- read_html(\"data/realclearpolitics_frag.html\")\n## Error: 'data/realclearpolitics_frag.html' does not exist in current working directory ('/home/susan/Projects/Class/unl-stat850/stat850-textbook').\npoll_results &lt;- xml_find_all(page, \"//td[@class='lp-results']\") # find all poll results in any table\n## Error in UseMethod(\"xml_find_all\"): no applicable method for 'xml_find_all' applied to an object of class \"function\"\nsearch_for_parent(poll_results[1], \"table\") # find the table that contains it\n## Error in nodeset_apply(x, function(x) .Call(node_parents, x)): object 'poll_results' not found\n\n\n\nYou may need to pip install lxml requests bs4 to run this code.\n\n# !pip install lxml requests bs4\nfrom bs4 import BeautifulSoup\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'bs4'\nimport requests as req\nimport numpy as np\n\n\ndef search_for_parent(node, tag):\n  # Get all of the parent nodes\n  parents = node.find_parents()\n  # get tag type for each parent node\n  tags = [x.name for x in parents]\n  print(tags)\n  \n  # Find matching tags\n  matches = np.array([i for i, val in enumerate(tags) if val == tag])\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match = np.min(matches)\n  if matches.size == 1:\n    ret = parents[min_match]\n  \n  return ret\n\n\nhtml_file = open('data/realclearpolitics_frag.html', 'r')\n## Error in py_call_impl(callable, dots$args, dots$keywords): FileNotFoundError: [Errno 2] No such file or directory: 'data/realclearpolitics_frag.html'\npage = html_file.read() \n# Read the page as HTML\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'html_file' is not defined\nsoup = BeautifulSoup(page, 'html')\n# Find all poll results in any table\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'BeautifulSoup' is not defined\npoll_results = soup.findAll('td', {'class': 'lp-results'})\n# Find the table that contains the first poll result\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'soup' is not defined\nsearch_for_parent(poll_results[0], 'table')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'poll_results' is not defined\n\n\n\n\nBy printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node.\n\n\n\n\n\n\n\n\nTry it out: Hurricanes in R\n\n\n\nNot all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully.\n\n\nSetup\nBuggy code\nSolution 1: Identification\nSolution 2: Fixing\nSolution 3: Verifying\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(maps)\nlibrary(ggthemes)\nworldmap &lt;- map_data(\"world\")\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\n\n\nThe code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nfor (i in 1:5) {\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, lets split the setup from the loop.\n\n# Make base map to be used for each iteration\nbasemap &lt;-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nprint(basemap) # make sure the basemap is fine\n\n\n\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\nstr(storms) # make sure the data exists and is formatted as expected\n## tibble [11,859 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ name                        : chr [1:11859] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n##  $ year                        : num [1:11859] 1975 1975 1975 1975 1975 ...\n##  $ month                       : num [1:11859] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day                         : int [1:11859] 27 27 27 27 28 28 28 28 29 29 ...\n##  $ hour                        : num [1:11859] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat                         : num [1:11859] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n##  $ long                        : num [1:11859] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n##  $ status                      : chr [1:11859] \"tropical depression\" \"tropical depression\" \"tropical depression\" \"tropical depression\" ...\n##  $ category                    : Ord.factor w/ 7 levels \"-1\"&lt;\"0\"&lt;\"1\"&lt;\"2\"&lt;..: 1 1 1 1 1 1 1 1 2 2 ...\n##  $ wind                        : int [1:11859] 25 25 25 25 25 25 25 30 35 40 ...\n##  $ pressure                    : int [1:11859] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n##  $ tropicalstorm_force_diameter: int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n##  $ hurricane_force_diameter    : int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n\nEverything looks ok in the setup chunk…\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(status == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, so from this we can see that something is going wrong with our filter statement - we have no rows of data.\n\n\n\nhead(storms)\n## # A tibble: 6 × 13\n##   name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n##   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;ord&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt;\n## 1 Amy    1975     6    27     0  27.5 -79   tropi… -1         25    1013      NA\n## 2 Amy    1975     6    27     6  28.5 -79   tropi… -1         25    1013      NA\n## 3 Amy    1975     6    27    12  29.5 -79   tropi… -1         25    1013      NA\n## 4 Amy    1975     6    27    18  30.5 -79   tropi… -1         25    1013      NA\n## 5 Amy    1975     6    28     0  31.5 -78.8 tropi… -1         25    1012      NA\n## 6 Amy    1975     6    28     6  32.4 -78.7 tropi… -1         25    1012      NA\n## # … with 1 more variable: hurricane_force_diameter &lt;int&gt;, and abbreviated\n## #   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter\n\nWhoops. I meant “category” when I typed “status”.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 13 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, that’s something, at least. We now have some data for category 1 storms…\n\nfilter(storms, year == 2013) %&gt;%\n  # Get max category for each named storm\n  group_by(name) %&gt;%\n  filter(category == max(category)) %&gt;%\n  ungroup() %&gt;%\n  # See what categories exist\n  select(name, category) %&gt;%\n  unique()\n## # A tibble: 14 × 2\n##    name      category\n##    &lt;chr&gt;     &lt;ord&gt;   \n##  1 Andrea    0       \n##  2 Barry     0       \n##  3 Chantal   0       \n##  4 Dorian    0       \n##  5 Erin      0       \n##  6 Fernand   0       \n##  7 Gabrielle 0       \n##  8 Eight     -1      \n##  9 Humberto  1       \n## 10 Ingrid    1       \n## 11 Jerry     0       \n## 12 Karen     0       \n## 13 Lorenzo   0       \n## 14 Melissa   0\n\nIt looks like 2013 was just an incredibly quiet year for tropical activity.\n\n\n2013 may have been a quiet year for tropical activity in the Atlantic, but 2004 was not. So let’s just make sure our code works by checking out 2004.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2004) %&gt;%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 45 ncol 13\"\n\n\n\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 39 ncol 13\"\n\n\n\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 29 ncol 13\"\n\n\n\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 32 ncol 13\"\n\n\n\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 12 ncol 13\"\n\n\n\n\nIf we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code).\n\nfor (i in 1:5) {\n  # print(paste0(\"Category \", i, \" storms\"))\n  \n  # Subset the data\n  subdata &lt;- storms %&gt;%\n    filter(year == 2013) %&gt;%\n    filter(category == i)\n  \n  # print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n  #       # str(subdata) works too, but produces more clutter. I started\n  #       # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot &lt;- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  \n  if (nrow(subdata) &gt; 0) print(plot) \n}\n\n\n\n\n\n\n\n\n\nOnce you’ve found your problem, go back and delete or comment out your print statements, as they’re no longer necessary. If you think you may need them again, comment them out, otherwise, just delete them so that your code is neat, clean, and concise.\n\n12.5.2 After an error has occurred - traceback()\n\ntraceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This may help you identify which function is actually causing the problem, which is especially useful when you have nested functions or are using package functions that depend on other packages.\n\n\n\n\n\n\nDemo: Using traceback\n\n\n\n\n\nR\nPython\n\n\n\n\naa &lt;- function(x) {\n  bb &lt;- function(y) {\n    cc &lt;- function(z) {\n     stop('there was a problem')  # This generates an error\n    }\n    cc()\n  }\n  bb()\n}\n\naa()\n## Error in cc(): there was a problem\n\nFor more information, you could run traceback\n\ntraceback()\n\nWhich will provide the following output:\n4: stop(\"there was a problem\") at #4\n3: c() at #6\n2: b() at #8\n1: a()\nReading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code.\nIf you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error.\n\n\nBoth Show Traceback and Rerun with Debug are useful tools\n\nIf you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. \n\n\n\nimport sys,traceback\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      try: \n        return y + z + tuple()[0] # This generates an error\n      except IndexError:\n        exc_type, exc_value, exc_tb = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_tb, file = sys.stdout)\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\nPython’s traceback information is a bit more low-level and requires a bit more from the programmer than R’s version.\n\n\n\n\n\n\n12.5.3 Interactive Debugging\n\n\n\n\n\n\n\n\n\nR browser()\nPython\n\n\n\nThe browser() function is useful for debugging your own code. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on.\n\n\n\n\n\n\nExample of using browser()\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in R.\nI start with\n\nlibrary(png)\nlibrary(xml2)\nlibrary(dplyr)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  \n  readPNG(source = image)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n## Error in readPNG(source = image): unable to open //imgs.xkcd.com/comics/division_notation.png\n\nHere’s the final function\n\nlibrary(png)\nlibrary(xml2)\n\n# get the most current xkcd\nget_xkcd &lt;- function() {\n  \n  url &lt;- \"http://xkcd.com\"\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  # Fix image address so that we can access the image\n  image &lt;- substr(image, 3, nchar(image))\n  \n  # Download the file to a temp file and read from there\n  file_location &lt;- tempfile(fileext = \".png\")\n  download.file(image, destfile = file_location, quiet = T)\n  \n  readPNG(source = file_location)\n}\n\nget_xkcd() %&gt;%\n  as.raster() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nIn python, the equivalent interactive debugger is ipdb. You can install it with pip install ipdb.\nIf you want to run Python in the interactive ipython console, then you can invoke the ipdb debugging with %debug get_xkcd(). This is similar to browser() in R. If you’re working in Python in RStudio, though, you have to get into debug mode in a more involved way.\nTo run code using ipdb when your code hits an error, add from ipdb import launch_ipdb_on_exception to the top of your python code chunk. Then, at the bottom, put any lines that may trigger the error after these two lines:\nif __name__ == \"__main__\":\n  with launch_ipdb_on_exception():\n    &lt;your properly indented code goes here&gt;\nThis ensures that ipdb is launched when an error is reached.\n\n\n\n\n\n\nExample using ipdb\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in python.\nI start with\n\nfrom bs4 import BeautifulSoup\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'bs4'\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'BeautifulSoup' is not defined\nplt.show()\n\n\n\n\nHere’s the final function\n\nfrom bs4 import BeautifulSoup\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'bs4'\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic &gt; img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'BeautifulSoup' is not defined\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nEach xkcd has a corresponding ID number (ordered sequentially from 1 to 2328 at the time this was written). Modify the XKCD functions above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic.\nUse interactive debugging tools to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL.\nWhat things might you add to make this function “defensive programming” compatible?\n\n\n\n# get the most current xkcd or the specified number\nget_xkcd &lt;- function(id = NULL) {\n  if (is.null(id)) {\n    # Have to get the location of the image ourselves\n    url &lt;- \"http://xkcd.com\"\n  } else if (is.numeric(id)) {\n    url &lt;- paste0(\"http://xkcd.com/\", id, \"/\")\n  } else {\n    # only allow numeric or null input\n    stop(\"To get current xkcd, pass in NULL, otherwise, pass in a valid comic number\")\n  }\n\n  page &lt;- read_html(url)\n  # Find the comic\n  image &lt;- xml_find_first(page, \"//div[@id='comic']/img\") %&gt;%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  # Fix image address so that we can access the image\n  image &lt;- substr(image, 3, nchar(image)) # cut the first 2 characters off\n\n  # make temp file\n  location &lt;- tempfile(fileext = \"png\")\n  download.file(image, destfile = location, quiet = T)\n\n  # This checks to make sure we saved the file correctly\n  if (file.exists(location)) {\n    readPNG(source = location)\n  } else {\n    # Give a good informative error message\n    stop(paste(\"Something went wrong saving the image at \", image, \" to \", location))\n  }\n}\n\nget_xkcd(2259) %&gt;%\n  as.raster() %&gt;% \n  plot()\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'bs4'\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd(id=''):\n  image = 0 # Defining a placeholder\n  \n  if id == '':\n    # Have to get the location of the image ourselves\n    url = \"http://xkcd.com\"\n  elif id.isnumeric():\n    url = \"http://xkcd.com/\" + id + \"/\"\n  else:\n    # only allow numeric or null input\n    raise TypeError(\"To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\")\n  \n  # Print debugging left in for your amusement\n  # print(type(id))\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imnode = soup.select('#comic &gt; img')\n  \n  try:\n    imlink = imnode[0].get('src')\n  except:\n    raise Exception(\"No comic could be found with number \" + id + \" (url = \"+ url+ \" )\")\n  \n  try: \n    # Format as a numpy array\n    image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n    return image\n  except: \n    raise Exception(\"Reading the image failed. Check to make sure an image exists at \" + url)\n    return(None)\n\n\nres = get_xkcd('')\n\nError in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'BeautifulSoup' is not defined\n\nplt.imshow(res)\n\nError in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'res' is not defined\n\nplt.show()\n\n\n\nres = get_xkcd('2500')\n\nError in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'BeautifulSoup' is not defined\n\nres = get_xkcd('abcd')\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\n\n\n\n\n\n\n\n\n12.5.4 R debug()\n\nIn the traceback() Rstudio output, the other option is “rerun with debug”. In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow.\ndebug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code.\n\n\n\n\n\n\ndebug() example\n\n\n\n\n\n\ndata(iris)\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n## \n## Call:\n## lm(formula = Species ~ ., data = iris)\n## \n## Residuals:\n## Error in quantile.default(resid): (unordered) factors are not allowed\n\nWe get this weird warning, and then an error about factors when we use summary() to look at the coefficients.\n\ndebug(lm) # turn debugging on\n\n\ntmp &lt;- lm(Species ~ ., data = iris)\nsummary(tmp)\n\nundebug(lm) # turn debugging off\n\n\n\nThe first thing I see when I run lm after turning on debug (screenshot)\n\n\n\nThe variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function\n\nAfter pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function.\n We can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong.\nWe probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened.\n\n\nI can hit “Stop” or type “Q” to exit the debug environment.\n\nBut, until I run undebug(lm), every call to lm will take me into the debug window.\n\n\n\nundebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run.\n\n\n\n\n\n\nTry it out: debug in R\n\n\n\n\n\nProblem\nSolution\n\n\n\nlarger(x, y) is supposed to return the elementwise maximum of two vectors.\n\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  x[y.is.bigger] &lt;- y[y.is.bigger] \n  x\n} \n\nlarger(c(1, 5, 10), c(2, 4, 11))\n## [1]  2  5 11\n\n\nlarger(c(1, 5, 10), 6)\n## [1]  6 NA 10\n\nWhy is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it.\n\n\nI’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined\n\n\nx &lt;- c(1, 5, 10)\ny &lt;- 6\n\n# Inside of larger() with x = c(1, 5, 10), y = 6\n(y.is.bigger &lt;- y &gt; x ) # putting something in () prints it out\n## [1]  TRUE  TRUE FALSE\ny[y.is.bigger] # This isn't quite what we were going for, but it's what's causing the issue\n## [1]  6 NA\nx[y.is.bigger] # What gets replaced\n## [1] 1 5\n\n\n# Better option\nlarger &lt;- function(x, y) { \n  y.is.bigger &lt;- y &gt; x \n  ifelse(y.is.bigger, y, x)\n} \n\n\n\n\n\n\n\n\n\n\n[1] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[2] \nH. Wickham et al., “Welcome to the tidyverse,” Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686. \n\n\n[3] \nK. Ushey, Renv: Project environments. 2022 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[4] \nH. Wickham and J. Bryan, R Packages: Organize, Test, Document, and Share Your Code, 1st ed. Sebastopol, CA: O’Reilly, 2015 [Online]. Available: https://r-pkgs.org/. [Accessed: Sep. 23, 2022]\n\n\n[5] \nT. Beuzen and T. Timbers, Python Packages, 1st edition. Boca Raton: Chapman; Hall/CRC, 2022 [Online]. Available: https://py-pkgs.org/\n\n\n\n[6] \nJ. Evans, “A debugging manifesto https://t.co/3eSOFQj1e1,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570060516839641092. [Accessed: Sep. 21, 2022]\n\n\n[7] \nNasser_Junior, “User.fist_name https://t.co/lxrf3IFO4x,” Twitter. Aug. 2020 [Online]. Available: https://twitter.com/Nasser_Junior/status/1295805928315531264. [Accessed: Sep. 21, 2022]\n\n\n[8] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nJ. Evans, “Debugging strategy: Reread the error message https://t.co/2BZHhPg04h,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570463473011920897. [Accessed: Sep. 21, 2022]\n\n\n[10] \nJ. Evans, “Debugging strategy: Shorten your feedback loop https://t.co/1cByDlafsK,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1549164800978059264. [Accessed: Sep. 21, 2022]\n\n\n[11] \nJ. Evans, “Debugging strategy: Write a tiny program https://t.co/Kajr5ZyeIp,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1547247776001654786. [Accessed: Sep. 21, 2022]\n\n\n[12] \nJ. Evans, “Debugging strategy: Change working code into broken code https://t.co/1T5uNDDFs0,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1545099244238946304. [Accessed: Sep. 21, 2022]\n\n\n[13] \nJ. Evans, “Debugging strategy: Come up with one question https://t.co/2Lytzl4laQ,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1554120424602193921. [Accessed: Sep. 21, 2022]\n\n\n[14] \nJ. Evans, “Debugging strategy: Write a unit test https://t.co/mC01DBNyM3,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1561718747504803842. [Accessed: Sep. 21, 2022]\n\n\n[15] \nS. Grimes, “This 500-Year-Old Piece of Advice Can Help You Solve Your Modern Problems,” Forge. Dec. 2019 [Online]. Available: https://forge.medium.com/the-500-year-old-piece-of-advice-that-will-change-your-life-1e580f115731. [Accessed: Sep. 21, 2022]"
  },
  {
    "objectID": "debugging.html#footnotes",
    "href": "debugging.html#footnotes",
    "title": "12  Debugging",
    "section": "",
    "text": "Some people use cats, but I find that they don’t meet the nonjudgmental criteria. Of course, they’re equally judgmental whether your code works or not, so maybe that works if you’re a cat person, which I am not. Dogs, in my experience, can work, but often will try to comfort you when they realize you’re upset, which both helps and lessens your motivation to fix the problem. A rubber duck is the perfect dispassionate listener.↩︎"
  },
  {
    "objectID": "documents.html#module11-objectives",
    "href": "documents.html#module11-objectives",
    "title": "13  Reproducibility and Professional Communication",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nCreate professional documents (slides, posters, CVs) using LaTeX and/or markdown\n\nThis chapter will be shorter in length than many of the rest, but you should not devote less time to it. Instead, you should spend the time playing with the different options presented here and deciding which one of each is your favorite. Rather than detailing all of the customization options in each package, I think you’ll have an easier time looking at examples, trying to customize them yourself to get the effect you want, and figuring out how to do that by reading the documentation, stackoverflow posts, and other help files – those are the skills you’ll need when you try to put this knowledge into action.\nAt the end of this chapter there are a few extras – for instance, how to use GitHub to host your documents, how to create a blog with blogdown, and more. You should feel free to investigate, but as long as you are able to create presentation slides, posters, and a CV, you’re good to go.\n\n\n\n\nReproducibility with Rmarkdown (by Allison Horst)"
  },
  {
    "objectID": "documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "href": "documents.html#literate-programming-knitr-rmarkdown-and-quarto",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.1 Literate Programming, knitr, rmarkdown, and quarto\n",
    "text": "13.1 Literate Programming, knitr, rmarkdown, and quarto\n\nLiterate programming is a programming method where you explain the code in natural language (e.g. English) in roughly the same space that you write the code (in a programming language). This solves two problems: code isn’t always clear as to what its goals are, and natural language descriptions of algorithms aren’t always clear enough to contain the details of how something is actually implemented.\nThe knitr, Rmarkdown, and quarto packages are all implementations of literate programming (and the packages tend to overlap a bit, because knitr and Rmarkdown were written by the same author, Yihui Xie, and quarto is next-generation Rmarkdown).\n\n\nknitr is primarily focused on the creation of Rnw (r no weave) files, which are essentially LaTeX files with R code inside. Rnw files are compiled into pdfs.\n\nrmarkdown uses Rmd or Rmarkdown files, which can then be compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nquarto uses qmd files, which are compiled into many different formats: pdf, html, markdown, Microsoft Word.\n\nAll of these programs work essentially the same way: code chunks are run in the specified language, figures are saved, tables are created, and the results are added to the intermediate file (.tex or .md). Then, another program (LaTeX or Pandoc) compiles the intermediate file into the final result. Understanding this process is key to being able to debug any errors you may encounter, because you need to identify which program is having the error - the code chunk? adding the results to the intermediate file? compiling from the intermediate file to the end result?\n\n\nknitr\nrmarkdown\nquarto\n\n\n\n\n\nKnitr uses R to produce a tex (.tex) file, which is then compiled to PDF using LaTeX.\n\n\n\n\n\nrmarkdown uses R to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\n\nquarto uses R or python to produce a markdown (.md) file, which is then compiled to PDF, DOC, HTML, or other formats using pandoc.\n\n\n\n\nOne major advantage of literate programming packages from a practical perspective is that it largely removes the need to keep track of graphs and charts when you’re writing a paper, making a presentation, etc. The charts and tables based on your method automatically update when the document is recompiled.\nIf you’re not reading this chapter early, you’ve been using quarto for the entire semester to submit your homework. Hopefully that’s been fairly easy - you’ve been creating quarto documents all semester. In this chapter, we’re going to explore some other applications of literate programming: creating slides, posters, and more."
  },
  {
    "objectID": "documents.html#review-quarto-formatting",
    "href": "documents.html#review-quarto-formatting",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.2 Review: Quarto Formatting",
    "text": "13.2 Review: Quarto Formatting\nThis section’s material is stolen copied directly from the Quarto documentation [1].\nText Formatting\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n*italics* and **bold**\n\nitalics and bold\n\n\n\nsuperscript^2^ / subscript~2~\nsuperscript2 / subscript2\n\n\n\n~~strikethrough~~\nstrikethrough\n\n\n`verbatim code`\nverbatim code\n\n\nHeadings\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n# Header 1\nHeader 1\n\n\n## Header 2\nHeader 2\n\n\n### Header 3\nHeader 3\n\n\n#### Header 4\nHeader 4\n\n\n##### Header 5\nHeader 5\n\n\n###### Header 6\nHeader 6\n\n\n\nLinks & Images\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n&lt;https://quarto.org&gt;\nhttps://quarto.org\n\n\n[Quarto](https://quarto.org)\nQuarto\n\n\n![Caption](elephant.png)\n\n\n\n[![Caption](elephant.png)](https://quarto.org)\n\n\n\n[![Caption](elephant.png)](https://quarto.org \"An elephant\")\n\n\n\n[![](elephant.png){fig-alt=\"Alt text\"}](https://quarto.org)\n\n\n\n\n\nLists\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n* unordered list\n    + sub-item 1\n    + sub-item 2\n        - sub-sub-item 1\n\n\nunordered list\n\nsub-item 1\n\nsub-item 2\n\nsub-sub-item 1\n\n\n\n\n\n\n\n*   item 2\n\n    Continued (indent 4 spaces)\n\n\nitem 2\nContinued (indent 4 spaces)\n\n\n\n\n1. ordered list\n2. item 2\n    i) sub-item 1\n         A.  sub-sub-item 1\n\nordered list\n\nitem 2\n\n\nsub-item 1\n\nsub-sub-item 1\n\n\n\n\n\n\n\n(@)  A list whose numbering\n\ncontinues after\n\n(@)  an interruption\n\n\nA list whose numbering\n\ncontinues after\n\nan interruption\n\n\n\n\nterm\n: definition\n\nterm\n\ndefinition\n\n\n\n\n\n\nTables\n\n\nMarkdown Syntax\nOutput\n\n\n\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|   12  |  12  |    12   |    12  |\n|  123  |  123 |   123   |   123  |\n|    1  |    1 |     1   |     1  |\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nLearn more in the article on Tables.\nEquations\nUse $ delimiters for inline math and $$ delimiters for display math. For example:\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\ninline math: $E = mc^{2}$\ninline math: \\(E=mc^{2}\\)\n\n\n\ndisplay math:\n\n$$E = mc^{2}$$\ndisplay math:\\[E = mc^{2}\\]\n\n\n\n\nIf you want to define custom TeX macros, include them within $$ delimiters enclosed in a .hidden block. For example:\n::: {.hidden}\n$$\n \\def\\RR{{\\bf R}}\n \\def\\bold#1{{\\bf #1}}\n$$\n\n:::\nFor HTML math processed using MathJax (the default) you can use the \\def, \\newcommand, \\renewcommand, \\newenvironment, \\renewenvironment, and \\let commands to create your own macros and environments."
  },
  {
    "objectID": "documents.html#a-very-brief-introduction-to-latex",
    "href": "documents.html#a-very-brief-introduction-to-latex",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.3 A Very Brief Introduction to LaTeX",
    "text": "13.3 A Very Brief Introduction to LaTeX\nLaTeX is a document preparation utility that attempts to take the focus off of layout (so you don’t have to spend 30 minutes trying to get the page break in the right place in e.g. Word) and bibliographic details.\n\n\nI’m not convinced LaTeX succeeds at freeing you from layout concerns, but it’s certainly true that it is much more powerful than Word for layout purposes.\nThe philosophy of LaTeX is that presentation shouldn’t get in the way of content: you should be able to change the presentation formatting systematically, without having to mess with the content. This allows you to switch templates easily, make document-wide changes in a single command, and more.\n\n\n\n\n\n\nTry it out\n\n\n\nIn Rstudio, copy the text in the document below, paste it into a text file in the editor window, and name it test.tex. You should see a Compile PDF button show up at the top of the document. Click that button to compile the document.\n\\documentclass{article} % this tells LaTeX what type of document to make\n% Note, comments are prefaced by a % sign. If you need to type the actual symbol\n% you will have to escape it with \\%.\n\n\\begin{document}\nHello \\LaTeX!\n\\end{document}\n\n\nMost commonly, you’ll use the article document class for papers, and beamer for presentations and posters. Other useful classes include moderncv (for CVs) and book, as well as the LaTeX class maintained by the UNL math department for thesis formatting. You can easily add R code chunks to a LaTeX file by changing the extension of any .tex file to .Rnw.\n\n\nThe Statistics graduate students maintain a bookdown version of the UNL thesis class on github here.\nThere are several types of latex commands:\n\n\nDeclarations: statements like \\documentclass, \\usepackage or \\small, which are stated once and take effect until further notice.\n\nEnvironments: statements with matching \\begin{xxx} and \\end{xxx} clauses that define a block of the document which is treated differently. Common environments include figures and tables.\n\nSpecial characters: another type of command that don’t define formatting or structure, but may print special characters, e.g. \\% to print a literal % character.\n\nBoth declarations and environments may come with both optional and required arguments. Required arguments are placed in {...} brackets, while optional arguments are placed in [...] brackets. You can, for instance, start your document with \\documentclass[12pt]{article} to specify the base font size.\nOne of the most useful features in LaTeX is math mode, which you can enter by enclosing text in $ ... $ (for inline statements), $$ ... $$ or \\[ ... \\] (for statements on their own line), or using other environments like \\begin{array} ... \\end{array} that come in math-specific packages. Once in math mode, you can use math symbol commands to get characters like \\(\\theta, \\pi, \\sum, \\int, \\infty\\), and more.\n\n\n\n\n\n\nTry it out\n\n\n\nWith any document creation software, the easiest way to learn how to do it is to find a sample document, tinker with it, see if you can make things the way you want them to be, and then google the errors when you inevitably screw something up.\n\n\nProblem\nSolution\n\n\n\nTake the sample document up above and see if you can do the following tasks: (I’ve linked to documentation that may be useful)\n\nAdd an image\nAdd the quadratic formula and the PDF of a normal distribution to the document\nIn extremely large text, print LaTeX using the \\LaTeX command\nIn extremely small, italic text, print your name\n\n\n\n\\documentclass{article} % this tells LaTeX what type of document to make\n\n% Add the graphicx package so that we can include images\n\\usepackage{graphicx}\n\n\\begin{document}\nHello \\LaTeX!\n\n% Include a figure\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=.5\\textwidth]{../image/IllusoryContour.png}\n\\caption{Illusory contour image}\n\\end{figure}\n\n% Add the quadratic formula and the normal PDF to the document\n$y = ax^2 + bx + c$ can be solved to get $$x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n\nThe PDF of a normal distribution is $$f(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n\n% In extremely large text, print \\LaTeX\n\n\\Huge\\LaTeX\n\n% In extremely small italic text, print your name\n\n\\tiny\\emph{Your name}\n\n\\end{document}\nYou can see the compiled pdf here.\n\n\n\n\n\n\n13.3.1 Knitr\nA LaTeX document has the file extension .tex, but it’s very easy to convert a LaTeX document into a .Rnw (R-no-weave) document: change the file extension. Then, you can add R code chunks, and the .Rnw document will be compiled to a .tex document in R, and then the .tex document will be compiled to .pdf using LaTeX.\nR code chunks are embedded in LaTeX documents using:\n% start of chunk\n&lt;&lt;chunk-name, ...options...&gt;&gt;=\n\n@\n% end of chunk\nYou can embed numerical results inline using \\Sexpr{...} where your R code goes in the ....\nYou could in theory use python within knitr via the reticulate package [2], but it will be easier by far to use quarto. Pick the tool that does the job well."
  },
  {
    "objectID": "documents.html#slides",
    "href": "documents.html#slides",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.4 Slides",
    "text": "13.4 Slides\n\n13.4.1 Beamer (LaTeX) and knitr\nBeamer is a powerful LaTeX class which allows you to create slides. The only change necessary to turn a beamer slide deck into a knitr slide deck is to add fragile as an option to any slide with verbatim content.\nYou can also create Beamer slides with Rmarkdown. Example presentation. Standard trade-offs (formatting details vs. document complexity) apply.\n\n\nCheck out the UNL-themed Beamer quarto template\n\n\n\n\n\n\nTry it out\n\n\n\nDownload and compile beamer-demo.Rnw.\nCan you change the theme of the presentation?\nAdd another slide, and on that slide, show an appropriate style ggplot2 graph of the distribution of board game ratings, reading in the board game ratings using the following code:\n\nboard_games &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2019/2019-03-12/board_games.csv\")\n\n\n\n\n\nKarl Broman has a set of slides that show how to use beamer + knitr to make reproducible slides with notes.\nYou can also create Beamer slides using Rmarkdown or quarto, if you want, but you may have more control over the fine details if you go straight to the Rnw file without going through markdown first. It’s a trade-off – the file will probably be simpler in markdown, but you won’t have nearly as much control.\n\n13.4.2 HTML slides\nRStudio has a host of other options for html slide presentations. There are some definite advantages to HTML presentations: they’re easy to share (via URL), you can add gifs, emojis, and interactive graphics, and you can set up github to host the presentations as well.\n\n\nI have a repository for all of the presentations I’ve given, and I use github pages to render the html presentations. Very easy, convenient, and I never have to carry a flash drive around at a conference or mess with the conference computers.\nThe downside to HTML slides is that there are approximately 100000 different javascript libraries that create HTML slides, and all of them have different capabilities. Many of these libraries have extensions that will let you create markdown slides, but they each have slightly different markdown syntax and capabilities.\n\n\nRmarkdown slide options available by default in RStudio\n\nYou can get the full details of any fully supported slide class in Rmarkdown by looking at the Rmarkdown book [3], which is freely available online. These guidelines will give you specifics about how to customize slides, add incremental information, change transitions, print your slides to PDF, and include speaker notes.\nQuarto has simplified the slide options available to you - for HTML slides, you have one option, which is to use reveal.js. While this may sound limiting, it’s really not - RStudio/Posit (the company behind quarto) has done a ton of work to make quarto a lovely experience, and that extends to the slides. I have almost entirely switched to using quarto for everything because it’s so much easier to arrange figures, add alt-text, and style presentations. See the quarto presentation documentation here [4]. If you have collaborators who are stuck on MS Office, quarto allows you to compile to a powerpoint presentation.\nRather than repeat the documentation for each slide package in this document, I think it is probably easier just to link you to the documentation and a sample presentation for each option.\nQuarto:\n\n\nreveal.js Example presentation\n\n\nRmarkdown:\n\n\nreveal.js Example presentation\n\n\nioslides Example presentation\n\n\nslidy Example presentation\n\n\nxaringan Example presentation, Example presentation 2 using UNL CSS theme\n\n\nIf you’re familiar with CSS (or happier tinkering to get the look of something exactly right) then xaringan and reveal.js are excellent full-featured options.\n\n\nI relied heavily on 2D slide layouts during my PhD prelim and defense.\nA nice feature of reveal.js presentations is support for 2D slide layouts, so you can have multiple sections in your presentation, and move vertically through each section, or horizontally between sections. That is useful for presentations where you may not plan on covering everything, but where you want to have all of the information available if necessary.\n\n\nUNL themed HTML presentations:\n\n\nxaringan (zip of all required files)\n\nquarto reveal.js (zip of all required files)\n\n\n\n\n\n\n\nTry it out\n\n\n\nTake a few minutes and try each of them out to see what feels right to you. Each one has a slightly different “flavor” of markdown, so read through the example to get a sense for what is different."
  },
  {
    "objectID": "documents.html#posters",
    "href": "documents.html#posters",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.5 Posters",
    "text": "13.5 Posters\nPosters are another common vehicle for presenting academic project results. Because posters are typically printed on paper or fabric, the standard file format is still PDF. As some venues move to digital posters, it is becoming more realistic to use HTML poster layouts that contain interactive elements.\n\n13.5.1 LaTeX\nOverleaf has a fantastic gallery of posters made in LaTeX.\nThere are several LaTeX options for making scientific posters: baposter, beamerposter, tikzposter are among the most common. We’ll focus on beamerposter here, but you are free to explore the other poster classes at will. As with beamer, you can easily integrate knitr code chunks into a document, so that you are generating your images reproducibly.\nBasic code for a poster in beamer (along with the necessary style files) that I’ve minimally customized to meet UNL branding requirements can be found here.\n\n\n\n\n\n\nTry it out\n\n\n\nDownload the beamer template and do the following:\n\nChange the 3-column span box to a 2-column span box.\nMake the “Block Colors” box purple\nMove the References block up to fill the 4th column.\n\n\n\n\n13.5.2 Markdown\nWhile most posters are still put together in PDF form, there is growing support for HTML posters, and many conferences have digital poster options for display. This may allow you to use interactive graphics and other features in a poster that would not translate well to PDF. Here is a list of Rmarkdown poster options; some even have PDF export capabilities so that you can have the interactive version plus a static version.\nPosterdown\nTo start, install posterdown with install.packages(\"posterdown\").\n\n\nUse the RStudio menu to create a posterdown presentation file – with a prefilled template\n\n\n\nUNL-themed posterdown template\nYou can also find additional customization options here. As with other markdown items, you can customize things even more using CSS. The nice thing about HTML posters, though, is that you can directly link to them if they’re hosted on a site.\nYou can also print a poster to PDF by running the following command: pagedown::chrome_print(\"myfile.Rmd\").\nPagedown\nThe pagedown package also has a couple of poster templates, including poster-relaxed and poster-jacobs.\nThere are also templates for letters, business cards, and more in pagedown, if you’re feeling ambitious.\n\n\n\n\n\n\nTry it out\n\n\n\nDownload the pagedown template and do the following:\n\nChange the 3-column layout to 4 columns. Adjust the breaks ({.mybreak}) accordingly to make the poster look good.\nMake the 2nd-level headers #249ab5 (cerulean)\nMove the References block to the 4th column.\nPrint your poster to a PDF"
  },
  {
    "objectID": "documents.html#resumecv",
    "href": "documents.html#resumecv",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.6 Resume/CV",
    "text": "13.6 Resume/CV\nYou can also create resumes and CVs in markdown and LaTeX. There is no real substitute for playing around with these classes, but I really like moderncv in LaTeX.\n\n\nYou can see my highly customized CV here, with timelines and numbered publications. It has to be compiled multiple times to get everything right.\nPagedown also comes with a html resume template (Use the menu -&gt; Rmarkdown -&gt; From Template -&gt; HTML Resume) that can be printed to html and pdf simultaneously. There is also the vitae package, which has even more templates, integration with other packages/sites, and more.\n\n\nAt this point, the biggest reason I haven’t switched to HTML is that I really like my timeline CV and I don’t have enough time to fiddle with it more."
  },
  {
    "objectID": "documents.html#hosting-content-with-github-pages",
    "href": "documents.html#hosting-content-with-github-pages",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.7 Hosting Content with Github Pages",
    "text": "13.7 Hosting Content with Github Pages\nGithub will host HTML content for you using Github pages (case in point: this textbook). This means you can version control your content (for instance, presentations or your CV) and have GitHub do the hosting (so you don’t have to find a webserver, buy a domain name, etc).\n\n\n\n\n\n\nSetting up Github Pages\n\n\n\n\nCreate a new repository named username.github.io on your personal github site (not the unl-stat850 classroom group)\nClone your repository\nModify your README.md file and push your changes\nGo to https://username.github.io and see your README.md file rendered as HTML.\n\n\n\n\n\n\n\nGithub will render any README.md file as actual HTML; it will also allow you to host plain HTML pages. By default, the README file is rendered first, but in subsequent directories, a file named index.html will be rendered as the “home page” for the subdirectory, if you have such a file. Otherwise you’ll have to know the file name.\nI tend to separate things out into separate repositories, but you can host HTML content on other repositories too, by enabling github pages in the repository settings. On my personal page, I have repositories for my CV, Presentations, etc. Each repository that has pages enabled can be accessed via https://srvanderplas.github.io/\\&lt;repository name\\&gt;/\\&lt;repository file path\\&gt;. So, to see my unl-stat850 repository, you’d go to https://srvanderplas.github.io/unl-stat850/ (but you’re already there!).\n\n\nI’ve been putting my presentations on Github since 2014, so it has a pretty good record of every set of slides I’ve created. I highly recommend this strategy - storing everything online makes it easy to share your work with others, reference later, and more importantly, easy for you to find in 3 years.\nThis mechanism provides a very convenient way to showcase your work, share information with collaborators, and more - instead of sending files, you can send a URL and no one has to download anything overtly.\n\n\n\n\n\n\nSetting up Github Pages in an existing repository\n\n\n\n\n\n\n\n\n\nIf you want to track your quarto/rmarkdown code and then render the output to a separate folder, you can use the docs/ folder. Github has this as an option as well – where we selected “main” branch above, we would select “docs/” instead (it’s grayed out b/c there isn’t a docs folder in the repo). That is how this book is hosted - the book compiles to the docs/ folder, and that way the book is rendered in final form and you don’t have to see all of the other crud that is in the repository."
  },
  {
    "objectID": "documents.html#additional-resources-to-explore",
    "href": "documents.html#additional-resources-to-explore",
    "title": "13  Reproducibility and Professional Communication",
    "section": "\n13.8 Additional Resources to Explore",
    "text": "13.8 Additional Resources to Explore\nThere are many other XXXdown packages made for Rmarkdown. Quarto is more multi-functional and contains blog, book, and website capabilities in a single package. However, most of the things which worked in Rmarkdown also work in Quarto, and Quarto has clearly been built off of the success of the ___down packages for Rmarkdown.\n\nblogdown\nbookdown (what I used to make this book in the SAS + R era)\npkgdown (to easily build documentation websites for R packages)\nROpenSci tutorial: How to set up hosting on github\nliftr - use Docker to make persistently reproducible documents\n\nIn addition, @mcanouil maintains a list of Quarto talks, topics, tools, and examples that is worth a look.\n\n\n\n\n[1] \nPosit, “Markdown Basics,” Quarto. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html#text-formatting. [Accessed: Oct. 17, 2022]\n\n\n[2] \nK. Ushey, J. Allaire, and Y. Tang, Reticulate: Interface to ’python’. 2022 [Online]. Available: https://CRAN.R-project.org/package=reticulate\n\n\n\n[3] \nY. Xie, J. J. Allaire, and G. Grolemund, “Chapter 4 Presentations,” in R Markdown: The Definitive Guide, 1st ed., CRC Press, 2018 [Online]. Available: https://bookdown.org/yihui/rmarkdown/presentations.html. [Accessed: Sep. 28, 2022]\n\n\n[4] \nRStudio, “Quarto - Presentations,” Quarto. [Online]. Available: https://quarto.org/docs/presentations/. [Accessed: Sep. 28, 2022]"
  },
  {
    "objectID": "simulation.html#module12-objectives",
    "href": "simulation.html#module12-objectives",
    "title": "14  Simulation and Reproducibility",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nProgram a simulation for a specific task, process, or model\nUnderstand the limitaitons of pseudorandom number generation\n\nSimulation is an extremely important part of computational statistics. Bayesian statistics, in particular, relies on Markov Chain Monte Carlo (MCMC) to get results from even the most basic of models. In this module, we’re going to touch on a few foundational pieces of simulation in computing, and you will get more exposure to simulation-based methods in other courses down the line."
  },
  {
    "objectID": "simulation.html#pseudorandom-number-generation",
    "href": "simulation.html#pseudorandom-number-generation",
    "title": "14  Simulation and Reproducibility",
    "section": "\n14.1 Pseudorandom Number Generation",
    "text": "14.1 Pseudorandom Number Generation\nComputers are almost entirely deterministic, which makes it very difficult to come up with “random” numbers. In addition to the deterministic nature of computing, it’s also somewhat important to be able to run the same code and get the same results every time, which isn’t possible if you rely on truly random numbers.\nHistorically, pseudorandom numbers were generated using linear congruential generators (LCGs) [1]. These algorithms aren’t typically used anymore, but they provide a good demonstration of how one might go about generating numbers that seem “random” but are actually deterministic. LCGs use modular arithmetic: \\[X_{n+1} = (aX_n + c) \\mod m\\] where \\(X_0\\) is the start value (the seed), \\(a\\) is the multiplier, \\(c\\) is the increment, and \\(m\\) is the modulus. When using a LCG, the user generally specifies only the seed.\n\n\nLCGs generate numbers which at first appear random, but once sufficiently many numbers have been generated, it is clear that there is some structure in the data. (Image from Wikimedia)\n\nThe important thing to note here is that if you specify the same generator values (a, c, m, and \\(X_0\\)), you will always get the same series of numbers. Since a, c, m are usually specified by the implementation, as a user, you should expect that if you specify the same seed, you will get the same results, every time.\n\n\n\n\n\n\nWarning\n\n\n\nIt is critically important to set your seed if you want the results to be reproducible and you are using an algorithm that depends on randomness.\n\n\nOnce you set your seed, the remaining results will only be reproducible if you generate the same set of random numbers every time.\n\n\nI once helped a friend fit a model for their masters thesis using Simulated Annealing (which relies on random seeds). We got brilliant results, but couldn’t ever reproduce them, because I hadn’t set the seed first and we never could figure out what the original seed was. Learn from my mistakes.\n\n\n\n\n\n\nExample: Setting Seeds for Reproducibility\n\n\n\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(50))\n## [1] -0.1095366\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  4 57 69 10 76 15 67  1  3 91\n\nCompare the results above to these results:\n\nset.seed(342512)\n\n# Get 10 numbers after the seed is set\nsample(1:100, 10)\n##  [1] 65 51 64 21 45 53  3  6 43  8\n\n# Compute something else that depends on randomness\nmean(rnorm(30))\n## [1] -0.1936645\n\n# Get 10 more numbers\nsample(1:100, 10)\n##  [1]  49  37   6  34   9   3 100  43   7  29\n\nNotice how the results have changed? To make my documents more reproducible, I will sometimes set a new seed at the start of an important chunk, even if I’ve already set the seed earlier. This introduces certain “fixed points” where results won’t change immediately after I’ve re-set the seed. This is particularly important when I’m generating bootstrap estimates, fitting models, or simulating data for graphics experiments.\n\n\nPick your seed in any way you want. I tend to just randomly wiggle my fingers over the number keys, but I have also heard of people using the date in yyyymmdd format, favorite people’s birthdays, the current time in hhmmss format… basically, you can use anything."
  },
  {
    "objectID": "simulation.html#built-in-simulations-from-distributions",
    "href": "simulation.html#built-in-simulations-from-distributions",
    "title": "14  Simulation and Reproducibility",
    "section": "\n14.2 Built-in simulations from distributions",
    "text": "14.2 Built-in simulations from distributions\nOften, we can get away with just simulating data from a known distribution. As both R and python are meant for statistical computing, this is extremely easy by design.\n\n\nR\nPython\n\n\n\nYou can see the various distribution options using ?Distributions. In general, dxxx is the PDF/PMF, pxxx is the CDF, qxxx is the quantile function, and rxxx gives you random nubmers generated from the distribution. (xxx, obviously, is whatever distribution you’re looking to use.)\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nset.seed(109025879)\n\ntibble(\n  norm = rnorm(500),\n  gamma = rgamma(500, shape = 3, scale = 1),\n  exp = rexp(500, rate = 1), # R uses a exp(-ax) \n  t = rt(500, df = 5),\n  chisq = rchisq(500, 5)\n) %&gt;%\n  pivot_longer(1:5, names_to = \"dist\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value)) + geom_density() + facet_wrap(~dist, scales = \"free\", nrow = 1)\n\n\n\n\n\n\n\nimport random\nrandom.seed(109025879)\n\nimport pandas as pd\nimport numpy as np\n\nwide_df = pd.DataFrame({\n  \"norm\": np.random.normal(size=500),\n  \"gamma\": np.random.gamma(size=500, shape = 3, scale = 1),\n  \"exp\": np.random.exponential(size = 500, scale = 1),\n  \"t\": np.random.standard_t(df = 5, size = 500),\n  \"chisq\": np.random.chisquare(df = 5, size = 500)\n})\n\nlong_df = pd.melt(wide_df, id_vars = None, var_name = \"dist\", value_name = \"value\")\n\nfrom plotnine import *\n\n(ggplot(long_df, aes(x = \"value\")) + geom_density() + facet_wrap(\"dist\", scales=\"free\", nrow = 1))\n## &lt;ggplot: (8761176073213)&gt;\n## \n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/utils.py:371: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n## /home/susan/Projects/Class/unl-stat850/stat850-textbook/renv/python/virtualenvs/renv-python-3.8/lib/python3.8/site-packages/plotnine/facets/facet.py:390: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\nProblem\n\n\nGenerate variables x and y, where x is a sequence from -10 to 10 and y is equal to \\(x + \\epsilon\\), \\(\\epsilon \\sim N(0, 1)\\). Fit a linear regression to your simulated data (in R, lm, in Python, sklearn.linear_model’s LinearRegression).\nHint: Sample code for regression using sklearn [2].\nR\n\nset.seed(20572983)\ndata &lt;- tibble(x = seq(-10, 10, .1), \n               y = x + rnorm(length(x)))\nregression &lt;- lm(y ~ x, data = data)\nsummary(regression)\n## \n## Call:\n## lm(formula = y ~ x, data = data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -3.14575 -0.70986  0.03186  0.65429  2.40305 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -0.01876    0.06869  -0.273    0.785    \n## x            0.99230    0.01184  83.823   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.9738 on 199 degrees of freedom\n## Multiple R-squared:  0.9725, Adjusted R-squared:  0.9723 \n## F-statistic:  7026 on 1 and 199 DF,  p-value: &lt; 2.2e-16\n\nPython\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nrandom.seed(20572983)\n\ndata = pd.DataFrame({'x': np.arange(-10, 10, .1)})\ndata['y'] = data.x + np.random.normal(size = data.x.size)\n\n# Fitting the regression and predictions\n# scikit-learn requires that we reshape everything into\n# nparrays before we pass them into the model.fit() function.\nmodel = LinearRegression().\\\n  fit(data.x.values.reshape(-1, 1),\\\n      data.y.values.reshape(-1, 1))\ndata['pred'] = model.predict(data.x.values.reshape(-1, 1))\n\n# Plotting the results\nimport matplotlib.pyplot as plt\nplt.clf()\n\nplt.scatter(data.x, data.y)\nplt.plot(data.x, data.pred, color='red')\nplt.show()"
  },
  {
    "objectID": "simulation.html#simulation-to-test-model-assumptions",
    "href": "simulation.html#simulation-to-test-model-assumptions",
    "title": "14  Simulation and Reproducibility",
    "section": "\n14.3 Simulation to test model assumptions",
    "text": "14.3 Simulation to test model assumptions\nOne of the more powerful ways to use simulation in practice is to use it to test the assumptions of your model. Suppose, for instance, that your data are highly skewed, but you want to use a method that assumes normally distributed errors. How bad will your results be? Where can you trust the results, and where should you be cautious?\nExample from Telling Stories with Data: Multilevel regression and post-stratification simulation with toddler bedtimes. This example talks about how to take a biased sample and then recover the original unbiased estimates – which is something you have to test using simulation to be sure it works, because you never actually know what the true population features are when you are working with real world data. When reading this example, you may not be all that interested with the specific model - but focus on the process of simulating data for your analysis so that you understand how and why you would want to simulate data in order to test a computational method.\n\n\n\n\n\n\nExample: Confidence Interval coverage rates\n\n\n\nSuppose, for instance, that we have a lognormal distribution (highly skewed) and we want to compute a 95% confidence interval for the mean of our data.\n\nset.seed(40295023)\n\nsim &lt;- tibble(\n  id = rep(1:100, each = 25), # generate 100 samples of 25 points each\n  ln_x = rnorm(25*100), # generate the normal deviates\n  x = exp(ln_x), # transform into lognormal deviates\n) %&gt;%\n  # this creates a 100-row data frame, with one row for each id. \n  # the columns x, ln_x are stored in the data list-column as a tibble.\n  nest(data = c(x, ln_x))\n  \nhead(sim)\n## # A tibble: 6 × 2\n##      id data             \n##   &lt;int&gt; &lt;list&gt;           \n## 1     1 &lt;tibble [25 × 2]&gt;\n## 2     2 &lt;tibble [25 × 2]&gt;\n## 3     3 &lt;tibble [25 × 2]&gt;\n## 4     4 &lt;tibble [25 × 2]&gt;\n## 5     5 &lt;tibble [25 × 2]&gt;\n## 6     6 &lt;tibble [25 × 2]&gt;\nsim$data[[1]]\n## # A tibble: 25 × 2\n##        x    ln_x\n##    &lt;dbl&gt;   &lt;dbl&gt;\n##  1 0.310 -1.17  \n##  2 0.622 -0.475 \n##  3 0.303 -1.19  \n##  4 1.05   0.0525\n##  5 0.529 -0.636 \n##  6 1.09   0.0891\n##  7 1.97   0.676 \n##  8 8.94   2.19  \n##  9 0.598 -0.514 \n## 10 0.183 -1.70  \n## # … with 15 more rows\n\nYou want to assess the coverage probability of a confidence interval computed under two different modeling scenarios:\n\nWorking with the log-transformed values, ln(x), and then transform the computed interval back\nWorking with the raw values, x, compute an interval assuming the data are symmetric, essentially treating the lognormal distribution as if it were normal.\n\nUnder scenario 1, our theoretical interval should be exp((-1.96/5, 1.96/5)) (because \\(\\mu\\) is 0, and \\(\\sigma\\) is 1, so \\(SE(\\overline x) = 1/\\sqrt{25} = 1/5\\)). \\((0.676,1.48)\\)\nUnder scenario 2, the expected value of the lognormal distribution is \\(\\exp(1/2) = 1.649\\), the variance is \\((\\exp(1) - 1)(\\exp(1)) = 4.671\\) and our theoretical interval should be \\((0.802, 2.496)\\). This interval contains 0, which is implausible for lognormally distributed data.\nOur expected values are different under scenario 1 and scenario 2: in scenario 1 we are computing an interval for \\(\\mu\\), in scenario 2, we are computing an interval for the population mean, which is \\(\\exp(\\mu + .5\\sigma^2)\\). Both are valid quantities we might be interested in, but they do not mean the same thing.\n\n\ncompute_interval &lt;- function(x) {\n  s1 &lt;- exp(mean(log(x)) + c(-1, 1) * qnorm(.975) * sd(log(x))/sqrt(length(x)))\n  s2 &lt;- mean(x) + c(-1, 1) * qnorm(.975) * sd(x)/sqrt(length(x))\n  tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n         mean = c(1, exp(1/2)),\n         lb = c(s1[1], s2[1]), ub = c(s1[2], s2[2]),\n         in_interval = (lb &lt; mean) & (ub &gt; mean))\n}\n\n\nsim_long &lt;- sim %&gt;%\n  # This line takes each data entry and computes an interval for x.\n  # .$x is code for take the argument you passed in to map and get the x column\n  mutate(res = purrr::map(data, ~compute_interval(.$x))) %&gt;%\n  # this \"frees\" res and we end up with two columns: lb and ub, for each scenario\n  unnest(res)\n  \n\nci_df &lt;- tibble(scenario = c(\"scenario_1\", \"scenario_2\"),\n                mu = c(1, exp(1/2)),\n                lb = c(exp(-1.96/5), exp(.5) - 1.96*sqrt((exp(1) - 1)*exp(1))/5),\n                ub = c(exp(1.96/5), exp(.5) + 1.96*sqrt((exp(1) - 1)*exp(1))/5))\n\n\nggplot() + \n  geom_rect(aes(xmin = lb, xmax = ub, ymin = -Inf, ymax = Inf), \n            data = ci_df,\n            fill = \"grey\", alpha = .5, color = NA) + \n  geom_vline(aes(xintercept = mu), data = ci_df) + \n  geom_segment(aes(x = lb, xend = ub, y = id, yend = id, color = in_interval),\n               data = sim_long) + \n  scale_color_manual(values = c(\"red\", \"black\")) + \n  theme_bw() + \n  facet_wrap(~scenario)\n\n\n\n\nFrom this, we can see that working with the log-transformed, normally distributed results has better coverage probability than working with the raw data and computing the population mean: the estimates in the latter procedure have lower coverage probability, and many of the intervals are much wider than necessary; in some cases, the interval actually lies outside of the domain.\n\n\n\nThe purrr::map notation specifies that we’re using the map function from the purrr package. When functions are named generically, and there may be more than one package with a function name, it is often more readable to specify the package name along with the function.\npurrr::map takes an argument and for each “group” calls the compute_interval function, storing the results in res. So each row in res is a 1x2 tibble with columns lb and ub.\nThis pattern is very useful in all sorts of applications. I wish we had time to cover purrr explicitly, but I at least want to expose you to how clean it makes your code.\n\nHere is a similar example worked through in SAS with IML. Note the use of BY-group processing to analyze each group at once - this is very similar to the use of purrr::map() in the R code.\n\n\n\n\n\n\n\nExample: Multilevel Regression and Post Stratification simulation\n\n\n\nMultilevel regression and post-stratification simulation with toddler bedtimes\nThis example talks about how to take a biased sample and then recover the original unbiased estimates – which is something you have to test using simulation to be sure it works, because you never actually know what the true population features are when you are working with real world data. When reading this example, you may not be all that interested with the specific model - but focus on the process of simulating data for your analysis so that you understand how and why you would want to simulate data in order to test a computational method.\n\n\n\n\n\n\n\n\nExample: Regression and high-leverage points\n\n\n\nWhat happens if we have one high-leverage point (e.g. a point which is an outlier in both x and y)? How pathological do our regression coefficient estimates get?\nThe challenging part here is to design a data generating mechanism.\n\ngen_data &lt;- function(n = 30, o = 1, error_sd = 2) {\n  # generate the main part of the regression data\n  data &lt;- tibble(x = rnorm(n = n - o, mean = seq(-10, 10, length.out = n - o), sd = .1),\n                 y = x + rnorm(length(x), mean = 0, sd = error_sd))\n  # generate the outlier - make it at ~(-10, 5)\n  outdata &lt;- tibble(x = rnorm(o, -10), y = rnorm(o, 5, error_sd))\n  bind_rows(data, outdata)\n}\n\nsim_data &lt;- tibble(\n  id = 1:300,\n  o = rep(0:2, each = 100),\n  # call gen_data for each row in sim_data, but don't really use id as a parameter.\n  data = purrr::map(o, ~gen_data(o = .)) \n)\n\nhead(sim_data)\n## # A tibble: 6 × 3\n##      id     o data             \n##   &lt;int&gt; &lt;int&gt; &lt;list&gt;           \n## 1     1     0 &lt;tibble [30 × 2]&gt;\n## 2     2     0 &lt;tibble [30 × 2]&gt;\n## 3     3     0 &lt;tibble [30 × 2]&gt;\n## 4     4     0 &lt;tibble [30 × 2]&gt;\n## 5     5     0 &lt;tibble [30 × 2]&gt;\n## 6     6     0 &lt;tibble [30 × 2]&gt;\n\n# plot a few datasets just to check they look like we expect:\nsim_data %&gt;%\n  filter(id %% 100 &lt; 3) %&gt;%\n  unnest(data) %&gt;%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  facet_grid(id %% 100 ~ o )\n\n\n\n\n\nlibrary(broom) # the broom package cleans up model objects to tidy form\n\nsim_data &lt;- sim_data %&gt;%\n  # fit linear regression\n  mutate(model = purrr::map(data, ~lm(y ~ x, data = .)))  %&gt;%\n  mutate(tidy_model = purrr::map(model, tidy))\n\n# Get the coefficients out\ntidy_coefs &lt;- select(sim_data, id, o, tidy_model) %&gt;%\n  unnest(tidy_model) %&gt;%\n  mutate(group = case_when(o == 0 ~ \"No HLPs\",\n                           o == 1 ~ \"1 HLP\",\n                           o == 2 ~ \"2 HLPs\") %&gt;%\n           factor(levels = c(\"No HLPs\", \"1 HLP\", \"2 HLPs\")))\n\nggplot(tidy_coefs, aes(x = estimate, color = group)) + \n  facet_grid(term ~ .) + \n  geom_density()\n\n\n\n\nObviously, you should experiment with different methods of generating a high-leverage point (maybe use a different distribution?) but this generating mechanism is simple enough for our purposes and shows that the addition of high leverage points biases the true values (slope = 1, intercept = 0).\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nHint\nGeneral Solution\nR Code\nPython Code\n\n\n\nLet’s explore what happens to estimates when certain observations are censored. Suppose we have a poorly-designed digital thermometer which cannot detect temperatures above 102\\(^\\circ F\\); for these temperatures, the thermometer will record a value of 102.0.\nIt is estimated that normal body temperature for dogs and cats is 101 to 102.5 degrees Fahrenheit, and values above 104 degrees F are indicative of illness. Given that you have this poorly calibrated thermometer, design a simulation which estimates the average temperature your thermometer would record for a sample of 100 dogs or cats, and determine the magnitude of the effect of the thermometer’s censoring.\n\n\nIf most pets have a normal body temperature between 101 and 102.5 degrees, can you use these bounds to determine appropriate parameters for a normal distribution? What if you assume that 101 and 102.5 are the 2SD bounds?\n\n\nIf 101 and 102.5 are the anchor points we have, let’s assume that 95% of normal pet temperatures fall in that range. So our average temperature would be 101.75, and our standard deviation would be .75/2 = 0.375.\nWe can simulate 1000 observations from \\(N(101.75, 0.375)\\), create a new variable which truncates them at 102, and compute the mean of both variables to determine just how biased our results are.\n\n\n\nset.seed(204209527)\ndogtemp &lt;- tibble(\n  actual = rnorm(1000, 101.75, 0.375),\n  read = pmin(actual, 102)\n) \ndogtemp %&gt;%\n  summarize_all(mean)\n## # A tibble: 1 × 2\n##   actual  read\n##    &lt;dbl&gt; &lt;dbl&gt;\n## 1   102.  102.\n\nThe effect of the thermometer’s censoring is around 0.06 degrees F.\n\n\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nrandom.seed(204209527)\ndogtemp = pd.DataFrame({\n  \"actual\": np.random.normal(size = 1000, loc = 101.75, scale = 0.375)\n})\ndogtemp['read'] = np.minimum(dogtemp.actual, 102)\n\nnp.diff(dogtemp.mean())\n## array([-0.0584365])\n\nThe effect of the thermometer’s censoring is around 0.06 degrees F."
  },
  {
    "objectID": "simulation.html#monte-carlo-methods",
    "href": "simulation.html#monte-carlo-methods",
    "title": "14  Simulation and Reproducibility",
    "section": "\n14.4 Monte Carlo methods",
    "text": "14.4 Monte Carlo methods\nMonte carlo methods [3] are methods which rely on repeated random sampling in order to solve numerical problems. Often, the types of problems approached with MC methods are extremely difficult or impossible to solve analytically.\nIn general, a MC problem involves these steps:\n\nDefine the input domain\nGenerate inputs randomly from an appropriate probability distribution\nPerform a computation using those inputs\nAggregate the results.\n\n\n\n\n\n\n\nExample: Sum of Uniform Random Variables\n\n\n\n\n\n\n\nProblem\nDefining Steps\nR Code\nPython Code\nLearn More\n\n\n\nLet’s try it out by using MC simulation to estimate the number of uniform (0,1) random variables needed for the sum to exceed 1.\nMore precisely, if \\(u_i \\sim U(0,1)\\), where _{i=1}^k u_i &gt; 1, what is the expected value of \\(k\\)?\n\n\n\nIn this simulation, our input domain is [0,1].\nOur input is \\(u_i \\sim U(0,1)\\)\n\nWe generate new \\(u_i\\) until \\(\\sum_{i=1}^k &gt; 1\\) and save the value of \\(k\\)\n\nWe average the result of \\(N\\) such simulations.\n\n\n\n\n# It's easier to think through the code if we write it inefficiently first\nsim_fcn &lt;- function() {\n  usum &lt;- 0\n  k &lt;- 0\n  # prevent infinite loops by monitoring the value of k as well\n  while (usum &lt; 1 & k &lt; 15) {\n    usum &lt;- runif(1) + usum\n    k &lt;- k + 1\n  }\n  return(k)\n}\n\nset.seed(302497852)\nres &lt;- tibble(k = replicate(1000, sim_fcn(), simplify = T))\n\nmean(res$k)\n## [1] 2.717\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nset.seed(20417023)\n\nsim_res &lt;- tibble(samp = replicate(250000, sim_fcn(), simplify = T)) \n\nsim_res &lt;- sim_res %&gt;%\n  mutate(running_avg_est = cummean(samp),\n         N = row_number())\n\nggplot(aes(x = N, y = running_avg_est), data = sim_res) + \n  geom_hline(yintercept = exp(1), color = \"red\") + \n  geom_line()\n\n\n\n\n\n\n\nimport numpy as np\nimport random\nimport pandas as pd\n\n\ndef sim_fcn():\n  usum = 0\n  k = 0\n  # prevent infinite loops by monitoring the value of k as well\n  while usum &lt; 1 and k &lt; 15:\n    # print(\"k = \", k)\n    usum = np.random.uniform(size=1) + usum\n    k += 1\n  return k\n\nrandom.seed(302497852)\nres = pd.DataFrame({\"k\": [sim_fcn() for _ in range(1000)]})\n\nIf we want to see whether the result converges to something, we can increase the number of trials we run:\n\nrandom.seed(20417023)\n\nsim_res = pd.DataFrame({\"k\": [sim_fcn() for _ in range(250000)]})\nsim_res['running_avg_est'] = sim_res.k.expanding().mean()\nsim_res['N'] = np.arange(len(sim_res))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = sim_res, x = 'N', y = 'running_avg_est', color = \"black\")\ngraph.axhline(y = np.exp(1), xmin = 0, xmax = 1, color = \"red\")\nplt.show()\n\n\n\n\n\n\nThe expected number of uniform RV draws required to sum to 1 is \\(e\\)!\nExplanation of why this works\n\n\n\n\n\n\nMonte Carlo methods are often used to approximate the value of integrals which do not have a closed-form (in particular, these integrals tend to pop up frequently in Bayesian methods).\n\n\n\n\n\n\nExample: Integration\n\n\n\n\n\nProblem\nR Code\nPython Code\nProof of Convergence\n\n\n\nSuppose you want to integrate \\[\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) dx\\]\n\n\n\n\n\nYou could set up Riemann integration and evaluate the integral using a sum over \\(K\\) points, but that approach only converges for smooth functions (and besides, that’s boring Calc 2 stuff, right?).\nInstead, let’s observe that this is equivalent to \\(\\int_0^1 x^2 \\sin \\left(\\frac{1}{x}\\right) \\cdot 1 dx\\), where \\(p(x) = 1\\) for a uniform random variable. That is, this integral can be written as the expected value of the function over the interval \\([0,1]\\). What if we just generate a bunch of uniform(0,1) variables, evaluate the value of the function at that point, and average the result?\n\n\n\nset.seed(20491720)\nfn &lt;- function(x)  x^2 * sin(1/x)\n\nsim_data &lt;- tibble(x = runif(100000),\n                   y = fn(x))\nmean(sim_data$y)\n## [1] 0.28607461\n\n\n\n\nrandom.seed(20491720)\n\ndef fn(x):\n  return x**2 * np.sin(1/x)\n\nsim_data = pd.DataFrame({\"x\": np.random.uniform(size = 100000)})\nsim_data['y'] = fn(sim_data.x)\n\nsim_data.y.mean()\n## 0.2853579113268654\n\n\n\nYou can use the law of large numbers to prove that this approach will converge. Example modified from this set of lecture notes [4]\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR code\nPython Code\n\n\n\nBuffon’s needle is a mathematical problem which can be boiled down to a simple physical simulation. Read this science friday description of the problem and develop a monte carlo simulation method which estimates \\(\\pi\\) using the Buffon’s needle method. Your method should be a function which\n\nallows the user to specify how many sticks are dropped\nplots the result of the physical simulation\nprints out a numerical estimate of pi.\n\n\n\nLet’s start out with horizontal lines at 0 and 1, and set our stick length to 1. We need to randomly generate a position (of one end of the stick) and an angle. The position in \\(x\\) doesn’t actually make much of a difference (since what we care about is the \\(y\\) coordinates), but we can draw a picture if we generate \\(x\\) as well.\n\nneedle_sim &lt;- function(sticks = 100) {\n  df &lt;- tibble(xstart = runif(sticks, 0, 10), \n         ystart = runif(sticks, 0, 1), \n         angle = runif(sticks, 0, 360),\n         xend = xstart + cos(angle/180*pi), \n         yend = ystart + sin(angle/180*pi)\n  ) %&gt;%\n    # We can see if a stick crosses a line if the floor() function of ystart is \n    # different than floor(yend). \n    # Note this only works for integer line values.\n  mutate(crosses_line = floor(ystart) != floor(yend)) \n  \n  \n  gg &lt;- ggplot() + \n  geom_hline(yintercept = c(0, 1)) + \n  geom_segment(aes(x = xstart, y = ystart, xend = xend, yend = yend,\n                   color = crosses_line), data = df) + \n  coord_fixed()\n  \n  return(list(est = 2 * sticks / sum(df$crosses_line), plot = gg))\n}\n\nneedle_sim(10)\n## $est\n## [1] 2.8571429\n## \n## $plot\n\n\n\n\nneedle_sim(100)\n## $est\n## [1] 2.8985507\n## \n## $plot\n\n\n\n\nneedle_sim(1000)\n## $est\n## [1] 3.1298905\n## \n## $plot\n\n\n\n\nneedle_sim(10000)\n## $est\n## [1] 3.1235358\n## \n## $plot\n\n\n\n\n\n\n\ndef needle_sim(sticks = 100):\n  df = pd.DataFrame({\n    \"xstart\": np.random.uniform(0, 10, size = sticks),\n    \"ystart\": np.random.uniform(0, 1, size = sticks),\n    \"angle\": np.random.uniform(0, 360, size = sticks)\n  })\n  \n  df['xend'] = df.xstart + np.cos(df.angle/180*np.pi)\n  df['yend'] = df.ystart + np.sin(df.angle/180*np.pi)\n  df['crosses_line'] = np.floor(df.ystart) != np.floor(df.yend)\n  \n  return df\n\ndata = needle_sim(100000)\ndata['N'] = np.arange(len(data)) + 1\ndata['cum_est'] = 2*data.N / data.crosses_line.expanding().sum()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.clf()\n\ngraph = sns.lineplot(data = data, x = \"N\", y = \"cum_est\", color = \"black\")\ngraph.axhline(y = np.pi, xmin = 0, xmax = 1, color = \"red\")\nplt.show()"
  },
  {
    "objectID": "simulation.html#other-resources",
    "href": "simulation.html#other-resources",
    "title": "14  Simulation and Reproducibility",
    "section": "\n14.5 Other Resources",
    "text": "14.5 Other Resources\n\nSimulation (R programming for Data Science chapter)\nSimulation - R Studio lesson\nSimulation, focusing on statistical modeling (R)\nSimulating Data with SAS (Excerpt)\nSimulating a Drunkard’s Walk in 2D in SAS\nSimulation from a triangle distribution (SAS)\nSimulating the Monty Hall problem (SAS)\nWhen to use purrr (part of the ‘teaching the tidyverse’ series) - essentially, purrr is a great intro to functional programming, but there are other ways to solve iterative problems in R as well, and some of them are easier than purrr (but purrr is a general approach that is very powerful).\n\n\n\n\n\n[1] \nWikipedia contributors, “Linear congruential generator.” Sep. 09, 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Linear_congruential_generator&oldid=1109285432\n\n\n\n[2] \nA. Menon, “Linear Regression in 6 lines of Python,” Medium. Oct. 2018 [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[3] \nWikipedia contributors, “Monte Carlo method,” Wikipedia. Oct. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Monte_Carlo_method&oldid=1116159451. [Accessed: Oct. 17, 2022]\n\n\n[4] \nY.-C. Chen, “Lecture 2: Monte Carlo Simulation,” Monte Carlo Simulation. 2017 [Online]. Available: http://faculty.washington.edu/yenchic/17Sp_403/Lec2_MonteCarlo.pdf"
  },
  {
    "objectID": "interactive-graphics.html#module13-objectives",
    "href": "interactive-graphics.html#module13-objectives",
    "title": "\n15  Animated and Interactive Graphics\n",
    "section": "\n15.1 Module Objectives",
    "text": "15.1 Module Objectives\n\nCreate animated and interactive charts using appropriate tools"
  },
  {
    "objectID": "interactive-graphics.html#plotly",
    "href": "interactive-graphics.html#plotly",
    "title": "\n15  Animated and Interactive Graphics\n",
    "section": "\n15.2 Plotly",
    "text": "15.2 Plotly\nPlotly [1] is a graphing library that uses javascript to add interactivity to graphics. There are several different ways to create plotly graphs in R or python. Here, we’ll discuss 3 approaches: - Working with plotly in R directly - Working with plotly in python directly - Using ggplotly, which converts a ggplot to a plotly plot automatically\nResources:\n\nR Plotly cheat sheet\nPython Plotly cheat sheet\n\nWe’ll demonstrate plotly’s capabilities using the volcanoes data from Tidy Tuesday.\n\n\n\n\n\n\nData set up\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"plotly\" %in% installed.packages()) \n  install.packages(\"plotly\")\n\nlibrary(plotly)\n\n\nlibrary(readr) # reading in data\nlibrary(dplyr) # cleaning data\nlibrary(tidyr) # merging data\nlibrary(lubridate) # dates and times\nlibrary(stringr) # string manipulation\nlibrary(ggplot2) # plotting\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano &lt;- read_csv(paste0(url_stub, \"volcano.csv\"))\neruptions &lt;- read_csv(paste0(url_stub, \"eruptions.csv\"))\nevents &lt;- read_csv(paste0(url_stub, \"events.csv\"))\nsulfur &lt;- read_csv(paste0(url_stub, \"sulfur.csv\"))\ntrees &lt;- read_csv(paste0(url_stub, \"tree_rings.csv\"))\n\n\n\n\n# Uncomment and run this line if you don't have plotly installed\n# %pip install plotly\n\nimport plotly.express as px\nimport plotly.io as pio # this allows plotly to play nice with markdown\n\nimport pandas as pd\n\n# all of the data is located in the same folder of a github repo\n# so let's not type it out 5x\nurl_stub = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/\"\nvolcano = pd.read_csv(url_stub + \"volcano.csv\")\neruptions = pd.read_csv(url_stub + \"eruptions.csv\")\nevents = pd.read_csv(url_stub + \"events.csv\")\nsulfur = pd.read_csv(url_stub + \"sulfur.csv\")\ntrees = pd.read_csv(url_stub + \"tree_rings.csv\")\n\n\n\n\n\n\n\nLet’s try out plotly while doing a bit of exploratory data analysis on this dataset.\n\n\n\n\n\n\nCleaning up volcano\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nvolcano &lt;- volcano %&gt;%\n  filter(tectonic_settings != \"Unknown\") %&gt;%\n  separate(tectonic_settings, into = c(\"zone\", \"crust\"), \n           sep = \"/\", remove = F) %&gt;%\n  # Remove anything past the first punctuation character \n  # catch (xx) and ?\n  mutate(volcano_type = str_remove(primary_volcano_type, \n                                   \"[[:punct:]].*$\"))\n\n\n\n\nvolcano = volcano.query(\"tectonic_settings != 'Unknown'\")\nvolcano[['zone', 'crust']] = volcano.tectonic_settings.\\\n                              str.split(\" / \", expand = True)\n# Remove anything after ( as well as ? if it exists\nvolcano = volcano.assign(volcano_type =\n            volcano['primary_volcano_type'].\\\n            str.replace(r\"(\\(.*)?\\??$\", \"\", regex = True))\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship Between Elevation and Zone\n\n\n\n\n\nLet’s start by seeing whether the elevation of a volcano changes based on the type of zone it’s on - we might expect that Rift zone volcanos (where plates are pulling away from each other) might not be as high.\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = zone, fill = zone, y = elevation)) +\n  geom_boxplot() +\n  coord_flip()\nggplotly(p)\n\n\n\n\n\n\n\nThe plot_ly function is pipe friendly.\nVariable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\nlibrary(plotly)\nplot_ly(volcano, color= ~zone, x = ~elevation, type = \"box\")\n\n\n\n\n\n\n\n\nfig = px.box(volcano, x = \"elevation\", color = \"zone\")\nfig.show()\n\n\n                                    \n\n\n\n\n\n\nIt doesn’t really look like there’s much difference.\n\n\n\n\n\n\n\n\n\nExamining Volcano Type\n\n\n\n\n\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug()\nggplotly(p)\n\n\n\n\n\n\n\nSince I’m trying to do this without the tidyverse, I’ll try out the new base R pipe, |&gt;, and the corresponding new anonymous function notation, \\().1\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type))\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\n# This creates a list of vectors, one for each type of volcano\nvolcano = volcano.groupby(\"volcano_type\")[\"elevation\"].count()\ntype_list = volcano.groupby(\"volcano_type\").elevation.apply(list)\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'SeriesGroupBy' object has no attribute 'elevation'\ntype_labels = volcano.volcano_type.unique()\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'Series' object has no attribute 'volcano_type'\nfig = ff.create_distplot(type_list, group_labels = type_labels, show_hist=False)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'type_list' is not defined\nfig.show()\n\n\n                                    \n\n\n\n\n\n\nHere, the interactivity actually helps a bit: we don’t need to use the legend to see what each curve corresponds to. We can see that submarine volcanoes are typically much lower in elevation (ok, duh), but also that subglacial volcanoes are found in a very limited range. If we double-click on a legend entry, we can get rid of all other curves and examine each curve one by one.\nI added the rug layer after the initial bout because I was curious how much data each of these curves were based on. If we want only curves with n &gt; 10 observations, we can do that:\n\n\nggplotly\nR + plotly\nPython\n\n\n\n\np &lt;- volcano %&gt;%\n  group_by(volcano_type) %&gt;% mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\n  ggplot(aes(x = elevation, color = volcano_type)) +\n  geom_density() +\n  # Rug plots show each observation as a tick just below the x axis\n  geom_rug(aes(text = paste0(volcano_name, \", \", country)))\nggplotly(p)\n\n\n\n\n\nIf we want to specify additional information that should show up in the tooltip, we can do that as well by adding the text aesthetic even though geom_rug doesn’t take a text aesthetic. You may notice that ggplot2 complains about the unknown aesthetic I’ve added to geom_rug: That allows us to mouse over each data point in the rug plot and see what volcano it belongs to. So we can tell from the rug plot that the tallest volcano is Ojas de Salvado, in Chile/Argentina (I believe that translates to Eyes of Salvation?).\n\n\n\n# First, compute the density\nelevation_dens &lt;- split(volcano, ~volcano_type) |&gt;\n  lapply(FUN = \\(df) {\n    tmp &lt;- density(df$elevation)[c(\"x\", \"y\", \"bw\")] |&gt; \n      as.data.frame()\n    tmp$n = nrow(df)\n    tmp\n  }) |&gt;\n  do.call(what = \"rbind\") |&gt;\n  as.data.frame()\nelevation_dens &lt;- cbind(volcano_type = row.names(elevation_dens), elevation_dens) |&gt;\n  transform(volcano_type = gsub(\"\\\\.\\\\d{1,}$\", \"\", volcano_type)) |&gt;\n  subset(n &gt; 15)\n\nplot_ly(data = elevation_dens, x = ~x, y = ~y, \n        type = \"scatter\", mode = \"line\", color = ~volcano_type)\n\n\n\n\n\n\n\n\nimport plotly.figure_factory as ff\nvolcano = volcano.assign(count = volcano.groupby(\"volcano_type\").\\\n                                  volcano_type.transform(\"count\"))\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'Series' object has no attribute 'assign'\ncommon_volcano = volcano.query(\"count &gt; 15\").sort_values([\"volcano_type\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'Series' object has no attribute 'query'\ncommon_volcano[\"label\"] = common_volcano.volcano_name + \", \" + common_volcano.country\n# This creates a list of vectors, one for each type of volcano\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'common_volcano' is not defined\ntype_list = common_volcano.groupby(\"volcano_type\").elevation.apply(list)\n# rug_text = common_volcano.groupby(\"volcano_type\").label.apply(list)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'common_volcano' is not defined\ntype_labels = common_volcano.volcano_type.unique()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'common_volcano' is not defined\nfig = ff.create_distplot(type_list, group_labels = type_labels, rug_text = rug_text, show_hist=False)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'type_list' is not defined\nfig.show()\n\n\n                                    \n\n\n\n\n\n\n\n\n\nAt any rate, there isn’t nearly as much variation as I was expecting in the elevation of different types of volcanoes.\nggplotly makes it very easy to generate plots that have a ggplot2 equivalent; you can customize these plots further using plotly functions that we’ll see in the next section. But first, try the interface out on your own.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nggplotly\nR + plotly\nPython\n\n\n\nConduct an exploratory data analysis of the eruptions dataset. What do you find?\n\n\n\nhead(eruptions)\n## # A tibble: 6 × 15\n##   volcan…¹ volca…² erupt…³ erupt…⁴ area_…⁵   vei start…⁶ start…⁷ start…⁸ evide…⁹\n##      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  \n## 1   266030 Soputan   22354 Confir… &lt;NA&gt;       NA    2020       3      23 Histor…\n## 2   343100 San Mi…   22355 Confir… &lt;NA&gt;       NA    2020       2      22 Histor…\n## 3   233020 Fourna…   22343 Confir… &lt;NA&gt;       NA    2020       2      10 Histor…\n## 4   345020 Rincon…   22346 Confir… &lt;NA&gt;       NA    2020       1      31 Histor…\n## 5   353010 Fernan…   22347 Confir… &lt;NA&gt;       NA    2020       1      12 Histor…\n## 6   273070 Taal      22344 Confir… &lt;NA&gt;       NA    2020       1      12 Histor…\n## # … with 5 more variables: end_year &lt;dbl&gt;, end_month &lt;dbl&gt;, end_day &lt;dbl&gt;,\n## #   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, and abbreviated variable names\n## #   ¹​volcano_number, ²​volcano_name, ³​eruption_number, ⁴​eruption_category,\n## #   ⁵​area_of_activity, ⁶​start_year, ⁷​start_month, ⁸​start_day,\n## #   ⁹​evidence_method_dating\n\nsummary(eruptions %&gt;% mutate(eruption_category = factor(eruption_category)))\n##  volcano_number   volcano_name       eruption_number\n##  Min.   :210010   Length:11178       Min.   :10001  \n##  1st Qu.:263310   Class :character   1st Qu.:12817  \n##  Median :290050   Mode  :character   Median :15650  \n##  Mean   :300284                      Mean   :15667  \n##  3rd Qu.:343030                      3rd Qu.:18464  \n##  Max.   :600000                      Max.   :22355  \n##                                                     \n##             eruption_category area_of_activity        vei       \n##  Confirmed Eruption  :9900    Length:11178       Min.   :0.000  \n##  Discredited Eruption: 166    Class :character   1st Qu.:1.000  \n##  Uncertain Eruption  :1112    Mode  :character   Median :2.000  \n##                                                  Mean   :1.948  \n##                                                  3rd Qu.:2.000  \n##                                                  Max.   :7.000  \n##                                                  NA's   :2906   \n##    start_year        start_month       start_day      evidence_method_dating\n##  Min.   :-11345.0   Min.   : 0.000   Min.   : 0.000   Length:11178          \n##  1st Qu.:   680.0   1st Qu.: 0.000   1st Qu.: 0.000   Class :character      \n##  Median :  1847.0   Median : 1.000   Median : 0.000   Mode  :character      \n##  Mean   :   622.8   Mean   : 3.451   Mean   : 7.015                         \n##  3rd Qu.:  1950.0   3rd Qu.: 7.000   3rd Qu.:15.000                         \n##  Max.   :  2020.0   Max.   :12.000   Max.   :31.000                         \n##  NA's   :1          NA's   :193      NA's   :196                            \n##     end_year      end_month         end_day         latitude      \n##  Min.   :-475   Min.   : 0.000   Min.   : 0.00   Min.   :-77.530  \n##  1st Qu.:1895   1st Qu.: 3.000   1st Qu.: 4.00   1st Qu.: -6.102  \n##  Median :1957   Median : 6.000   Median :15.00   Median : 17.600  \n##  Mean   :1917   Mean   : 6.221   Mean   :13.32   Mean   : 16.866  \n##  3rd Qu.:1992   3rd Qu.: 9.000   3rd Qu.:21.00   3rd Qu.: 40.821  \n##  Max.   :2020   Max.   :12.000   Max.   :31.00   Max.   : 85.608  \n##  NA's   :6846   NA's   :6849     NA's   :6852                     \n##    longitude      \n##  Min.   :-179.97  \n##  1st Qu.: -77.66  \n##  Median :  55.71  \n##  Mean   :  31.57  \n##  3rd Qu.: 139.39  \n##  Max.   : 179.58  \n## \n\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet’s start out seeing what month most eruptions occur in…\n\n# Note, I'm using the original month, so 0 = unknown\np &lt;- ggplot(erupt, aes(x = factor(start_month))) + geom_bar()\nggplotly(p)\n\n\n\n\n# I could rename some of the factors to make this pretty, but... nah\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index,\np &lt;- ggplot(erupt, aes(x = vei)) + geom_bar()\nggplotly(p)\n\n\n\n\n\nWe can also look at the frequency of eruptions over time. We’ll expect some historical bias - we don’t have exact dates for some of these eruptions, and if no one was around to write the eruption down (or the records were destroyed) there’s not going to be a date listed here.\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  filter(start_year &gt; 0) %&gt;%\n\nggplot(aes(x = start_date, xend = start_date,\n                  y = 0, yend = duration,\n                  color = evidence_method_dating)) +\n  geom_segment() +\n  geom_point(size = .5, aes(text = volcano_name)) +\n  xlab(\"Eruption Start\") +\n  ylab(\"Eruption Duration (days)\") +\n  facet_wrap(~vei, scales = \"free_y\")\nggplotly(p)\n\n\n\n\n\nAs expected, it’s pretty rare to see many eruptions before ~1800 AD, which is about when we have reliable historical records2 for most of the world (exceptions include e.g. Vestuvius, which we have extensive written information about).\n\np &lt;- erupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\nggplot(aes(x = factor(vei), y = duration)) +\n  geom_violin() +\n  xlab(\"VEI\") +\n  ylab(\"Eruption Duration (days)\") +\n  scale_y_sqrt()\nggplotly(p)\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully).\n\n\n\nhead(eruptions)\n## # A tibble: 6 × 15\n##   volcan…¹ volca…² erupt…³ erupt…⁴ area_…⁵   vei start…⁶ start…⁷ start…⁸ evide…⁹\n##      &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  \n## 1   266030 Soputan   22354 Confir… &lt;NA&gt;       NA    2020       3      23 Histor…\n## 2   343100 San Mi…   22355 Confir… &lt;NA&gt;       NA    2020       2      22 Histor…\n## 3   233020 Fourna…   22343 Confir… &lt;NA&gt;       NA    2020       2      10 Histor…\n## 4   345020 Rincon…   22346 Confir… &lt;NA&gt;       NA    2020       1      31 Histor…\n## 5   353010 Fernan…   22347 Confir… &lt;NA&gt;       NA    2020       1      12 Histor…\n## 6   273070 Taal      22344 Confir… &lt;NA&gt;       NA    2020       1      12 Histor…\n## # … with 5 more variables: end_year &lt;dbl&gt;, end_month &lt;dbl&gt;, end_day &lt;dbl&gt;,\n## #   latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, and abbreviated variable names\n## #   ¹​volcano_number, ²​volcano_name, ³​eruption_number, ⁴​eruption_category,\n## #   ⁵​area_of_activity, ⁶​start_year, ⁷​start_month, ⁸​start_day,\n## #   ⁹​evidence_method_dating\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n\nfix_date &lt;- function(yyyy, mm, dd) {\n  # First, negative years (BCE) are a bit of a problem.\n  neg &lt;- yyyy &lt; 0\n  subtract_years &lt;- pmax(-yyyy, 0) # Years to subtract off later\n  # for now, set to 0\n  year_fixed &lt;- pmax(yyyy, 0) # this will set anything negative to 0\n\n  # sometimes the day or month isn't known, so just use 1 for both.\n  # recorded value may be NA or 0.\n  day_fixed &lt;- ifelse(is.na(dd), 1, pmax(dd, 1))\n  month_fixed &lt;- ifelse(is.na(mm), 1, pmax(mm, 1))\n\n  # Need to format things precisely, so use sprintf\n  # %0xd ensures that you have at least x digits, padding the left side with 0s\n  # lubridate doesn't love having 3-digit years.\n  date_str &lt;- sprintf(\"%04d/%02d/%02d\", year_fixed, month_fixed, day_fixed)\n  # Then we can convert the dates and subtract off the years for pre-CE dates\n  date &lt;- ymd(date_str) - years(subtract_years)\n}\n\nerupt &lt;- eruptions %&gt;%\n  # Don't work with discredited eruptions\n  filter(eruption_category == \"Confirmed Eruption\") %&gt;%\n  # Create start and end dates\n  mutate(\n    start_date = fix_date(start_year, start_month, start_day),\n    end_date = fix_date(end_year, end_month, end_day),\n    # To get duration, we have to start with a time interval,\n    # convert to duration, then convert to a numeric value\n    duration = interval(start = start_date, end = end_date) %&gt;%\n      as.duration() %&gt;%\n      as.numeric(\"days\"))\n\nLet’s start out seeing what month most eruptions occur in…\n\n# Note, I'm using the original month, so 0 = unknown\nerupt %&gt;%\n  count(start_month) %&gt;%\n  plot_ly(\n    data = .,\n    x = ~start_month,\n    y = ~n,\n    type = \"bar\"\n)\n\n\n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nerupt %&gt;%\n  count(vei) %&gt;%\n  plot_ly(x = ~vei, y = ~n, type = \"bar\")\n\n\n\n\n\n\nerupt %&gt;%\n  filter(!is.na(end_date)) %&gt;%\n  # Account for recency bias (sort of)\n  filter(start_year &gt; 1800) %&gt;%\n  plot_ly(x = ~ factor(vei),\n          y = ~ duration, \n          split = ~factor(vei),\n          type = \"violin\") %&gt;%\n  layout(yaxis = list(type=\"log\"))\n\n\n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully).\n\n\nIn Python, negative dates are even more of a pain to work with if you’re using standard libraries, so we’ll install the astropy class with pip install astropy. BCE dates are still a pain in the … but they at least work.\n\neruptions.head()\n\n# Historical (very historical) dates are a bit of a pain to work with, so I\n# wrote a helper function which takes year, month, and day arguments and formats\n# them properly\n##    volcano_number            volcano_name  ...  latitude longitude\n## 0          266030                 Soputan  ...     1.112   124.737\n## 1          343100              San Miguel  ...    13.434   -88.269\n## 2          233020  Fournaise, Piton de la  ...   -21.244    55.708\n## 3          345020      Rincon de la Vieja  ...    10.830   -85.324\n## 4          353010              Fernandina  ...    -0.370   -91.550\n## \n## [5 rows x 15 columns]\nfrom astropy.time import Time,TimeDelta\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'astropy'\nimport numpy as np\nimport math\n\ndef fix_date(yyyy, mm, dd):\n  # The zero, one columns allow using pd.max(axis = 1) where we'd use pmax in R\n  neg = yyyy &lt;= 0\n  nyear = -yyyy\n  \n  year = max([yyyy, 1])\n  subtract_year = max([nyear, 0]) + neg\n  \n  day = dd\n  month = mm\n  \n  if math.isnan(day): \n    day = 1\n  if math.isnan(month): \n    month = 1\n      \n  if day == 0: \n    day = 1\n  if month == 0: \n    month = 1\n  \n  dateformat = \"%04d-%02d-%02d\" % (year, month, day)\n  \n  date = Time(dateformat, format = \"iso\", scale = 'ut1')\n  datefix = date - TimeDelta(subtract_year*365, format= 'jd')\n  return datefix\n\nerupt = eruptions.query(\"eruption_category == 'Confirmed Eruption'\")\nerupt.fillna(0, inplace = True)\nerupt['start_date'] = erupt.apply(lambda x: fix_date(x.start_year, x.start_month, x.start_day), axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'Time' is not defined\nerupt['end_date'] = erupt.apply(lambda x: fix_date(x.end_year, x.end_month, x.end_day), axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'Time' is not defined\nerupt['duration'] = erupt.end_date - erupt.start_date\n# Convert back to numeric\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrame' object has no attribute 'end_date'\nerupt['duration'] = erupt.duration.apply(lambda x: x.to_value(\"jd\", \"decimal\")) # Julian day\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrame' object has no attribute 'duration'\n\nLet’s start out seeing what month most eruptions occur in…\n\nimport plotly.express as px\ntmp = erupt.groupby(\"start_month\").count()\ntmp = tmp.reset_index()\n# Note, I'm using the original month, so 0 = unknown\nfig = px.bar(tmp, x = 'start_month', y = 'volcano_number')\nfig.show()\n\n\n                                    \n\n\n\nAnother numerical variable is VEI, volcano explosivity index. A VEI of 0 is non-explosive, a VEI of 4 is about what Mt. St. Helens hit in 1980, and a VEI of 5 is equivalent to the Krakatau explosion in 1883. A VEI of 8 would correspond to a major Yellowstone caldera eruption (which hasn’t happened for 600,000 years). Basically, VEI increase of 1 is an order of magnitude change in the amount of material the eruption released.\n\n# VEI is volcano explosivity index\nfig = px.bar(\n  erupt.groupby(\"vei\").count().reset_index(),\n  x = \"vei\", y = \"volcano_number\")\nfig.show()\n\n\n                                    \n\n\n\n\nerupt[\"duration_yr\"] = erupt.duration/365.25\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrame' object has no attribute 'duration'\nfig = px.box(\n  erupt,\n  x = \"vei\",\n  y = \"duration_yr\",\n  points = \"all\"\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Value of 'y' is not the name of a column in 'data_frame'. Expected one of ['volcano_number', 'volcano_name', 'eruption_number', 'eruption_category', 'area_of_activity', 'vei', 'start_year', 'start_month', 'start_day', 'evidence_method_dating', 'end_year', 'end_month', 'end_day', 'latitude', 'longitude'] but received: duration_yr\nfig.show()\n\n\n                                    \n\n\n\nIt seems that the really big eruptions might be less likely to last for a long time, but it is hard to tell because there aren’t that many of them (thankfully)\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Interactivity\n\n\n\n\n\nPlotly integration with ggplot2 is nice, but obviously not a universal summary of what it can do. Let’s look at another example of plotly in R/python without ggplot2 integration.\nWe start with a scatterplot of volcanoes along the earth’s surface:\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, lat = volcano$latitude)\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, lon = \"longitude\", lat = \"latitude\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Value of 'lat' is not the name of a column in 'data_frame'. Expected one of ['elevation'] but received: latitude\nfig.show()\n\n\n                                    \n\n\n\n\n\n\nAnd then we can start customizing.\n\n\nR\nPython\n\n\n\n\nplot_ly(type = \"scattergeo\", lon = volcano$longitude, \n        lat = volcano$latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano$volcano_name, \"\\n\",\n                      \"Last Erupted: \", volcano$last_eruption_year),\n        # Change the markers because why not?\n        marker = list(color = \"#d00000\", opacity = 0.25)\n        )\n\n\n\n\n\n\n\n\nfig = px.scatter_geo(volcano, \n  lon = \"longitude\", lat = \"latitude\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Value of 'hover_name' is not the name of a column in 'data_frame'. Expected one of ['elevation'] but received: volcano_name\nfig.update_traces(marker=dict(size=12, opacity = 0.25, color = 'red'),\n                  selector=dict(mode='markers'))\n\n\n                                    \n\n\nfig.show()\n\n\n                                    \n\n\n\n\n\n\nPlotly will handle some variable mappings for you, depending on which “trace” (type of plot) you’re using.\n\n\nR\nPython\n\n\n\nThe plot_ly function is also pipe friendly. Variable mappings are preceded with ~ to indicate that the visual appearance changes with the value of the variable.\n\n# Load RColorBrewer for palettes\nlibrary(RColorBrewer)\n\nvolcano %&gt;%\n  group_by(volcano_type) %&gt;% \n  mutate(n = n()) %&gt;%\n  filter(n &gt; 15) %&gt;%\nplot_ly(type = \"scattergeo\", lon = ~longitude, lat = ~latitude,\n        mode = \"markers\",\n        # Add information to mouseover\n        text = ~paste(volcano_name, \"\\n\",\n                      \"Last Erupted: \", last_eruption_year),\n        color = ~ volcano_type,\n        # Specify a palette\n        colors = brewer.pal(length(unique(.$volcano_type)), \"Paired\"),\n        # Change the markers because why not?\n        marker = list(opacity = 0.5)\n        )\n\n\n\n\n\n\n\n\nvolc_sub = volcano.groupby(\"volcano_type\").agg({'volcano_number': ['size']})\n## Error in py_call_impl(callable, dots$args, dots$keywords): pandas.core.base.SpecificationError: nested renamer is not supported\nvolc_sub.columns = [\"n\"]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.reset_index()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'volc_sub' is not defined\nvolc_sub = volc_sub.query(\"n &gt;= 15\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'volc_sub' is not defined\nvolc_sub = pd.merge(volc_sub['volcano_type'], volcano, on = 'volcano_type', how = 'inner')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'volc_sub' is not defined\nfig = px.scatter_geo(volc_sub, \n  lon = \"longitude\", lat = \"latitude\", \n  color = \"volcano_type\",\n  hover_name = \"volcano_name\",\n  hover_data = [\"last_eruption_year\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'volc_sub' is not defined\nfig.update_traces(marker=dict(size=12, opacity = 0.25),\n                  selector=dict(mode='markers'))\n\n\n                                    \n\n\nfig.show()"
  },
  {
    "objectID": "interactive-graphics.html#leaflet-maps",
    "href": "interactive-graphics.html#leaflet-maps",
    "title": "\n15  Animated and Interactive Graphics\n",
    "section": "\n15.3 Leaflet maps",
    "text": "15.3 Leaflet maps\n\n\nI’m sorry, but I haven’t managed to redo this chapter in python as well as R. Hopefully I’ll get to it soon.\nLeaflet is another javascript library that allows for interactive data visualization. We’re only going to briefly talk about it here, but there is extensive documentation that includes details of how to work with different types of geographical data, chloropleth maps, plugins, and more.\n\n\n\n\n\n\nBigfoot Sightings\n\n\n\n\n\nTo explore the leaflet package, we’ll start out playing with a dataset of Bigfoot sightings assembled from the Bigfoot Field Researchers Organization’s Google earth tool\n\nif (!\"leaflet\" %in% installed.packages()) install.packages(\"leaflet\")\n\nlibrary(leaflet)\nlibrary(readr)\n\nbigfoot_data &lt;- read_csv(\"https://query.data.world/s/egnaxxvegdkzzrhfhdh4izb6etmlms\")\n\nWe can start out by plotting a map with the location of each sighting. I’ve colored the points in a seasonal color scheme, and added the description of each incident as a mouseover label.\n\nbigfoot_data %&gt;%\n  filter(classification == \"Class A\") %&gt;%\n  mutate(seasoncolor = str_replace_all(season, c(\"Fall\" = \"orange\",\n                                                 \"Winter\" = \"skyblue\",\n                                                 \"Spring\" = \"green\",\n                                                 \"Summer\" = \"yellow\")),\n         # This code just wraps the description to the width of the R terminal\n         # and inserts HTML for a line break into the text at appropriate points\n         desc_wrap = purrr::map(observed, ~strwrap(.) %&gt;%\n                                  paste(collapse = \"&lt;br/&gt;\") %&gt;%\n                                  htmltools::HTML())) %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~longitude, ~latitude, color = ~seasoncolor, label = ~desc_wrap)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSquirrels of New York City\n\n\n\n\n\nOf course, because this is an interactive map library, we aren’t limited to any one scale. We can also plot data at the city level:\n\nif(!\"nycsquirrels18\" %in% installed.packages()) {\n  devtools::install_github(\"mine-cetinkaya-rundel/nycsquirrels18\")\n}\n\nlibrary(nycsquirrels18)\n\ndata(squirrels)\n\nhead(squirrels)\n## # A tibble: 6 × 35\n##    long   lat unique_sq…¹ hectare shift date       hecta…² age   prima…³ highl…⁴\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;date&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  \n## 1 -74.0  40.8 13A-PM-101… 13A     PM    2018-10-14       4 &lt;NA&gt;  Gray    &lt;NA&gt;   \n## 2 -74.0  40.8 15F-PM-101… 15F     PM    2018-10-10       6 Adult Gray    &lt;NA&gt;   \n## 3 -74.0  40.8 19C-PM-101… 19C     PM    2018-10-18       2 Adult Gray    Cinnam…\n## 4 -74.0  40.8 21B-AM-101… 21B     AM    2018-10-19       4 &lt;NA&gt;  &lt;NA&gt;    &lt;NA&gt;   \n## 5 -74.0  40.8 23A-AM-101… 23A     AM    2018-10-18       2 Juve… Black   &lt;NA&gt;   \n## 6 -74.0  40.8 38H-PM-101… 38H     PM    2018-10-12       1 Adult Gray    &lt;NA&gt;   \n## # … with 25 more variables: combination_of_primary_and_highlight_color &lt;chr&gt;,\n## #   color_notes &lt;chr&gt;, location &lt;chr&gt;, above_ground_sighter_measurement &lt;chr&gt;,\n## #   specific_location &lt;chr&gt;, running &lt;lgl&gt;, chasing &lt;lgl&gt;, climbing &lt;lgl&gt;,\n## #   eating &lt;lgl&gt;, foraging &lt;lgl&gt;, other_activities &lt;chr&gt;, kuks &lt;lgl&gt;,\n## #   quaas &lt;lgl&gt;, moans &lt;lgl&gt;, tail_flags &lt;lgl&gt;, tail_twitches &lt;lgl&gt;,\n## #   approaches &lt;lgl&gt;, indifferent &lt;lgl&gt;, runs_from &lt;lgl&gt;,\n## #   other_interactions &lt;chr&gt;, zip_codes &lt;dbl&gt;, community_districts &lt;dbl&gt;, …\n\nsquirrels %&gt;%\n  mutate(color = tolower(primary_fur_color)) %&gt;%\n  leaflet() %&gt;%\n  addTiles() %&gt;%\n  addCircleMarkers(~long, ~lat, color = ~color)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEcological Regions\n\n\n\n\n\nWe can also plot regions, instead of just points. I downloaded a dataset released by the US Forest Service, Bailey’s Ecoregions and Subregions dataset, which categorizes the US into different climate and ecological zones.\nTo map colors to variables, we have to define a color palette and variable mapping ourselves, and pass that function into the leaflet object we’re adding.\nI’ve set this chunk to not evaluate because it causes the book to be painfully large.\n\nlibrary(sf)\necoregions &lt;- st_read(\"data/Bailey_s_Ecoregions_and_Subregions_Dataset.geojson\")\n\n# Define a palette\nregion_pal &lt;- colorFactor(c(\"#E67E22\", \"#0B5345\", \"#229954\", \"#B3B6B7\"), ecoregions$DOMAIN)\n\necoregions %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = TRUE, fillOpacity = 0.25,\n              fillColor = ~region_pal(DOMAIN), color = ~region_pal(DOMAIN), label = ~SECTION)\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nStarter R code\nR solution\n\n\n\nDownload the Shapefiles for the 116th Congress Congressional Districts. Unzip the file and read it in using the code below (you’ll have to change the file path). Use the MIT Election Data and Science Lab’s US House election results, and merge this data with the shapefiles to plot the results of the 2018 midterms in a way that you think is useful (you can use any of the available data).\nSome notes:\n\nFIPS codes are used to identify the state and district, with 00 indicating at-large districts (one district for the state) and 98 indicating non-voting districts.\nIf you would like to add in the number of citizens of voting age, you can get that information here but you will have to do some cleaning in order to join the table with the others.\nMinnesota’s Democratic-farmer-labor party caucuses with the Democrats but maintains its name for historical reasons. You can safely recode this if you want to.\n\n\n\n\nlibrary(sf)\n# Read in the districts\nziptemp &lt;- tempfile(fileext=\".zip\")\nshapeurl &lt;- \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_cd116_5m.zip\"\ndownload.file(shapeurl, destfile = ziptemp, mode = \"wb\")\nunzip(ziptemp, exdir = \"data/116_congress\")\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\")\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/unl-stat850/stat850-textbook/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Read in the results\nelection_results &lt;- read_csv(\"data/1976-2020-house.csv\") %&gt;%\n  filter(year == 2018) %&gt;%\n  mutate(state_fips = sprintf(\"%02d\", as.integer(state_fips)),\n         district = sprintf(\"%02d\", as.integer(district)))\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n\n\n\nlibrary(sf)\nlibrary(htmltools) # to mark labels as html code\n\n# Read in the results\nelection_results &lt;- election_results %&gt;%\n  group_by(state, state_fips, state_po, district, stage) %&gt;%\n  arrange(candidatevotes) %&gt;%\n  mutate(pct = candidatevotes/totalvotes) %&gt;%\n  mutate(party = str_to_lower(party)) %&gt;%\n  # Keep the winner only\n  filter(pct == max(pct)) %&gt;%\n  # Fix Minnesota\n  mutate(party = ifelse(party == \"democratic-farmer-labor\", \"democrat\", party))\n\n# Read in the districts\ncongress_districts &lt;- st_read(\"data/116_congress/cb_2018_us_cd116_5m.shp\") %&gt;%\n  mutate(geometry = st_transform(geometry, crs = st_crs(\"+proj=longlat +datum=WGS84\")))\n## Reading layer `cb_2018_us_cd116_5m' from data source \n##   `/home/susan/Projects/Class/unl-stat850/stat850-textbook/data/116_congress/cb_2018_us_cd116_5m.shp' \n##   using driver `ESRI Shapefile'\n## Simple feature collection with 441 features and 8 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: -179.1473 ymin: -14.55255 xmax: 179.7785 ymax: 71.35256\n## Geodetic CRS:  NAD83\n\n# Clean up congress districts\ncongress_districts &lt;- congress_districts %&gt;%\n  # Convert factors to characters\n  mutate(across(where(is.factor), as.character)) %&gt;%\n  # Handle at-large districts\n  mutate(district = ifelse(CD116FP == \"00\", \"01\", CD116FP))\n\n# Merge\ncongress_districts &lt;- congress_districts %&gt;%\n  left_join(election_results, by = c(\"STATEFP\" = \"state_fips\", \"CD116FP\" = \"district\")) %&gt;%\n  mutate(party = factor(party, levels = c(\"republican\", \"democrat\")),\n         short_party = ifelse(party == \"republican\", \"R\", \"D\"),\n         label = paste0(state_po, \"-\", district, candidate, \" (\", short_party, \")\"))\n\n# Define a palette\nregion_pal &lt;- colorFactor(c(\"#e9141d\", \"#0015bc\"), congress_districts$party)\n\ncongress_districts %&gt;%\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addPolygons(stroke = TRUE, fillOpacity = ~pct/2,\n              # still want to see what's underneath, even in safe districts\n              fillColor = ~region_pal(party), color = ~region_pal(party),\n              label = ~label)"
  },
  {
    "objectID": "interactive-graphics.html#shiny",
    "href": "interactive-graphics.html#shiny",
    "title": "\n15  Animated and Interactive Graphics\n",
    "section": "\n15.4 Shiny",
    "text": "15.4 Shiny\nTake a few minutes and poke around the RStudio Shiny user showcase. It helps to have some motivation, and to get a sense of what is possible before you start learning something. One of the more amusing ones I found was an exploration of lego demographics.\nShiny is a framework for building interactive web applications in R (and now in Python too!). Unlike plotly and other graphics engines, Shiny depends on an R instance on a server to do computations. This means Shiny is much more powerful and has more capabilities, but also that it’s harder to share and deploy - you have to have access to a web server with R installed on it. If you happen to have a server like that, though, Shiny is pretty awesome. RStudio runs a service called shinyapps.io that will provide some limited free hosting, as well as paid plans for apps that have more web traffic, but you can also create Shiny apps for local use - I often do this for model debugging when I’m using neural networks, because they’re so complicated.\nRStudio has a set of well produced video tutorials to introduce Shiny. I’d recommend you at least listen to the introduction if you’re a visual/audio learner (the whole tutorial is about 2 hours long). There is also a written tutorial if you prefer to learn in written form (7 lessons, each is about 20 minutes long).\nI generally think it’s better to send you to the source when there are well-produced resources, rather than trying to rehash something to put my own spin on it.\nOne other interesting feature to keep in mind when using Shiny - you can integrate Shiny reactivity into Rmarkdown by adding runtime: shiny to the markdown header.\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nShiny articles\nReactivity in Shiny\nLeaflet introduction for R\n\n\n15.4.1 Other interactive tools\n\nhtmlwidgets - a generic wrapper for any Javascript library (htmlwidgets is used under the hood in both Leaflet and Plotly R integration)\ndash - Another dashboard program supported by plotly. dash is the python equivalent of shiny, but also has R integration (though I’m not sure how well it’s supported).\n\n15.4.2 Debugging\n\nDebugging with Dean - Shiny debugging - YouTube video with debugging in realtime.\nShinyJS - Using Shiny and JavaScript together\nUsing Shiny in Production - Joe Cheng\n\n\n\n\n\n\n\n\n[1] \nPlotly, “Plotly Open Source Graphing Libraries.” 2022 [Online]. Available: https://plotly.com/api/. [Accessed: Oct. 18, 2022]"
  },
  {
    "objectID": "interactive-graphics.html#footnotes",
    "href": "interactive-graphics.html#footnotes",
    "title": "\n15  Animated and Interactive Graphics\n",
    "section": "",
    "text": "This is me experimentally trying to replace the tidyverse, and honestly, I’m not a fan.↩︎\nThere are obviously exceptions - we can figure out the exact date and approximate time that there was an earthquake along the Cascadia subduction zone based on a combination of oral histories of the indigenous people and records of a massive tsunami in Japan Excellent read, if you’re interested, and the Nature paper.↩︎"
  },
  {
    "objectID": "functional-programming.html#module14-objectives",
    "href": "functional-programming.html#module14-objectives",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "Module Objectives",
    "text": "Module Objectives\n\nUse functional programming techniques to create code which is well organized and easier to understand and maintain"
  },
  {
    "objectID": "functional-programming.html#review",
    "href": "functional-programming.html#review",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.1 Review",
    "text": "16.1 Review\n\n16.1.1 Lists and Vectors\nA vector is a 1-dimensional data structure that contains items of the same simple (‘atomic’) type (character, logical, integer, factor).\n\n\nR\nPython\n\n\n\n\n(logical_vec &lt;- c(T, F, T, T))\n## [1]  TRUE FALSE  TRUE  TRUE\n(numeric_vec &lt;- c(3, 1, 4, 5))\n## [1] 3 1 4 5\n(char_vec &lt;- c(\"A\", \"AB\", \"ABC\", \"ABCD\"))\n## [1] \"A\"    \"AB\"   \"ABC\"  \"ABCD\"\n\n\n\nNotice that in python, we define each of these things as a list first, and then convert to a numpy array, which is equivalent to an R vector.\n\nimport numpy as np\n\nlogical_vec = np.array([True, False, True, True])\nlogical_vec\n## array([ True, False,  True,  True])\nnumeric_vec = np.array([3, 1, 4, 5])\nnumeric_vec\n## array([3, 1, 4, 5])\nchar_vec = np.array(['A', 'AB', 'ABC', 'ABCD'])\nchar_vec\n## array(['A', 'AB', 'ABC', 'ABCD'], dtype='&lt;U4')\n\n\n\n\nYou index a vector using brackets: to get the \\(i\\)th element of the vector x, you would use x[i] in R or x[i-1] in python (Remember, python is 0-indexed, so the first element of the vector is at location 0).\n\n\nR\nPython\n\n\n\n\nlogical_vec[3]\n## [1] TRUE\nnumeric_vec[3]\n## [1] 4\nchar_vec[3]\n## [1] \"ABC\"\n\n\n\n\nlogical_vec[2]\n## True\nnumeric_vec[2]\n## 4\nchar_vec[2]\n## 'ABC'\n\n\n\n\nYou can also index a vector using a logical vector:\n\n\nR\nPython\n\n\n\n\nnumeric_vec[logical_vec]\n## [1] 3 4 5\nchar_vec[logical_vec]\n## [1] \"A\"    \"ABC\"  \"ABCD\"\nlogical_vec[logical_vec]\n## [1] TRUE TRUE TRUE\n\n\n\n\nnumeric_vec[logical_vec]\n## array([3, 4, 5])\nchar_vec[logical_vec]\n## array(['A', 'ABC', 'ABCD'], dtype='&lt;U4')\nlogical_vec[logical_vec]\n## array([ True,  True,  True])\n\n\n\n\nA list is a 1-dimensional data structure that has no restrictions on what type of content is stored within it. A list is a “vector”, but it is not an atomic vector - that is, it does not necessarily contain things that are all the same type.\n\n\nR\nPython\n\n\n\n\n(\n  mylist &lt;- list(\n    logical_vec, \n    numeric_vec, \n    third_thing = char_vec[1:2]\n  )\n)\n## [[1]]\n## [1]  TRUE FALSE  TRUE  TRUE\n## \n## [[2]]\n## [1] 3 1 4 5\n## \n## $third_thing\n## [1] \"A\"  \"AB\"\n\nIn R, list components may have names (or not), be homogeneous (or not), have the same length (or not).\n\n\nIn python, lists are similar, but do not have named entries.\n\nmylist = [logical_vec, numeric_vec, char_vec[0:1]]\n\nIn python, list components are unnamed, but can be homogeneous (or not) and may have the same length (or not).\n\n\n\n\n16.1.2 Indexing\nIndexing necessarily differs between R and python, and since the list types are also somewhat different (e.g. lists cannot be named in python), we will treat list indexing in the two languages separately.\n\n\nR\nPython\n\n\n\n\n\n\n\n\nAn unusual pepper shaker which we’ll call pepper\n\n\n\n\n\nWhen a list is indexed with single brackets, pepper[1], the return value is always a list containing the selected element(s).\n\n\n\n\n\nWhen a list is indexed with double brackets, pepper[[1]], the return value is the selected element.\n\n\n\n\n\nTo actually access the pepper, we have to use double indexing and index both the list object and the sub-object, as in pepper[[1]][[1]].\n\n\n\nFigure 16.1: The types of indexing in R are made most memorable with a fantastic visual example from [1], which I have repeated here. This example may be familiar from Section 4.8, but hopefully at this point it makes a lot more sense.\n\n\nIn R, there are 3 ways to index a list:\n\nWith single square brackets, just like we index atomic vectors. In this case, the return value is always a list.\n\n\nmylist[1]\n## [[1]]\n## [1]  TRUE FALSE  TRUE  TRUE\n\nmylist[2]\n## [[1]]\n## [1] 3 1 4 5\n\nmylist[c(T, F, T)]\n## [[1]]\n## [1]  TRUE FALSE  TRUE  TRUE\n## \n## $third_thing\n## [1] \"A\"  \"AB\"\n\n\nWith double square brackets. In this case, the return value is the thing inside the specified position in the list, but you also can only get one entry in the main list at a time. You can also get things by name.\n\n\nmylist[[1]]\n## [1]  TRUE FALSE  TRUE  TRUE\n\nmylist[[\"third_thing\"]]\n## [1] \"A\"  \"AB\"\n\n\nUsing x$name. This is equivalent to using x[[\"name\"]]. Note that this does not work on unnamed entries in the list.\n\n\nmylist$third_thing\n## [1] \"A\"  \"AB\"\n\nTo access the contents of a list object, we have to use double-indexing:\n\nmylist[[\"third_thing\"]][[1]]\n## [1] \"A\"\n\n\n\n\n\n\n\nNote\n\n\n\nYou can get a more thorough review of vectors and lists from Jenny Bryan’s purrr tutorial introduction [2].\n\n\n\n\nBecause Python lists are unnamed, indexing is pretty straightforward and works exactly like indexing vectors.\n\nhhgtg = ['the', 'answer', 'to', 'life', 'is', 42]\n\nhhgtg[5]\n## 42\nhhgtg[0:5]\n## ['the', 'answer', 'to', 'life', 'is']\n\nIndexing nested lists is just a matter of appending multiple sets of indexes.\n\n# The quote above is an abbreviation... here's the full version\nhhgtg = ['the', 'answer', 'to', 'the', \n         'ultimate', 'question', 'of', \n         ['life', 'the universe', 'and everything'], \n         'is', 42]\n\nhhgtg[7]\n## ['life', 'the universe', 'and everything']\nhhgtg[7][0]\n## 'life'"
  },
  {
    "objectID": "functional-programming.html#vectorized-operations",
    "href": "functional-programming.html#vectorized-operations",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.2 Vectorized Operations",
    "text": "16.2 Vectorized Operations\nOperations in R and numpy are (usually) vectorized - that is, by default, they operate on vectors. This is primarily a feature that applies to atomic vectors (and we don’t even think about it):\n\n\nR\nNumpy\n\n\n\n\n(rnorm(10) + rnorm(10, mean = 3))\n##  [1] 4.096248 3.182472 4.160299 2.167633 2.849340 3.004994 1.883960 3.282649\n##  [9] 1.372756 1.108213\n\n\n\n\nimport numpy as np\n\nnpx = np.array([1,2,3])\nnpy = np.array([4,5,6])\nnpx + npy # numpy uses vectorized arithmetic operations\n## array([5, 7, 9])\n\n\n\n\nWith vectorized functions, we don’t have to use a for loop to add these two vectors with 10 entries each together. In languages which don’t have implicit support for vectorized computations, this might instead look like:\n\n\nR\n(Base) Python\n\n\n\n\na &lt;- rnorm(10)\nb &lt;- rnorm(10, mean = 3)\n\nresult &lt;- rep(0, 10)\nfor (i in 1:10) {\n  result[i] &lt;- a[i] + b[i]\n}\n\nresult\n##  [1] 3.8859799 3.2256176 2.3621698 3.4717828 3.2511672 4.3374697 4.0713542\n##  [8] 2.2581989 3.3939106 0.1578225\n\n\n\n\nx = [1, 2, 3]\ny = [4, 5, 6]\n\nx + y # This just appends the lists... which is not what we want\n\n# This is what we actually want\n## [1, 2, 3, 4, 5, 6]\nz = [0, 0, 0]\nfor i in range(3):\n  z[i] = x[i] + y[i]\n\nz\n## [5, 7, 9]\n\n\n\n\nThat is, we would apply or map the + function to each entry of a and b. For atomic vectors, it’s easy to do this by default; with a list, however, we need to be a bit more explicit (because everything that’s passed into the function may not be the same type).\nThe R package purrr (and similar base functions apply, lapply, sapply, tapply, and mapply) are based on extending “vectorized” functions to a wider variety of vector-like structures.\n\n\nI find the purrr package easier to work with, but you may use the base package versions if you want, and you can find a side-by-side comparison in the purrr tutorial.\nIn python, similar methods apply but are built in to pandas [3]. As a result, I have structured this section based on the purrr package, but will show equivalent python code where possible."
  },
  {
    "objectID": "functional-programming.html#functional-programming",
    "href": "functional-programming.html#functional-programming",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.3 Functional Programming",
    "text": "16.3 Functional Programming\n\n\n\n\n\n\nNote\n\n\n\nA much more thorough treatment of functional programming is available in Advanced R [4]. This is a bare summary in comparison.\n\n\nOne last concept that is relevant to this chapter is the idea of functional programming. This concept is a bit hard to define rigorously at the level we’re working at, but generally, functional programming is concerned with pure functions: functions that have an input value that determines the output value and create no other side effects.\nWhat this means is that you describe every step of the computation using a function, and chain the functions together. At the end of the computations, you might save the program’s results to an object, but (in general), the goal is to not change things outside of the “pipeline” along the way.\nThis has some advantages:\n\nEasier parallelization\n“Side effects” generally make it hard to parallelize code because e.g. you have to update stored objects in memory, which is hard to do with multiple threads accessing the same memory.\nFunctional programming tends to be easier to read\nYou can see output and input and don’t have to work as hard to keep track of what is stored where .\nEasier Debugging\nYou can examine the input and output at each stage to isolate which function is introducing the problem.\n\nThe introduction of the pipe in R has made chaining functions together in a functional programming-style pipeline much easier. purrr is just another step in this process: by making it easy to apply functions to lists of things (or to use multiple lists of things in a single function), purrr makes it easier to write clean, understandable, debuggable code.\n\n\nWhile I’m not quite ready to introduce it in class, because the syntax has changed significantly from V1 to V2, there is also a Pipe package for python [5] that may be worth investigating.\n\n\n\n\n\n\nFunctional Programming Example\n\n\n\nThis example is modified from the motivation section of the Functional Programming chapter in Advanced R [4].\n\n\nProblem\nNaive Approach\nWriting a function\nMapping a function\n\n\n\nSuppose we want to replace every -99 in the following sample dataset with an NA. (-99 is sometimes used to indicate missingness in datasets).\n\n# Generate a sample dataset\nset.seed(1014)\ndf &lt;- data.frame(replicate(6, sample(c(1:10, -99), 6, rep = TRUE)))\nnames(df) &lt;- letters[1:6]\ndf\n##   a   b   c   d  e f\n## 1 7   5 -99   2  5 2\n## 2 5   5   5   3  6 1\n## 3 6   8   5   9  9 4\n## 4 4   2   2   6  6 8\n## 5 6   7   6 -99 10 6\n## 6 9 -99   4   7  5 1\n\n\n\nThe “beginner” approach is to just replace each individual -99 with an NA:\n\ndf1 &lt;- df\ndf1[6,2] &lt;- NA\ndf1[1,3] &lt;- NA\ndf1[5,4] &lt;- NA\n\ndf1\n##   a  b  c  d  e f\n## 1 7  5 NA  2  5 2\n## 2 5  5  5  3  6 1\n## 3 6  8  5  9  9 4\n## 4 4  2  2  6  6 8\n## 5 6  7  6 NA 10 6\n## 6 9 NA  4  7  5 1\n\nThis is tedious, and painful, and won’t work if we have a slightly different dataset where the -99s are in different places. So instead, we might consider being a bit more general:\n\ndf2 &lt;- df\ndf2$a[df2$a == -99] &lt;- NA\ndf2$b[df2$b == -99] &lt;- NA\ndf2$c[df2$c == -99] &lt;- NA\ndf2$d[df2$d == -99] &lt;- NA\ndf2$e[df2$e == -99] &lt;- NA\ndf2$f[df2$f == -99] &lt;- NA\ndf2\n##   a  b  c  d  e f\n## 1 7  5 NA  2  5 2\n## 2 5  5  5  3  6 1\n## 3 6  8  5  9  9 4\n## 4 4  2  2  6  6 8\n## 5 6  7  6 NA 10 6\n## 6 9 NA  4  7  5 1\n\nThis requires a few more lines of code, but is able to handle any data frame with 6 columns a - f. It also requires a lot of copy-paste and can leave you vulnerable to making mistakes.\n\n\nThe standard rule is that if you copy-paste the same code 3x, then you should write a function, so let’s try that instead:\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf3 &lt;- df\ndf3$a &lt;- fix_missing(df$a)\ndf3$b &lt;- fix_missing(df$b)\ndf3$c &lt;- fix_missing(df$c)\ndf3$d &lt;- fix_missing(df$d)\ndf3$e &lt;- fix_missing(df$e)\ndf3$f &lt;- fix_missing(df$f)\ndf3\n##   a  b  c  d  e f\n## 1 7  5 NA  2  5 2\n## 2 5  5  5  3  6 1\n## 3 6  8  5  9  9 4\n## 4 4  2  2  6  6 8\n## 5 6  7  6 NA 10 6\n## 6 9 NA  4  7  5 1\n\nThis still requires a lot of copy-paste, and doesn’t actually make the code more readable. We can more easily change the missing value, though, which is a bonus.\n\n\nWe have a function that we want to apply or map to every column in our data frame. We could use a for loop:\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf4 &lt;- df\nfor (i in 1:ncol(df)) {\n  df4[,i] &lt;- fix_missing(df4[,i])\n}\ndf4\n##   a  b  c  d  e f\n## 1 7  5 NA  2  5 2\n## 2 5  5  5  3  6 1\n## 3 6  8  5  9  9 4\n## 4 4  2  2  6  6 8\n## 5 6  7  6 NA 10 6\n## 6 9 NA  4  7  5 1\n\nThis is more understandable and flexible than the previous function approach as well as the naive approach - we don’t need to know the names of the columns in our data frame, or even how many there are. It is still quite a few lines of code, though.\nIterating through a list (or columns of a data frame) is a very common task, so R has a shorthand function for it. For these purposes, I’ll use the base function lapply, since it doesn’t require any additional packages and demonstrates the concept nicely.\nlapply(x, f, ...) is a function that takes arguments x, a list, f, a function, and ... - a special parameter in R that can handle one or more arguments that are passed to the function.\n\nfix_missing &lt;- function(x, missing = -99){\n  x[x == missing] &lt;- NA\n  x\n}\n\ndf5 &lt;- df\ndf5[] &lt;- lapply(df5, fix_missing)\ndf5\n##   a  b  c  d  e f\n## 1 7  5 NA  2  5 2\n## 2 5  5  5  3  6 1\n## 3 6  8  5  9  9 4\n## 4 4  2  2  6  6 8\n## 5 6  7  6 NA 10 6\n## 6 9 NA  4  7  5 1\n\nBy default, lapply returns a list. By assigning the results to df5[], we tell R we want the results to be a data frame, instead.\nWe’ve replaced 6 lines of code that only worked for 6 columns named a - f with a single line of code that works for any data frame with any number of rows and columns, so long as -99 indicates missing data. In addition to being shorter, this code is also somewhat easier to read and much less vulnerable to typos."
  },
  {
    "objectID": "functional-programming.html#introduction-to-map",
    "href": "functional-programming.html#introduction-to-map",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.4 Introduction to map\n",
    "text": "16.4 Introduction to map\n\n\n\nThe source data for this example comes from An API of Ice and Fire and is fairly typical for API (automatic programming interface) data in both cleanliness and complexity.\n\n16.4.1 Data Setup\nFirst, let’s export the data from repurrrsive to a JSON file that we can read into R or Python, so that we start out in both languages at approximately the same place.\n\nlibrary(repurrrsive) # example data\ndata(got_chars)\n\nlibrary(rjson)\nwrite(toJSON(got_chars), \"data/got_chars.json\")\n\n\n\nR\nPython\n\n\n\n\nlibrary(tidyverse) \nlibrary(purrr) # functions for working with lists\ngot_chars &lt;- fromJSON(file = \"data/got_chars.json\")\n\nWe’ll use one of the datasets in repurrsive, got_chars, to start playing with the map_ series of functions.\n\nlength(got_chars)\n## [1] 30\ngot_chars[[1]][1:6] # Only show the first 6 fields\n## $url\n## [1] \"https://www.anapioficeandfire.com/api/characters/1022\"\n## \n## $id\n## [1] 1022\n## \n## $name\n## [1] \"Theon Greyjoy\"\n## \n## $gender\n## [1] \"Male\"\n## \n## $culture\n## [1] \"Ironborn\"\n## \n## $born\n## [1] \"In 278 AC or 279 AC, at Pyke\"\nnames(got_chars[[1]]) # How many total fields? names?\n##  [1] \"url\"         \"id\"          \"name\"        \"gender\"      \"culture\"    \n##  [6] \"born\"        \"died\"        \"alive\"       \"titles\"      \"aliases\"    \n## [11] \"father\"      \"mother\"      \"spouse\"      \"allegiances\" \"books\"      \n## [16] \"povBooks\"    \"tvSeries\"    \"playedBy\"\n\nIt appears that each entry in this 30-item list is a character from Game of Thrones, and there are several sub-fields for each character.\n\n\nWe can read the data in to Python as a pandas DataFrame. Note that the default structure here is very different than the equivalent R structure for reading in a JSON file.\n\nimport pandas as pd\ngot_chars = pd.read_json('data/got_chars.json')\n\nfrom skimpy import skim\nskim(got_chars) \n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 30     │ │ object      │ 16    │                       │\n## │ │ Number of columns │ 18     │ │ int64       │ 1     │                       │\n## │ └───────────────────┴────────┘ │ bool        │ 1     │                       │\n## │                                └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━┳━━━━━┳━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n## │ ┃    ┃ missing ┃ complete  ┃ mean ┃ sd  ┃ p0 ┃ p25 ┃ p75  ┃ p100 ┃ hist   ┃  │\n## │ ┃    ┃         ┃ rate      ┃      ┃     ┃    ┃     ┃      ┃      ┃        ┃  │\n## │ ┡━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━╇━━━━━╇━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n## │ │ id │       0 │         1 │  710 │ 500 │ 60 │ 220 │ 1100 │ 2100 │ █▃▅▅ ▁ │  │\n## │ └────┴─────────┴───────────┴──────┴─────┴────┴─────┴──────┴──────┴────────┘  │\n## │                                    bool                                      │\n## │ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓  │\n## │ ┃                ┃ true        ┃ true rate              ┃ hist            ┃  │\n## │ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩  │\n## │ │ alive          │          20 │                   0.67 │     ▄    █      │  │\n## │ └────────────────┴─────────────┴────────────────────────┴─────────────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\nAs most of the columns are object types, using skimpy to examine the data structure really doesn’t help us much.\n\n\n\n\n16.4.2 Exploring the Data\nWhat characters do we have? How is the data structured?\nList data can be incredibly hard to work with because the structure is so flexible. It’s important to have a way to visualize the structure of a complex list object: the View() command in RStudio is one good way to explore and poke around a list.\n\n\nR\nPython\n\n\n\nWe can use purrr::map(x, \"name\") to get a list of all characters’ names. Since they are all the same type, we could also use an extension of map, map_chr, which will coerce the returned list into a character vector (which may be simpler to operate on).\n\n\n\n\n\n\nNote\n\n\n\nThere are several packages with map() functions including functions that are meant to actually plot maps; it generally saves time and effort to just type the function name with the package you want in package::function notation. You don’t have to do so, but if you have a lot of other (non tidyverse, in particular) packages loaded, it will save you a lot of grief.\n\n\n\npurrr::map(got_chars, \"name\")[1:5]\n## [[1]]\n## [1] \"Theon Greyjoy\"\n## \n## [[2]]\n## [1] \"Tyrion Lannister\"\n## \n## [[3]]\n## [1] \"Victarion Greyjoy\"\n## \n## [[4]]\n## [1] \"Will\"\n## \n## [[5]]\n## [1] \"Areo Hotah\"\npurrr::map_chr(got_chars, \"name\")[1:5]\n## [1] \"Theon Greyjoy\"     \"Tyrion Lannister\"  \"Victarion Greyjoy\"\n## [4] \"Will\"              \"Areo Hotah\"\n\nSimilar shortcuts work to get the nth item in each sub list:\n\npurrr::map_chr(got_chars, 4)\n##  [1] \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Female\"\n##  [9] \"Female\" \"Male\"   \"Female\" \"Male\"   \"Female\" \"Male\"   \"Male\"   \"Male\"  \n## [17] \"Female\" \"Female\" \"Female\" \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Male\"  \n## [25] \"Male\"   \"Female\" \"Male\"   \"Male\"   \"Male\"   \"Female\"\n\nSpecifying the output type using e.g. map_chr works if each item in the list is an atomic vector of length 1. If the list is more complicated, though, these shortcuts will issue an error:\n\npurrr::map(got_chars, \"books\")[1:5]\n## [[1]]\n## [1] \"A Game of Thrones\" \"A Storm of Swords\" \"A Feast for Crows\"\n## \n## [[2]]\n## [1] \"A Feast for Crows\"         \"The World of Ice and Fire\"\n## \n## [[3]]\n## [1] \"A Game of Thrones\" \"A Clash of Kings\"  \"A Storm of Swords\"\n## \n## [[4]]\n## [1] \"A Clash of Kings\"\n## \n## [[5]]\n## [1] \"A Game of Thrones\" \"A Clash of Kings\"  \"A Storm of Swords\"\npurrr::map_chr(got_chars, \"books\")[1:5]\n## Error in `stop_bad_type()`:\n## ! Result 1 must be a single string, not a character vector of length 3\n\nWhat if we want to extract several things? This trick works off of the idea that [ is a function: that is, the single brackets we used before are actually a special type of function. In R functions, there is often the argument ..., which is a convention that allows us to pass arguments to other functions that are called within the main function we are using (you’ll see … used in plotting and regression functions frequently as well).\nHere, we use ... to pass in our list of 3 things we want to pull from each item in the list.\n\npurrr::map(got_chars, `[`, c(\"name\", \"gender\", \"born\"))[1:5]\n## [[1]]\n## [[1]]$name\n## [1] \"Theon Greyjoy\"\n## \n## [[1]]$gender\n## [1] \"Male\"\n## \n## [[1]]$born\n## [1] \"In 278 AC or 279 AC, at Pyke\"\n## \n## \n## [[2]]\n## [[2]]$name\n## [1] \"Tyrion Lannister\"\n## \n## [[2]]$gender\n## [1] \"Male\"\n## \n## [[2]]$born\n## [1] \"In 273 AC, at Casterly Rock\"\n## \n## \n## [[3]]\n## [[3]]$name\n## [1] \"Victarion Greyjoy\"\n## \n## [[3]]$gender\n## [1] \"Male\"\n## \n## [[3]]$born\n## [1] \"In 268 AC or before, at Pyke\"\n## \n## \n## [[4]]\n## [[4]]$name\n## [1] \"Will\"\n## \n## [[4]]$gender\n## [1] \"Male\"\n## \n## [[4]]$born\n## [1] \"\"\n## \n## \n## [[5]]\n## [[5]]$name\n## [1] \"Areo Hotah\"\n## \n## [[5]]$gender\n## [1] \"Male\"\n## \n## [[5]]$born\n## [1] \"In 257 AC or before, at Norvos\"\n\nIf this is ugly syntax to you, that’s fine - the magrittr package also includes an extract function that works the same way.\n\npurrr::map(got_chars, magrittr::extract, c(\"name\", \"gender\", \"born\"))[1:5]\n## [[1]]\n## [[1]]$name\n## [1] \"Theon Greyjoy\"\n## \n## [[1]]$gender\n## [1] \"Male\"\n## \n## [[1]]$born\n## [1] \"In 278 AC or 279 AC, at Pyke\"\n## \n## \n## [[2]]\n## [[2]]$name\n## [1] \"Tyrion Lannister\"\n## \n## [[2]]$gender\n## [1] \"Male\"\n## \n## [[2]]$born\n## [1] \"In 273 AC, at Casterly Rock\"\n## \n## \n## [[3]]\n## [[3]]$name\n## [1] \"Victarion Greyjoy\"\n## \n## [[3]]$gender\n## [1] \"Male\"\n## \n## [[3]]$born\n## [1] \"In 268 AC or before, at Pyke\"\n## \n## \n## [[4]]\n## [[4]]$name\n## [1] \"Will\"\n## \n## [[4]]$gender\n## [1] \"Male\"\n## \n## [[4]]$born\n## [1] \"\"\n## \n## \n## [[5]]\n## [[5]]$name\n## [1] \"Areo Hotah\"\n## \n## [[5]]$gender\n## [1] \"Male\"\n## \n## [[5]]$born\n## [1] \"In 257 AC or before, at Norvos\"\n\nWhat if we want this to be a data frame instead? We can use map_dfr to get a data frame that is formed by row-binding each element in the list.\n\npurrr::map_dfr(got_chars, `[`, c(\"name\", \"gender\", \"born\")) \n## # A tibble: 30 × 3\n##    name               gender born                                    \n##    &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;                                   \n##  1 Theon Greyjoy      Male   \"In 278 AC or 279 AC, at Pyke\"          \n##  2 Tyrion Lannister   Male   \"In 273 AC, at Casterly Rock\"           \n##  3 Victarion Greyjoy  Male   \"In 268 AC or before, at Pyke\"          \n##  4 Will               Male   \"\"                                      \n##  5 Areo Hotah         Male   \"In 257 AC or before, at Norvos\"        \n##  6 Chett              Male   \"At Hag's Mire\"                         \n##  7 Cressen            Male   \"In 219 AC or 220 AC\"                   \n##  8 Arianne Martell    Female \"In 276 AC, at Sunspear\"                \n##  9 Daenerys Targaryen Female \"In 284 AC, at Dragonstone\"             \n## 10 Davos Seaworth     Male   \"In 260 AC or before, at King's Landing\"\n## # … with 20 more rows\n\n# Equivalent to\npurrr::map(got_chars, `[`, c(\"name\", \"gender\", \"born\")) %&gt;%\n  dplyr::bind_rows()\n## # A tibble: 30 × 3\n##    name               gender born                                    \n##    &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;                                   \n##  1 Theon Greyjoy      Male   \"In 278 AC or 279 AC, at Pyke\"          \n##  2 Tyrion Lannister   Male   \"In 273 AC, at Casterly Rock\"           \n##  3 Victarion Greyjoy  Male   \"In 268 AC or before, at Pyke\"          \n##  4 Will               Male   \"\"                                      \n##  5 Areo Hotah         Male   \"In 257 AC or before, at Norvos\"        \n##  6 Chett              Male   \"At Hag's Mire\"                         \n##  7 Cressen            Male   \"In 219 AC or 220 AC\"                   \n##  8 Arianne Martell    Female \"In 276 AC, at Sunspear\"                \n##  9 Daenerys Targaryen Female \"In 284 AC, at Dragonstone\"             \n## 10 Davos Seaworth     Male   \"In 260 AC or before, at King's Landing\"\n## # … with 20 more rows\n\nIf we want to more generally convert the entire data set to a data frame, we can use a couple of handy functions to do that:\n\n\npurrr::transpose transposes a list, so that x[[1]][[2]] becomes x[[2]][[1]]. This turns the list into a set of columns.\n\ntibble::as_tibble turns an object into a tibble. This creates a rectangular, data frame like structure\n\npurrr::unnest takes columns and “ungroups” them, so that each entry in the sub-lists of the column gets a row in the data frame. Here, I’ve used this to unwrap lists that are all single items so that we can see some of the data.\n\nThese operations make the data look more or less like it does in Python by default.\n\ngot_df &lt;- got_chars %&gt;%\n  transpose() %&gt;%\n  as_tibble() %&gt;%\n  unnest(c(\"url\", \"id\", \"name\", \"gender\", \n           \"culture\", \"born\", \"died\", \"alive\", \n           \"father\", \"mother\", \"spouse\"))\n\n\n\nIn python, because Pandas read things in as a DataFrame, we don’t need to go to any great measures to get characters names - we can just access the column.\n\ngot_chars.name[0:5]\n## 0        Theon Greyjoy\n## 1     Tyrion Lannister\n## 2    Victarion Greyjoy\n## 3                 Will\n## 4           Areo Hotah\n## Name: name, dtype: object\n\nSimilarly, we can use normal indexing to access the fourth item in each sub-list (which corresponds to the 4th column), assuming we remember that Python is 0-indexed, so we need the column with index 3.\n\ngot_chars.iloc[0:5,3] \n## 0    Male\n## 1    Male\n## 2    Male\n## 3    Male\n## 4    Male\n## Name: gender, dtype: object\n\nBut what happens when our columns are made up of more complicated objects, like when we examine which books each character appears in?\n\ngot_chars.books[0:5]\n## 0    [A Game of Thrones, A Storm of Swords, A Feast...\n## 1       [A Feast for Crows, The World of Ice and Fire]\n## 2    [A Game of Thrones, A Clash of Kings, A Storm ...\n## 3                                     A Clash of Kings\n## 4    [A Game of Thrones, A Clash of Kings, A Storm ...\n## Name: books, dtype: object\n\nThis is a little bit messier. What if we want to know the first book the character appears in? We have to handle the fact that some entries are lists, some are strings, and some are entirely empty. We can use list comprehensions to do this in shorthand, or we can write out a full for loop:\n\nz1 = [x if type(x) == str else x[0] if len(x) &gt; 0  else \"\" for x in got_chars.books]\n\nz2 = ['']*len(got_chars.books)\nfor i in got_chars.books.index:\n  j = got_chars.books[i]\n  if type(j) == str:\n    z2[i] = j\n  elif len(j) &gt; 0:\n    z2[i] = j[0]\n  else:\n    z2[i] = ''\nz1\n\n# Check, both methods are the same\n## ['A Game of Thrones', 'A Feast for Crows', 'A Game of Thrones', 'A Clash of Kings', 'A Game of Thrones', 'A Game of Thrones', 'A Storm of Swords', 'A Game of Thrones', 'A Feast for Crows', 'A Feast for Crows', '', 'A Game of Thrones', 'A Game of Thrones', 'A Game of Thrones', 'A Storm of Swords', 'A Feast for Crows', 'A Clash of Kings', 'A Feast for Crows', 'A Game of Thrones', 'A Clash of Kings', 'A Game of Thrones', 'A Storm of Swords', 'A Feast for Crows', 'A Game of Thrones', 'A Game of Thrones', 'A Clash of Kings', 'A Game of Thrones', 'A Game of Thrones', 'A Game of Thrones', 'A Dance with Dragons']\nz1 == z2\n\n  \n## True\n\nThe first version is more compact and more ‘pythonic’, but the second is more readable.\n\n\n\n\n16.4.3 Map inside Mutate\nA very powerful way to work with data is to use a map function inside of a mutate statement: to simplify data and create a new column all in one go. Let’s use this to create a more human-readable (though somewhat less “clean”) data frame:\n\nfunction to simplify a character list-column,\n\nreplace any 0-length/NULL entries with an empty string\npaste all of the entries together, separated by “,”\nensure that the resulting list is coerced to a character vector\n\n\nThen, we can apply the above function to each list column in our data frame.\n\n\n\nR\nPython\n\n\n\n\npaste_entries &lt;- function(x) {\n  # Replace any null entries of x with \"\"\n  x[map_int(x, length) == 0] &lt;- \"\"\n  \n  map_chr(x, ~paste(., collapse = \", \"))\n}\n\ngot_df &lt;- got_df %&gt;%\n  mutate(across(where(is.list), paste_entries))\n\n\n\n\nimport numpy as np\n\ndef paste_entries(x):\n  # Copy x to y so that x is not modified directly\n  y = x.copy()\n  \n  # Replace any null entries of x with \"\"\n  ylen = np.array([len(i) for i in x])\n  y[ylen == 0] = \"\"\n  \n  # if the entry in y is a list, collapse it, otherwise do nothing\n  # this for loop iterates over the index and the item at the same time\n  for idx, i in enumerate(y):\n    if isinstance(i, list):\n      y[idx] = ', '.join(map(str, i))\n  \n  return y\n\n## Approach 1: split the data frame then rejoin\n# split off non-object columns\ndf1 = got_chars.select_dtypes(exclude=['object'])\ndf2 = got_chars.select_dtypes(include=['object'])\n# apply our function to object columns\ndf2fix = df2.apply(paste_entries, axis = 1)\n# join back together\ngot_df = pd.concat([df2fix, df1], axis = 1)\n\n## Approach 2: keep the data frame together and mutate only some cols\n# Get a logical vector of columns to mutate\ndfcols = got_df.columns[np.array(got_chars.dtypes == 'object')]\n# Avoid copy on write errors by copying the data frame to a new object\ngot_df = got_chars.copy()\n# Replace any columns that are objects with the pasted version\ngot_df[dfcols] = got_df[dfcols].apply(paste_entries, axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: object of type 'int' has no len()"
  },
  {
    "objectID": "functional-programming.html#creating-and-using-list-columns",
    "href": "functional-programming.html#creating-and-using-list-columns",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.5 Creating (and Using) List-columns",
    "text": "16.5 Creating (and Using) List-columns\nData structures in R are typically list-based in one way or another. Sometimes, more complicated data structures are actually lists of lists, or tibbles with a list-column, or other variations on “list within a ____”. In combination with purrr, this is an incredibly powerful setup that can make working with simulations and data very easy. In Python, list columns come along with pandas by default and are incredibly common when you’re working with data stored as a JSON, YAML, XML, and other markup formats.\n\n\n\n\n\n\nExample: Benefits of List columns\n\n\n\n\n\nR\nPython\n\n\n\nSuppose, for instance, I want to simulate some data for modeling purposes, where I can control the number of outliers in the dataset:\n\ndata_sim &lt;- function(n_outliers = 0) {\n  tmp &lt;- tibble(x = seq(-10, 10, .1),\n                y = rnorm(length(x), mean = x, sd = 1))\n  \n  \n  outlier_sample &lt;- c(NULL, sample(tmp$x, n_outliers))\n  \n  # Create outliers\n  tmp %&gt;% \n    mutate(\n      is_outlier = x %in% outlier_sample,\n      y = y + is_outlier * sample(c(-1, 1), n(), replace = T) * runif(n(), 5, 10)\n    )\n}\ndata_sim()\n## # A tibble: 201 × 3\n##        x      y is_outlier\n##    &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt;     \n##  1 -10   -10.8  FALSE     \n##  2  -9.9 -11.4  FALSE     \n##  3  -9.8  -8.86 FALSE     \n##  4  -9.7  -9.52 FALSE     \n##  5  -9.6  -9.36 FALSE     \n##  6  -9.5  -7.88 FALSE     \n##  7  -9.4  -9.29 FALSE     \n##  8  -9.3  -9.43 FALSE     \n##  9  -9.2 -11.1  FALSE     \n## 10  -9.1  -9.38 FALSE     \n## # … with 191 more rows\n\nNow, lets suppose that I want 100 replicates of each of 0, 5, 10, and 20 outliers.\n\nsim &lt;- crossing(rep = 1:100, n_outliers = c(0, 5, 10, 20)) %&gt;%\n  mutate(sim_data = purrr::map(n_outliers, data_sim))\n\nI could use unnest(sim_data) if I wanted to expand my data a bit to see what I have, but in this case, it’s more useful to leave it in its current, compact form. Instead, suppose I fit a linear regression to each of the simulated data sets, and store the fitted linear regression object in a new list-column?\n\nsim &lt;- sim %&gt;%\n  mutate(reg = purrr::map(sim_data, ~lm(data = ., y ~ x)))\n\nHere, we use an anonymous function in purrr: by using ~{expression}, we have defined a function that takes the argument . (which is just a placeholder). So in our case, we’re saying “use the data that I pass in to fit a linear regression of y using x as a predictor”.\nLet’s play around a bit with this: We might want to look at our regression coefficients or standard errors to see how much the additional outliers affect us. We could use a fancy package for tidy modeling, such as broom, but for now, lets do something a bit simpler and apply the purrr name extraction functions we used earlier.\nIt can be helpful to examine one of the objects just to see what you’re dealing with:\n\nstr(sim$reg[[1]])\n## List of 12\n##  $ coefficients : Named num [1:2] -0.117 1.001\n##   ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"x\"\n##  $ residuals    : Named num [1:201] 0.584 -0.721 0.696 1.282 -0.627 ...\n##   ..- attr(*, \"names\")= chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n##  $ effects      : Named num [1:201] 1.662 82.367 0.733 1.318 -0.591 ...\n##   ..- attr(*, \"names\")= chr [1:201] \"(Intercept)\" \"x\" \"\" \"\" ...\n##  $ rank         : int 2\n##  $ fitted.values: Named num [1:201] -10.13 -10.03 -9.93 -9.83 -9.73 ...\n##   ..- attr(*, \"names\")= chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n##  $ assign       : int [1:2] 0 1\n##  $ qr           :List of 5\n##   ..$ qr   : num [1:201, 1:2] -14.1774 0.0705 0.0705 0.0705 0.0705 ...\n##   .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. ..$ : chr [1:201] \"1\" \"2\" \"3\" \"4\" ...\n##   .. .. ..$ : chr [1:2] \"(Intercept)\" \"x\"\n##   .. ..- attr(*, \"assign\")= int [1:2] 0 1\n##   ..$ qraux: num [1:2] 1.07 1.11\n##   ..$ pivot: int [1:2] 1 2\n##   ..$ tol  : num 1e-07\n##   ..$ rank : int 2\n##   ..- attr(*, \"class\")= chr \"qr\"\n##  $ df.residual  : int 199\n##  $ xlevels      : Named list()\n##  $ call         : language lm(formula = y ~ x, data = .)\n##  $ terms        :Classes 'terms', 'formula'  language y ~ x\n##   .. ..- attr(*, \"variables\")= language list(y, x)\n##   .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. ..$ : chr [1:2] \"y\" \"x\"\n##   .. .. .. ..$ : chr \"x\"\n##   .. ..- attr(*, \"term.labels\")= chr \"x\"\n##   .. ..- attr(*, \"order\")= int 1\n##   .. ..- attr(*, \"intercept\")= int 1\n##   .. ..- attr(*, \"response\")= int 1\n##   .. ..- attr(*, \".Environment\")=&lt;environment: 0x55c0f382a350&gt; \n##   .. ..- attr(*, \"predvars\")= language list(y, x)\n##   .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. ..- attr(*, \"names\")= chr [1:2] \"y\" \"x\"\n##  $ model        :'data.frame':   201 obs. of  2 variables:\n##   ..$ y: num [1:201] -9.55 -10.75 -9.23 -8.55 -10.36 ...\n##   ..$ x: num [1:201] -10 -9.9 -9.8 -9.7 -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 ...\n##   ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language y ~ x\n##   .. .. ..- attr(*, \"variables\")= language list(y, x)\n##   .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n##   .. .. .. ..- attr(*, \"dimnames\")=List of 2\n##   .. .. .. .. ..$ : chr [1:2] \"y\" \"x\"\n##   .. .. .. .. ..$ : chr \"x\"\n##   .. .. ..- attr(*, \"term.labels\")= chr \"x\"\n##   .. .. ..- attr(*, \"order\")= int 1\n##   .. .. ..- attr(*, \"intercept\")= int 1\n##   .. .. ..- attr(*, \"response\")= int 1\n##   .. .. ..- attr(*, \".Environment\")=&lt;environment: 0x55c0f382a350&gt; \n##   .. .. ..- attr(*, \"predvars\")= language list(y, x)\n##   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n##   .. .. .. ..- attr(*, \"names\")= chr [1:2] \"y\" \"x\"\n##  - attr(*, \"class\")= chr \"lm\"\n\nIf we pull out the coefficients by name we get a vector of length two. So before we unnest, we need to change that so that R formats it as a row of a data frame.\n\nsim$reg[[1]]$coefficients %&gt;% as_tibble_row()\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1        -0.117  1.00\n\nThis will make our formatting a lot easier and prevent any duplication that might occur if we unnest a vector that has length &gt; 1.\n\nsim &lt;- sim %&gt;%\n  mutate(coefs = purrr::map(reg, \"coefficients\") %&gt;%\n           purrr::map(as_tibble_row))\n\nsim$coefs[1:5]\n## [[1]]\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1        -0.117  1.00\n## \n## [[2]]\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1        -0.105 0.977\n## \n## [[3]]\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1        -0.125 0.961\n## \n## [[4]]\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1         0.211 0.966\n## \n## [[5]]\n## # A tibble: 1 × 2\n##   `(Intercept)`     x\n##           &lt;dbl&gt; &lt;dbl&gt;\n## 1         0.108  1.01\n\nThen, we can plot our results:\n\nsim %&gt;%\n  unnest(coefs) %&gt;%\n  select(rep, n_outliers, `(Intercept)`, x) %&gt;%\n  pivot_longer(-c(rep, n_outliers), names_to = \"coef\", values_to = \"value\") %&gt;%\n  ggplot(aes(x = value, color = factor(n_outliers))) + geom_density() + \n  facet_wrap(~coef, scales = \"free_x\")\n\n\n\n\nSo as there are more and more outliers, the coefficient estimates get a wider distribution, but remain (relatively) centered on the “true” values of 0 and 1, respectively.\nNotice that we keep our data in list column form right up until it is time to actually unnest it - which means that we have at the ready the simulated data, the simulated model, and the conditions under which it was simulated, all in the same data structure. It’s a really nice, organized system.\n\n\n\ndef data_sim(n_outliers = 0):\n  tmp = pd.DataFrame({'x': range(-100, 100, 1), 'y': 0})\n  tmp['x'] = tmp.x/10\n  tmp['y'] = np.random.normal(size = len(tmp.x), loc = tmp.x, scale = 1)\n  tmp['is_outlier'] = False\n  \n  # Choose sample\n  outlier_sample = list(np.random.choice(tmp.index, size = n_outliers))\n  tmp.loc[outlier_sample, 'is_outlier'] = True\n  # Create outliers\n  tmp['y'] = tmp.y + tmp.is_outlier * np.random.choice([-1,1], len(tmp.index), replace = True) * (np.random.sample(size = len(tmp.index))*5 + 5)\n  \n  return tmp\n\ndata_sim()\n##         x          y  is_outlier\n## 0   -10.0  -8.615630       False\n## 1    -9.9  -9.282386       False\n## 2    -9.8 -11.125694       False\n## 3    -9.7  -9.517860       False\n## 4    -9.6  -9.653798       False\n## ..    ...        ...         ...\n## 195   9.5  11.883267       False\n## 196   9.6   9.297179       False\n## 197   9.7   7.945535       False\n## 198   9.8  11.113908       False\n## 199   9.9   9.923770       False\n## \n## [200 rows x 3 columns]\n\nNow, lets suppose that I want 100 replicates of each of 0, 5, 10, and 20 outliers.\n\nimport itertools as it\n\nsim = pd.DataFrame(it.product(range(100), [0,5,10,20]))\nsim.columns = ['rep', 'n_outliers']\nsim['data'] = sim.n_outliers.apply(data_sim)\n\nI could use sim.explode('data'), which is the pandas equivalent of unnest() in R, if I wanted to expand my data a bit to see what I have, but in this case, it’s more useful to leave it in its current, compact form. Instead, suppose I fit a linear regression to each of the simulated data sets, and store the fitted linear regression object in a new column?\n\nfrom sklearn.linear_model import LinearRegression\n\n# This function just makes it a bit simpler to run linear regressions \n# by moving the reshaping from pd.Series to np.array inside the function.\ndef lreg(df):\n  X = df.loc[:,'x'].values.reshape(-1,1)\n  Y = df.loc[:,'y'].values.reshape(-1,1)\n  lregobj = LinearRegression()\n  lregobj.fit(X, Y)\n  return lregobj\n\nsim['reg'] = sim.data.apply(lreg)\n\nIn a more simple example, we could use an anonymous lambda function here as well (I’ve demonstrated this in other places in this book), but it’s a good idea to know when to use anonymous functions, and if they take more than a line of code, it’s better to define a named function instead.\nLet’s play around a bit with this: We might want to look at our regression coefficients or standard errors to see how much the additional outliers affect us.\nIt can be helpful to examine one of the objects just to see what you’re dealing with:\n\nsim.reg[0]\n\n# We can use the regression object to make predictions\n\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLinearRegressionLinearRegression()\n\n\n\nsim.reg[0].predict(sim.data[0].x.values.reshape(-1,1))[0:10]\n## array([[-9.96234581],\n##        [-9.86210883],\n##        [-9.76187185],\n##        [-9.66163487],\n##        [-9.56139788],\n##        [-9.4611609 ],\n##        [-9.36092392],\n##        [-9.26068694],\n##        [-9.16044995],\n##        [-9.06021297]])\nsim.reg[0].coef_ # Slope\n## array([[1.00236982]])\nsim.reg[0].intercept_ # Intercept\n## array([0.06135242])\n\nIf we pull out the coefficients by name we get a vector of length two. So before we unnest, we need to change that so that R formats it as a row of a data frame.\n\nsim['coef'] = sim.reg.apply(lambda x: x.coef_[0])\nsim['intercept'] = sim.reg.apply(lambda x: x.intercept_)\nsim[['coef','intercept']]\n##                      coef                intercept\n## 0    [1.0023698231233278]    [0.06135241846652731]\n## 1    [1.0110948633547137]    [-0.3352445668499911]\n## 2    [1.0052749068195836]   [-0.20413176000098113]\n## 3    [1.0302173118405402]   [-0.11580709112777833]\n## 4    [0.9866128945470576]   [0.011473162003821986]\n## ..                    ...                      ...\n## 395  [0.9858802082967044]  [-0.011756628213911928]\n## 396  [0.9949013921221167]    [0.03225461650765866]\n## 397  [1.0170495910756496]    [0.05694564748206274]\n## 398   [1.012342843368514]    [0.07840335880278498]\n## 399  [0.9847894940809924]   [-0.12237977359705118]\n## \n## [400 rows x 2 columns]\n\nThen, we can plot our results:\n\nsimcoefs = sim.explode(['coef', 'intercept'])\ntmp = pd.melt(simcoefs[['rep', 'n_outliers', 'intercept', 'coef']], ['rep', 'n_outliers'])\n\nimport seaborn as sns\ng = sns.FacetGrid(tmp, col = 'variable', hue = 'n_outliers', sharex = False)\ng.map_dataframe(sns.kdeplot, x='value', palette = \"crest\")\n\n\n\ng.add_legend()\n\n\n\nplt.show()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'plt' is not defined\n\nSo as there are more and more outliers, the coefficient estimates get a wider distribution, but remain (relatively) centered on the “true” values of 0 and 1, respectively.\nNotice that we keep our data in nested form right up until it is time to actually “explode” it - which means that we have at the ready the simulated data, the simulated model, and the conditions under which it was simulated, all in the same data structure. It’s a really nice, organized system."
  },
  {
    "objectID": "functional-programming.html#ways-to-use-map-and-apply",
    "href": "functional-programming.html#ways-to-use-map-and-apply",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.6 Ways to use map and apply\n",
    "text": "16.6 Ways to use map and apply\n\nThere are 3 main use cases for map (and its cousins pmap, map2, etc.):\n\nUse with an existing function\nUse with an anonymous function, defined on the fly\nUse with a formula (which is just a concise way to define an anonymous function)\n\nTwo of these use cases also are relevant to apply in python:\n\nUse with an existing function\nUse with an anonymous (lambda) function, defined on the fly\n\n(Python doesn’t seem to use formulas in quite the same way R does)\nI’ll use a trivial example to show the difference between these options:\n\n\nR\nPython\n\n\n\n\n# An existing function\nres &lt;- tibble(x = 1:10, y1 = map_dbl(x, log10))\n\n# An anonymous function\nres &lt;- res %&gt;% mutate(y2 = map_dbl(x, function(z) z^2/10))\n\n# A formula equivalent to function(z) z^5/(z + 10)\n# the . is the variable you're manipulating\nres &lt;- res %&gt;% mutate(y3 = map_dbl(x, ~.^5/(.+10)))\n\nIt can be a bit tricky to differentiate between options 2 and 3 in practice - the biggest difference is that you’re not using the keyword function and your variable is the default placeholder variable . used in the tidyverse.\n\n\n\nres = pd.DataFrame({'x': range(10)})\n\nres['y'] = res.x.apply(np.log10)\n\nres['y1'] = res.x.apply(lambda x: x**2/10)\n\nThe lambda function is something of a hybrid between anonymous functions and formulas in R: it’s defined on the fly, but has a bit more structure than the formula option in R.\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nCreate a new column containing a single string of all of the books each character was in.\nTo do this, you’ll need to collapse the list of books for each character into a single string, which you can do with the paste function and the collapse argument in R, or with some modification to the paste_entries function we defined earlier in Python, which I’ve copied here for convenience. (The function won’t work out of the box, because it was designed to work on each column of a DataFrame, and here we’d be applying it to each row.)\n\nletters[1:10] %&gt;% paste(collapse = \"|\")\n## [1] \"a|b|c|d|e|f|g|h|i|j\"\n\n\n\ndef paste_entries(x):\n  # Copy x to y so that x is not modified directly\n  y = x.copy()\n  \n  # Replace any null entries of x with \"\"\n  ylen = np.array([len(i) for i in x])\n  y[ylen == 0] = \"\"\n  \n  # if the entry in y is a list, collapse it, otherwise do nothing\n  # this for loop iterates over the index and the item at the same time\n  for idx, i in enumerate(y):\n    if isinstance(i, list):\n      y[idx] = ', '.join(map(str, i))\n  \n  return y\n\nx = [chr(i) for i in range(ord('a'), ord('k'))]\nx = pd.Series([x])\nx\n## 0    [a, b, c, d, e, f, g, h, i, j]\n## dtype: object\npaste_entries(x)\n## 0    a, b, c, d, e, f, g, h, i, j\n## dtype: object\n\nStart with this data frame of character names and book list-columns:\n\ndata(got_chars)\n\ngot_df &lt;- tibble(name = map_chr(got_chars, \"name\"),\n                 id = map_int(got_chars, \"id\"),\n                 books = map(got_chars, \"books\"))\n\n\nimport pandas as pd\ngot_chars = pd.read_json('data/got_chars.json')\n\ngot_df = got_chars[[\"name\", \"id\", \"books\"]]\n\n\n\n\n# Define a function\nmy_collapse &lt;- function(x) paste(x, collapse = \" | \")\n\ndata(got_chars)\n\ngot_df &lt;- tibble(name = map_chr(got_chars, \"name\"),\n                 id = map_int(got_chars, \"id\"),\n                 books = map(got_chars, \"books\"))\n\ngot_df &lt;- got_df %&gt;%\n  mutate(\n    fun_def_res = map_chr(books, my_collapse),\n    # Here, I don't have to define a function, I just pass my additional \n    # argument in after the fact...\n    fun_base_res = map_chr(books, paste, collapse = \" | \"),\n    \n    # Here, I can just define a new function without a name and apply it to \n    # each entry\n    fun_anon_res = map_chr(books, function(x) paste(x, collapse = \" | \")),\n    \n    # And here, I don't even bother to specifically say that I'm defining a \n    # function, I just apply a formula to each entry\n    fun_formula_res = map_chr(books, ~paste(., collapse = \" | \"))\n  ) \n\nhead(got_df)\n## # A tibble: 6 × 7\n##   name                 id books     fun_def_res          fun_b…¹ fun_a…² fun_f…³\n##   &lt;chr&gt;             &lt;int&gt; &lt;list&gt;    &lt;chr&gt;                &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n## 1 Theon Greyjoy      1022 &lt;chr [3]&gt; A Game of Thrones |… A Game… A Game… A Game…\n## 2 Tyrion Lannister   1052 &lt;chr [2]&gt; A Feast for Crows |… A Feas… A Feas… A Feas…\n## 3 Victarion Greyjoy  1074 &lt;chr [3]&gt; A Game of Thrones |… A Game… A Game… A Game…\n## 4 Will               1109 &lt;chr [1]&gt; A Clash of Kings     A Clas… A Clas… A Clas…\n## 5 Areo Hotah         1166 &lt;chr [3]&gt; A Game of Thrones |… A Game… A Game… A Game…\n## 6 Chett              1267 &lt;chr [2]&gt; A Game of Thrones |… A Game… A Game… A Game…\n## # … with abbreviated variable names ¹​fun_base_res, ²​fun_anon_res,\n## #   ³​fun_formula_res\n\n\n\nThe equivalent of apply for a single row or column is transform - it is applied rowwise (or columnwise).\nThen we use .assign() to get around the copy-vs-view warnings.\n\n\ndef collapse_entries(x):\n  if len(x) &lt; 1:\n    return x\n  elif isinstance(x, str):\n    return x\n  else: \n    return ' | '.join(map(str, x))\n  \n  return NULL\n\ngot_df = got_df.assign(books_simple = got_df['books'].transform(collapse_entries))"
  },
  {
    "objectID": "functional-programming.html#beyond-map-functions-with-multiple-inputs",
    "href": "functional-programming.html#beyond-map-functions-with-multiple-inputs",
    "title": "16  Lists, Nested Lists, and Functional Programming",
    "section": "\n16.7 Beyond map: Functions with multiple inputs",
    "text": "16.7 Beyond map: Functions with multiple inputs\nSometimes, you might need to map a function over two vectors/lists in parallel. purrr has you covered with the map2 function. As with map, the syntax is map2(thing1, thing2, function, other.args); the big difference is that function takes two arguments.\nIn Python, you can use lambda functions to specify which columns of your data go into which function arguments.\n\n\n\n\n\n\nExample\n\n\n\n\n\nR\nPython\n\n\n\nLet’s create a simple times-table:\n\ncrossing(x = 1:10, y = 1:10) %&gt;%\n  mutate(times = map2_int(x, y, `*`)) %&gt;%\n  pivot_wider(names_from = y, names_prefix = 'y=', values_from = times)\n## # A tibble: 10 × 11\n##        x `y=1` `y=2` `y=3` `y=4` `y=5` `y=6` `y=7` `y=8` `y=9` `y=10`\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;\n##  1     1     1     2     3     4     5     6     7     8     9     10\n##  2     2     2     4     6     8    10    12    14    16    18     20\n##  3     3     3     6     9    12    15    18    21    24    27     30\n##  4     4     4     8    12    16    20    24    28    32    36     40\n##  5     5     5    10    15    20    25    30    35    40    45     50\n##  6     6     6    12    18    24    30    36    42    48    54     60\n##  7     7     7    14    21    28    35    42    49    56    63     70\n##  8     8     8    16    24    32    40    48    56    64    72     80\n##  9     9     9    18    27    36    45    54    63    72    81     90\n## 10    10    10    20    30    40    50    60    70    80    90    100\n# we could use `multiply_by` instead of `*` if we wanted to\n\nIf you are using formula notation to define functions with map2, you will need to refer to your two arguments as .x and .y. You can determine this from the Usage section when you run map2, which shows you map2(.x, .y, .f, ...) - that is, the first argument is .x, the second is .y, and the third is the function.\nLike map, you can specify the type of the output response using map2. This makes it very easy to format the output appropriately for your application.\nYou can use functions with many arguments with map by using the pmap variant; here, you pass in a list of functions, which are identified by position (..1, ..2, ..3, etc). Note the .. - you are referencing the list first, and the index within the list argument 2nd.\n\n\n\n\nimport itertools as it\n\nx10 = pd.DataFrame(it.product(range(11), range(11)))\nx10.columns = ['x', 'y']\nx10['times'] = x10.apply(lambda x: x['x']*x['y'], axis = 1)\n\nx10.pivot(index = 'x', columns = 'y', values = 'times')\n## y   0   1   2   3   4   5   6   7   8   9    10\n## x                                              \n## 0    0   0   0   0   0   0   0   0   0   0    0\n## 1    0   1   2   3   4   5   6   7   8   9   10\n## 2    0   2   4   6   8  10  12  14  16  18   20\n## 3    0   3   6   9  12  15  18  21  24  27   30\n## 4    0   4   8  12  16  20  24  28  32  36   40\n## 5    0   5  10  15  20  25  30  35  40  45   50\n## 6    0   6  12  18  24  30  36  42  48  54   60\n## 7    0   7  14  21  28  35  42  49  56  63   70\n## 8    0   8  16  24  32  40  48  56  64  72   80\n## 9    0   9  18  27  36  45  54  63  72  81   90\n## 10   0  10  20  30  40  50  60  70  80  90  100\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nDetermine if each Game of Thrones character has more titles than aliases. Start with this code:\n\nlibrary(repurrrsive)\nlibrary(tidyverse)\n\ndata(got_chars)\ngot_names &lt;- tibble(name = purrr::map_chr(got_chars, \"name\"),\n                    titles = purrr::map(got_chars, \"titles\"),\n                    aliases = purrr::map(got_chars, \"aliases\"))\n\n\ngot_names = got_chars[['name', 'titles', 'aliases']]\n\n\n\n\ngot_names %&gt;%\n  mutate(more_titles = map2_lgl(titles, aliases, ~length(.x) &gt; length(.y)))\n## # A tibble: 30 × 4\n##    name               titles    aliases    more_titles\n##    &lt;chr&gt;              &lt;list&gt;    &lt;list&gt;     &lt;lgl&gt;      \n##  1 Theon Greyjoy      &lt;chr [3]&gt; &lt;chr [4]&gt;  FALSE      \n##  2 Tyrion Lannister   &lt;chr [2]&gt; &lt;chr [11]&gt; FALSE      \n##  3 Victarion Greyjoy  &lt;chr [2]&gt; &lt;chr [1]&gt;  TRUE       \n##  4 Will               &lt;chr [1]&gt; &lt;chr [1]&gt;  FALSE      \n##  5 Areo Hotah         &lt;chr [1]&gt; &lt;chr [1]&gt;  FALSE      \n##  6 Chett              &lt;chr [1]&gt; &lt;chr [1]&gt;  FALSE      \n##  7 Cressen            &lt;chr [1]&gt; &lt;chr [1]&gt;  FALSE      \n##  8 Arianne Martell    &lt;chr [1]&gt; &lt;chr [1]&gt;  FALSE      \n##  9 Daenerys Targaryen &lt;chr [5]&gt; &lt;chr [11]&gt; FALSE      \n## 10 Davos Seaworth     &lt;chr [4]&gt; &lt;chr [5]&gt;  FALSE      \n## # … with 20 more rows\n\n\n\n\n# Because len(list) and len(string) are both valid in python we have to be more careful\n\ndef cflen(i):\n  if isinstance(i, list):\n    return(len(i))\n  return bool(i) # this is 1 if there's something stored in i, 0 otherwise\n\nnewdf = got_names.copy()\nnewdf['more_titles'] = got_names.apply(lambda x: cflen(x['titles']) &gt; cflen(x['aliases']), axis = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nLearn More About Purrr\n\n\n\n\nThe Joy of Functional Programming (for Data Science): Hadley Wickham’s talk on purrr and functional programming. ~1h video and slides.\n(The Joy of Cooking meets Data Science, with illustrations by Allison Horst)\nPirating Web Content Responsibly with R and purrr (a blog post in honor of international talk like a pirate day) [6]\nHappy R Development with purrr\nWeb mining with purrr\nText Wrangling with purrr\nSetting NAs with purrr (uses the naniar package)\nMappers with purrr - handy ways to make your code simpler if you’re reusing functions a lot.\nFunction factories - code optimization with purrr\nStats and Machine Learning examples with purrr\n\n\n\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nJ. Bryan, “Lessons and Examples,” purrr tutorial. [Online]. Available: https://jennybc.github.io/purrr-tutorial/index.html. [Accessed: Nov. 14, 2022]\n\n\n[3] \nD. B. Koneswarakantha, “Moving from R to python - 2/7 - pandas,” datistics. Aug. 2018 [Online]. Available: https://www.datisticsblog.com/2018/08/r2py_pandas/. [Accessed: Nov. 14, 2022]\n\n\n[4] \nH. Wickham, “Functional programming,” in Advanced R, 2nd ed., Chapman; Hall/CRC, 2019 [Online]. Available: http://adv-r.had.co.nz/Functional-programming.html. [Accessed: Nov. 15, 2022]\n\n\n[5] \nK. Tran, “Write Clean Python Code Using Pipes,” Medium. Oct. 2021 [Online]. Available: https://towardsdatascience.com/write-clean-python-code-using-pipes-1239a0f3abf5. [Accessed: Nov. 15, 2022]\n\n\n[6] \nB. Rudis, “Pirating Web Content Responsibly With R,” rud.is. Sep. 2017 [Online]. Available: https://rud.is/b/2017/09/19/pirating-web-content-responsibly-with-r/. [Accessed: Nov. 16, 2022]"
  },
  {
    "objectID": "app-sas.html#setting-up-sas",
    "href": "app-sas.html#setting-up-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.1 Setting up SAS",
    "text": "A.1 Setting up SAS\nIn this class, you have several options for how to use SAS:\n\nInstall SAS on your machine\n\n\n See Steve Westerholt, pay a small fee, and he will install SAS on your machine.\n\n Steve Westerholt will help you acquire Parallels desktop (to emulate Windows) and SAS. You’ll pay license fees for both products.\n\n Talk with me and I’ll help you interface with Steve to get SAS for Linux installed on your system. Steve doesn’t support Linux installs, but I can help you get SAS set up if you’re using a common distribution like Ubuntu or RedHat/Fedora.\n\n\nUse SAS University Edition/SAS OnDemand for Academics. Get more information here.\n\nPros - this is free\nCons - limited storage, may cause issues with paths and file upload/download (so it may be hard to ensure your homework assignments are reproducible).\n\n\n\nUse the Guacamole environment I’ve set up for the class via HCC. More information will be provided outside the textbook for this option. (This is not an option in Fall 2022)\n\nPros - free, allows you access to SAS and Rstudio on the same machine (so you can use SASmarkdown seamlessly for homework). You can also use this to test the reproducibility of your assignment.\nCons - This interface is only available for Stat850 - it will not be available after the class concludes."
  },
  {
    "objectID": "app-sas.html#looking-around-sas",
    "href": "app-sas.html#looking-around-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.2 Finding Your Way around in SAS",
    "text": "A.2 Finding Your Way around in SAS\nSAS is another extensively used statistical programming language. It is primarily used for mixed models and in the biostatistics community (for e.g. drug trials).\nNote\nSAS looks different on different machines. On Linux, SAS looks like you took a time machine back to the early 1990s. Screenshots from SAS will likely look very different on your machine than on mine. I will try to integrate screenshots from other OS’s where it matters.^[As I write this in late July 2020, I do not yet have access to a Windows or Mac machine. Sigh. XXX TODO: Get Windows/Mac screenshots XXX.\nI still don’t have access to a Windows or Mac machine, and it’s now April 2021. Sigh.]\n\nA.2.1 SAS Windows/Panes\n\n\nThe SAS toolbox has options to create new files, submit code, and more (but I mostly use keyboard shortcuts in the individual windows). This is probably one of the biggest things that’s different on Linux…\n\n\n\nThe SAS Log gives you lots of detailed information about how your code ran – look here for errors and warnings.\n\n\n\nThe SAS program editor is where you’ll be writing your code. If you want, you can write code in a more advanced text editor and copy/paste it into the log when you’re ready to run it.\n\nThere are two different places your output may end up: if you’re using the old output system, you’ll get text output in the output pane.\n\n\nThe old output system output pane.\n\n\n\nThe new output system uses HTML and will output to your browser.\n\n\n\nYou can navigate through your results using the results window\n\n\n\nThe explorer window lets you access documentation, data, and more\n\nIf you click on libraries, you get to this list:\n\n\nThis area of the SAS explorer shows all of the libraries you have access to.\n\n\n\nClicking on any one library will show you a list of datasets in that library\n\nYou can then click on a dataset and you will get a spreadsheet-like view.\n\nA.2.2 SAS Modules\nSAS is a very large set of programs. In this class, we’re primarily working with base SAS (the underlying language and interpreter), SAS/STAT (the statistical procedures, like PROC GLM), and SAS/IML, which is SAS’s version of a scripting language. IML allows you to implement your own procedures “from scratch”.\nInitially, we’ll primarily focus on SAS/IML, because it contains information parallel to what you need to know to start programming in R. It’s easier to teach general programming concepts at the same time, even if your typical SAS course would probably introduce you to the DATA step and simple PROC statements first."
  },
  {
    "objectID": "app-sas.html#sas-computer-basics",
    "href": "app-sas.html#sas-computer-basics",
    "title": "Appendix A — SAS",
    "section": "\nA.3 SAS computer basics",
    "text": "A.3 SAS computer basics\nIn SAS, you can set your working directory in the Program Editor, Log, or Explorer windows by clicking on Tools &gt; Options &gt; Change Directory, navigating to your preferred directory, and clicking ok.\n\nIn SAS, you’ll want to store your .sas code file in the RStudio project folder as well, and I believe that should be sufficient to set your working directory for any SAS code you may write as part of e.g. a homework assignment."
  },
  {
    "objectID": "app-sas.html#introduction-to-sas-programming",
    "href": "app-sas.html#introduction-to-sas-programming",
    "title": "Appendix A — SAS",
    "section": "\nA.4 Introduction to SAS Programming",
    "text": "A.4 Introduction to SAS Programming\nA few notes on programming concepts and R vs. SAS\n\n\nHistorical view\n\nBoth R and SAS have long histories. SAS in particular dates back to the 1960s, and has syntax which is unique compared to more modern languages such as C, python, Java, and R. R’s predecessor, S, dates back to 1976 and was designed for internal use at Bell Labs. The histories of both languages are useful in understanding why they are optimized for their respective tasks, but are not essential for this course (so read them at your leisure).\n\n\nDifference between R and SAS\n\nThe biggest difference between R and SAS (at a fundamental level) is that R is a functional language - it consists mainly of functions, which can (and do) manipulate objects, including other functions. SAS, on the other hand, is a procedural language - most SAS programs follow a specific series of steps, known as “proc”s. Procs are essentially functions (or compositions of multiple functions), but in SAS, it is simpler to think of an analysis as a series of procedural steps; in R, there are steps, but they may be implemented in a more flexible way (depending on the analysis).\n\n\nAnother interesting feature of SAS is that it’s really several languages - some commands work in PROC IML (interactive matrix language) but not in a DATA step. When looking for help in SAS, make sure you’re referencing the correct part of the language documentation.\n\n\n\nI’m teaching SAS very differently\n\nI’m definitely teaching SAS differently than it is normally taught. This is so that we don’t have to do half the semester in SAS and half in R - I’d rather teach the concepts and show you how they’re implemented than split them up by language. BUT, this means that some of the things we’re doing first in SAS are things you wouldn’t normally do until you were already proficient in SAS. It also means that SAS is probably going to seem even more oddly organized when taught this way than it actually is (and it is oddly organized, in my opinion).\n\n\nWe’re going to start with SAS IML (programming concepts) and then talk about the DATA step. We’ll use some procedures implicitly along the way, but hopefully that will make sense in context. Then, we’ll work on the PROCs (SQL, Transpose, and graphing) - in greater detail.\n\n\n\n\nA.4.1 Variable Types in SAS\nIn SAS, there are two basic variable types: numeric and character variables. SAS does not differentiate between integers and floats and doubles. Functionally, though, the same basic operations can be performed in SAS. As with R, SAS does attempt to implicitly convert variable types, and will notify you that the conversion has taken place in the log file.\nType Conversions\nSAS will attempt to implicitly convert variables when:\n\na character value is assigned to a previously defined numeric variable\na character value is used in arithmetic operations\na character value is compared to a numeric value using a comparison operator (&lt;, &gt;, &lt;=, &gt;=)\na character value is specified in a function that takes numeric arguments\n\n\nImplicit conversion does not occur in WHERE statements. (This will make more sense later, but is here for reference)\n\n\nManual type conversions\nIf you want to manually convert a value, use the INPUT statement. Unlike in R, the INPUT statement has the ability to read numbers which are formatted differently. For instance\ndata set1;\n  x = 3;\n  y = '3.1415';\n  z = x * y;\n  put z;\nrun;\ndata set2;\n  x = 3;\n  y = '3.1415';\n  z = x * y;\n  put z; /* print to log */\n\n  x = '3.14159';\n  /* x previously had a number in it, \n     so it will be converted to a number here */\n  put x; /* print to log */\n\n  zz = y &lt;= 2;\n  /* comparison operator: y will be converted */\n  put zz; /* print to log */\n  \nrun;\nNotice that in SAS, zz, which is the result of the logical statement y&lt;=2, is a numeric variable. The value 0 signifies that the comparison was false. SAS does not have a logical data type, it uses the numeric variable with 0:=FALSE, 1:=TRUE.\nTry it out\n\nCreate variables string1 and string2 that each have text/character values. “Bob” and “Jane” might be good options. How does logical operation work with actual character values?\nWhat happens if you use string1 and add 3 to it?\n\n\nSolutions\ndata set1; \n  string1 = 'Bob';\n  string2 = 'Jane';\n  x = string1 &lt; string2;\n  put x=; /* This prints the result to the log */\nrun;\n\nSAS will actually compare strings based on the first letter: Bob comes before Jane, so Bob &lt; Jane.\ndata set2;\n  string1 = 'Bob';\n  y = string1 + 3;\n  put y=;\nrun;\nThe . in SAS is a missing value (like NA in R). So SAS is behaving basically like R does: it complains about the fact that you asked it to add a string to a number, and then it stores the result as a missing value.\nBasic List Syntax in SAS\nThere are also lists in SAS IML which function similarly to lists in R. To create a named object in a list, precede the name with #. In SAS, the $ operator can be used to get items from a list, using either name or numeric references.\n\nproc iml;\n  grocery_list = [\n    #dairy  = [\"asiago\", \"fontina\", \"mozzarella\", \"blue cheese\"], \n    #baking = [\"flour\", \"yeast\", \"salt\"], \n    #canned = [\"pepperoni\", \"pizza sauce\", \"olives\"], \n    #meat   = [\"bacon\", \"sausage\", \"anchovies\"], \n    #veggies= [\"bell pepper\", \"onion\", \"scallions\", \"tomatoes\", \"basil\"]\n  ];\n\n  /* print only works on matrices and vectors */\n  /* so we'll cheat and load another library to print lists */\n  \n  package load ListUtil;\n  \n  /* run ListPrint(grocery_list); */ \n  /* This would print the thing, but it's long */\n  \n  ick = [grocery_list$\"canned\"$3, grocery_list$4$2, grocery_list$4$3];\n  crust = grocery_list$\"baking\";\n  call ListAddItem(crust, \"water\"); /* add an item to a list */\n  essential_toppings = [grocery_list$\"dairy\"$3, grocery_list$\"canned\"$2];\n  yummy_toppings = [grocery_list$\"dairy\"$1, grocery_list$\"dairy\"$2, \n    grocery_list$\"dairy\"$4, grocery_list$\"meat\"$1, grocery_list$5$3] ;\n  /* The || is a concatenation operator, like c(). */\n  /* It is inefficient for large data sets */\n  \n  run ListPrint(ick);\n  run ListPrint(crust);\n  run ListPrint(yummy_toppings);\nquit;\n\nData Sets (SAS)\nThe SAS data set structure is similar to a R data frame.\n\nIn SAS, missing values are indicated with .\nSAS datasets also come with a description which is attached to the table. The descriptor portion of the data set records names of variables (and attributes), numbers of observations, and date/time stamps of creation and updates.\n\nCreating a SAS data set\nIn the next code chunk, we’ll create a data set using a SAS Data step. We’ll talk more about the anatomy of a SAS command later, but for now, notice that I’m specifying some metadata (the title), telling SAS what the variable names are (Drugs, Score), and then providing some data (indicated by the datalines statement).\ndata mathLSD;\ntitle 'Average math test scores under the influence of LSD';\ninput Drugs Score;\ndatalines;\n1.17 78.93\n2.97 58.20\n3.26 67.47\n4.69 37.47\n5.83 45.65\n6.00 32.92\n6.41 29.97\n;\n\n/* Describe the dataset */\nproc datasets;\n  contents data = mathLSD;\nrun;\n\nproc print data = mathLSD;\nrun;\nThe last two blocks are SAS procedures (PROCs). In the first block, I’m asking SAS to describe the contents of the mathMJ dataset. In the second block, I’m telling SAS to print the whole mathMJ dataset out.\n\nA.4.2 Indexing in SAS\nIn SAS, the same basic code works (though matrix definition is a bit more manual).\nproc iml; /* Interactive Matrix Language */\n  x = {1 2 3 4 5, 6 7 8 9 10, 11 12 13 14 15, 16 17 18 19 20};\n  y = x[3:4, 1:2];\n  print x; /* Here, print is used instead of put */\n  print y;\nquit; /* exit proc IML */\nTry it out\n(From project Euler)\nIf we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000.\nHint: The modulo operator, %%, gives the integer remainder of one number divided by another. So a %% b gives the integer remainder when dividing a by b. Modular division is often used to find multiples of a number.\n\nSAS solution\ndata tmp;\n  do x = 1 to 999;\n  output;\n  end;\nrun;\n\nproc summary data=tmp; /* Summarize data */\n  where (mod(x, 3) = 0) | (mod(x, 5) = 0); \n  /* Keep only obs where x is divisible by 3 or 5 */\n  \n  var x; /* what variable we want the summary for */\n  \n  output out=sum_x sum=; /* output sum_x to a new dataset */\nrun;\n\nproc print data = sum_x; /* print our sum_x dataset */\nrun;\nNote on the SAS code: where statements allow you to select part of the data for further processing. There was a note earlier about the fact that type conversion doesn’t happen in where clauses… this is one of those clauses. We’ll get into where clauses in more detail later, in module 5.\n\nA.4.3 If Statements\n\nIn SAS\nIn a data step:\ndata santa;\n  input name $ status $;\n  datalines;\n  Edison nice\n  Alex naughty\n  Susan .\n  Ryan neutral\n;\n\n/* Modify santa_list and make a new dataset, present_list */\ndata presents;\n  set santa;\n  if status = \"naughty\" then present = \"coal\";\n  else present = \"toy\";\nrun; /* must end with run if no datalines option */\n\nproc print data=presents;\nrun;\nNote that ., or missing data is handled the same as ‘nice’. That might not be what we wanted… this is the natural thing to do, right?\ndata santa;\n  input name $ status $;\n  datalines;\n  Edison nice\n  Alex naughty\n  Susan .\n  Ryan neutral\n;\n\n/* Modify santa_list and make a new dataset, present_list */\ndata presents;\n  set santa;\n  if status = \"naughty\" then present = \"coal\";\n  else (if status = \"nice\" then present = \"toy\" else present = .);\nrun; /* must end with run if no datalines option */\n\nproc print data=presents;\nrun;\nSAS doesn’t handle nested if statements very well - they can be ambiguous. Instead, SAS documentation suggests using do; and end; to denote the start and end points of each if statement (like the {} in R).\ndata santa;\n  input name $ status $;\n  datalines;\n  Edison nice\n  Alex naughty\n  Susan .\n  Ryan neutral\n;\n  \ndata presents;\n  set santa;\n  if status = \"naughty\" then \n    do;\n      present = \"coal\";\n    end;\n  else if status = \"nice\" then\n    do;\n      present = \"toy\";\n    end;\n  else \n    do;\n      present = .;\n    end;\nrun;\n          \nproc print data=presents;\nrun;\nInterestingly, if you set a character variable to be missing, SAS converts it to ‘.’. So, if we actually want to have the value be missing, we can set it to an empty string.\ndata santa;\n  input name $ status $;\n  datalines;\n  Edison nice\n  Alex naughty\n  Susan .\n  Ryan neutral\n;\ndata presents; \n  set santa;\n  if status = \"naughty\" then \n    do;\n      present = \"coal\";\n    end;\n  else if status = \"nice\" then\n    do;\n      present = \"toy\";\n    end;\n  else \n    do;\n      present = '';\n    end;\nrun;\n          \nproc print data=presents;\nrun;\nNow things work the way we expected them to work.\n\nSwitch statments: SAS case statement documentation\n\n\nTry it out\nThe sample() function selects a random sample of entries from a vector. Suppose we sample a random vector \\(x\\) with 10 entries. Write one or more if statements to fulfill the following conditions\n\nif \\(x\\) is divisible by 2, \\(y\\) should be positive; otherwise, it should be negative.\nif \\(x\\) is divisible by 3, \\(y\\) should have a magnitude of 2; otherwise, it should have a magnitude of 1.\n\nIt may be helpful to define separate variables y_mag and y_sign and then multiply them afterwards. Once you have found the value of \\(y\\) compute \\(\\text{sum}(x * y)\\).\nYou may use the following code skeletons to set the problem up.\nproc iml;\n  call randseed(342502837);\n  x = sample(1:50, 20)`;\n  create sampledata from x [colname = \"x\"];\n  append from x;\n  close;\nquit;\n\ndata xy;\n  set sampledata;\n\n\n  /* Conditional statements go here */\n  \n  \n  /* Leave this so that the code below works */\n  res = x * y;\nrun;\n\nproc summary data=xy; /* Summarize data */\n  var res; /* what variable we want the summary for */\n  \n  output out=tmpsum sum=; /* output tmpsum to a new dataset */\nrun;\n\nproc print data = xy; /* print our original dataset to check result */\n  var x y res;\n  sum res;\nrun;\n\nproc print data = tmpsum; /* print our tmpsum dataset */\nrun;\n\nSAS Solution\nproc iml;\n  call randseed(342502837);\n  x = sample(1:50, 20)`; \n  create sampledata from x [colname = \"x\"];\n  append from x;\n  close;\nquit;\n\ndata xy;\n  set sampledata;\n\n  y_sign = 0 * x;\n  y_mag = 0 * x;\n\n  /* Conditional statements go here */\n  if MOD(x, 2) = 0 then y_sign = 1; \n    else y_sign = -1;\n  if MOD(x, 3) = 0 then y_mag = 2; \n    else y_mag = 1;\n\n  y = y_sign * y_mag;\n  res = x * y;\nrun;\n\nproc summary data=xy; /* Summarize data */\n  var res; /* what variable we want the summary for */\n  \n  output out=tmpsum sum=; /* output tmpsum to a new dataset */\nrun;\n\n\nproc print data = xy; /* print our original dataset to check result */\n  var x y res;\n  sum res;\nrun;\n\nproc print data = tmpsum; /* print our tmpsum dataset */\nrun;\nSee this to understand how the print statement works and how to add column summary values.\n\nA.4.4 For Loops\n\n“For loops” in SAS IML (using do)\n/* SAS IML example loop */\nproc iml;\n  do i = 1 to 10;\n    print i; \n  end; /* This ends the loop definition */\nquit;\n\n“For loops” in a SAS DATA step\ndata A;\n  do i = 1 to 10; \n    put i=;\n  end; /* This ends the loop definition */\nrun;\n\nOther sequence structures in SAS for loops\nWe can iterate by non-integer values:\ndata A;\ny = 0;\ndo i = 5 to 0 by -0.5;\n    put i=; \n  end;\nrun;\nWe can even add additional conditions:\ndata A;\ny = 0;\ndo i = 5 to 0 by -0.5 while (i**2 &gt; 1);\n    put i=;\n  end; \nrun;\nTry it out (in SAS)\nWrite a for loop which will output the first 30 fibbonacci numbers. You can use the following code as a starting point:\n/* SAS IML example loop */\nproc iml;\n  current = 1; \n  prev = 0;\n\nquit;\n\n\nSolution\n/* SAS IML example loop */\nproc iml;\n  current = 1;\n  prev = 0;\n\n  do i = 1 to 30;\n    new = current + prev;\n    prev = current;\n    current = new;\n    print current; \n  end; /* This ends the loop definition */\nquit;\n\n\nA.4.5 Condition-controlled loops (WHILE, DO WHILE)\nExample: The Basel Problem\nLet’s solve the Basel problem in SAS using WHILE loops - we’ll repeat the calculation until the value changes by less than 0.000001. The Basel problem is the problem of calculating the precise infinite summation \\[\\sum_{n=1}^\\infty \\frac{1}{n^2}\\]\nWe’ll stick to calculating it computationally.\n\nIn SAS\nproc iml;\n  i = 1;\n  basel = 0;\n  prev = -1;\n  do while((basel - prev) &gt; 1e-6);\n    prev = basel;\n    basel = basel + 1/i**2; /* ** is the exponent operator */\n    i = i + 1;\n    \n    if i &gt; 1e6 then\n      do;\n        leave;\n    end;\n      \n    if MOD(i, 200) = 0 then\n      do;\n        print i, prev, basel;\n    end; \n  end;\n  \n  print i, basel;\nquit;\n  \nTry it out\nWrite a while loop in in SAS to calculate \\(\\displaystyle \\lim_{x \\rightarrow 4} \\frac{2 - \\sqrt{x}}{4-x}\\) by starting at 3 and halving the distance to 4 with each iteration. Exit the loop when you are within 1e-6 of the value computed on the previous iteration, or when you are within 1e-6 from 4. Which exit condition did you hit first? How do you know?\n\nSolution\nproc iml;\n  x = 3;\n  dist = 4 - x;\n  fx = 0;\n  prev_fx = 1;\n  dfx = abs(fx - prev_fx);\n  do while(dfx &gt; 1e-6 & dist &gt; 1e-6);\n    prev_fx = fx;\n    dist = dist/2; \n    x = 4 - dist; \n    fx = (2 - sqrt(x))/(4 - x);\n    dfx = abs(fx - prev_fx);\n  end;\n  \n  print x, dist, fx, dfx;\nquit;"
  },
  {
    "objectID": "app-sas.html#eda-in-sas",
    "href": "app-sas.html#eda-in-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.5 EDA in SAS",
    "text": "A.5 EDA in SAS\n\n\nProc Freq generates frequency tables for variables or interactions of variables.\n\n\nPROC FREQ demo\nThis can help you to see whether there is missing information. Using those frequency tables, you can create frequency plots and set up chi squared tests.\nlibname classdat \"sas/\";\n\nODS GRAPHICS ON;\nPROC FREQ DATA=classdat.poke ORDER=FORMATTED;\n  TABLES generation / CHISQ PLOTS=freqplot(type=dotplot);\nRUN;\nPROC FREQ DATA=classdat.poke ORDER=FREQ;\n  TABLES type_1 status / MAXLEVELS=10 PLOTS=freqplot(type=dotplot scale=percent);\nRUN;\nODS GRAPHICS OFF;\n\nProc Means can be used to get more useful summary statistics for numeric variables.\n\n\nPROC MEANS demo\nNote that the Class statement identifies a categorical variable; the summary statistics are computed for each level of this variable.\nODS HTML style= HTMLBlue; \nlibname classdat \"sas/\";\n\nPROC MEANS DATA = classdat.poke;\nrun;\n\nproc means data = classdat.poke;\nclass status;\nrun;\n\nFor even higher levels of detail, Proc Univariate will provide variability, tests for location, quantiles, skewness, and will identify the extreme observations for you.\n\n\nPROC UNIVARIATE demo\nYou can also get histograms for variables, even specifying distributions you’d like to be fit to the data (if that’s something you want).\nODS HTML style= HTMLBlue; \nlibname classdat \"sas/\";\n\nODS GRAPHICS ON;\nPROC UNIVARIATE DATA = classdat.poke;\nVAR attack defense sp_attack sp_defense speed;\nHISTOGRAM attack defense sp_attack sp_defense speed;\nRUN;\nODS GRAPHICS OFF;\n\nProc Corr allows you to examine the relationship between two quantitative variables.\n\n\nPROC CORR demo\nlibname classdat \"sas/\";\n\nODS GRAPHICS ON;\nPROC CORR DATA = classdat.poke PLOTS( MAXPOINTS=200000)=MATRIX(HISTOGRAM);\nVAR attack defense sp_attack sp_defense speed ;\nRUN;\nODS GRAPHICS OFF;\nThe plot here is called a scatterplot matrix. It contains histograms on the diagonal, and pairwise scatterplots on off-diagonals. It can be useful for spotting strong correlations among multiple variables which may affect the way you build a model.\nTry it out\nOne of the datasets we read in above records incidents of police violence around the country. Explore the variables present in this dataset (see code in the spreadsheets section to read it in). Note that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.\nIt is useful to memorize the SAS PROC options you use most frequently, but it’s also a good idea to reference the SAS documentation - it provides a list of all viable options for each procedure, and generally has decent examples to show how those options are used.\n\nSolution\nODS HTML style= HTMLBlue; \n\nlibname classdat \"sas/\";\n\nODS GRAPHICS ON;\nPROC CONTENTS DATA = classdat.police; /* see what's in the dataset */\nRUN;\n\nPROC FREQ DATA = classdat.police ORDER=FREQ; /* Examine Freq of common vars */\nTABLES Victim_s_gender Victim_s_race State Cause_of_death \n        Unarmed Geography__via_Trulia_methodolog / MAXLEVELS = 10;\nRUN;\n\nPROC FREQ DATA = classdat.police ORDER=FREQ; /* Combinations of vars */\nTABLES Unarmed * Criminal_Charges_ / NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10;\nRUN;\n\nPROC MEANS DATA = classdat.police; /* Numeric variable exploration */\nVAR num_age; /* Only numeric variable in this set */\nRUN;\n\nPROC UNIVARIATE DATA = classdat.police; /* Investigating age/date info */\nHISTOGRAM num_age date;\nRUN;\nODS GRAPHICS OFF;\nOddities to note:\n\nGender - Unknown should be recoded as missing (’ ’)\nVictim_s_race - Unknown race and Unknown Race should be recoded as missing\nState - might need to check to make sure all states are valid (but top 10 are, at least)\nCause of death - sometimes, there are multiple causes. Also, varying capitalizations…\nGeography - #N/A should be recoded as missing\nCriminal_Charges_ - What does No/NO mean? (would need to look up in the codebook)\nAge - the maximum age recorded is 107, which bears some investigation… other extreme observations between 89 and 95 are also fairly interesting and could be investigated further. There are also several infants/young children included, which is horribly sad, but believable.\nDate - PROC UNIVARIATE doesn’t display date results with a meaningful format, even though format is specified.\n\nConclusions (ok, probably obvious before this analysis):\n\nIt’s much more likely for charges to be filed if the suspect was unarmed (but still very rare)\nData is relatively evenly distributed between 2013 and 2019.\nIt’s fairly rare for police to kill female or transgender individuals - around 5% of all victims\nCalifornia, Texas, and Florida, while populous, seem to have a disproportionate number of killings, especially compared to e.g. NY, which is also a high population state. To really make the state numbers meaningful, though, we’d need to know population counts. There’s also an issue of accurate comparisons - some states may not report police killings with the same standards as other states."
  },
  {
    "objectID": "app-sas.html#data-cleaning-in-sas",
    "href": "app-sas.html#data-cleaning-in-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.6 Data Cleaning in SAS",
    "text": "A.6 Data Cleaning in SAS\nIn SAS, as in SQL, the filter() operation is accomplished using a where clause. Multiple clauses can be connected using and, and compound statements can be grouped with parentheses.\n\nA.6.1 Demonstration of where in SAS\nRather than output the whole data table (which would take up a lot of space), I’ve linked the log file from each chunk below the chunk. If you are running this code in SAS, you should NOT copy the proc printto line.\nlibname classdat \"sas/\";\n\n/* SAS limits dataset names to 8 characters, which is super annoying. */\n/* Sorry the names aren't descriptive... */\n\nDATA tmp1; /* this is the out dataset */\n/* By not having a library attached, SAS places this in WORK */\n/* It's a temporary dataset */\n  set classdat.starwars;\n  where (species = 'Human');\n  run;\nSee the log file here\nlibname classdat \"sas/\";\n\nDATA tmp2; \n  set classdat.starwars;\n  where (species = 'Human') and (homeworld = 'Tatooine');\n  run;\nSee the log file here\nAt this point, you’ve seen the traditional SAS Data step options, but there is another SAS PROC that may be more useful (and more similar to dplyr). dplyr was developed to provide SQL-like syntax while enabling the use of more advanced computations than are supported in SQL. While SAS doesn’t have anything quite the same as dplyr, it does have PROC SQL.\nTry it out\nUsing the pokemon data, can you create a new data frame that has only water type pokemon? Can you write a filter statement that looks for any pokemon which has water type for either type1 or type2?\nlibname classdat \"sas/\";\n\nDATA water1;\nSET classdat.poke;\nWHERE type_1 = \"Water\";\nRUN;\n\nDATA water2;\nSET classdat.poke;\nWHERE (type_1 = \"Water\" OR type_2 = \"Water\");\nRUN;\nIn the interests of only showing the parts of the log that are useful, I’ve just pasted them into this chunk. Not reproducible, but faster to read.\nNOTE: There were 134 observations read from the data set CLASSDAT.POKE.\n      WHERE type_1='Water';\nNOTE: The data set WORK.WATER1 has 134 observations and 49 variables.\nNOTE: DATA statement used (Total process time):\n      real time           0.00 seconds\n      cpu time            0.00 seconds\n\nNOTE: There were 153 observations read from the data set CLASSDAT.POKE.\n      WHERE (type_1='Water') or (type_2='Water');\nNOTE: The data set WORK.WATER2 has 153 observations and 49 variables.\nNOTE: DATA statement used (Total process time):\n      real time           0.00 seconds\n      cpu time            0.00 seconds\nSAS PROC SQL\nIn SQL, as in the SAS DATA step, filter() operations are performed using the keyword WHERE.\nTo limit the output I’m going to cheat a bit and use SELECT statements before I officially teach them to you - this is mostly so you don’t get a table with all 49 variables in it. Similarly, I’m limiting the dataset to the first 5 observations that meet the condition so that we don’t have to see all the water type pokemon.\nlibname classdat \"sas/\";\n\nPROC SQL;\nSELECT pokedex_number, name, type_1, type_number FROM classdat.poke (obs=5)\nWHERE type_1 = \"Water\";\nIf we want to store the output of our query to a new table, we can do that by starting our query with CREATE TABLE &lt;table name&gt; AS - this creates a table with our results.\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE aquapoke AS\nSELECT pokedex_number, name, type_1, type_2, type_number FROM classdat.poke\nWHERE (type_1 = \"Water\" OR type_2 = \"Water\");\n\nPROC PRINT DATA=aquapoke (obs=10);\nRUN;\n\nA.6.2 Common Cleaning Tasks\nKeeping only certain rows\nIn SAS, to use a variable, you have to define it in one data step, then make another data step in order to use that variable. But, like dplyr, SAS has a row number counter that we can use for this purpose.\nlibname classdat \"sas/\";\n\nDATA tmp;\nSET classdat.poke;\n  rownum=_n_; /* SAS shorthand for row number */\nRUN;\n\nDATA evenrow;\n  SET WORK.tmp;\n  WHERE MOD(rownum, 2) = 0;\n  DROP rownum; /* ditch temp variable */\nRUN;\nTop N values\nWe’re going to want to use PROC SORT to get the data arranged before we take the top N values. According to this, we can’t use _n_ in a where statement, and the proposed solution isn’t reliable. So we’ll do it the long way.\nlibname classdat \"sas/\";\n\nPROC SORT DATA = classdat.poke\n  OUT = pokesort;\n  BY descending total_points;\nRUN;\n\nDATA poken;\n  SET WORK.pokesort;\n  rownum = _n_;\nRUN;\n\nDATA poken;\n  SET WORK.poken;\n  WHERE rownum &lt;= 5;\n  DROP rownum;\nRUN;\n\nPROC PRINT DATA = poken;\n  VAR pokedex_number name status species type_1 total_points;\nRUN;\nIn both cases, the SAS statements required to perform the task require a WHERE clause, but also a few other statements to get things working. The equivalent base R code would be about the same (though tricky in different spots).\nPROC SQL filter statements\nSQL doesn’t have an intrinsic notion of ordered rows, so in order to select even rows, we need to create a temporary dataset with _n_ copied into a variable (just like last time).\nlibname classdat \"sas/\";\n\nDATA poke;\n  SET classdat.poke;\n  rownum=_n_;\nRUN;\n\nPROC SQL;\nSELECT * FROM poke(obs=5)\nWHERE mod(rownum, 2) = 0;\nSELECT * says to select all variables. We’ll talk about SELECT in the next section, but with SQL it’s not really possible to avoid using SELECT.\nIf we want the 5 pokemon with the highest total points, we can use ORDER BY to sort the table, and then specify that we only want 5 rows.\nlibname classdat \"sas/\";\n\nPROC SQL;\nSELECT pokedex_number, name, status, species, type_1, total_points\nFROM classdat.poke(obs=5)\nORDER BY total_points DESC;\nAs a reminder, if we want to store this new data into a new dataset, we have to start our statement with CREATE TABLE  AS, and then follow the statement with our query.\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE poketmp AS\nSELECT pokedex_number, name, status, species, type_1, total_points\nFROM classdat.poke(obs=5)\nORDER BY total_points DESC;\n\nPROC PRINT DATA=poketmp;\nRUN;\n\nA.6.3 Column Selection\nUnfortunately, SAS doesn’t make column selection quite as easy. It’s still not hard, but it can be tedious. In SAS, there are two primary methods to select variables: KEEP selects variables, DROP removes variables.\n\n# Export flights data for SAS\nflights %&gt;%\nsample_frac(size = .25) %&gt;% # Keep file from being too big\nwrite_csv(\"data/flights.csv\", na = \".\")\n\n/* Read in data */\nlibname classdat \"sas/\";\nfilename fileloc '~/Projects/Class/unl-stat850/2020-stat850/data/flights.csv';\nPROC IMPORT  datafile = fileloc out=classdat.flights\nDBMS = csv; /* comma delimited file */\nGETNAMES = YES;\nRUN;\nIn SAS, a partial variable name either preceded or followed by : serves as a wildcard. Ranges of variables can be specified with two dashes, e.g. var3 -- var5.\nUnfortunately, the wildcard doesn’t work on both ends, so to get the equivalent of matches(\"dep\"), we have to use two different options in our KEEP statement (plus the extra variables that don’t have dep in them).\nlibname classdat \"sas/\";\n\nDATA tmpfly;\n  KEEP flight year--day tailnum origin dep: sched_dep:;\n  SET classdat.flights;\nRUN;\n\nPROC PRINT DATA = tmpfly (obs=10);\nRUN;\nNote also that SAS doesn’t reorder the columns for us like select() does.\nIf we’d prefer to carve out columns (rather than assembling a new dataset with the columns we want to keep), we can use a DROP statement, which works exactly the same way. Let’s see what columns we removed implicitly last time by dropping everything we’d previously kept:\nlibname classdat \"sas/\";\n\nDATA tmpfly;\n  DROP flight year--day tailnum origin dep: sched_dep:;\n  SET classdat.flights;\nRUN;\n\nPROC PRINT DATA = tmpfly (obs=10);\nRUN;\nAs with the filter statements, we can also use PROC SQL instead of a SAS DATA step. There are even ways to (sort-of) use elements of both.\nSAS PROC SQL SELECT statement\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE tmpfly\nAS\nSELECT flight, year, month, day, tailnum, origin\nFROM classdat.flights;\n\nPROC PRINT DATA = tmpfly(obs=10);\nRUN;\nNote that PROC SQL doesn’t have a RUN statement - it is executed immediately. But, using the PROC SQL syntax, we still have to list out all of the variables, and that’s a drag.\nLuckily, PROC SQL will also let us use some of the DATA step options, if we’re careful about it:\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE tmpfly\nAS\nSELECT *\nFROM classdat.flights(drop=year--day flight tailnum origin dep: sched_dep:);\n\nPROC PRINT DATA = tmpfly(obs=10);\nRUN;\nNote the difference - we’re selecting everything (in SQL) but dropping columns when we tell SQL where to look for the data.\nFor the most part, that is what you need to functionally replicate select() syntax. It may be a bit more work because there aren’t the same convenience functions, but it’ll do and you don’t have to remember as many keywords, so that’s a plus.\n\nA.6.4 Creating New Variables\nSAS DATA STEP\nWe can create our variable a couple of different ways in SAS: - Use the TRANWRD function for find and replace. - Use an if statement and define the replacement ourselves\nBoth are demonstrated below:\nlibname classdat \"sas/\";\n\nDATA pvtmp;\n  SET classdat.police;\n  race = tranwrd(victim_s_race, \"Race\", \"race\");\n  race2 = victim_s_race; /* initialize it to current value */\n  IF victim_s_race='Unknown Race' THEN race2 = 'Unknown race';\nRUN;\n\nPROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */\nTABLES victim_s_race * race victim_s_race * race2 /\n  NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10;\nRUN;\nIn both cases we can see that the recode worked the way we wanted and we’ve now gotten rid of the extra “unknown” category”.\nProc SQL\nWe can also use PROC SQL to create new variables using relatively complex logic if necessary.\nIn SQL, you define new variables using AS. In SELECT statements, this definition has the computation on the left and the variable on the right1.\nCASE WHEN is the if-else statement in SQL. When (victim_s_race = ‘Unknown Race’), our variable value will be “Unknown race”, otherwise it will be what ever value is in victim_s_race.\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE WORK.pvtmp AS\nSELECT * ,\nCASE WHEN victim_s_race='Unknown Race' THEN 'Unknown race' ELSE victim_s_race END AS race\nFROM classdat.police;\n\n\nPROC FREQ DATA = pvtmp ORDER=FREQ; /* Combinations of vars */\nTABLES victim_s_race * race /\n  NOCUM NOPERCENT NOCOL NOROW MAXLEVELS=10;\nRUN;\nComparison\nThe choice of which method to use (DATA step or PROC SQL) involves weighing these competing factors:\n\ncomputational time\ncode readability\nprogrammer time\n\nPersonally, I find PROC SQL easier to work with, but I think the code is ugly. There are similar sql-syntax packages in R, but I don’t feel the need to use them, because (for me) dplyr code is much easier to read (and thus, easier to maintain). dplyr code is not as efficient as SQL (or other packages, like data.table) on big datasets, but there are variants such as dbplyr to handle some of those cases, and I find that they don’t come up very often in my work or research. If I were working at Google or Amazon, my opinion might be very different\n\n\nA.6.5 Summarize\nIn SAS, we can do something similar:\nlibname classdat \"sas/\";\n\nDATA pv;\n  SET classdat.police;\n  age = INPUT(victim_s_age, 3.);\n  name_len = LENGTH(victim_s_name);\nRUN;\n\nPROC MEANS DATA=pv;\nVAR age name_len;\nRUN;\nBy default, with SAS, we get a bit more than we bargained for; we can turn the extra output off with options.\nAnother option is to use PROC SQL in SAS, which will have a logical flow similar to dplyr.\n\nlibname classdat \"sas/\";\n\nDATA pv;\n  SET classdat.police;\n  age = INPUT(victim_s_age, 3.);\n  name_len = LENGTH(victim_s_name);\nRUN;\n\nPROC SQL;\nSELECT AVG(age) as age, AVG(name_len) as name_len\nFROM pv;\n\nA.6.6 Group By + (?) = Power\nlibname classdat \"sas/\";\n\nDATA pv;\n  SET classdat.police;\n  age = INPUT(victim_s_age, 3.);\n  IF victim_s_gender=' ' THEN victim_s_gender='Unknown';\n  race = victim_s_race; /* initialize it to current value */\n  IF victim_s_race='Unknown Race' THEN race = 'Unknown race';\nRUN;\n\nPROC SQL;\n  SELECT victim_s_gender AS gender, race, count(*) AS n\n  FROM pv\n  GROUP BY race;\n\nPROC SQL;\n  SELECT victim_s_gender AS gender, race, count(*) AS n, min(age) AS min_age, max(age) AS max_age\n  FROM pv\n  GROUP BY gender, race;\n\nA.6.7 Storm Example\nlibname classdat \"sas/\";\nfilename fileloc 'data/storms.csv';\nPROC IMPORT  datafile = fileloc out=classdat.storms REPLACE\nDBMS = csv; /* comma delimited file */\nGUESSINGROWS=500;\nGETNAMES = YES;\nRUN;\n\nDATA classdat.storms;\nSET classdat.storms;\ndate = MDY(month, day, year);\ntime = DHMS(date, hour, 0, 0);\nFORMAT time DATETIME.;\nRUN;\n\nProc SQL in SAS\nIn SAS, this is going to require some work. Specifically, while dplyr commands are stated in recipe order (do this, then this), SQL statements… aren’t. WHERE comes after SELECT xxx FROM yyy, and GROUP BY comes after that again.\nThere are a couple of ways to handle that: sub-queries, and creating temporary tables. I think the temporary tables approach will be easier to demonstrate, read, and understand, so lets go with that.\nAnother challenge will be the fact that SAS PROC SQL doesn’t handle missing data quite as easily as dplyr does (na.rm is a very nice function, all things considered). We can think through the steps we need to take: 1. Create a table where wind and pressure observations aren’t missing. We’ll call that tmp1. 2. Filter tmp1, keeping only rows with minimum pressure and maximum wind for each storm/year combination (HAVING is like WHERE, but after the GROUP BY clause has been applied). We’ll call that tmp2. We can also select the variables we care about in this step. 3. Summarize tmp2, keeping columns name, year, pressure, wind, category, status, and time, where time is the mean of all maximum-power observations. The other variables should have only one value each. We can accomplish this task using the combination of SELECT and DISTINCT. DISTINCT says “keep only rows with new combinations of these values”.\n(Note also that we can format values inline in proc SQL Select statements. That forces SAS to treat time as a date-time variable, which will force it to format correctly in e.g. plots.)\nlibname classdat \"sas/\";\n\n\nPROC SQL;\n  CREATE TABLE tmp1 AS\n  SELECT *\n  FROM classdat.storms\n  WHERE (NOT missing(pressure)) AND (NOT missing(wind));\n  \n  CREATE TABLE tmp2 AS \n  SELECT name, year, pressure, wind, category, status, time, \n         min(pressure) AS minpressure, max(wind) AS maxwind\n  FROM tmp1 \n  GROUP BY year, name\n  HAVING pressure = minpressure AND wind = maxwind;\n  \n  CREATE TABLE maxpwr AS\n  SELECT DISTINCT name, year, pressure, wind, category, status, \n                  mean(time) AS time format=DATETIME.\n  FROM tmp2\n  GROUP BY year, name;\n  \nQUIT;\n  \nPROC PRINT DATA=maxpwr (obs=5);\n  RUN;\n\nODS GRAPHICS ON;\nODS TRACE ON; /* this allows us to select only the plot and not tables */\nODS SELECT HISTOGRAM;\nPROC UNIVARIATE DATA=maxpwr;\n  VAR pressure;\n  HISTOGRAM;\n  RUN;\nODS TRACE OFF;\n  \nPROC SGPLOT DATA=maxpwr;\n  scatter X = time Y = pressure;\nRUN;\n  \nODS GRAPHICS OFF;\nQUIT;\nIn SAS, we have to know that datetimes are stored in seconds. So if we subtract two date time values, and we want our answer in days, then we need to divide by the number of seconds in a day: 24*60*60 = 86400. R has helper functions to do this for us, but it’s not that much harder to just do the computuation ourselves.\nlibname classdat \"sas/\";\nPROC SQL;\nCREATE TABLE stormlencat AS\nSELECT name, year, (max(time) - min(time))/86400 AS duration, max(category) AS max_strength\nFROM classdat.storms\nGROUP BY year, name\nORDER BY max_strength;\n\nPROC BOXPLOT DATA=stormlencat;\n  PLOT duration * max_strength;\nRUN;\nQUIT;\n/* Clean log and output */\ndm log \"clear\";\ndm output \"clear\";\nods html close;\nods html;\nWe can do something similar in SAS; this time, I decided to get rid of any storm which never had hurricane-force winds - that will get rid of a lot of lines that never leave the x-axis.\nlibname classdat \"sas/\";\nPROC SQL;\nCREATE TABLE stormevo AS\nSELECT name, year, (time - min(time))/86400 AS time_since_start, category, status, hu_diameter, ts_diameter, max(hu_diameter) AS max_hu_diameter\nFROM classdat.storms\nWHERE NOT MISSING(hu_diameter)\nGROUP BY year, name\nHAVING max_hu_diameter &gt; 0\nORDER BY year, name, time_since_start;\n\nPROC SGPANEL DATA=stormevo;\nPANELBY year / COLUMNS = 4 ROWS = 3;\nSERIES X = time_since_start Y = hu_diameter / GROUP = name;\nRUN;\nQUIT;\nPROC SGPANEL in SAS does essentially the same thing as facet_wrap() in R - it allows you to select one or more variables to create sub-plots for. We do have to manually specify how many rows and columns (or SAS will give us 3 separate plots with 4 panels each). The essential components of the graph specification are the same - instead of specifying the use of a line, we specify “series” (which means plot a line). We specify the same x, y, and group variables, though the syntax differs a bit.\nThe SAS code for this is fairly similar (though I’ll admit to not having the finesse with SAS to get a truly nice looking plot). At this point, we’re going for quick-and-dirty graphics that show us what we want to know - we can figure out how to customize things later.\nODS HTML style= HTMLBlue; /* needed for color graphs in bookdown */\n\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE ike AS\nSELECT * FROM classdat.storms WHERE name = \"Ike\"\nORDER BY time;\n\nPROC SGPLOT DATA=ike;\nSCATTER X = time Y = hu_diameter / \n  COLORRESPONSE=category /* color by another variable */\n  MARKERATTRS=(symbol=CircleFilled) /* use circles for points */\n  DATALABEL=category; /* label the circles with the value */\nRUN;\nQUIT;\nWhile I’m tempted to plot out the diameter and location on a map, it’s a bit excessive for this particular problem. Luckily, Wikipedia has us covered: \nIt looks like Ike went long-ways across Cuba, which weakened it. When hurricanes weaken, often their wind fields expand (as they no longer have the angular momentum to maintain a tight structure). Ike crossed into the Gulf of Mexico, restrengthened, and then hit Houston just about dead-on. I was living just northwest of Houston when it hit (in College Station), and I can verify that it was not a fun time.\n\nA.6.8 Gapminder: Try it Out\n\nreadr::write_csv(gapminder_unfiltered, \"data/gapminder.csv\", na = '.')\n## Error in is.data.frame(x): object 'gapminder_unfiltered' not found\n\nlibname classdat \"sas/\";\n\nfilename fileloc 'data/gapminder.csv';\nPROC IMPORT  datafile = fileloc out=classdat.gapminder REPLACE\nDBMS = csv; /* comma delimited file */\nGUESSINGROWS=500;\nGETNAMES = YES;\nRUN;\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE gapsummary AS\nSELECT DISTINCT country, COUNT(*) AS n, \nSUM(MISSING(lifeExp)) AS missinglifeExp, \nSUM(MISSING(pop)) AS missingpop,\nSUM(MISSING(gdpPercap)) AS missingGDP\nFROM classdat.gapminder\nGROUP BY country;\n\n/* Print the problem countries only */\nPROC PRINT DATA = gapsummary;\nWHERE n ^= 12;\nRUN;\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE gap5 AS\nSELECT *\nFROM classdat.gapminder\nWHERE MOD(year, 5) = 2;\n\n/* Aus had too much data, so use it to see if the command worked */\nPROC PRINT DATA = gap5;\nWHERE country = \"Australia\";\nRUN;\nlibname classdat \"sas/\";\n\nPROC SQL;\nCREATE TABLE gap5 AS\nSELECT *\nFROM classdat.gapminder\nWHERE MOD(year, 5) = 2;\n\nCREATE TABLE gap_clean AS\nSELECT *, COUNT(*) as n \nFROM gap5\nGROUP BY country\nHAVING n = 12;\n\n/* Clean up extra column */\nALTER TABLE gap_clean \nDROP n;\n\nPROC PRINT DATA = gap_clean;\nRUN;"
  },
  {
    "objectID": "app-sas.html#graphics",
    "href": "app-sas.html#graphics",
    "title": "Appendix A — SAS",
    "section": "\nA.7 Graphics",
    "text": "A.7 Graphics\n\nSAS Examples\nOriginal SAS graphics engine\nNote: This code runs, but it causes every other sas chunk in this document to go haywire… so you’ll have to copy the code and run it on your own to see what it looks like.\nlibname classdat \"sas/\";\n\n/* This step creates a constant variable in the data frame, \nso that all generations can be stacked in one bar */\n  DATA poketmp;\nSET classdat.poke;\ni = 1;\nRUN;\n\nPROC GCHART data=poketmp;\npie generation / other = 0;\nRUN;\nQUIT; \n\nPROC GCHART data=poketmp;\nVBAR i / SUBGROUP = generation;\nRUN;\nQUIT; \n\nODS Graphics\nNote: This is a terrible example in SAS because there isn’t an easy way to create a pie chart2. We have to resort to using SAS Graph Template Language.\nlibname classdat \"sas/\";\n\nDATA poketmp;\nSET classdat.poke;\ni = 1;\nRUN;\n\n/* Define a pie chart template */\n  PROC TEMPLATE;\nDEFINE STATGRAPH WORK.simplepie;\nBEGINGRAPH;\nLAYOUT REGION;\nPIECHART category=generation;\nENDLAYOUT;\nENDGRAPH;\nEND;\nRUN;\n\n/* Make the pie chart */ \n  PROC SGRENDER data=classdat.poke template=WORK.simplepie;\nRUN;\nQUIT; \n\n/* Use SGPLOT to make a stacked bar chart */\n  PROC SGPLOT data=poketmp;\nVBAR i / GROUP = generation;\nRUN;\nQUIT; \nAs in base R, the syntax between the two types of charts is different, even though the underlying operations required to make the plots are very similar. This is one example of why I don’t agree with the assertion that ODS graphics is like ggplot2 syntax - the functionality may be similar, but the structure is not."
  },
  {
    "objectID": "app-sas.html#debugging-tools-in-sas",
    "href": "app-sas.html#debugging-tools-in-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.8 Debugging Tools in SAS",
    "text": "A.8 Debugging Tools in SAS\nIn SAS, there are two stages that occur after you submit lines to the console.\n\nThe Compilation Phase: code is parsed. In this step, SAS will catch the logic errors, misspellings, missing key words, etc.\nThe Execution Phase: program is run. In this step, SAS will catch any wrong assignments, loop issues, etc.\n\nSas recognizes four types of errors:\n\nSyntax errors - violations of the language structure\nSemantic errors - structure of the statement is incorrect, but the syntax is correct. e.g. trying to reference an index that doesn’t exist.\nExecution-time errors - errors that occur when the compiled function is run on data values – e.g. division by zero\nData errors - errors that occur when statements are correct but data is invalid (taking the log of a negative number, etc.)\n\nSAS is built around enterprise users, as opposed to R’s open-source philosophy. SAS code also is more formulaic than R code, which means it is usually easier to figure out what is going wrong with the code. As a result, you may find that errors in your SAS code are much easier to diagnose than errors in your R code. Generally, it will tell you exactly where you are missing a semicolon, or exactly what word it thinks you’ve misspelled (and usually, it tries to correct that for you, but it doesn’t always succeed). In my experience with SAS (which is very limited and mostly contained in this book), SAS error messages are much easier to google and find solutions, right up until you’re working in Linux or some other not-well-supported system and the error is related to how the underlying OS handles some task. As a downside, though, trying to do a task SAS doesn’t think you need to do can be much more difficult than necessary.\nSee the references section for a couple of good guides to SAS error statements and warnings. These guides are likely sufficient for most of your SAS debugging needs.\nThere are certainly other errors which can occur in SAS – logic errors are not something SAS can protect you from . These errors can have dramatic consequences, as demonstrated in this twitter thread about a JAMA retraction due to a coding error. To debug these types of errors, you can use the same print() techniques demonstrated in R. For these types of errors, there’s nothing special about what language you’re using (outside of the usual quirks of every language) - the error is in the logic, not the encoding of that logic."
  },
  {
    "objectID": "app-sas.html#simulation",
    "href": "app-sas.html#simulation",
    "title": "Appendix A — SAS",
    "section": "\nA.9 Simulation",
    "text": "A.9 Simulation\nYou can see the various distribution options in the RAND documentation.\n%let N=500; /* size of sample */\n  \nDATA sample;\ncall streaminit(12532);\nDO i = 1 to &N; /* &N is the value of the macro variable defined above */\n  id = i;\n  norm = rand(\"Normal\", 0, 1);\n  gamma = rand(\"Gamma\", 3, 1);\n  exp = rand(\"Exponential\", 1); /* SAS uses 1/a exp(-x/a) */\n  t = rand(\"T\", 5);\n  chisq = rand(\"Chisq\", 5);\n  OUTPUT;\nEND;\nRUN;\n\nPROC TRANSPOSE data=sample out=longsample\n  (rename=(COL1 = value)) /* rename output variable ('values_to') */\n  NAME = dist /* where the column names go ('names_to') */\n;\n  BY id;\n  VAR norm gamma exp t chisq;\nRUN;\n\nPROC SGPANEL data=longsample;\nPANELBY dist / COLUMNS = 5 UNISCALE = ROW NOVARNAME;\n  DENSITY value / TYPE = KERNEL;\nRUN;"
  },
  {
    "objectID": "app-sas.html#refs-sas",
    "href": "app-sas.html#refs-sas",
    "title": "Appendix A — SAS",
    "section": "\nA.10 References",
    "text": "A.10 References\n\nSAS quick start guide\nSAS Cheatsheet (from another class like this)\nSAS Cheatsheet (by SAS)\nSAS Programming for R Users (free book)\nSAS Matrix reference\nSAS Data set documentation\nCreating SAS Data Sets from IML (also this friendly guide and this blog post)\nSAS Data Step options\nSAS Mathematical Operators\nLists and Data Structures in SAS\nLoops in SAS and SAS documentation for DO WHILE loops\nRandom number generation in SAS\n\n\nA.10.1 Data Files\n\n\nReading JSON in SAS – You know SAS documentation is getting weird when they advertise a method as “the sexiest way to import JSON data into SAS”.\nReading Rdata files in SAS\nCommon problems with SAS data files\n\nA.10.2 Data Cleaning\n\nSAS Dates and Times.\nCommon String operations in SAS\nRegular Expressions in SAS\nUsing WHERE with SAS Procedures\nPROC SQL documentation\nSAS data manipulation"
  },
  {
    "objectID": "app-sas.html#footnotes",
    "href": "app-sas.html#footnotes",
    "title": "Appendix A — SAS",
    "section": "",
    "text": "This is equivalent to using right assignment in R with -&gt;, which you shouldn’t do unless you have a really good reason, because it’s hard to read.↩︎\nIt’s not often you’ll find me approving of SAS graphics, but making it hard to make pie charts is definitely a point in SAS’s favor↩︎"
  }
]