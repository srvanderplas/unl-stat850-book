---
editor_options: 
  markdown: 
    wrap: 72
---

# Exploratory Data Analysis

## Module Objectives   {- #module7-objectives}

- Understand the main goals of exploratory data analysis
- Generate and answer questions about a new dataset using charts, tables, and numerical summaries

::: callout-note
## Extra Reading {.unnumbered}

[The EDA chapter in R for Data Science](https://r4ds.had.co.nz/exploratory-data-analysis.html) [@r4ds] is very good at
explaining what the goals of EDA are, and what types of questions you
will typically need to answer in
EDA. Much of the
material in this chapter is based at least in part on the R4DS chapter.
:::

![(Image from https://www.mrdbourke.com) 
The EDA lifecycle starts with data collection and is
primarily a cycle between checking data types, assessing distributions,
feature engineering, and model iteration. These tasks are supported by
summary statistics, visualization, and
modeling.](images/eda/an-EDA-lifecycle.png){fig-align="center"
width="75%"}
<!-- https://www.mrdbourke.com/content/images/size/w1000/2019/09/an-EDA-lifecycle.png -->

Major components of Exploratory Data Analysis (EDA):

-   generating questions about your data
-   look for answers to the questions (visualization, transformation,
    modeling)
-   use answers to refine the questions and generate new questions

EDA is an iterative process. It is like brainstorming - you start with
an idea or question you might have about the data, investigate, and then
generate new ideas. EDA is useful even when you are relatively familiar
with the type of data you're working with: in any dataset, it is good to
make sure that you know the quality of the data as well as the
relationships between the variables in the dataset.

EDA is important because it helps us to know what challenges a
particular data set might bring, what we might do with it. Real data is
often messy, with large amounts of cleaning that must be done before
statistical analysis can commence.

While in many classes you'll be given mostly clean data, you do need to
know how to clean your own data up so that you can use more interesting
data sets for projects (and for fun!). EDA is an important component to
learning how to work with messy data.

::: .callout-note
In this section, I will mostly be using the plot commands that come with
base R/python and require no extra packages. The R for Data Science book
[@r4ds] shows plot commands which use the `ggplot2` library. I'll show
you some plots from ggplot here as well, but you don't have to
understand how to generate them yet. We will learn more about ggplot2
later, though if you want to start using it now, you may.
:::

## A Note on Language Philosophies {.unnumbered}

It is usually relatively easy to get summary statistics from a dataset,
but the "flow" of EDA is somewhat different depending on the language
patterns.

> You must realize that R is written by experts in statistics and
> statistical computing who, despite popular opinion, do not believe
> that everything in SAS and SPSS is worth copying. Some things done in
> such packages, which trace their roots back to the days of punched
> cards and magnetic tape when fitting a single linear model may take
> several days because your first 5 attempts failed due to syntax errors
> in the JCL or the SAS code, still reflect the approach of "give me
> every possible statistic that could be calculated from this model,
> whether or not it makes sense". The approach taken in R is different.
> The underlying assumption is that the useR is thinking about the
> analysis while doing it. -- Douglas Bates

I provide this as a historical artifact, but it does explain the
difference between the approach to EDA and model output in R and Python,
and the approach in SAS, which you may see in your other statistics
classes. This is not (at least, in my opinion) a criticism -- the SAS
philosophy dates back to the mainframe and punch card days, and the
syntax and output still bear evidence of that -- but it is worth noting.

In R and in Python, you will have to specify each piece of output you
want, but in SAS you will get more than you ever wanted with a single
command. Neither approach is wrong, but sometimes one is preferable over
the other for a given problem.

## Generating EDA Questions

I very much like the two quotes in the @r4ds section on EDA Questions:

> There are no routine statistical questions, only questionable
> statistical routines. --- Sir David Cox

> Far better an approximate answer to the right question, which is often
> vague, than an exact answer to the wrong question, which can always be
> made precise. --- John Tukey

As statisticians, we are concerned with variability by default. This is
also true during EDA: we are interested in variability (or sometimes,
lack thereof) in the variables in our dataset, including the
co-variability between multiple variables.

We may assess variability using pictures or numerical summaries:

-   histograms or density plots (continuous variables)
-   column plots (categorical variables)
-   boxplots
-   5 number summaries (min, 25%, mean, 75%, max)
-   tabular data summaries (for categorical variables)

In many cases, this gives us a picture of both variability and the
"typical" value of our variable.

Sometimes we may also be interested in identifying unusual values:
outliers, data entry errors, and other points which don't conform to our
expectations. These unusual values may show up when we generate pictures
and the axis limits are much larger than expected.

We also are usually concerned with missing values - in many cases, not
all observations are complete, and this missingness can interfere with
statistical analyses. It can be helpful to keep track of how much
missingness there is in any particular variable and any patterns of
missingness that would impact the eventual data
analysis[^exploratory-data-analysis-1].

[^exploratory-data-analysis-1]: One package for this process in R is
    `naniar` [@tierneyNaniarDataStructures2021].

If you are having trouble getting started on EDA,
@danielbourkeGentleIntroductionExploratory2019 provides a nice checklist
to get you thinking:

> 1.  What question(s) are you trying to solve (or prove wrong)?
> 2.  What kind of data do you have and how do you treat different
>     types?
> 3.  What's missing from the data and how do you deal with it?
> 4.  Where are the outliers and why should you care about them?
> 5.  How can you add, change or remove features to get more out of your
>     data?

## Useful EDA Techniques

In this chapter, we'll explore the [pokemon
data](https://github.com/shahinrostami/pokemon_dataset/blob/master/pokemon_gen_1_to_8.csv)
in [shahinrostami](https://github.com/shahinrostami/pokemon_dataset)'s
github repository. This data has a number of categorical and continuous
variables that should allow for a reasonable demonstration of a number
of techniques for exploring data.

::: panel-tabset
### R {.unnumbered}

```{r read-poke-data}
library(readr)
url <- "https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv"
poke <- read_csv(url)[,-c(1, 4, 5)] # skip extra columns
```

### Python {.unnumbered}

```{python read-poke-data-py}
import pandas as pd
poke = pd.read_csv("https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv")
poke = poke.drop(["Unnamed: 0", "german_name", "japanese_name"], axis = 1) # Drop some extra cols
```
:::

### Numerical Summary Statistics

::: panel-tabset
#### R: summary {.unnumbered}

The first, and most basic EDA command in R is `summary()`.

For numeric variables, `summary` provides 5-number summaries plus the
mean. For categorical variables, `summary` provides the length of the
variable and the Class and Mode. For factors, `summary` provides a table
of the most common values, as well as a catch-all "other" category.

```{r readr-eda}
# Make types into factors to demonstrate the difference
poke$type_1 <- factor(poke$type_1)
poke$type_2 <- factor(poke$type_2)

summary(poke[,3:12])
```

One common question in EDA is whether there are missing values or other
inconsistencies that need to be handled. `summary()` provides you with
the NA count for each variable, making it easy to identify what
variables are likely to cause problems in an analysis.

There is one pokemon who appears to not have a weight specified. Let's
investigate further:

```{r poke-weight}
poke[is.na(poke$weight_kg),] # Show any rows where weight.kg is NA
```

This is the last row of our data frame, and this pokemon appears to have
many missing values.

#### Python: describe {.unnumbered}

The most basic EDA command in pandas is `df.describe()` (which operates
on a DataFrame named `df`). Like `summary()` in R, `describe()` provides
a 5-number summary for numeric variables. For categorical variables,
`describe()` provides the number of unique values, the most common
value, and the frequency of that common value.

```{python poke}
poke.iloc[:,2:11].describe() # describe only shows numeric variables by default

# You can get categorical variables too if that's all you give it to show
poke['status'].describe()
```

#### R: skimr {.unnumbered}

An R package that is incredibly useful for this type of dataset
exploration is `skimr`.

```{r skimr-poke-demo}
library(skimr)
skim(poke)
```

`skim` provides a beautiful table of summary statistics along with a
sparklines-style histogram of values, giving you a sneak peek at the
distribution.

#### python: skimpy {.unnumbered}

There is a similar package to `skimr` in R called `skimpy` in Python.

```{python skimpy-poke-demo}
from skimpy import skim
skim(poke)
```
:::

### Assessing Distributions

We are often also interested in the distribution of values.

#### Categorical Variables

One useful way to assess the distribution of values is to generate a
cross-tabular view of the data. This is mostly important for variables
with a relatively low number of categories - otherwise, it is usually
easier to use a graphical summary method.

##### Tabular Summaries {.unnumbered}

::: panel-tabset
###### R {.unnumbered}

We can generate cross-tabs for variables that we know are discrete (such
as generation, which will always be a whole number). We can even
generate cross-tabular views for a combination of two variables (or
theoretically more, but this gets hard to read and track).

```{r poke-distribution}
table(poke$generation)

table(poke$type_1, poke$type_2)
```

###### Python {.unnumbered}

```{python poke-distribution-py}
import numpy as np
# For only one factor, use .groupby('colname')['colname'].count()
poke.groupby(['generation'])['generation'].count()

# for two or more factors, use pd.crosstab
pd.crosstab(index = poke['type_1'], columns = poke['type_2'])
```
:::

##### Frequency Plots {.unnumbered}

::: panel-tabset
###### Base R {.unnumbered}

```{r poke-distribution-plots}
plot(table(poke$generation)) # bar plot
```

###### R: ggplot2 {.unnumbered}

We generate a bar chart using `geom_bar`. It helps to tell R that
generation (despite appearances) is categorical by declaring it a factor
variable. This ensures that we get a break on the x-axis at each
generation.

```{r poke-dist-plots-ggplot2}
library(ggplot2)

ggplot(poke, aes(x = factor(generation))) +
  geom_bar() +
  xlab("Generation") + ylab("# Pokemon")
```


###### Python: matplotlib {.unnumbered}

We generate a bar chart using the contingency table we generated earlier
combined with matplotlib's `plt.bar()`.

```{python matplotlib-categorical-chart}
import matplotlib.pyplot as plt

tab = poke.groupby(['generation'])['generation'].count()

plt.bar(tab.keys(), tab.values, color = 'grey')
plt.xlabel("Generation")
plt.ylabel("# Pokemon")
plt.show()
```


###### Python: plotnine {.unnumbered}

Plotnine is a ggplot2 clone for python, and for the most part, the code
looks almost exactly the same, minus a few python-specific tweaks to
account for different syntax conventions in each language.

We generate a bar chart using `geom_bar`. It helps to tell R that
generation (despite appearances) is categorical by declaring it a factor
variable. This ensures that we get a break on the x-axis at each
generation.

```{python poke-dist-plots-plotnine}
from plotnine import *

(ggplot(aes(x = "factor(generation)"), data = poke) +
  geom_bar() +
  xlab("Generation") + ylab("# Pokemon"))
```

:::

#### Quantitative Variables

We covered some numerical summary statistics in the numerical summary
statistic section above. In this section, we will primarily focus on
visualization methods for assessing the distribution of quantitative
variables.

::: callout-note
##### Note: R pipe {.unnumbered}

The code in this section uses the R pipe, `%>%`. The left side of the
pipe is passed as an argument to the right side. This makes code easier
to read because it becomes a step-wise "recipe" instead of a nested mess
of functions and parentheses.

![In each step, the left hand side of the pipe is put into the first
argument of the function. Source: [Arthur Welle
(Github)](https://github.com/arthurwelle/VIS/blob/master/Pipe_Cake/)](https://github.com/arthurwelle/VIS/blob/master/Pipe_Cake/Pipe_baking_magrittr_backAssign.gif?raw=true){width="50%"}
:::

We can generate histograms[^exploratory-data-analysis-2] or kernel
density plots (a continuous version of the histogram) to show us the
distribution of a continuous variable.

[^exploratory-data-analysis-2]: A histogram is a chart which breaks up a
    continuous variable into ranges, where the height of the bar is
    proportional to the number of items in the range. A bar chart is
    similar, but shows the number of occurrences of a discrete variable.

::: panel-tabset
##### Base R {.unnumbered}

By default, R uses ranges of $(a, b]$ in histograms, so we specify which
breaks will give us a desirable result. If we do not specify breaks, R
will pick them for us.

```{r poke-hp-out, out.width = "48%", fig.show = "hold"}
hist(poke$hp)
```

For continuous variables, we can use histograms, or we can examine
kernel density plots.

```{r poke-graphs-base-cts, fig.show = "hold", collapse = T}
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 2
#| fig-cap: Histogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
#| fig-subcap: 
#|   - Histogram of Pokemon Height (m)
#|   - Histogram of Pokemon Height (m, log 10)
#|   - Density of Pokemon Height (m)
#|   - Density of Pokemon Height (m, log 10)

library(magrittr) # This provides the pipe command, %>%

hist(poke$weight_kg)

poke$weight_kg %>%
  log10() %>% # Take the log - will transformation be useful w/ modeling?
  hist(main = "Histogram of Log10 Weight (Kg)") # create a histogram

poke$weight_kg %>%
  density(na.rm = T) %>% # First, we compute the kernel density
  # (na.rm = T says to ignore NA values)
  plot(main = "Density of Weight (Kg)") # Then, we plot the result


poke$weight_kg %>%
  log10() %>% # Transform the variable
  density(na.rm = T) %>% # Compute the density ignoring missing values
  plot(main = "Density of Log10 pokemon weight in Kg") # Plot the result,
    # changing the title of the plot to a meaningful value
```

##### Python: matplotlib {.unnumbered}

```{python, include = F}
plt.clf()
```

```{python poke-graphs-matplotlib-cts}
#| fig-width: 6
#| fig-height: 6
#| fig-cap: Histogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
import matplotlib.pyplot as plt

# Create a 2x2 grid of plots with separate axes
# This uses python multi-assignment to assign figures, axes
# variables all in one go
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)

poke.weight_kg.plot.hist(ax = ax1) # first plot
ax1.set_title("Histogram of Weight (kg)")


np.log10(poke.weight_kg).plot.hist(ax = ax2)
ax2.set_title("Histogram of Log10 Weight (kg)")

poke.weight_kg.plot.density(ax = ax3)
ax3.set_title("Density of Weight (kg)")

np.log10(poke.weight_kg).plot.density(ax = ax4)
ax4.set_title("Density of Log10 Weight (kg)")

plt.tight_layout()
plt.show()
```

##### R: ggplot2

```{r poke-ggplot2-cts-r}
#| fig-show: hold
#| collapse: true
#| fig-align: center
#| fig-cap:  Histogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
#| fig-subcap: 
#|   - Histogram of Pokemon Height (m)
#|   - Histogram of Pokemon Height (m, log 10)
#|   - Density of Pokemon Height (m)
#|   - Density of Pokemon Height (m, log 10)
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 2

library(ggplot2)
ggplot(poke, aes(x = height_m)) +
  geom_histogram(bins = 30)
ggplot(poke, aes(x = height_m)) +
  geom_histogram(bins = 30) +
  scale_x_log10()
ggplot(poke, aes(x = height_m)) +
  geom_density()
ggplot(poke, aes(x = height_m)) +
  geom_density() +
  scale_x_log10()
```

Notice that in ggplot2/plotnine, we transform the axes instead of the
data. This means that the units on the axis are true to the original,
unlike in base R and matplotlib.

##### Python: plotnine

```{python poke-ggplot2-cts-py}
#| collapse: true
#| fig-align: center
#| fig-cap:  Histogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.
#| fig-subcap: 
#|   - Histogram of Pokemon Height (m)
#|   - Histogram of Pokemon Height (m, log 10)
#|   - Density of Pokemon Height (m)
#|   - Density of Pokemon Height (m, log 10)
#| fig-show: hold
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 2

ggplot(poke, aes(x = 'height_m')) + geom_histogram(bins = 30)

(ggplot(poke, aes(x = 'height_m')) +
  geom_histogram(bins = 30) +
  scale_x_log10())

(ggplot(poke, aes(x = 'height_m')) +
  geom_density())

(ggplot(poke, aes(x = 'height_m')) +
  geom_density() +
  scale_x_log10())

```

Notice that in ggplot2/plotnine, we transform the axes instead of the
data. This means that the units on the axis are true to the original,
unlike in base R and matplotlib.
:::

### Relationships Between Variables
#### Categorical - Categorical Relationships

::: panel-tabset

##### R: ggplot2 {.unnumbered}

We can generate a (simple) mosaic plot (the equivalent of a 2-dimensional
cross-tabular view) using `geom_bar` with `position = 'fill'`, which
scales each bar so that it ends at 1. I've flipped the axes using
`coord_flip` so that you can read the labels more easily.

```{r poke-dist-plots-ggplot2-mosaic}
library(ggplot2)

ggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +
  geom_bar(color = "black", position = "fill") +
  xlab("Type 1") + ylab("Proportion of Pokemon w/ Type 2") +
  coord_flip()
```

Another way to look at this data is to bin it in x and y and shade the
resulting bins by the number of data points in each bin. We can even add
in labels so that this is at least as clear as the tabular view!

```{r poke-dist-plots-ggplot2-tile}
ggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +
  # Shade tiles according to the number of things in the bin
  geom_tile(aes(fill = ..count..), stat = "bin2d") +
  # Add the number of things in the bin to the top of the tile as text
  geom_text(aes(label = ..count..), stat = 'bin2d') +
  # Scale the tile fill
  scale_fill_gradient2(limits = c(0, 100), low = "white", high = "blue", na.value = "white")
```

##### Base R {.unnumbered}

Base R mosaic plots aren't nearly as pretty as the ggplot version, but I will at least show you how to create them. 

```{r mosaic-base-r}
plot(table(poke$type_1, poke$type_2)) 
# mosaic plot - hard to read b/c too many categories
```

##### Python: matplotlib {.unnumbered}

To get a mosaicplot, we need an additional library, called
`statsmodels`, which we install with `pip install statsmodels` in the
terminal.

```{python mosaic-plot-poke-statsmodels}
import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic

mosaic(poke, ['type_1', 'type_2'], title = "Pokemon Types")
plt.show()
```

This obviously needs a bit of cleaning up to remove extra labels, but
it's easy to get to and relatively functional. Notice that it does not,
by default, show NA values.

##### Python: plotnine {-}
We can generate a mosaic plot (the equivalent of a 2-dimensional
cross-tabular view) using `geom_bar` with `position = 'fill'`, which
scales each bar so that it ends at 1. I've flipped the axes using
`coord_flip` so that you can read the labels more easily.

```{python poke-dist-plots-plotnine-mosaic}
# Convert everything to categorical/factor variable ahead of time
# this stops an error: TypeError: '<' not supported between instances of 'float' and 'str'
poke['type_1'] = pd.Categorical(poke['type_1'].astype(str))
poke['type_2'] = pd.Categorical(poke['type_2'].astype(str))

( ggplot(aes(x = 'type_1', fill = 'type_2'), data = poke) +
  geom_bar(color = "black", position = "fill") +
  xlab("Type 1") + ylab("Proportion of Pokemon w/ Type 2") +
  coord_flip() +
  # This says 85% of the plot is for the main plot and 15% is for the legend.
  theme(subplots_adjust={'right':0.85})
  )
```

Another way to look at this data is to bin it in x and y and shade the
resulting bins by the number of data points in each bin. We can even add
in labels so that this is at least as clear as the tabular view!

```{python poke-dist-plots-plotnine-tile}
(ggplot(aes(x = 'type_1', y = 'type_2'), data = poke) +
  # Shade tiles according to the number of things in the bin
  stat_bin2d(aes(fill = after_stat('count')), geom = 'tile'))
```

:::

#### Categorical - Continuous Relationships

::: panel-tabset
##### Base R {.unnumbered}

In R, most models are specified as `y ~ x1 + x2 + x3`, where the
information on the left side of the tilde is the dependent variable, and
the information on the right side are any explanatory variables.
Interactions are specified using `x1*x2` to get all combinations of x1
and x2 (x1, x2, x1\*x2); single interaction terms are specified as e.g.
`x1:x2` and do not include any component terms.

To examine the relationship between a categorical variable and a
continuous variable, we might look at box plots:

```{r boxplot-graphs}
#| out-width: 100%
#| fig-align: center
#| fig-show: hold
#| collapse: true
par(mfrow = c(1, 2)) # put figures in same row
boxplot(log10(height_m) ~ status, data = poke)
boxplot(total_points ~ generation, data = poke)
```

In the second box plot, there are far too many categories to be able to
resolve the relationship clearly, but the plot is still effective in
that we can identify that there are one or two species which have a much
higher point range than other species. EDA isn't usually about creating
pretty plots (or we'd be using `ggplot` right now) but rather about
identifying things which may come up in the analysis later.

##### R: ggplot2 {.unnumbered}

```{r boxplot-ggplot2}
#| out-width: 100%
#| fig-align: center
#| fig-show: hold
#| collapse: true

ggplot(data = poke, aes(x = status, y = height_m)) + 
  geom_boxplot() + 
  scale_y_log10()

ggplot(data = poke, aes(x = factor(generation), y = total_points)) + 
  geom_boxplot()
```

##### Python: matplotlib {.unnumbered}

```{python boxplot-matplotlib}

```
XXX TODO XXX

##### Python: plotnine {.unnumbered}
```{python boxplot-plotnine-py}
ggplot(aes(x = "status", y = "height_m"), data = poke) +\
geom_boxplot()
```

:::

You can find more on boxplots and ways to customize boxplots in the [Data Visualization](data-vis.qmd) chapter.

#### Continuous - Continuous Relationships

::: panel-tabset
##### Base R {.unnumbered}

To look at the relationship between numeric variables, we could compute
a numeric correlation, but a plot may be more useful, because it allows
us to see outliers as well.

```{r}
#| out-width: 48%
#| fig-align: center
#| collapse: true
plot(defense ~ attack, data = poke, type = "p")

cor(poke$defense, poke$attack)
```

Sometimes, we discover that a numeric variable which may seem to be
continuous is actually relatively quantized - there are only a few
values of base_friendship in the whole dataset.

```{r}
plot(x = poke$base_experience, y = poke$base_friendship, type = "p")
```

A scatterplot matrix can also be a useful way to visualize relationships
between several variables.

```{r, fig.width = 8, fig.height = 8, out.width = "100%"}
#| fig.cap: "A scatterplot matrix of hit points, attack, defense, special attack, and special defense characteristics for all generation 1-8 Pokemon."
pairs(poke[,16:20]) # hp - sp_defense columns
```

::: callout-note
[There's more information on how to customize base R scatterplot
matrices
here](http://www.sthda.com/english/wiki/scatter-plot-matrices-r-base-graphs).
:::

##### R: ggplot2 {.unnumbered}

XXX TODO XXX

##### Python: matplotlib {.unnumbered}

XXX todo XXX

##### Python: plotnine {.unnumbered}

XXX todo XXX

:::

If you want summary statistics by group, you can get that using the
`dplyr` package functions `select` and `group_by`, which we will learn
more about in the next section. (I'm cheating a bit by mentioning it
now, but it's just so useful!)

```{python, include = F}
# Clear python plot framework
plt.clf()
```

::: callout-tip
## Try it out: EDA {#EDA-police-violence}

::: panel-tabset
### Problem {.unnumbered}

Explore the variables present in [the police violence
data](data/police_violence.xlsx).

Note that some variables may be too messy to handle with the things that
you have seen thus far - that is ok. As you find irregularities,
document them - these are things you may need to clean up in the dataset
before you conduct a formal analysis.

```{r police-violence-tryitout-r-read-data}
if (!"readxl" %in% installed.packages()) install.packages("readxl")
library(readxl)
police_violence <- read_xlsx("data/police_violence.xlsx", sheet = 1, guess_max = 7000, skip = 1)
```

```{python police-violence-tryitout-py-read-data}
import pandas as pd
police_violence = pd.read_excel("data/police_violence.xlsx", skiprows=1)
```

### R solution {.unnumbered}

```{r police-violence-tryitout-R}
police_violence$`Victim's age` <- as.numeric(police_violence$`Victim's age`)

skim(police_violence)
```

Let's examine the numeric and date variables first:

```{r police-violence-numeric-date}
hist(police_violence$`Victim's age`)

# hist(police_violence$`Date of Incident (month/day/year)`)
# This didn't work - it wants me to specify breaks

# Instead, lets see if ggplot handles it better - from R4DS
library(ggplot2)
ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) +
  geom_histogram()
ggplot(police_violence, aes(x = `Date of Incident (month/day/year)`)) +
  geom_density()
```

Let's look at the victims' gender and race:

```{r police-violence-gender-race}
table(police_violence$`Victim's race`, useNA = 'ifany')
table(police_violence$`Victim's gender`)
table(police_violence$`Victim's race`, police_violence$`Victim's gender`)

plot(table(police_violence$`Victim's race`, police_violence$`Victim's gender`),
     main = "Police Killing by Race, Gender")
```

We can also look at the age range for each race:

```{r}
police_violence %>%
  # get groups with at least 100 observations that aren't unknown
  subset(`Victim's race` %in% c("Asian", "Black", "Native American", "Hispanic", "White")) %>%
  boxplot(`Victim's age` ~ `Victim's race`, data = .)
```

And examine the age range for each gender as well:

```{r police-violence-age}
police_violence %>%
  boxplot(`Victim's age` ~ `Victim's gender`, data = .)
```

The thing I'm honestly most surprised at with this plot is that there
are so many elderly individuals (of both genders) shot. That's not a
realization I'd normally construct this plot for, but the visual
emphasis on the outliers in a boxplot makes it much easier to focus on
that aspect of the data.

### Python solution {.unnumbered}

```{python police-violence-tryitout-py}
police_violence["Victim's age"] = pd.to_numeric(police_violence["Victim's age"], errors = 'coerce')

# police_violence.describe()
skim(police_violence)
```

Let's examine the numeric and date variables first:

```{python police-violence-numeric-date-py}
police_violence["Victim's age"].plot.hist()
plt.show()

(ggplot(police_violence, aes(x = "Date of Incident (month/day/year)")) +
  geom_histogram())
(ggplot(police_violence, aes(x = "Date of Incident (month/day/year)")) +
  geom_density())
```

Let's look at the victims' gender and race:

```{python police-violence-gender-race-py}
police_violence["Victim's race"].groupby(police_violence["Victim's race"]).count()

police_violence["Victim's gender"].groupby(police_violence["Victim's gender"]).count()

pd.crosstab(index = police_violence["Victim's race"], columns = police_violence["Victim's gender"])


import matplotlib.pyplot as plt
from statsmodels.graphics.mosaicplot import mosaic

mosaic(police_violence, ["Victim's race", "Victim's gender"], title = "Police Killing by Race, Gender")
plt.show()
```

We can also look at the age range for each race:

```{python police-age-race-py}
# get groups with at least 100 observations that aren't unknown
race_sub = ["Asian", "Black", "Native American", "Hispanic", "White"]

police_sub = police_violence.loc[police_violence["Victim's race"].isin(race_sub)]
police_sub = police_sub.assign(v_race = pd.Categorical(police_sub["Victim's race"], categories = race_sub))

police_sub.boxplot("Victim's age", by = "v_race")
plt.show()
```

And examine the age range for each gender as well:

```{python police-violence-age-py, eval = F}
police_violence.boxplot("Victim's age", "Victim's gender")
plt.show()
```

The thing I'm honestly most surprised at with this plot is that there
are so many elderly individuals (of both genders) shot. That's not a
realization I'd normally construct this plot for, but the visual
emphasis on the outliers in a boxplot makes it much easier to focus on
that aspect of the data.

:::
:::


::: callout-note
## Learn More: Janitor R package

The janitor package [@janitorpkg] has some very convenient functions for cleaning up messy data. 
One of its best features is the `clean_names()` function, which creates names based on a capitalization/separation scheme of your choosing.

![janitor and clean_names() by Allison
Horst](https://github.com/allisonhorst/stats-illustrations/blob/main/rstats-artwork/janitor_clean_names.png?raw=true){fig-alt="A cartoon beaver putting shapes with long, messy column names (pulled from a bin labeled “MESS” and “not so awesome column names”) into a contraption that converts them to lower snake case. The output has stylized text reading “Way more deal-withable column names.” Title text reads “janitor::clean_names(): convert all column names to *_case!”"}
:::

## References
