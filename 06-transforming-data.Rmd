# Transforming Data {#transforming-data}

> “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy

> “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham

## Module Objectives {-}

- Reshape data
- Transform variables to support analysis and visualization of data
- Join tables together in order to create a single coherent dataset

Most of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data *viewing*, rather than for statistical analysis - they may be laid out so that there are multiple observations in a single row (e.g., commonly a year's worth of data, with monthly observations in each column). 

Unfortunately, this type of data structure is not usually useful to us when we analyze or visualize the data. 

::: note
This section is going to seem like it drags on forever. It covers a lot of material, and a few different concepts. I highly recommend separating it out into 3 different "sessions" - Strings, Pivot operations, and Table Joins. 

For now, you need to know this material well enough to 1) identify what operation needs to happen, and 2) know where to find the sample code for that operation. It will get easier to remember the specific syntax with practice. 
:::

## Identifying the problem: Messy data

These datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.
```{r, message = F}
library(tidyverse)
```

```{r, echo = F}
knitr::kable(table1, caption = "Table 1")
```

Here, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column). 


```{r, echo = F}
knitr::kable(table2, caption = "Table 2")
```

Here, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1. 


```{r, echo = F}
knitr::kable(table3, caption = "Table 3")
```

This form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can't do *anything* with this format as it stands, because we can't do math on data stored as characters. However, this form might be easier to read and record for a human being. 


```{r, echo = F}
knitr::kable(table4a, caption = "Table 4a")
knitr::kable(table4b, caption = "Table 4b")
```

In this form, we have two tables - one for population, and one for cases. Each year's observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we'll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year. 


```{r, echo = F}
knitr::kable(table5, caption = "Table 5")
```

Table 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns  (or date and time in separate columns), often to deal with the fact that spreadsheets don't always handle dates the way you'd hope they would. 


These variations highlight the principles which can be said to define a tidy dataset:
1. Each variable must have its own column
2. Each observation must have its own row
3. Each value must have its own cell

<div class="tryitout">
### Try it out {-}
Go back through the 5 tables and determine whether each table is tidy, and if it is not, which rule or rules it violates. Figure out what you would have to do in order to compute a standardized TB infection rate per 100,000 people. 
<details><summary>Solution</summary>

1. table1 - this is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.

2. table2 - each variable does not have its own column (so a single year's observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.

3. table3 - each value does not have its own cell (and each variable does not have its own column). In Table 3, you'd have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1. 

4. table4a and table 4b - there are multiple observations in each row because there is not a column for year. To compute the rate, you'd need to "stack" the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1. 

5. table 5 - each variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren't actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you'd need to combine the two year columns together first). 
</details>
</div>

It is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be: 

1. Each dataset goes into its own table (or tibble, if you are using R)
2. Each variable gets its own column

By the end of this module, you should have the skills to "tidy" each of these tables. 

## String operations: Creating new variables and separating multi-variable columns

Nearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different "levels" of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated). 

> Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski

![Alternately, the xkcd version of the above quote](https://imgs.xkcd.com/comics/perl_problems.png)

The tidyverse package to deal with strings is [`stringr`](https://stringr.tidyverse.org/). The functions in stringr take the form of `str_XXX` where XXX is a verb. So `str_split()`, `str_replace()`, `str_remove()`, `str_to_lower()` all should make some sense.

```{r, eval = F, include = F}
college_data <- read_csv("https://ed-public-download.app.cloud.gov/downloads/Most-Recent-Cohorts-All-Data-Elements.csv") 
college_data2 <- college_data %>%
  select(UNITID, INSTNM, CITY, STABBR, ZIP, ACCREDAGENCY, INSTURL, PREDDEG, MAIN, NUMBRANCH, HIGHDEG, CONTROL, ST_FIPS, LOCALE, LATITUDE, LONGITUDE) %>%
  mutate(PREDDEG = factor(PREDDEG, levels = 0:4, labels = c("Not classified", "Predominantely certificate-degree granting", "Predominantely associate's-degree granting", "Predominantly bachelor's-degree granting", "Entirely graduate-degree granting")),
         MAIN = factor(MAIN, levels = 0:1, labels = c("Not main campus", "main campus")),
         HIGHDEG = factor(HIGHDEG, levels = 0:4, labels = c("Non-degree granting", "Certificate", "Associate", "Bachelors", "Graduate")),
         CONTROL = factor(CONTROL, levels = 1:3, labels = c("Public", "Private Non Profit", "Private For Profit")))
readr::write_csv(college_data2, "data/College_Data_Abbrev.csv")


college_fips <- read_csv("data/CollegeFips.csv", col_names = F) %>%
  set_names(c("ST_FIPS", "State")) %>%
  mutate_all(as.character)

college_data2 %>% select(STABBR, ST_FIPS) %>%
  mutate(ST_FIPS = as.character(ST_FIPS)) %>%
  unique() %>%
  left_join(college_fips) %>%
  write_csv("data/College_FIPS_Abbr.csv")

college_data2 <- left_join(college_data2, college_fips)
readr::write_csv(college_data2, "data/College_Data_Abbrev.csv", na = '.')
```

For this example, we'll use a subset of the US Department of Education College Scorecard data. [Documentation](https://collegescorecard.ed.gov/data/documentation/), [Data](https://collegescorecard.ed.gov/data/). I've selected a few columns from the institution-level data available on the College Scorecard site. 

<details><summary>Let's take a look (Read in the data)</summary>
```{r}
college <- read_csv("data/College_Data_Abbrev.csv", guess_max = 5000, na = '.')
str(college)
```

```{sashtml sas-college-read-data}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

filename fileloc '~/Projects/Class/unl-stat850/2020-stat850/data/College_Data_Abbrev.csv';
PROC IMPORT  datafile = fileloc out=classdat.college REPLACE
DBMS = csv; /* comma delimited file */
GUESSINGROWS=500;
GETNAMES = YES;
RUN;

PROC PRINT DATA = classdat.college (obs = 5);
RUN;
```
</details>

### Basic String Operations

<details><summary>What proportion of the schools operating in each state have the state's name in the school name?</summary>

We'll use `str_detect()` to look for the state name in the college name. 
```{r, fig.width = 4, fig.height = 8, out.width = 4}
# Outside the pipe
str_detect(college$INSTNM, pattern = college$State)

# Using the pipe and mutate:
college <- college %>%
  mutate(uses_st_name = str_detect(INSTNM, State))

# By state - percentage of institution names
college %>%
  group_by(State) %>%
  summarize(pct_uses_st_name = mean(uses_st_name), n = n()) %>%
  filter(n > 5) %>% # only states/territories with at least 5 schools
  # Reorder state factor level by percentage that uses state name
  mutate(State = reorder(State, -pct_uses_st_name)) %>%
  ggplot(data = ., aes(x = State, y = pct_uses_st_name)) + 
  geom_col() + coord_flip() + 
  geom_text(aes(y = 1, label = paste("Total Schools:", n)), hjust = 1)
```

In SAS, we use `find(x, pattern, 't')` to find the location of the pattern, which is 0 if the pattern is not found. To get something equivalent to `str_detect`, we just test whether this quantity is greater than 0. (The R equivalent of `find` is `str_locate()`). 

Note that SAS pads character fields with spaces so that they are all the same length. So if we want to test for "Alabama    " we could omit the 't' option in the command, but since we usually don't want that, we need to tell SAS to trim the fields before searching for the pattern. 

```{sashtml sas-college-find}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

DATA collegetmp;
set classdat.college;
uses_st_name = find(INSTNM, State, 't') GT 0;
RUN;


PROC PRINT DATA = collegetmp (obs = 5);
RUN;
```
</details>

<details><summary>What are some common substrings in a set of text?</summary>
For this, we'll start with working with the single column `INSTNM`. 

```{r}
head(college$INSTNM) %>% str_split(., " ") # Split on every space

# We may need to fix certain things that should stay together
# But doing too much of that gets tedious...
str_replace(college$INSTNM, "A & M", "A&M") %>%
  head() %>% 
  str_split(., "[ -]") # This pattern says "either ' ' or '-'" 
                       # (but the - has to be at the start or the end)
```
So we could take the time to clean up everything, making sure that e.g. San Diego is treated as a single word, but that's a pain in the rear. Instead, let's just see what happens if we brute-force it. 
```{r}
tmp <- college %>%
  select(INSTNM, State) %>% 
  mutate(name_words = str_split(INSTNM, '[ -]')) # This is a list-column
tmp
unnest(tmp) # Unnest duplicates rows so that the expanded data frame has the 
            # same structure as the original data
```
List columns are one way to maintain tidy data. They allow you to have several "sub-observations" for each observation and are useful for precisely cases like this, where there are uneven numbers of words in each university's name. We're not going to focus on list columns, but if you're interested, check out `purrr` and this [excellent tutorial](https://jennybc.github.io/purrr-tutorial/). 
```{r}
unnest(tmp) %>% 
  pull(name_words) %>% # this pulls out a single column
  table() %>% 
  sort(decreasing = T) %>% 
  head(50)
```

In SAS, this is a bit more tricky. Most people I know that use both SAS and R will do the data cleaning in R once things get complicated, and then read the clean data in to SAS. That's a valid approach, but it's worth seeing what has to be done in SAS this once. As we get further into this class, I'll probably be more willing to say "we're just going to use R for this" for two reasons - 1. I know R better, and 2. R is generally better at handling the weird stuff; SAS is built to quickly handle things that are already formatted in a reasonable way. SAS seems to be highly preferred for e.g. fitting mixed/linear models, but it isn't the easiest tool to use for data cleaning. 

But, in this particular case, there is [documentation about how to break a sentence into words](https://blogs.sas.com/content/iml/2016/07/11/break-sentence-into-words-sas.html) in SAS. 
```{sashtml sas-college-split-string-words}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";
DATA collegename;
SET classdat.college;
numWords = countw(INSTNM, " ");
DO i = 1 TO numWords;
  word = scan(INSTNM, i, " ");
  OUTPUT;
END;
KEEP word numWords;
;

PROC PRINT DATA=collegename (obs = 30);
run;

PROC FREQ DATA=collegename ORDER=FREQ;
TABLES word / MAXLEVELS=30;
RUN;
```
</details>

### Regular Expressions
Matching exact strings is easy - it's just like using find and replace.

```{r}
human_talk <- "blah, blah, blah. Do you want to go for a walk?"
dog_hears <- str_extract(human_talk, "walk")
dog_hears
```

But, if you can master even a small amount of regular expression notation, you'll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you're creative, but often they're much simpler. 

You may find it helpful to follow along with this section using this [web  app](https://spannbaueradam.shinyapps.io/r_regex_tester/) built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS) can be found [here](https://regex101.com/). The subset of regular expression syntax we're going to cover here is fairly limited (and common to both SAS and R, with a few adjustments), but [you can find regular expressions to do just about anything string-related](https://stackoverflow.com/questions/tagged/regex?tab=Votes). As with any tool, there are situations where it's useful, and situations where you should not use a regular expression, no matter how much you want to. 


<details><summary>Short Regular Expression Primer (with R examples)</summary>

- `[]` enclose sets of characters    
Ex: `[abc]` will match any single character `a`, `b`, `c`
  - `-` specifies a range of characters (`A-z` matches all upper and lower case letters)
  - to match `-` exactly, precede with a backslash (outside of `[]`) or put the `-` last (inside `[]`)
- `.` matches any character (except a newline)
- To match special characters, escape them using `\` (in most languages) or `\\` (in R). So `\\.` will match a literal `.`, `\\$` will match a literal `$`. 

```{r}
num_string <- "phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789"

ssn <- str_extract(num_string, "[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]")
ssn
```

Listing out all of those numbers can get repetitive, though. How do we specify repetition?

- `*` means repeat between 0 and inf times
- `+` means 1 or more times
- `?` means 0 or 1 times -- most useful when you're looking for something optional
- `{a, b}` means repeat between `a` and `b` times, where `a` and `b` are integers. `b` can be blank. So `[abc]{3,}` will match `abc`, `aaaa`, `cbbaa`, but not `ab`, `bb`, or `a`. For a single number of repeated characters, you can use `{a}`. So `{3, }` means "3 or more times" and `{3}` means "exactly 3 times"

```{r}
str_extract("banana", "[a-z]{1,}") # match any sequence of lowercase characters
str_extract("banana", "[ab]{1,}") # Match any sequence of a and b characters
str_extract_all("banana", "(..)") # Match any two characters
str_extract("banana", "(..)\\1") # Match a repeated thing
```

```{r}
num_string <- "phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23"

ssn <- str_extract(num_string, "[0-9]{3}-[0-9]{2}-[0-9]{4}")
ssn
phone <- str_extract(num_string, "[0-9]{3}.[0-9]{3}.[0-9]{4}")
phone
nuid <- str_extract(num_string, "[0-9]{8}")
nuid
bank_balance <- str_extract(num_string, "\\$[0-9,]+\\.[0-9]{2}")
bank_balance
```

There are also ways to "anchor" a pattern to a part of the string (e.g. the beginning or the end)

- `^` has multiple meanings:
  - if it's the first character in a pattern, `^` matches the beginning of a string
  - if it follows `[`, e.g. `[^abc]`, `^` means "not" - for instance, "the collection of all characters that aren't a, b, or c". 
- `$` means the end of a string

Combined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses
```{r}
address <- "1600 Pennsylvania Ave NW, Washington D.C., 20500"

house_num <- str_extract(address, "^[0-9]{1,}")

 # Match everything alphanumeric up to the comma
street <- str_extract(address, "[A-z0-9 ]{1,}")
street <- str_remove(street, house_num) %>% str_trim() # remove house number

city <- str_extract(address, ",.*,") %>% str_remove_all(",") %>% str_trim()

zip <- str_extract(address, "[0-9-]{5,10}$") # match 5 and 9 digit zip codes
```


- `()` are used to capture information. So `([0-9]{4})` captures any 4-digit number
- `a|b` will select a or b. 

If you've captured information using (), you can reference that information using backreferences. In most languages, those look like this: `\1` for the first reference, `\9` for the ninth. In R, though, the `\` character is special, so you have to escape it. So in R, `\\1` is the first reference, and `\\2` is the second, and so on. 

```{r}
phone_num_variants <- c("(123) 456-7980", "123.456.7890", "+1 123-456-7890")
phone_regex <- "\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})"
# \\( and \\) match literal parentheses if they exist
# ([0-9]{3})? captures the area code, if it exists
# .? matches any character
# ([0-9]{3}) captures the exchange code
# ([0-9]{4}) captures the 4-digit individual code

str_extract(phone_num_variants, phone_regex)
str_replace(phone_num_variants, phone_regex, "\\1\\2\\3")
# We didn't capture the country code, so it remained in the string

human_talk <- "blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. "
dog_hears <- str_extract_all(human_talk, "walk|treat")
dog_hears
```
</details>

In SAS, much the same information is true, though you do not have to double-escape special characters. SAS uses PERL-compatible regular expressions (PCRE for short) (these can also be enabled in base-R string functions). 

In PCRE regular expressions, '/' are used as delimiters. SAS assigns each sequential regular expression a number (so that you can reference them if necessary).

<details><summary>PRXMATCH returns the first position of a string where a match is found (0 otherwise)</summary>

```{sashtml}
DATA strings;
  INFILE DATALINES DSD; /* This allows quoted strings */
  INPUT string ~ $150.; /* ~ says deal with quoted strings */
  DATALINES;
"abcdefghijklmnopqrstuvwxyzABAB"
"banana orange strawberry apple"
"ana went to montana to eat a banana"
"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard."
"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23"
"1600 Pennsylvania Ave NW, Washington D.C., 20500"
RUN;

DATA info;
set strings;
  IF PRXMATCH("/\(?([0-9]{3})?\)?.?([0-9]{3}).([0-9]{4})/", string) GT 0 THEN phone = 1;
    ELSE phone = 0;
  IF PRXMATCH("/(walk|treat)/", string) GT 0 THEN dog = 1;
    ELSE dog = 0;
  IF PRXMATCH("/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\. ]{3,}), ([0-9]{5})/", string) GT 0 THEN addr = 1;
    ELSE addr = 0; /* Changed to require at least 3 characters in street and city names */
  IF PRXMATCH("/(..)\1/", string) GT 0 THEN abab = 1;
    ELSE abab = 0;
;

PROC PRINT DATA=info;
RUN;
```

Note that the equivalent syntax in R would be:
```{r}
strings <- c("abcdefghijklmnopqrstuvwxyzABAB",
"banana orange strawberry apple",
"ana went to montana to eat a banana",
"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.",
"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23",
"1600 Pennsylvania Ave NW, Washington D.C., 20500")

phone_regex <- "\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})"
dog_regex <- "(walk|treat)"
addr_regex <- "([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), ([0-9]{5})"
abab_regex <- "(..)\\1"

tibble(
  text = strings,
  phone = str_detect(strings, phone_regex),
  dog = str_detect(strings, dog_regex),
  addr = str_detect(strings, addr_regex),
  abab = str_detect(strings, abab_regex))
```
</details>

When doing various operations with regular expressions, it can be useful to save the regular expression for later use. 

- PRXPARSE saves a regex for use later
- PRXSUBSTR saves the starting location and length of a string match
- SUBSTR extracts the string given the starting location and length

<details><summary>PRXPARSE, PRXSUBSTR, SUBSTR</summary>
```{sashtml}
DATA strings;
  INFILE DATALINES DSD; /* This allows quoted strings */
  INPUT string ~ $150.; /* ~ says deal with quoted strings */
  DATALINES;
"abcdefghijklmnopqrstuvwxyzABAB"
"banana orange strawberry apple"
"ana went to montana to eat a banana"
"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard."
"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23"
"1600 Pennsylvania Ave NW, Washington D.C., 20500"
RUN;

DATA info;
  SET strings;
  /* This says use these variables for all rows */
  RETAIN REphone REdog REaddr REabab; 
  
  /* This block defined our variables */
  IF _N_ = 1 THEN DO;
    REphone = PRXPARSE("/\(?([0-9]{3})?\)?.?([0-9]{3}).([0-9]{4})/");
    REdog = PRXPARSE("/(walk|treat)/");
    REaddr = PRXPARSE("/([0-9]*) ([A-z0-9 ]{3,}), ([A-z\. ]{3,}), ([0-9]{5})/");
    REabab = PRXPARSE("/(..)\1/");
  END;
  
  /* This block identifies string start and length for matches */
  /* Note that phonestart, phonelength, dogstart, doglength, ... 
     are all defined implicitly in this block */
  CALL PRXSUBSTR(REphone, string, phonestart, phonelength);
  CALL PRXSUBSTR(REdog, string, dogstart, doglength);
  CALL PRXSUBSTR(REaddr, string, addrstart, addrlength);
  CALL PRXSUBSTR(REabab, string, ababstart, abablength);

  /* This block extracts all of the matches */
  IF phonestart GT 0 THEN DO;
    phonenumber = SUBSTR(string, phonestart, phonelength);
  END;
  IF dogstart GT 0 THEN DO;
    dogword = SUBSTR(string, dogstart, doglength);
  END;
  IF addrstart GT 0 THEN DO;
    addr = SUBSTR(string, addrstart, addrlength);
  END;
  IF ababstart GT 0 THEN DO;
    abab = SUBSTR(string, ababstart, abablength);
  END;


  /* This block keeps only rows with a phone number, dog keyword, or address */
  IF (phonestart GT 0) OR (dogstart GT 0) OR (addrstart GT 0) OR (ababstart GT 0) THEN DO;
    OUTPUT;
  END;

  /* This keeps only the variables we care about */
  KEEP string phonenumber dogword addr abab;
;

PROC PRINT DATA=info;
RUN;
```

Note that the equivalent syntax in R would be:
```{r}
strings <- c("abcdefghijklmnopqrstuvwxyzABAB",
"banana orange strawberry apple",
"ana went to montana to eat a banana",
"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.",
"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23",
"1600 Pennsylvania Ave NW, Washington D.C., 20500")

phone_regex <- "\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})"
dog_regex <- "(walk|treat)"
addr_regex <- "([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\. ]{3,}), ([0-9]{5})"
abab_regex <- "(..)\\1"

tibble(
  text = strings,
  phone = str_extract(strings, phone_regex),
  dog = str_extract(strings, dog_regex),
  addr = str_extract(strings, addr_regex),
  abab = str_extract(strings, abab_regex))
```
</details>

<details><summary>Find and Replace with PRXCHANGE</summary>
The next major task is find and replace, where we get to see another feature of perl-style regular expressions. 's/xxx/yyy/' is the general form of a find-and-replace regular expression. Think "substitue yyy for xxx". 

```{sashtml}
DATA ducks;
INFILE DATALINES DSD;
INPUT line ~ $50.;
DATALINES;
"Five little ducks went out one day,"
"Over the hills and far away."
"Mother duck said, quack quack quack quack,"
"But only four little ducks came back."
RUN;

DATA cats;
SET ducks;
/* define lengths of output strings */
LENGTH new_text $ 50 new_text2 $ 50;

IF _N_ = 1 THEN DO;
  REanimal = PRXPARSE("s/duck/cat/");
  REnoise = PRXPARSE("s/quack/meow/");
END;
RETAIN REanimal REnoise;

/* First, replace duck with cat */
CALL PRXCHANGE(REanimal, -1, line, new_text, r_length, trunc, n_of_changes);

/* Then, fix noises */
CALL PRXCHANGE(REnoise, -1, new_text, new_text2, r_length2, trunc2, n_of_changes2);

/* Warnings if anything was truncated */
IF trunc THEN PUT "Note: new_text was truncated";
IF trunc2 THEN PUT "Note: new_text2 was truncated";
RUN;

PROC PRINT DATA=cats;
RUN;
```

The equivalent R code:
```{r}
line <- c(
"Five little ducks went out one day,",
"Over the hills and far away.",
"Mother duck said, quack quack quack quack,",
"But only four little ducks came back."
)

str_replace_all(line, "duck", "cat") %>%
  str_replace_all("quack", "meow")

# Or, in one line... 
str_replace_all(line, c("duck" = "cat", "quack" = "meow"))
```


In PCRE, a backreference in the same expression would be `\1`, `\2`, etc., but if you are in the replace block of the regex, you would use `$1`, `$2`, .... 
</details>

::: note
Don't expect too much out of yourself as far as regular expressions go. I used them for almost a decade before I (mostly) quit googling "regex for ..." to find somewhere to start. 

Another thing to realize - regular expressions are 100% a language you write, but don't ever expect to read. So leave yourself lots of comments. 
:::


#### Try it out {- .tryitout}

The Squirrel Census (https://www.thesquirrelcensus.com/) is a multimedia science, design, and storytelling project focusing on the Eastern gray (Sciurus carolinensis) in NYC's Central Park. They count squirrels and present their findings to the public. This table contains squirrel data for each of the 3,023 sightings, including location coordinates, age, primary and secondary fur color, elevation, activities, communications, and interactions between squirrels and with humans.

```{r include = F, eval = F}
tmp <- read_csv("data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv")
```
<details><summary>Task 1: Fix the date!</summary>
In both SAS and R, read in the data ([link](data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv)) and format the date correctly. You can do this by carefully specifying how the date is read in (?read_csv in R, [informat](https://documentation.sas.com/?docsetId=etsug&docsetTarget=etsug_intervals_sect009.htm&docsetVersion=15.1&locale=en) in SAS)

<details><summary>R solution</summary>
```{r}
library(readr)
library(lubridate)

squirrels <- read_csv("data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv") %>%
  mutate(Date = as.character(Date) %>% mdy())
```
</details>

<details><summary>SAS solution</summary>
```{sashtml}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

filename fileloc '~/Projects/Class/unl-stat850/2020-stat850/data/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv';
PROC IMPORT  datafile = fileloc out=classdat.squirrel REPLACE
DBMS = csv; /* comma delimited file */
GUESSINGROWS=500;
GETNAMES = YES;
RUN;

PROC CONTENTS DATA=classdat.squirrel;
RUN;

DATA classdat.cleanSquirrel;
SET classdat.squirrel;
month = FLOOR(date/1000000);
day = MOD(FLOOR(date/10000), 100);
year = MOD(date, 10000);
date = MDY(month, day, year);
format date MMDDYY10.;
RUN;
```
</details>
</details>

<details><summary>Task 2: Clean up the Combination of primary and highlight fur color column</summary>
A. Get rid of leading and trailing `+` characters
B. Where two highlight colors exist, add the primary color to both of them (so `Gray+Cinnamon, White` becomes `Gray+Cinnamon, Gray+White`)

You can do this by working with the original values or the combination values; whatever is easiest. <details><summary>R solution</summary>
```{r}
squirrels_colorfix <- squirrels %>%
  # Make it easier to join things back together...
  mutate(id = 1:n()) %>% 
  # keep the stuff we need for this
  select(id, primary = `Primary Fur Color`, highlight = `Highlight Fur Color`, combo = `Combination of Primary and Highlight Color`) %>%
  
  # Remove all single character strings. 
  # ^ means "front of string", $ means "end of string", and . is a wildcard
  mutate(combo = str_remove(combo, "^.$")) %>%
  
  # Remove trailing + signs
  mutate(combo = str_remove(combo, "\\+$")) %>%
  
  # This allows only two highlight colors
  mutate(combo = str_replace(
    combo, 
    "^(Black|Cinnamon|Gray)\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$", 
    "\\1+\\2, \\1+\\3")) %>%
  
  # This allows three highlight colors
  mutate(combo = str_replace(
    combo, 
    "^(Black|Cinnamon|Gray)\\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$", 
    "\\1+\\2, \\1+\\3, \\1+\\4"))

table(squirrels_colorfix$combo)
```
</details>

<details><summary>SAS solution</summary>
```{sashtml}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

DATA squirrelcolor;
SET classdat.cleanSquirrel;

LENGTH orig  $50 new_text1 $ 50 new_text2 $ 50 new_text3 $ 50;
orig = Combination_of_Primary_and_Highl;

IF _N_ = 1 THEN DO;
  REthree = PRXPARSE("s/^(Gray|Cinnamon|Black)\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3, $1+$4/");
  REtwo = PRXPARSE("s/^(Gray|Cinnamon|Black)\+(Black|Cinnamon|Gray|White), (Black|Cinnamon|Gray|White)$/$1+$2, $1+$3/");
  REplus = PRXPARSE("s/[^A-z]$//");
END;
RETAIN REplus REtwo REthree;

CALL PRXCHANGE(REplus, -1, trim(orig), new_text1);
CALL PRXCHANGE(REthree, -1, trim(new_text1), new_text2);
CALL PRXCHANGE(REtwo, -1, trim(new_text2), new_text3);

keep orig new_text1 new_text2 new_text3;
RUN;

/* print all combinations that occur w/ new value */
PROC SQL; 
SELECT DISTINCT orig, new_text3 FROM squirrelcolor;
```
</details>
</details>

### Joining and Splitting Variables

There's another string-related task that is fairly commonly encountered: separating variables into two different columns (as in Table 3 above).
```{r, echo = F, out.width = "50%", fig.cap = "A visual representation of what separating variables means for data set operations."}
knitr::include_graphics("image/tidyr_separate.png")
```
<details><summary>Separating Variables</summary>
We can use `str_extract()` if we want, but it's actually faster to use `separate()`, which is part of the `tidyr` package. There is also `extract()`, which is another `tidyr` function that uses regular expressions and capture groups to split variables up. 

```{r}
table3 %>%
  separate(col = rate, into = c("cases", "population"), sep = "/", remove = F)
```

I've left the rate column in the original data frame just to make it easy to compare and verify that yes, it worked. 

`separate()` will also take a full on regular expression if you want to capture only parts of a string to put into new columns. 

[The `scan()` function in SAS can be used similarly](https://communities.sas.com/t5/SAS-Procedures/Splitting-a-delimited-column-into-multiple-columns/td-p/351130), though it doesn't have quite the simplicity and convenience of `separate`.
```{sashtml sas-table3-scan}
data table3;
length country $12 rate $20;
input country $ year rate $;
datalines;
Afghanistan  1999 745/19987071     
Afghanistan  2000 2666/20595360    
Brazil       1999 37737/172006362  
Brazil       2000 80488/174504898  
China        1999 212258/1272915272
China        2000 213766/1280428583
;
data table3split;
set table3;
length var1-var2 $10.;
array var(2) $;
do i = 1 to dim(var);
  var[i]=scan(rate, i, '/', 'M');
end;
count = var1;
population = var2;
run;
```
</details>

And, of course, there is a complementary operation, which is when it's necessary to join two columns to get a useable data value. 
```{r, echo = F, out.width = "50%", fig.cap = "A visual representation of what uniting variables means for data set operations."}
knitr::include_graphics("image/tidyr_unite.png")
```

<details><summary>Joining Variables</summary>
`separate()` has a complement, `unite()`, which is useful for handling situations like in table5:
```{r}
table5 %>%
  unite(col = "year", century:year, sep = '') %>%
  separate(col = rate, into = c("cases", "population"), sep = "/")
```

Note that separate and unite both work with character variables - it's not necessarily true that you'll always be working with character formats when you need to do these operations. For instance, it's relatively common to need to separate dates into year, month, and day as separate columns (or to join them together). 

Of course, it's much easier just to do a similar two-step operation (we have to convert to numeric variables to do math)
```{r}
table5 %>%
  mutate(year = as.numeric(century)*100 + as.numeric(year)) %>% 
  select(-century)
```

(Handy shortcut functions in `dplyr` don't completely remove the need to think).

Similarly, it is possible to do this operation in SAS as well (by string concatenation or using the numeric approach), as shown below:

```{sashtml sas-table5-concatenate}
/* read in the data */
DATA table5;
LENGTH country $12 century year rate $20;
INPUT country $ century year rate $;
DATALINES;
Afghanistan 19      99   745/19987071      
Afghanistan 20      00   2666/20595360     
Brazil      19      99   37737/172006362   
Brazil      20      00   80488/174504898   
China       19      99   212258/1272915272 
China       20      00   213766/1280428583 
;

/* Format the data */
DATA table5split;
  SET table5;
  LENGTH v1-v2 $10. yyc $3. centc $3. yyyyc $4.;
  ARRAY v(2) $;
  DO i = 1 TO dim(v);
    v[i]=scan(rate, i, '/', 'M');
  END;
  count = v1;
  population = v2;
  /* Numeric version */
  year = century*100 + year;
  /* Character version */
  yyc = PUT(year, 2.); /* convert to character */
  centc = PUT(century, 2.); /* convert to character */
  yyyyc = CATT('', centc, yearc); /* catt is truncate, then concatenate */
RUN;

/* Print the data */
PROC PRINT DATA=table5split;
  VAR country year yyyyc count population rate centc century yyc;
RUN;
```
</details>


## Pivot operations

It's fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren't necessarily the most friendly for analysis. 

### Longer

In many cases, the data come in what we might call "wide" form - some of the column names are not names of variables, but instead, are themselves values of another variable. Let's look at this data from the WHO ([download page here](https://apps.who.int/gho/data/view.main.22500A?lang=en)).
<details><summary>Data set up</summary>
```{r rw-hiv-data-R}
url <- "https://apps.who.int/gho/athena/data/xmart.csv?target=GHO/HIV_0000000026,SDGHIV&profile=crosstable&filter=COUNTRY:*;REGION:*;AGEGROUP:-&x-sideaxis=COUNTRY&x-topaxis=GHO;YEAR;SEX&x-collapse=true"

# create colnames in shorter form
hiv <- read_csv(url, na = "No data", ) %>%
  select(-2) # get rid of only column that has raw totals

# work with the names to make them shorter and more readable
# otherwise, they're too long for SAS
newnames <- names(hiv)  %>%
  str_remove("New HIV infections \\(per 1000 uninfected population\\); ") %>%
  str_replace_all("(\\d{4}); (Male|Female|Both)( sexes)?", "Rate_\\1_\\2")
hiv <- set_names(hiv, newnames) %>%
  # transliterate - get rid of non-ascii characters, replace w/ closest equiv
  mutate(Country = iconv(Country, to="ASCII//TRANSLIT"))

write_csv(hiv, path = "data/who_hiv.csv", na = '.') # make it easy for SAS
```
Since I've cheated a bit to make this easier to read in using SAS... hopefully that will be uneventful. 
```{sashtml read-hiv-data-sas}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

filename fileloc '~/Projects/Class/unl-stat850/2020-stat850/data/who_hiv.csv';
PROC IMPORT  datafile = fileloc out=classdat.hiv REPLACE
DBMS = csv; /* comma delimited file */
GUESSINGROWS=500;
GETNAMES = YES;
RUN;

```
</details>
Here, the column names (except for the first column) contain information about both group (Male, Female, total) and year. If we want to plot values over time, we're not going to have much fun. 

The solution to this is to rearrange the data into "long form": to take the columns which contain values and "stack" them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn't being stacked (e.g. country, in both the example above and the image below). 

```{r, echo = F, out.width = "50%", fig.cap = "A visual representation of what the pivot_longer operation looks like in practice."}
knitr::include_graphics("image/tidyr_pivot_longer.png")
```

Once our data are in long form, we can separate values that once served as column labels into actual variables, and we'll have tidy(er) data. 

<details><summary>In R, this operation is performed using `pivot_longer`</summary>
The columns are moved to a variable with the name passed to the argument "names_to" (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument "values_to" (again, hopefully easy to remember). 
```{r}
hiv_tidy <- hiv %>%
  pivot_longer(-Country, names_to = "key", values_to = "rate")
```
From this point, it's pretty easy to use things we've used in the past (regular expressions, separate, extract)

```{r}
hiv_tidier <- hiv_tidy %>%
  # Split the key into Rate (don't keep), year, and group (F, M, Both)
  separate(key, into = c(NA, "year", "group"), sep = "_", convert = T) %>%
  # Fix the HTML sign for less than - we could remove it as well
  mutate(rate = str_replace_all(rate, "&lt;", "<")) %>%
  # Split the rate into estimate, lower bound, and upper bound
  extract(rate, into = c("est", "lb", "ub"), regex = "([\\d\\.]{1,}) .([<\\d\\.]{1,}) - ([<\\d\\.]{1,}).", remove = F)
```
</details>
<details><summary>In SAS, this operation is performed using `PROC TRANSPOSE`. </summary>

[[Friendly guide to PROC TRANSPOSE](https://libguides.library.kent.edu/SAS/TransposeData)]{.learn-more}

```{sashtml sas-transpose-hiv-clean, error = T, warning = T}
libname classdat "/home/susan/Projects/Class/unl-stat850/2020-stat850/sas/";

PROC TRANSPOSE DATA=classdat.hiv OUT = hivtidy;
BY Country NOTSORTED; /* specify notsorted unless you know your data are sorted */
VAR Rate_2018_Both--Rate_1990_Female; /* variables to transpose */
RUN;

title 'Intermediate result 1'; 
PROC PRINT DATA=hivtidy(obs=5); RUN;

/* Data step to clean up */
DATA hivtidy;
SET hivtidy (rename=col1=rate);
group = scan(_name_,3,"_");
year = input(scan(_name_, 2, "_"), 4.);
drop _name_;
RUN;

title 'Intermediate result 2'; 
PROC PRINT DATA=hivtidy(obs=5); RUN;

/* Data step to clean up */
DATA hivtidy;
SET hivtidy;
/* just get rid of the less than sign */
rate = PRXCHANGE("s/&lt;//", -1, rate);
rate = PRXCHANGE("s/[\[\]-]//", -1, rate);

length v1-v3 4.2;
array v(3) $;
do i = 1 to dim(v);
  v[i]=scan(rate, i, ' ');
end;

rename v1 = est v2 = lb v3 = ub;
drop i rate;
RUN;

title 'Final result'; 
PROC PRINT DATA=hivtidy(obs=5); RUN;

```
</details>

#### Try it out {- .tryitout}
In the next section, we'll be using the WHO surveillance of disease incidence data ([link](https://www.who.int/immunization/monitoring_surveillance/data/en/) - 3.1, [Excel link](http://www.who.int/entity/immunization/monitoring_surveillance/data/incidence_series.xls))

It will require some preprocessing before it's suitable for a demonstration. I'll do some of it, but in this section, you're going to do the rest :)

```{r}
library(readxl)
library(purrr) # This uses the map() function as a replacement for for loops. 
# It's pretty sweet

sheets <- excel_sheets("data/incidence_series.xls")
sheets <- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name

# This command says "for each sheet, read in the excel file with that sheet name"
# map_df means paste them all together into a single data frame
disease_incidence <- map_df(sheets, ~read_xls(path ="data/incidence_series.xls", sheet = .))

# export for SAS (and R, if you want)
write_csv(disease_incidence, path = "data/who_disease_incidence.csv", na = ".")
```
Download the exported data [here](data/who_disease_incidence.csv) and import it into SAS and R. Transform it into long format, so that there is a date column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that). 

Can you make a line plot of cases of Measles in France over time?

<details><summary>R solution</summary>
```{r}
who_disease <- read_csv("data/who_disease_incidence.csv", na = ".")

who_disease_long <- who_disease %>%
  pivot_longer(matches("\\d{4}"), names_to = "year", values_to = "cases") %>%
  rename(Country = Cname) %>%
  mutate(Disease = str_replace(Disease, "CRS", "Congenital Rubella"),
         year = as.numeric(year))
```
</details>

<details><summary>SAS solution</summary>
```{r}

```
</details>

### Wider

While it's very common to need to transform data into a longer format, it's not that uncommon to need to do the reverse operation. Sometimes, you do one and then the other, to get the data into the format you want. In any case, to do that, we use PROC TRANSPOSE in SAS (again) or `pivot_wider` in R with `dplyr`. 


## Table Joins


## References {- .learn-more}

String manipulation

- [R4DS chapter](https://r4ds.had.co.nz/strings.html)
- [SAS and Perl regular expressions](https://support.sas.com/resources/papers/proceedings/proceedings/sugi29/265-29.pdf)
- [PCRE tester](https://regex101.com/)
- [R regex tester](https://spannbaueradam.shinyapps.io/r_regex_tester/) - has a short timeout period and will disconnect you if you're idle too long. But you can also clone the repo [here](https://github.com/AdamSpannbauer/r_regex_tester_app) and run it locally. 
- [SAS scan statement](https://documentation.sas.com/?docsetId=ds2ref&docsetTarget=p13adatt2vvhcxn1ext6w6eet24p.htm&docsetVersion=9.4&locale=en#p1ub8nub953289n1dpsvh7gm88qs)

Other stuff
- [SAS rename statement](https://documentation.sas.com/?docsetId=lestmtsref&docsetTarget=n0x16kvqkxxdx5n1t04voifvo8wo.htm&docsetVersion=9.4&locale=en)
